The Information Bottleneck Problem and Its
Applications in Machine Learning
Ziv Goldfeld and Yury Polyanskiy
Abstract
Inference capabilities of machine learning (ML) systems skyrocketed in recent years, now playing a pivotal role in various aspect of society. The goal in statistical learning is to use data to obtain simple algorithms for predicting a random variable Y from a correlated observation X. Since the dimension of X is typically huge, computationally feasible solutions should summarize it into a lower-dimensional feature vector T, from which Y is predicted. The algorithm will successfully make the prediction if T is a good proxy of Y, despite the said dimensionality-reduction.
A myriad of ML algorithms (mostly employing deep learning (DL)) for finding such representations T based on real-world data are now available. While these methods are often effective in practice, their success is hindered by the lack of a comprehensive theory to explain it. The information bottleneck (IB) theory recently emerged as a bold information-theoretic paradigm for analyzing DL systems. Adopting mutual information as the figure of merit, it suggests that the best representation T should be maximally informative about Y while minimizing the mutual information with X. In this tutorial we survey the information-theoretic origins of this abstract principle, and its recent impact on DL. For the latter, we cover implications of the IB problem on DL theory, as well as practical algorithms inspired by it. Our goal is to provide a unified and cohesive description. A clear view of current knowledge is particularly important for further leveraging IB and other information-theoretic ideas to study DL models.
I. INTRODUCTION
The past decade cemented the status of machine learning (ML) as a method of choice for a variety of inference tasks. The general learning problem considers an unknown probability distribution PX,Y that generates a target variable Y and an observable correlated variable X. Given a dataset from PX,Y, the goal is to learn a representation
T(X) of X that is useful for inferring Y.1 While these ideas date back to the late 1960's,, it was not until the 21st century that ML, and specifically, deep learning (DL), began to revolutionize data science practice.
Fueled by increasing computational power and data availability, deep neural networks (DNNs) are often unmatched in their effectiveness for classification, feature extraction and generative modeling.
This practical success, however, is not coupled with a comprehensive theory to explain how and why deep models work so well on real-world data. In recent years, information theory emerged as a popular lens through which to study fundamental properties of DNNs and their learning dynamics â€“. In particular, the information bottleneck
Z. Goldfeld is with the Electrical and Computer Engineering Department, Cornell University, Ithaca, NY, 14850, US (e-mail: goldfeld@cornell.edu). Y. Polyanskiy is with the Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, MA, 02139, US (e-mail: yp@mit.edu).
1The reader is referred to for background on statistical learning theory. arXiv:2004.14941v2 [cs.LG] 1 May 2020(IB) theory, received significant attention. In essence, the theory extends an information-theoretic framework introduced in 1999 to account for modern DL systems,. It offers a novel paradigm for analyzing DNNs in an attempt to shed light on their layered structure, generalization capabilities and learning dynamics.
The IB theory for DL includes claims about the computational benefit of deep architectures, that generalization is driven by the amount of information compression the network attains, and more,. This perspective inspired many followup works, â€“,, some corroborating (and building on) and others refuting different aspects of the theory. This tutorial surveys the information-theoretic origins of the IB problem and its recent applications to and influences on DL. Our goal is to provide a comprehensive description that accounts for the multiple facets of this opinion-splitting topic.
A. Origins in Information Theory
The IB problem was introduced in as an information-theoretic framework for learning. It considers extracting information about a target signal Y through a correlated observable X. The extracted information is quantified by a variable T = T(X), which is (a possibly randomized) function of X, thus forming the Markov chain Y â†” X â†” T.
The objective is to find a T that minimizes the mutual information I(X; T), while keeping I(Y ; T) above a certain threshold; the threshold determines how informative the representation T is of Y. Specifically, the IB problem for a pair (X, Y ) (with a know joint probability law) is given by inf I(X; T) subject to: I(Y ; T) â‰¥ Î±, where minimization is over all randomized mappings of X to T. This formulation provides a natural approximate version of minimal sufficient statistic (MSS) (cf. ). Notably, the same problem was introduced back in 1975 by Witsenhausen and Wyner in a different context: as a tool for analyzing common information.
Beyond formulating the problem, showed that, when alphabets are discrete, optimal T can be found by iteratively solving a set of self-consistent equations. An algorithm (a generalization of Blahut-Arimoto, ) for solving the equations was also provided. Extensions of the IB problem and the aforementioned algorithm to the distributed setup were given in. The only continuous-alphabet adaptation of this algorithm is known for jointly Gaussian (X, Y ) (cf. for the distributed Gaussian IB problem). In this case, the solution reduces to a canonical correlation analysis (CCA) projection with tunable rank. Solving the IB problem for complicated
PX,Y distributions seems impossible (even numerically), even more so when only samples of PX,Y are available.
The IB formulation, its solution, relations to MSSs, and the Gaussian case study are surveyed in Section II.
Optimized information measures under constraints often appear in information theory as solutions to operational coding problems. It is thus natural to ask whether there is an operational setup whose solution is the IB problem.
Indeed, the classic framework of remote source coding (RSC) fits this description. RSC considers compressing a source Xn so as to recover a correlated sequence Y n from that compression, subject to a distortion constraint,. Choosing the distortion measure as the logarithmic loss and properly setting the compression threshold, 3 recovers the IB problem as the fundamental RSC rate. With that choice of loss function, RSC can roughly be seen as a multi-letter extension of the IB problem. The connection between the two problems is covered in Section III.
The reader is referred to for a recent survey covering additional connections between the IB problem and other information-theoretic setups.
B. The Information Bottleneck Theory for Deep Learning
Although first applications of the IB problem to ML, e.g., for clustering, date two decades ago, it recently gained much traction in the context of DL. From the practical perspective, the IB principle was adopted as a design tool for DNN classifiers, and generative models, with all three works published concurrently.
Specifically, optimized a DNN to solve the IB Lagrangian via gradient-based methods. Termed deep variational IB (VIB), the systems learns stochastic representation rules that were shown to generalize better and enjoy robustness to adversarial examples. The same objective was studied in, who argued it promotes minimality, sufficiency and disentanglement of representations. The disentanglement property was also employed in for generative modeling purposes, where the Î²-variational autoencoder was developed. These applications of the IB framework are covered in Section IV-A.
The IB problem also had impact on DL theory. The main characterizing property of DNNs, as compared to general learning algorithms, is their layered structure. The IB theory suggests that deeper layers correspond to smaller I(X; T) values, thus providing increasingly compressed sufficient statistics. In a classification task, the feature X might contain information that is redundant for determining the label Y. It is therefore desirable to find representations T of X that shed redundancies, while retaining informativeness about Y. The argument of, was that the IB problem precisely quantifies the fundamental tradeoff between informativeness (of T for Y ) and compression (of X into T). the IB theory for DL was first presented in, followed by the supporting empirical study. The latter relied on a certain synthetic binary classification task as a running example. Beyond testing claims made in, the authors of further evolved the IB theory. A main new argument was that classifiers trained with cross-entropy loss and stochastic gradient descent (SGD) inherently (try to) solve the IB optimization problem. As such, posed the information plane (IP), i.e., the trajectory in R2 of the mutual information pair
ï¿½
I(X; T), I(Y ; T)
ï¿½ across training epochs, as a potent lens through which to analyze DNNs.
Based on this IP analysis, the IB theory proposed certain predictions about DNN learning dynamics. Namely, argued that there is an inherent phase transition during DNN training, starting from a quick 'fitting' phase that aims to increase I(Y ; T) and followed by a long 'compression' phase that shrinks I(X; T). This observation was explained as the network shedding redundant information and learning compressed representations, as described above. This is striking since the DNN has no explicit mechanism that encourages compressed representations.
The authors of further used the IP perspective to reason about computational benefits of deep architectures, phase transitions in SGD dynamics, and the network's generalization error. The IB theory for DL is delineated in section IV-B.
C. Recent Results and Advances
The bold IB perspective on DL, inspired followup research aiming to test and understand the observations therein. In, the empirical findings from were revisited. The goal was to examine their validity in additional settings, e.g., across different nonlinearities, replacing SGD with batch gradient-descent, etc. The main conclusion of was that the findings from do not hold in general. Specifically, provided experiments showing that IP dynamics undergo a compression phase only when double-sided saturating nonlinearities (e.g., tanh, as used in ) are employed. Retraining the same network but with ReLU activations, produces a similarly-performing classifier whose IP trajectory does not exhibit compression. This refuted the fundamental role of compression in learning deep models, as posed in. The results of, along with additional counterexamples they provided to claims from, are covered in Section V-A.
Theoretical aspect of the IB theory for DL were also reexamined. It was noted in, that the mutual information measures of interest
ï¿½
I(X; T), I(Y ; T)
ï¿½ are vacuous in deterministic DNNs (i.e., networks that define a deterministic mapping for each fixed set of parameters). This happens because deterministic DNNs with strictly monotone activations (e.g., tanh or sigmoid) can encode the entire input dataset into arbitrarily fine variations of T.
Consequently, no information about X is lost when traversing the network's layers and I(X; T) is either the constant
H(X) (discrete X) or infinite (continuous X); a vacuous quantity, independent of the network parameters, either way.2 Similar degeneracies occur in DNNs with any bi-Lipschitz nonlinearities, as well as for ReLUs.
This implies that the (estimated) IP trajectories presented in, for deterministic DNNs, cannot be fluctuating(e.g., undergoing fitting/compression phases) due to changes in mutual information. Indeed, showed that these fluctuations are an artifact of the quantization-based method employed in, to approximate the true mutual information. We describe the technical caveats in applying information-theoretic reasoning to deterministic DL systems and the misestimation of mutual information in Sections IV-B3 and V-B, respectively.
To circumvent these issues, proposed an auxiliary framework, termed noisy DNNs, over which I(X; T) and I(Y ; T) are meaningful, parameter-dependent quantities. In this framework, additive Gaussian noise is injected to the output of each of the network's neurons. showed that noisy DNNs approximate deterministic ones both in how they generalize (just as well and sometimes better), and in terms of the learned representations. From a theoretical standpoint, the noise renders X ï¿½â†’ T a stochastic parametrized channel, which makes I(X; T) and I(Y ; T) functions of the network's parameters. Once the degeneracy of the considered mutual information terms was alleviated, focused on accurately measuring their values across training epochs.
Building on recent results on differential entropy estimation under Gaussian smoothing,,, a rateoptimal estimator of I(X; T) over noisy DNNs was developed. This enabled tracking I(X; T) across training of noisy DNN classifiers and empirically show that it also undergoes compression (just like the quantization-based estimate of mutual information over deterministic networks). To understand the relation between compression and the geometry of latent representations, described an analogy between I(X; T) and data transmission over additive
2A similar degeneracy occurs for I(Y ; T), which, e.g., equals I(Y ; X), whenever X is discrete and the activations are injective. Note that, except for synthetic models, it is customary to replace the unknown joint distribution PX,Y with its empirical version, thus making X discrete.
5 white Gaussian noise (AWGN) channels. The analogy gave rise to an argument that compression of I(X; T) is driven by the progressive clustering of internal representations of equilabeled inputs.
Armed with the connection between compression and clustering, traced back to deterministic DNNs and showed that, while compression of I(X; T) is impossible therein, these networks nonetheless cluster equilabeled samples. They further demonstrated that the quantization-based proxy of mutual information used in, in fact measures clustering in the latent spaces. This identified clustering of representations as the fundamental phenomenon of interest, while elucidating some of the machinery DNNs employ for learning. Section V-C elaborates on noisy
DNNs, mutual information estimation, and the relation to clustering.
The IB problem remains an active area of research in ML and beyond. Its rich and opinion-splitting history inspired a myriad of followup works aiming to further explore and understand it. This tutorial surveys a non-exhaustive subset of these works as described above3, that contributed to different facets of IB research, both historically and more recently. We aim to provide a balanced description that clarifies the current status of the IB problem applied to DL theory and practice. Doing so would help further leveraging the IB and other information-theoretic concepts for the study of DL systems.
II. INFORMATION BOTTLENECK FORMULATION AND GAUSSIAN SOLUTION
The IB framework was proposed in as a principled approach for extracting 'relevant' or 'useful' information from an observed signal about a target one. Consider a pair of correlated random variables (X, Y ), where X is the observable and Y is the object on interest. The goal is to compress X into a representation T that preserves as much information about Y as possible.
The formulation of this idea uses mutual information. For a random variable pair (A, B) âˆ¼ PA,B with values in A Ã— B, set
I(A; B) :=
ï¿½
AÃ—B log
ï¿½ dPA,B dPA âŠ— PB(a, b)
ï¿½ dPA,B(a, b), as the mutual information between A and B, where dPA,B dPAâŠ—PB is the Radon-Nikodym derivative of PA,B with respect to (w.r.t.) PA âŠ— PB. Mutual information is a fundamental measure of dependence between random variables with many desirable properties. For instance, it nullifies if and only if A and B are independent, and is invariant to bijective transformations. In fact, mutual information can be obtained axiomatically as a unique (up to a multiplicative constant) functional satisfying several natural 'informativeness' conditions.
A. The Information Bottleneck Framework
The IB framework for extracting the relevant information an X-valued random variable X contains about a Y-valued Y is described next. For a set T, let PT |X be a transition kernel from X to T.4 The kernel PT |X can be viewed as transforming X âˆ¼ PX (e.g., via quantization) into a representation of T âˆ¼ PT (Â·) :=
ï¿½
PT |X(Â·|x) dPX(x)
3Others relevant papers include,,,,,,.
4Given two measurable spaces (A, A) and (B, B), PA|B(Â·|Â·) : A Ã— B â†’ R is a transition kernel from the former to the latter if PA|B(Â·|b) is a probability measure on (A, A), for all b âˆˆ B, and PA|B(a|Â·) is B-measurable for all a âˆˆ A.
ğ‘Œ
ğ‘‹
ğ‘‡
ğ‘ƒğ‘‹,ğ‘Œ
ğ‘ƒğ‘‡|ğ‘‹
Fig. 1: Graphical representation of probabilistic relations in the IB framework. The triple (X, Y, T) is jointly distributed according to PX,Y PT |X, thus forming a Markov chain Y â†” X â†” T. The goal is to find a compressed representation T of X (via the transition kernel PT |X), i.e., minimize I(X; T), while preserving at least Î± bits of information about Y, i.e., I(Y ; T) â‰¥ Î±. in the T space. The triple Y â†” X â†” T forms a Markov chain in that order w.r.t. the joint probability measure
PX,Y,T = PX,Y PT |X. This joint measure specifies the mutual information terms I(X; T) and I(Y ; T), that are interpreted as the amount of information T conveys about X and Y, respectively.
The IB framework concerns finding a PT |X that extracts information about Y, i.e., high I(Y ; T), while maximally compressing X, which is quantified as keeping I(X; T) small. Since the Data Processing Inequality (DPI) implies
I(Y ; T) â‰¤ I(X; Y ), the compressed representation T cannot convey more information than the original signal.
This gives rise to a tradeoff between compressed representations and preservation of meaningful information. The tradeoff is captured by a parameter Î± âˆˆ Râ‰¥0 that specifies the lowest tolerable I(Y ; T) values. Accordingly, the IB problem is formulated through the constrained optimization inf
PT |X: I(Y ;T )â‰¥Î± I(X; T), (1) where the underlying joint distribution is PX,Y PT |X. Thus, we pass the information that X contains about Y through a 'bottleneck' via the representation T (see Fig. 1). An extension of the IB problem to the distributed setup was proposed in,. In that problem, multiple sources X1,..., XK are compressed separately into representations T1,..., TK, that, collectively, preserve as much information as possible about Y.
A slightly different form of the problem in (1), namely inf
PT |X: H(X|T )â‰¥Î± H(Y |T), (2) first appeared in a seminal paper of Witsenhausen and Wyner, who used it to simplify the proof of GÂ´acs and KÂ¨orner's result on common information.
B. Lagrange Dual Form and Information Bottleneck Curve
Note that the optimization problem (1) is not convex (in PT |X), however it can be made into a convex ratedistortion problem (cf. (14)-(15)). In any case, (1) is commonly solved by introducing the Lagrange multiplier Î² and considering the functional
LÎ²(PT |X) := I(X; T) âˆ’ Î²I(T; Y ).
With this definition, the IB problem can be recast as minimizing LÎ²(PT |X) over all possible PT |X kernels. Here Î² controls the amount of compression in the representation T of X: small Î² implies more compression (sacrificing informativeness), while larger Î² pushes towards finer representation granularity that favor informativeness. Varying Î² âˆˆ [0, +âˆ) regulates the tradeoff between informativeness and compression.
For any Î² âˆˆ [0, +âˆ), conditions for a stationary point of LÎ²(PT |X) can be expressed via the following selfconsistent equations. Namely, a stationary point P (Î²)
T |X must satisfy:
P (Î²)
T(t) =
ï¿½
X
P (Î²)
T |X(t|x) dPX(x)(4a)
P (Î²)
Y |T (y|t) =
P (Î²)
T(t)
ï¿½
X
PY |X(y|x)P (Î²)
T |X(t|x) dPX(x)(4b)
P (Î²)
T |X(t|x) = P (Î²)
T(t)
ZÎ²(x) eâˆ’Î²DKL
ï¿½
PY |X(Â·|x)
ï¿½ï¿½P (Î²)
Y |T (Â·|t)
ï¿½, (4c) where ZÎ²(x) is the normalization constant (partition function). If X, Y and T take values in finite sets, and PX,Y is known, then alternating iterations of (4) locally converges to a solution, for any initial PT |X. This is reminiscent of the Blahut-Arimoto algorithm for computing rate distortion functions or channel capacity,. The discrete alternating iterations algorithms was later adapted to the Gaussian IB.5 While the algorithm is infeasible for general continuous distributions, in the Gaussian case it translates into a parameter updating procedure.
The IB curve is obtained by solving infPT |X LÎ²(PT |X), for each Î² âˆˆ [0, +âˆ), and plotting the mutual information pair
ï¿½
IÎ²(X; T), IÎ²(Y ; T)
ï¿½ for an optimal P (Î²)
T |X. In Section II-D we show the IB curve for jointly Gaussian (X, Y ) variables (Fig. 2). The two-dimensional plane in which the IB curve resides, was later coined as the information plane (IP).
C. Relation to Minimal Sufficient Statistics
A elementary concept that captures the notion of 'compressed relevance' is that of MSS, as defined next.
Definition 1 (Minimal Sufficient Statistic) Let (X, Y ) âˆ¼ PX,Y. We call T := t(X), where t is a deterministic function, a sufficient statistic of X for Y if Y â†” T â†” X forms a Markov chain. A sufficient statistic T is minimal if for any other sufficient statistic S, there exists a function f, such that T = f(S), PX-almost surely (a.s.).
The need to define minimally arises because T = X is trivially sufficient, and one would like to avoid such degenerate choices. However, sufficient statistics themselves are rather restrictive, in the sense that their dimension always depends on the sample size, except when the data comes from an exponential family (cf., the PitmanKoopmanDarmois Theorem ). It is therefore useful to consider relaxations of the MSS framework.
Such a relaxation is given by the IB framework, which is evident by relating it to MSSs as follows. Allowing T to be a stochastic (rather than deterministic) function of X, i.e., defined through a transition kernel PT |X, we have
5see, for extensions of both the discrete and the Gaussian cases to the distributed setup.
8 that T is sufficient for Y if and only if I(X; Y ) = I(X; T). Furthermore, set of MSSs coincides with the set of solutions of inf
PT |XâˆˆF I(X; T), (5) where F :=
ï¿½
PT |X : I(Y ; T) = supQT â€²|X I(Y ; T â€²)
ï¿½. Here, the mutual information terms I(X; T) and I(Y ; T) are w.r.t. PX,Y PT |X, while I(Y ; T â€²) is w.r.t. PX,Y QT â€²|X.
The IB framework thus relaxes the notion of MSS in two ways. First, while a MSS is defined as a deterministic function of X, IB solutions are randomized mappings. Such mapping can attain strictly smaller values of LÎ²(PT |X) as compared to deterministic ones.6 Second, in view of (5), the IB framework allows for Î²-approximate MSSs by regulating the amount of information T retains about Y (see also (1)).
D. Gaussian Information Bottleneck
The Gaussian IB has a closed form solution for (3), which is the focus of this section. Let X âˆ¼ N(0, Î£X) and Y âˆ¼ N(0, Î£Y ) be centered multivariate jointly Gaussian (column) vectors of dimensions dx and dy, respectively.
Their cross-covariance matrix is Î£XY := E
ï¿½
XY âŠ¤ï¿½
âˆˆ RdxÃ—dy.7
1) Analytic solution: The first step towards analytically characterizing the optimal value of LÎ²(PT |X) is showing that a Gaussian T is optimal. Using the entropy power inequality (EPI) in a similar vein to shows that infPT |X LÎ²(PT |X) is achieved by P (Î²)
T |X for which (X, Y, T) are jointly Gaussian. Since Y â†” X â†” T forms a Markov chain, we may represent T = AX + Z, where Z âˆ¼ N(0, Î£Z) is independent of (X, Y ). Consequently, the IB optimization problem reduces to inf
PT |X LÎ²(PT |X) = inf
A,Î£Z I(X; AX + Z) âˆ’ Î²I(AX + Z; Y )
= 1
2 inf
A,Î£Z (1 âˆ’ Î²) log
ï¿½ï¿½ï¿½AÎ£XAâŠ¤ + Î£Z
ï¿½ï¿½
ï¿½ï¿½Î£Z
ï¿½ï¿½
ï¿½
+ Î² log
ï¿½ï¿½ï¿½AÎ£X|Y AâŠ¤ + Î£Z
ï¿½ï¿½
ï¿½
ï¿½
ï¿½ï¿½
ï¿½
:=L(G) Î² (A,Î£Z), (6) where (X Y )âŠ¤ âˆ¼ N
ï¿½
0, ï¿½
Î£X
Î£XY
Î£âŠ¤
XY
Î£Y
ï¿½ï¿½ and Z âˆ¼ N(0, Î£Z) are independent. The second equality above follows by direct computation of the mutual information terms under the specified Gaussian law.
The optimal projection T = AX + Z (namely, explicit structure for A and Î£Z) was characterized in [25, Theorem 3.1], and is restated in the sequel. Let Î£X|Y = Î£X âˆ’ Î£XY Î£âˆ’1
Y Î£Y X be the mean squared error (MSE) matrix for estimating X from Y. Consider its normalized form Î£X|Y Î£âˆ’1 and let (vi)k i=1, 1 â‰¤ k â‰¤ dx be the left eigenvectors of Î£X|Y Î£âˆ’1 with eigenvalues (Î»i)k i=1. We assume (Î»i)k i=1 are sorted in ascending order (with (vi)k i=1 ordered accordingly). For each i = 1,..., k, define Î²â‹† i =
1âˆ’Î»i and set Î±i(Î²) := Î²(1âˆ’Î»i)âˆ’1 Î»iri, where ri = vâŠ¤ i Î£Xvi.
6Consider, e.g., (X, Y ) as correlated Bernoulli random variables.
7For simplicity, we assume all matrices are full rank.
The tradeoff parameter Î² âˆˆ [0, âˆ) defines the optimal A matrix through its relation to the critical (Î²â‹† i )k i=1 values.
Fix i = 1,..., k + 1, and let Î² âˆˆ [Î²â‹† iâˆ’1, Î²â‹† i ), where Î²â‹†
0 = 0 and Î²â‹† k+1 = âˆ. For this Î², define
Ai(Î²) =
ï¿½ Î±1(Î²)v1,..., Î±i(Î²)vi, 0,..., 0
ï¿½âŠ¤
âˆˆ RdxÃ—dx.
Thus, Ai(Î²) has Î±1(Î²)vâŠ¤
1,..., Î±i(Î²)vâŠ¤ i as its first i rows, followed by dx âˆ’ i rows of all zeros.
Theorem 1 (Gaussian IB ) Fix Î² âˆˆ [0, âˆ). Choosing Î£Z = Idx and A(Î²) :=
ï¿½
ï¿½
ï¿½
ï¿½
ï¿½
ï¿½
ï¿½
ï¿½
ï¿½
ï¿½
ï¿½
ï¿½
ï¿½
ï¿½
ï¿½
ï¿½
ï¿½
ï¿½
ï¿½
A0(Î²), Î² âˆˆ [0, Î²â‹†
A1(Î²), Î² âˆˆ [Î²â‹†
1, Î²â‹†
Ak(Î²), Î² âˆˆ [Î²â‹† k, Î²â‹† k+1), (8) where A0(Î²) is the all-zero matrix, achieves the infimum in (6).
The proof of Theorem 1 is not difficult. First one shows that Î£Z = Idx is optimal by a standard whitening argument. This step assumes Î£Z is non-singular, which does not lose generality. Indeed, if Î£Z is low rank then
L(G) Î² (A, Î£Z) = âˆ. The result then follows by differentiating L(G) Î² (A, Idx) with respect to A and solving analytically.
In light of Theorem 1, the optimal representation T â‹† Î² = A(Î²)X + Z, is a noisy linear projection to eigenvectors of the normalized correlation matrix Î£X|Y Î£âˆ’1
X. These eigenvectors coincide with the well-known CCA basis.
However, in the Gaussian IB, Î² determines the dimensionality of the projection (i.e., how many CCA basis vectors are used). As Î² grows in a continuous manner, the dimensionality of T â‹† Î² increases discontinuously, with jumps at the critical Î²â‹† i points. Larger Î² implies higher-dimensional projections, while for Î² â‰¤ Î²â‹†
1, we have that T â‹† Î² = Idx comprises only noise. The Gaussian IB thus serves as a complexity measure with Î²-tunable projection rank.
2) Information bottleneck curve: The analytic solution to the Gaussian IB problem enables plotting the corresponding IB curve. Adapted from [25, Fig. 3], Fig. 2 illustrates this curve. Before discussing it we explain how it is computed. This is done by substituting the optimal projection A(Î²) and Î£Z = Idx into the mutual information pair of interest. Upon simplifying, for any Î² âˆˆ [0, âˆ), one obtains
I(X; T â‹† Î²) = 1
2 nÎ²
ï¿½ i=1 log
ï¿½(Î² âˆ’ 1)1 âˆ’ Î»i Î»i
ï¿½(9a)
I(T â‹† Î²; Y ) = I(X; TÎ²) âˆ’ 1
2 nÎ²
ï¿½ i=1 log
ï¿½ Î²(1 âˆ’ Î»i)
ï¿½, (9b) where nÎ² is the maximal index i such that Î² â‰¥
1âˆ’Î»i. Define the function
FIB(x) := x âˆ’ nÎ²(x)
2 log
ï¿½nÎ²(x)
ï¿½ i=1(1 âˆ’ Î»i)
1 nÎ²(x) + e
2x nÎ²(x) nÎ²(x)
ï¿½ i=1 Î»
1 nÎ²(x) i
ï¿½, (10) where Î²(x) is given by isolating Î² from (9a) as a function of I(X; T â‹† Î²) = x. Rearranging (9) one can show that
FIB assigns each I(X; T â‹† Î²) with its corresponding I(T â‹† Î²; Y ), i.e., FIB
ï¿½
I(X; T â‹† Î²)
ï¿½
= I(T â‹† Î²; Y ), for all Î² âˆˆ [0, âˆ)
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½(ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½; ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½)
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½(ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½; ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½) log ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½
Î£
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ = 1 âˆ’ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½
Suboptimal curve with:
--- 1 eigenvalue
--- 2 eigenvalues
--- 3 eigenvalues
Fig. 2 [Adapted from Fig. 3 of ]: The Gaussian IB curve (computed with four eigenvalues Î»1 = 0.1, Î»2 = 0.5, Î»3 = 0.7, Î»4 = 0.9 and Î² âˆˆ [0, 103)) is shown in blue. Suboptimal curves that use only the first 1, 2, or 3 eigenvalues are illustrated by the dashed green, red and orange lines, respectively. Mutual information pairs at the critical values Î²â‹† i =
1âˆ’Î»i, i = 1, 2, 3, 4, are marked by red circles. The slope of the curve at each point is the corresponding 1/Î². The tangent at zero, whose slope is 1 âˆ’ Î»1, is shown in black.(see [25, Section 5]).
The Gaussian IB curve, as shown in blue in Fig. 2, is computed using (10). An interesting property of this curve is that while
ï¿½
I(X; T â‹† Î²), I(T â‹† Î²; Y )
ï¿½ changes continuously with Î², the dimensionality of T â‹† Î² (or, equivalently, the number of eigenvectors used in A(Î²)) increases discontinuously at the critical points Î²â‹† i, i = 1,..., k, and is fixed between them. Restricting the dimension results in a suboptimal curve, that coincides with the optimal one up to a critical Î² value and deviates from it afterwards. Some suboptimal curves are shown in Fig. 2 by the dashed, horizontally saturating segments. The critical values of ï¿½
I(X; T â‹† Î²), I(T â‹† Î²; Y )
ï¿½ after which the suboptimal curve deviate from the optimal one are marked with red circles. Notably, the optimal curve moves from one analytic segment to another in a smooth manner. Furthermore, one readily verifies that FIB is a concave function with slope tends to 0 as Î² â†’ âˆ. This reflects the law of diminishing returns: encoding more information about X in T (higher
I(T; X)) yields smaller increments in I(T; Y ).
III. REMOTE SOURCE CODING
The IB framework is closely related to the classic information-theoretic problem of RSC. RSC dates back to the 1962 paper by Dobrushin and Tsybakov (see also for the additive noise case). As subsequently shown, the RSC rate-distortion function under logarithmic loss effectively coincides with 1. We start by setting up the operational problem.
A. Operational Setup
Consider a source sequence Y n := (Y1,..., Yn) of n independent and identically distributed (i.i.d.) copies of a random variable Y âˆ¼ PY âˆˆ P(Y). An encoder observes the source Y n through a memoryless noisy channel PX|Y.
Namely, PX|Y stochastically maps each Yi to an X-valued random variable Xi, where i = 1,..., n. Denoting
PX,Y = PY PX|Y, the pair (Xn, Y n) is distributed according to its n-fold product measure P âŠ—n
X,Y.
The sequence Xn := (X1,..., Xn) is encoded through fn : X n â†’ Mn, where |Mn| < âˆ, producing the representation M := fn(Xn) âˆˆ Mn. The goal is to reproduce Y n from M via a decoder gn : Mn â†’ Ë†Yn, where Ë†Yn is called the reproduction space, subject to a distortion constraint.8 The system is illustrated in Fig. 3
We adopt the logarithmic loss as the distortion measure. To set it up, let the reproduction alphabet be Ë†Y = P(Y), i.e., the set of probability measures on Y. Thus, given a reproduction sequence Ë†yn âˆˆ Ë†Y, each Ë†yi, i = 1,..., n, is a probability measure on Y. This corresponds to a 'soft' estimates of the source sequence. The symbol-wise logarithmic loss distortion measure is d(y, Ë†y) := log
ï¿½ 1
Ë†y(y)
ï¿½, (11a) which gives rise to d(yn, Ë†yn) := 1 n n
ï¿½ i=1 log
ï¿½
Ë†yi(yi)
ï¿½(11b) as the distortion measure between the source yn âˆˆ Yn and its reproduction Ë†yn âˆˆ Ë†Y.
An RSC block code of length n comprises a pair of encoder-decoder functions (fn, gn). A rate-distortion pair(R, D) is achievable if for every Ïµ > 0 there exists a large enough blocklength n and a code (fn, gn), such that
1 n log |Mn| < R + Ïµ
E
ï¿½ d(Y n, Ë†Y n)
ï¿½
< D + Ïµ, (12) where Ë†Y n = gn
ï¿½ fn(Xn)
ï¿½ and (Xn, Y n) âˆ¼ P âŠ—n
X,Y. The rate-distortion function R(D) is the infimum of rates R such that (R, D) is achievable for a given distortion D [37, Section 10.2].
On the surface, RSC appears to be a new type of (lossy) compression problem, but it turns out to be a special case of it [41, Section 3.5]). Indeed, let us introduce another distortion metric Ëœd(x, Ë†y) := E[d(Y, Ë†y)|X = x] and its n-fold extension d(xn, Ë†yn) = 1 n
ï¿½n i=1 Ëœd(xi, Ë†yi). One readily sees that E
ï¿½ d(Y n, Ë†Y n)
ï¿½
= E
ï¿½ Ëœd(Xn, Ë†Y n)
ï¿½. Thus, the standard rate-distortion theory implies that (subject to technical conditions, cf. [42, Chapter 26])
R(D) = inf
P Ë†
Y |X: E[ Ëœd(X, Ë†Y )]â‰¤D
I(X; Ë†Y ).
B. From Rate-Distortion Function to Information Bottleneck
Adapted to logarithmic loss, (13) can be further simplified. Indeed, consider an arbitrary distribution PX, Ë†Y and extend it to PY,X, Ë†Y = PX, Ë†Y PY |X, so that Y â†” X â†” Ë†Y forms a Markov chain. Consider any Ë†y and denote by Ë†y0 the following distribution:
Ë†y0(Â·) := PY | Ë†Y =Ë†y.
8Rate distortion theory [37, Section 10] often considers Ë†Y = Y, but this is not the case under logarithmic loss distortion, as described below.
ğ‘ƒğ‘‹,ğ‘Œ ğ‘“ğ‘›
ğ‘Œğ‘›
ğ‘ƒğ‘‹|ğ‘Œ
âŠ—ğ‘›
ğ‘‹ğ‘›
ğ‘€ ğ‘”ğ‘›
à· ğ‘Œğ‘› ğ‘‘(ğ‘Œğ‘›, à· ğ‘Œğ‘›) â‰¤ ğ·
Fig. 3: Remote source coding operational setup.
From convexity, we can easily see that E
ï¿½ Ëœd(X, Ë†y)
ï¿½ï¿½Y = Ë†y
ï¿½
â‰¤ E
ï¿½ Ëœd(X, Ë†y0)
ï¿½ï¿½Y = Ë†y
ï¿½. Therefore, any distortion minimizing PX, Ë†Y should satisfy the condition
Ë†y = PY | Ë†Y =Ë†y, P Ë†Y -a.s..
In other words, we have E
ï¿½ d(Y, Ë†Y )
ï¿½
= E
ï¿½ Ëœd(X, Ë†Y )
ï¿½
â‰¥ H(Y | Ë†Y ) with equality holding whenever condition (14) is satisfied. Relabeling Ë†Y as T we obtain
R(D) = inf
PT |X: H(Y |T )â‰¤D I(X; T), (15) where the underlying joint distribution is PX,Y PT |X, i.e., so that Y â†” X â†” T forms a Markov chain.
Yet another way to express IB and R(D) is the following. Let Î 1 be a random variable valued in P(Y) â€” the space of probability measures on Y. Namely, Î 1 = PY |X=x0 with probability PX(x0) (and, more generally, Î 1 = PY |X=X for a regular branch of conditional law PY |X). Then, we have
R(D) = inf
PÎ 2|Î 1
ï¿½
I(Î 1; Î 2) : E[DKL(Î 1âˆ¥Î 2)] â‰¤ D âˆ’ H(Y |X)
ï¿½, (16) where DKL is the Kullback-Leibler (KL) divergence and the minimization is over all conditional distributions of Î 2 âˆˆ P(Y) given Î 1.
An interesting way to show the achievability of R(D) in the RSC problem is via Berger-Tung multiterminal source coding (MSC) inner bound,. RSC is a special case of a 2-user MSC obtained by nullifying the communication rate of one source and driving to infinity the distortion constraint on the other. Alternatively, one may employ an explicit coding scheme: first quantize (encode) the observation Xn to a codeword T n, such that the empirical distribution of (Xn, T n) is a good approximation of PXPT |X, and then communicate this quantization to the decoder via Slepian-Wolf coding. The reader is referred to [46, Section IV-B] for a detailed treatment of MSC under logarithmic loss.
Setting D = H(Y )âˆ’Î± in (15) one recovers the IB problem (1). Thus, RSC with logarithmic loss can be viewed as the operational setup whose solution is given by the IB optimization problem. This provides an operational interpretation to the ad hoc definition of the IB framework as presented in Section II. Additional connections between the IB problem and other information-theoretic setups can be found in the recent survey.
IV. INFORMATION BOTTLENECK IN MACHINE LEARNING
The IB framework had impact on both theory and practice of ML. While its first applications in the field was for clustering, more recently it is often explored as a learning objective for deep models. This was concurrently
13 done in,, by optimizing the IB Lagrangian (3) via a variational bound compatible with gradient-based methods. Applications to classification and generative modeling were explored in, and, respectively.
A common theme in and was that the IB objective promotes disentanglement of representations.
From a theoretical standpoint, the IB gave rise to a bold information-theoretic DL paradigm,. It was argued that, even when the IB objective in not explicitly optimized, DNNs trained with cross-entropy loss and SGD inherently solve the IB compression-prediction trade-off. This 'IB theory for DL' attracted significant attention, culminating in many follow-up works that tested the proclaimed narrative and its accompanying empirical observations. This section surveys the practical and theoretical roles of IB in DL, focusing on classification tasks.
A. Information Bottleneck as Optimization Objective
1) Variational approximation and efficient optimization: In, a variational approximation to the IB objective(3) was proposed. The approximation parametrized the objective using a DNN and proposed an efficient training algorithm. As opposed to classic models, the obtained system is stochastic in the sense that it maps inputs to internal representations via randomized mappings. Empirical tests of the VIB system showed that it generalized better and was more robust to adversarial examples compared to competing methods.
IB objective. Given a DNN, regard its â„“th internal representation Tâ„“, abbreviated as T, as a randomized mapping operating on the input feature X; the corresponding label is Y. Denote by X, Y and T the sets in which X, Y and T take values. The encoding of X into T is defined through a conditional probability distribution, which the VIB parametrizes as P (Î¸)
T |X, Î¸ âˆˆ Î˜. Together with PX,Y, P (Î¸)
T |X defines the joint distribution of ï¿½
X, Y, T (Î¸)) âˆ¼ P (Î¸)
X,Y,T :=
PX,Y P (Î¸)
T |X. With respect to this distribution, one may consider the optimization objective
L(VIB) Î²(Î¸) := max Î¸âˆˆÎ˜ I
ï¿½
T (Î¸) â„“
; Y
ï¿½
âˆ’ Î²I
ï¿½
X; T (Î¸) â„“
ï¿½
In accordance to the original IB problem, the goal here is to learn representations that are maximally informative about the label Y, subject to a compression requirement on X. However, since the data distribution PX,Y is unknown and (even knowing it) direct optimization of (17) is intractable â€” a further approximation is needed.
Variational approximation. To overcome intractability of (17), the authors of lower bound it by a form that is easily optimized via gradient-based methods. Using elementary properties of mutual information, entropy and KL divergence, (17) is lower bounded by
EP (Î¸)
Y,T
ï¿½ log Q(Ï†)
Y |T
ï¿½
Y
ï¿½ï¿½T (Î¸)ï¿½ï¿½
âˆ’ Î²DKL
ï¿½
P (Î¸)
T |X
ï¿½ï¿½P (Î¸)
T
ï¿½ï¿½PX
ï¿½, (18) where Q(Ï†)
Y |T is a conditional distribution from T to Y parametrized by a NN with parameters Ï† âˆˆ Î¦. This distribution plays the role of a variational approximation of the decoder P (Î¸)
Y |T. Another difficulty is that the marginal
P (Î¸)
T (Â·) =
ï¿½
P (Î¸)
T |X(Â·|x) dPX(x), which is defined by P (Î¸)
T |X and PX, it is typically intractable. To circumvent this, one may replace it with some reference measure RT âˆˆ P(T ), thus further lower bounding (18). This is the strategy that was employed in.(a)(b)(c)
Fig. 4 [Fig. 2 from ]: 2-dimensional VIB embedding of 103 MNIST images for: (a) Î² = 10âˆ’3 ; (b) Î² = 10âˆ’1; and (c) Î² = 1. The ellipses in each figure illustrate 95% confidence intervals of the Gaussian encoding P (Î¸)
T |X(Â·|x) =
N
ï¿½ ÂµÎ¸(x), Î£Î¸(x)
ï¿½, for 103 input MNIST images x. The larger Î² is, the more compressed representations become.
This is expressed in larger covariance matrices for the encoder. The background grayscale illustrates H
ï¿½
Q(Ï†)
Y |T (Â·|t)
ï¿½, for each t âˆˆ R2, which measures classification uncertainty at a given point.
Using the reparametrization trick from and replacing PX,Y in (18) with its empirical proxy Pn :=
1 n
ï¿½n i=1 Î´(xi,yi), where Î´x is the Dirac measure centered at x and Dn :=
ï¿½(xi, yi)
ï¿½n i=1 is the dataset, we arrive at the empirical loss function
Ë†L(VIB) Î²(Î¸, Ï†, Dn) := 1 n n
ï¿½ i=1
E
ï¿½
âˆ’ log Q(Ï†)
Y |T
ï¿½ yn
ï¿½ï¿½f(xn, Z)
ï¿½ï¿½
+ Î²DKL
ï¿½
P (Î¸)
T |X(Â·|xn)
ï¿½ï¿½ï¿½RT (Â·)
ï¿½
Here Z is an auxiliary noise variable (see ) and the expectation is w.r.t. its law. The first term is the average cross-entropy loss, which is commonly used in DL. The second term serves as a regularizer that penalizes the dependence of X on T, thus encouraging compression. The empirical estimator in (19) of the variational lower bound is differentiable and easily optimized via standard stochastic gradient-based methods. The obtained gradient is an unbiased estimate of the true gradient.
Empirical study. For their experiments, the authors of set the encoder distribution to P (Î¸)
T |X(Â·|x) =
N
ï¿½ ÂµÎ¸(x), Î£Î¸(x)
ï¿½, with mean and convariance matrix parametrized by a DNN. The variational decoder Q(Ï†)
Y |T was set to a logistic regression function, while RT was chosen as a (fixed) standard Gaussian distribution. The performance of VIB was tested on the MNIST and ImageNet datasets; we focus on the MNIST results herein. First, it was shown that a VIB classifier outperforms a multi-layer perceptron (MLP) fitted using (penalized) maximum likelihood estimation (MLE). Considered penalization techniques include dropout, confidence penalty, and label smoothing (see [31, Table 1] for accuracy results).
To illustrate the operation of VIB, a 2-dimensional embedding of internal representations is examined for different Î² values. The results are shown in Fig. 4 (reprinted from [31, Fig. 2]). The posteriors P (Î¸)
T |X are represented as
Gaussian ellipses (representing the 95% confidence region) for 103 images from the test set. Colors designate true class labels. The grayscale shade in the background corresponds to the entropy of the variational classifier Q(Ï†)
Y |T
15 at a given point, i.e., H
ï¿½
Q(Ï†)
Y |T (Â·|t)
ï¿½, for t âˆˆ R2. This entropy quantifies the uncertainty of the decoder regarding the class assignment to each point. Several interesting observations are: (i) as Î² increases (corresponds to more compression in (17)), the Gaussian encoder covariances increase in relation to the distance between samples, and the classes start to overlap; (ii) beyond some critical Î² value, the encoding 'collapses' essentially losing class information; and (iii) there is uncertainty in class predictions, as measured by H
ï¿½
Q(Ï†)
Y |T (Â·|t)
ï¿½, in the areas between the class embeddings. This illustrates the ability of VIB to learn meaningful and interpretable representations of data, while preserving good classification performance.
While Î² = 10âˆ’1 (Fig. 4(b)) has relatively large covariance matrices, showed that this this system has reasonable classification performance (3.44% test error). This implies there is significant in-class uncertainty about locations of internal representations, although the classes themselves are well-separated. Such encoding would make it hard to infer which input image corresponds to a given internal representation. This property leads to consider model robustness, which was demonstrated as another virtue of VIB.
To test model robustness, employed the fast gradient sign (FGS) and L2 optimization attacks for perturbing inputs to fool the classifier. Robustness was measured as classification accuracy of adversarial examples.
The (stochastic) VIB classifier showed increasing robustness to these attacks, compared to competing deterministic models which misclassified all the perturbed inputs. This is an outcome of the randomized VIB mapping from X to T, which suppresses the ability to tailor adversarial examples that flip the classification label. Robustness is also related to compression: indeed, larger Î² values correspond to more robust systems, which can withstand adversarial attacks with large perturbation norms (see [31, Fig. 3]).
2) IB objective for sufficiency, minimality and disentanglement: Another perspective on the IB objective was proposed in, that was published concurrently with. This work provided an information-theoretic formulation of some desired properties of internal representations, such as sufficiency, minimality and disentanglement, and showed that IB optimization favors models that posses them. To optimize VIB, presented a method that closely resembles that of. The method reparametrizes T (Î¸) as a product of some function of X and a noise variable
Z. The noise distribution is for the system designer to choose. Setting Z âˆ¼ Bern(p), recovers the popular dropout regularization, hence the authors of called their method 'information dropout'. The noise distribution employed in is log-normal with zero mean, and variance that is a parametrized function of X. We omit further details due to similarity to VIB optimization (Section IV-A1).
Minimality and sufficiency. That IB solutions are approximate MSSs is inherent to the problem formulation(see Section II-C). The authors of discussed the relation between minimality of representation and invariance to nuisance factors. Such factors affect the data, but the label is invariant to them. By penalizing redundancy of representations (i.e., enforcing small I(X; T (Î¸))) it was heuristically argued that the model's sensitive to nuisances is mitigated. Some experiments to support this claim were provided, demonstrating the ability of VIB to classify hand-written digits from the cluttered MNIST dataset, or CIFAR-10 images occluded by MNIST digits.
Disentanglement. Another property considered in is disentanglement, which refers to weak dependence
16 between elements of an internal representation vector. This idea was formalized using total correlation (TC):
DKL
ï¿½
P (Î¸)
T
ï¿½ï¿½ï¿½ï¿½
ï¿½d j=1 Qj
ï¿½(20) where ï¿½d j=1 Qj is some product measure on the elements of the d-dimensional representation T. Adding a TC regularizer to (18) will encourage the system to learn disentangled representation in the sense of small (20). If the TC regularization parameter is set equal to Î² in (18), it trivially simplifies to
EP (Î¸)
Y,T
ï¿½ log Q(Ï†)
Y |T
ï¿½
Y
ï¿½ï¿½T (Î¸)ï¿½ï¿½
âˆ’ Î²DKL
ï¿½
P (Î¸)
T |X
ï¿½ï¿½ï¿½ï¿½
ï¿½d j=1 Qj
ï¿½ï¿½ï¿½ï¿½PX
ï¿½
Thus, choosing RT âˆˆ P(T ), in the framework of, as a product measure is equivalent to regularizing for disentanglement. While the term 'disentanglement' was not used in, they do set RT as a product measure.
Altogether, Section IV-A demonstrates the practical usefulness of the IB as an optimization objective. It is easy to optimize under proper parameterization and learns representation with various desired properties. The work of took these observation a step further. They claimed that DNNs trained with SGD and cross-entropy loss inherently solve the IB problem, even when there in no explicit reference to the IB problem in the system design. We elaborate on this theory next.
B. Information Bottleneck Theory for Deep Learning
Recently, a information-theoretic paradigm for DL based on the IB framework was proposed,. It claimed that DNN classifiers trained with cross-entropy loss and SGD inherently (try to) solve the IB optimization problem.
Namely, the system aims to find internal representations that are maximally informative about the label Y, while compressing X, as captured by (3). Coupled with an empirical case study, this perspective was leveraged to reason about the optimization dynamics in the IP, properties of SGD training, and the computational benefit of deep architectures. This section reviews these ideas as originally presented in,, with the exception of Section
IV-B3 that discusses the incompatibility of the IB framework to deterministic DNNs. Whether the proposed theory holds is general was challenged by several follow-up works, which are expounded upon in Section V.
1) Setup and Preliminaries: Consider a feedforward DNN with L layers, operating on an input x âˆˆ Rd0 according to: Ï†â„“(x) = Ïƒ(Aâ„“Ï†â„“âˆ’1(x) + bâ„“), Ï†0(x) = x, â„“ = 1,..., L, (21) where Aâ„“ âˆˆ Rdâ„“Ã—dâ„“âˆ’1 and bâ„“ âˆˆ Rdâ„“ are, respectively, the â„“th weight matrix and bias vector, while Ïƒ : R â†’ R is the activation function operating on vectors element-wise. Letting (X, Y ) âˆ¼ PX,Y be the feature-label pair, we call Tâ„“ â‰œ Ï†â„“(X), â„“ = 1,..., L âˆ’ 1, the â„“th internal representation. The output layer TL is a reproduction9 of Y, sometimes denoted by Ë†Y.
The goal of the DNN classifier is to learn a reproduction Ï†L(X) = Ë†Y that is a good approximation of the true label Y. Let Î¸ represent the network parameters (i.e., weight matrices and bias vectors) and write Ï†(Î¸)
L for Ï†L to
9Perhaps up to an additional soft-max operation.
17 stress the dependence of the DNN's output on Î¸. Statistical learning theory measures the quality of the reproduction by the population risk
LPX,Y (Î¸) := Ec
ï¿½ Ï†(Î¸)
L (X), Y
ï¿½
=
ï¿½ c
ï¿½ Ï†(Î¸)
L (x), y) dPX,Y (x, y), where c is the cost/loss function. Since PX,Y is unknown, a learning algorithm cannot directly compute LPX,Y (Î¸) for a given Î¸ âˆˆ Î˜. Instead, it can compute the empirical risk of Î¸ on the dataset Dn :=
ï¿½(xi, yi)
ï¿½n i=1, which comprises n i.i.d. samples from PX,Y. The empirical risk is given by
LDn(Î¸) := 1 n n
ï¿½ i=1 c(Ï†(Î¸)
L (xi), yi).
Minimizing LDn(Î¸) over Î¸ is practically feasible, but the end goal is to attain small population risk. The gap between them is captured by the generalization error gen(PX,Y, Î¸) := E
ï¿½
LPX,Y (Î¸) âˆ’ LDn(Î¸)
ï¿½, where the expectation is w.r.t P âŠ—n
X,Y. The sample complexity nâ‹†(PX,Y, Ïµ, Î´) is the least number of samples n needed to ensure LPX,Y (Î¸) âˆ’ LDn(Î¸) â‰¤ Ïµ with probability at least 1 âˆ’ Î´. To reason about generalization error and sample complexity, the IB theory for DL views the learning dynamics through the so-called 'information plane'.
2) The information plane: Consider the joint distribution of the label, feature, internal representation and output random variables. By (21), they form a Markov chain Y â†” X â†” T1 â†” T2 â†”... â†” TL, i.e., their joint law factors as PX,Y,T1,T2,...,TL = PX,Y PT1|XPT2|T1Â·Â·Â· PTL|TLâˆ’1.10 Based on this Markov relation, the data processing inequality (DPI) implies
I(X; Y ) â‰¥ I(T1; Y ) â‰¥ I(T2; Y ) â‰¥... â‰¥ I(TL; Y )
H(X) â‰¥ I(X; T1) â‰¥ I(X; T2) â‰¥... â‰¥ I(X; TL).(22) illustrating how information inherently dissipates deeper in the network. The IB theory for DL centers around how each internal representation Tâ„“ tries to balance its two associated mutual information term.
Fix â„“ = 1,..., L âˆ’ 1, and consider the conditional marginal distributions PTâ„“|X and PTL|Tâ„“. Respectively, these distributions define an 'encoder' (of X into Tâ„“) and 'decoder' (of Tâ„“ into TL = Ë†Y ) for an IB problem associated with the â„“th hidden layer (see Fig. 5). The argument of and is that the IB framework captures the essence of learning to classify Y from X: to reconstruct Y from X, the latter has to go through the bottleneck Tâ„“. This Tâ„“ should shed information about X that is redundant/irrelevant for determining Y, while staying maximally informative about Y itself. The working assumption of, is that by optimizing the DNN's layers {Tâ„“}L â„“=1 for that task via standard SGD, the encoder-decoder pair for each hidden layer converges to its optimal IB solution.
With this perspective, conducted an empirical case study of a DNN classifier trained on a synthetic task, reporting several striking findings. The study is centered around IP visualization of the learning dynamics, i.e., 10Marginal distributions of this law are designated by keeping only the relevant subscripts.
â†” ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½1 â†”
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½2 â†” â‹¯ â†” ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½âˆ’1 â†” ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ = ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½2|ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½|ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½2
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ â†” ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½
Fig. 5 [Adapted from Fig. 1 from ]: IB framework for a DNN classifier with L layers. The label, feature, hidden representations and output form a Markov chain Y â†” X â†” T1 â†” T2 â†”... â†” TL. An encoder PTâ„“|X and a decoder TTL|Tâ„“ are associated with each hidden layer Tâ„“, â„“ = 1,..., L âˆ’ 1. tracking the evolution of ï¿½
I(X; Tâ„“), I(Tâ„“; Y )
ï¿½ across training epochs. First, they observed that the training process comprises two consecutive phases, termed 'fitting' and 'compression': after a quick initial fitting phase, almost the entire training process is dedicated to compressing X, while staying informative about Y. Second, it was observed that the compression phase starts when the mean and variance of the stochastic gradient undergoes a phase transition from high to low signal-to-noise (SNR). Third, observed that the two training phases accelerate when more layers are added to the network, which led to an argument that the benefit of deeper architectures is computational.
Central to the empirical results of is the ability to measure I(X; Tâ„“) and I(Tâ„“; Y ) in a DNN with fixed parameters. We explain next that these information measures are ill-posed in deterministic networks (i.e., networks that define a deterministic mapping from input to output). Afterwards, we describe how circumvented this issue by instead evaluating the mutual information terms after quantizing the internal representation vectors. We then turn to expound upon the empirical findings of based on these quantized measurements.
3) Vacuous mutual information in deterministic deep networks: We address theoretical caveats in applying IBbased reasoning to deterministic DNNs. A deterministic DNN is one whose output, as well as every internal representation, is a deterministic function of its input X. The setup described in Section IV-B1, which is standard for DNNs and the one used in,,, adheres to the deterministic framework. In deterministic DNNs with continuous and strictly monotone nonlinearities (e.g., tanh or sigmoid) or bi-Lipschitz (e.g., leaky-ReLU), the mutual information I(X; Tâ„“) is provably either infinite (continuous X) or a constant that does not depend on the network parameters (discrete X). The behavior for ReLU or step activation functions is slightly different, though other issues arise, such as the IB functional being piecewise constant.
We start from continuous inputs. Elementary information-theoretic arguments show that I(X; Tâ„“) = âˆ a.s.11 if X is a continuous Rd-valued random variable and nonlinearities are continuous and strictly monotone. Indeed, in 11With respect to, e.g., the Lebesgue measure on the parameter space of ï¿½(Ai, bi)
ï¿½â„“ i=1, or the entry-wise i.i.d. Gaussian measure.
19 this case the dâ„“-dimensional Tâ„“ = Ï†â„“(X) is a.s. continuous whenever dâ„“ â‰¤ d, and so I(X; Tâ„“) â‰¥ I(Tâ„“; Tâ„“) = âˆ(cf. [42, Theorems 2.3 and 2.4]). A more general statement was given in [15, Theorem 1] and is restated next.
Theorem 2 (Theorem 1 of ) Let X be a d-dimensional random variable, whose distribution has an absolutely continuous component with density function that is continuous on a compact subset of Rd. Assume that the activation function Ïƒ in (21) is either bi-Lipschitz or continuously differentiable with strictly positive derivative. Then, for every â„“ = 1,..., L and almost all weight matrices A1,..., Aâ„“, we have I(X; Tâ„“) = âˆ.
The proof uses the notion of correlation dimension. It shows that X has a positive correlation dimension that remains positive throughout the DNN, from which the conclusion follows. This result broadens the conditions for which I(X; Tâ„“) = âˆ beyond requiring that Tâ„“ is continuous. This, for instance, accounts for cases when the number of neurons dâ„“ in Tâ„“ exceeds the dimension of X. Theorem 2 implies that for continuous features X, the true mutual information I(X; Tâ„“) = âˆ, for any hidden layer â„“ = 1,..., L in the tanh network from.
To avoid this issue, model X âˆ¼ Unif(Xn), where Xn = {xi}n i=1. While having a discrete distribution for
X ensures mutual information is finite, as I(X; Tâ„“) â‰¤ H(X) = log n, a different problem arises. Specifically, whenever nonlinearities are injective (e.g., strictly monotone), the map from Xn to Ï†â„“(Xn) = {Ï†â„“(x) : x âˆˆ Xn}(as a mapping between discrete sets) is a.s. injective. As such, we have that I(X; Tâ„“) = H(X) = log n and I(Tâ„“; Y ) = I(X; Y ), which are constants independent of the network parameters.
Both continuous and discrete degeneracies are a consequence of the deterministic DNN's ability to encode information about X in arbitrarily fine variations of Tâ„“, essentially without loss, even if deeper layers have fewer neurons. Consequently, no information about X is lost when traversing the network's layers, which renders I(X; Tâ„“) a vacuous quantity for almost all network parameters. In such cases approximating or estimating I(X; Tâ„“) and I(Tâ„“; Y ) to study DNN learning dynamics is unwarranted. Indeed, the true value (e.g., infinity or H(X) for
I(X; Tâ„“)) is known and does not depend on the network.
4) Mutual information measurement via quantization: The issues described above are circumvented in by imposing a concrete model on (X, Y ) and quantizing internal representation vectors. Namely, X is assumed to be uniformly distributed over the dataset Xn = {xi}n i=1, while PY |X is defined through a logistic regression with respect to a certain spherically symmetric real-valued function of X. With this model, quantize Tâ„“ to compute
ï¿½
I(X; Tâ„“), I(Tâ„“, Y )
ï¿½, as described nex.
Specifically, consider a DNN with bounded nonlinearities Ïƒ : R â†’ [a, b]. Let Qm[Tâ„“] be the quantized version of the dâ„“-dimensional random vector Tâ„“, which dissects its support (the hypercube [a, b]dâ„“) into mdâ„“ equal-sized cells (m in each direction). The two mutual information terms are then approximated by
I(X; Tâ„“) â‰ˆ I
ï¿½
X; Qm[Tâ„“]
ï¿½
= H
ï¿½
Qm[Tâ„“]
ï¿½(23a)
I(Tâ„“; Y ) â‰ˆ I
ï¿½
Qm[Tâ„“]; Y
ï¿½
= H
ï¿½
Qm[Tâ„“]
ï¿½
âˆ’
ï¿½ y=0,1
PY (y)H
ï¿½
Qm[Tâ„“]
ï¿½ï¿½Y = y
ï¿½(23b) where the equality in (23a) is because Tâ„“ (and thus Qm[Tâ„“]) is a deterministic function of X, while PY in (23a) is given by PY (1) =
1 n
ï¿½n i=1 PY |X(1|xi). Computing H
ï¿½
Qm[Tâ„“]
ï¿½ amounts to counting how many inputs xi, ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½â„“ ; ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½; ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½â„“
Fig. 6 [Fig. 3 from ]: IP dynamics for the fully connected 12â€“10â€“7â€“5â€“4â€“3â€“2 binary DNN classifier from.
The horizontal and vertical axis correspond to I
ï¿½
X; Qm[Tâ„“]
ï¿½ and I
ï¿½
Qm[Tâ„“]; Y
ï¿½, respectively. The thick curves show
ï¿½
I
ï¿½
X; Qm[Tâ„“]
ï¿½, I
ï¿½
Qm[Tâ„“]; Y
ï¿½ï¿½ values across training epoch (designated by the color map) for the different hidden layers. Curves for deeper layers appear more to the left. The thin lines between the curves connect
ï¿½
I
ï¿½
X; Qm[Tâ„“]
ï¿½, I
ï¿½
Qm[Tâ„“]; Y
ï¿½ï¿½ values across layers at a given epoch. The two training phases of 'fitting' and 'compression' are clearly seen in this example, as the curve for each layer exhibits an elbow effect. The green marks correspond to the transition between the 'drift' and 'diffusion' phases of SGD (see Section IV-B6 and Fig. 7) i = 1,..., n, have their internal representation Ï†â„“(xi) fall inside each of the mdâ„“ quantization cells under Qm. For the conditional entropy, counting is restricted to xi's whose corresponding label is yi = y.
The quantized mutual information proxies are motivated by the fact that for any random variable pair (A, B), I(A; B) = lim j,kâ†’âˆ I
ï¿½
[A]j; [B]k
ï¿½, (24) where [A]j and [B]k are any sequences of finite quantizations of A and B, respectively, such that the quantization errors tend to zero as j, k â†’ âˆ (see [54, Section 2.3]). Thus, at the limit of infinitely-fine quantization, the approximations in (23) are exact. In practice, fix the resolution m in Qm and perform the computation w.r.t. this resolution. Doing so generally bounds the approximation away from the true value, resulting in a discrepancy between the computed values and the true DNN model (where activations are not quantized during training or inference). This discrepancy and its effect on the observations of are discussed in detail in Section V. The remainder of the current section focuses on describing the findings of, temporarily overlooking these subtleties.
5) Information plane dynamics and two training phases:
Using the above methodology for approximating
ï¿½
I(X; Tâ„“), I(Tâ„“, Y )
ï¿½, explores IP dynamics during training of a DNN classifier on a certain synthetic task.
The task is binary classification of 12-dimensional inputs using a fully connected 12â€“10â€“7â€“5â€“4â€“3â€“2 architecture with tanh nonlinearities.12 Training is performed via standard SGD and cross-entropy loss. The IP visualizations are produced by subsampling training epochs and computing
ï¿½
I
ï¿½
X; Qm[Tâ„“]
ï¿½, I
ï¿½
Qm[Tâ„“]; Y
ï¿½ï¿½, with m = 30, w.r.t. the (fixed) DNN parameters at each epoch. To smooth out the curves, trajectories are averaged over 50 runs.
12For the full experimental setup see [6, Section 3.1].
Fig. 6 (reprinted from [6, Fig. 3]), demonstrates the IP dynamics for this experiment. The thick trajectories show
ï¿½
I
ï¿½
X; Qm[Tâ„“]
ï¿½, I
ï¿½
Qm[Tâ„“]; Y
ï¿½ï¿½ evolution across training epochs (which are designated by the color map) for the different hidden layers of the network. Deeper layers are bound from above (in the Pareto sense) by shallower ones, in accordance to the DPI (see Eq. (22)). The thin lines between the IP curves connect the mutual information pair values across layers at a given epoch. These IP dynamics reveal a remarkable trend, as the trajectories of ï¿½
I
ï¿½
X; Qm[Tâ„“]
ï¿½, I
ï¿½
Qm[Tâ„“]; Y
ï¿½ï¿½ exhibit two distinct phases: an increase in both I
ï¿½
X; Qm[Tâ„“]
ï¿½ and I
ï¿½
Qm[Tâ„“]; Y
ï¿½ at the beginning of training, followed by along-term decrease in I
ï¿½
X; Qm[Tâ„“]
ï¿½ that subsumes most of the training epochs. These two phases were termed, respectively, 'fitting' and 'compression'. While the increase in I
ï¿½
Qm[Tâ„“]; Y
ï¿½ during the fitting phase is expected, there was no explicit regularization to encourage compression of representations.
Inspired by the classic IB problem (Section II), the compression phase was interpreted as the network 'shedding' information about X that is 'irrelevant' for the classification task. The authors of then argued that the observed two-phased dynamics are inherent to DNN classifiers trained with SGD, even when the optimization method/objective have no explicit reference to the IB principle. The compression phase was further claimed to be responsible for the outstanding generalization performance of deep networks, although no rigorous connection between I
ï¿½
X; Qm[Tâ„“]
ï¿½ and the generalization error is currently known.
6) Connection to stochastic gradient descent dynamics: To further understand the two IP phases of training, compared them with SGD dynamics. For each layer â„“ = 1,..., L, define Âµâ„“ :=
ï¿½ï¿½ï¿½ï¿½
ï¿½ âˆ‚c
âˆ‚Aâ„“
ï¿½ï¿½ï¿½ï¿½ï¿½
F
; Ïƒâ„“ :=
ï¿½ï¿½ï¿½ï¿½STD
ï¿½ âˆ‚c
âˆ‚Aâ„“
ï¿½ï¿½ï¿½ï¿½ï¿½
F, where âŸ¨Â·âŸ© and STD(Â·) denote, respectively, the mean and the element-wise standard deviation (std) across samples within a minibatch, and âˆ¥ Â· âˆ¥F is the Frobenius norm. Thus, Âµâ„“ and Ïƒâ„“ capture the gradient's mean and std w.r.t. to the â„“th weight matrix. Since weights tend to grow during training, Âµâ„“ and Ïƒâ„“ were normalized by âˆ¥Aâ„“âˆ¥F. The evolution of the normalized mean and std during training is displayed in Fig. 7 (reprinted from [6, Fig. 4]).
The figure shows a clear phase transition around epoch 350, marked with the vertical grey line. At the first phase, termed 'drift', the gradient mean Âµâ„“ is much larger than its fluctuations, as measured by Ïƒâ„“. Casting Âµâ„“/Ïƒâ„“ as the gradient signal-to-noise ratio (SNR) at the â„“th layer, the drift phase is characterized by high SNR. This corresponds to SGD exploring the high-dimensional loss landscape, quickly converging from the random initialization to a near (locally) optimal region. In the second phase, termed 'diffusion', the gradient SNR abruptly drops. The low
SNR regime, as explained in, is a consequence of empirical error saturating and SGD being dominated by its fluctuations. This observation corresponds to the earlier work of,, where two phases of gradient descent (convergence towards a near-optimal region and oscillation in that region) were also identified and described in greater generality. Rather than 'drift' and 'compression', these phases are sometimes termed 'transient' and 'stochastic' or 'search' and 'convergence'.
The correspondence between the two SGD phases and the IP trajectories from Fig. 6 was summarized in as follows. First, the transition from 'fitting' to 'compression' in Fig. 6 happens roughly at the same epoch when SGD transitions from 'drift' to 'diffusion' (Fig. 7) â€” this is illustrated by the green marks in Fig. 6. The SGD drift
Epochs
Norm of gradient mean and std
Layer
Mean
Std
â€“â€“â€“
- -â€“â€“â€“
- -â€“â€“â€“
- -â€“â€“â€“
- -â€“â€“â€“
- -â€“â€“â€“
- -Fig. 7 [Fig. 4 from ]: Norms of gradient mean (solid lines) and standard deviation (dashed lines) for the different layers. For each layer, the values are normalized by the L2 norm of the corresponding weight matrix. The grey vertical line marks the epoch when a phase transition between a high-SNR to a low-SNR occurs. phase quickly reduces empirical error, thereby increasing I(Tâ„“; Y ) (as captured by its approximation I
ï¿½
Qm[Tâ„“]; Y
ï¿½ in Fig. 6). The connection between the SGD diffusion phase and compression of I
ï¿½
X; Qm[Tâ„“]
ï¿½ in the IP is less clear. A heuristic argument given in is that SGD diffusion mostly adds random noise to the weights, evolving them like Wiener processes. This diffusion-like behaviour inherently relies on the randomness in SGD (as opposed to, e.g., full gradient descent). Based on this hypothesis, claimed that SGD diffusion can be described by the Fokker-Planck equation, subject to a small training error constraint. Together with the maximum entropy principle, this led to the conclusion that the diffusion phase maximizes the conditional entropy H(X|Tâ„“), or equivalently, minimizes I(X; Tâ„“) (approximated by I
ï¿½
X; Qm[Tâ„“]
ï¿½ in Fig. 6). We note that presented no rigorous derivations to support this explanation.
7) Computation benefit of deep architectures: It is known that neural networks gain in representation power with the addition of layers â€“. The argument of is that deep architectures also result in a computational benefit.
Specifically, adding more layers speeds up both the fitting and the compression phases in the IP dynamics.
Recall the synthetic binary classification task of 12-dimensional inputs described in Section IV-B5. Consider 6 neural network architectures of increasing depth (1 to 6 hidden layers) trained to solve that task. The first hidden layer has 12 neurons and each succeeding one has two neurons less. For example, the 3rd architecture is a fully connected 12â€“12â€“10â€“8â€“2 networks. Fig. 8 (reprinted from [6, Fig. 5]) shows the IP dynamics of these 6 architectures, each averaged over 50 runs.
The figure shows that additional layers speed up the IP dynamics. For instance, while last hidden layer of the deepest network (bottom-right subfigure) attains its maximal I
ï¿½
Qm[T6]; Y
ï¿½
â‰ˆ 0.7 after about 400 epochs, ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½; ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½â„“
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½; ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½â„“
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½; ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½â„“
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½â„“ ; ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½â„“ ; ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½
Fig. 8 [Fig. 5 from ]: IP dynamics for 6 neural networks with increasing depths. Each network has 12 input and 2 output units. The width of the hidden layers start from 12 and reduces by 2 with each added layer. the shallowest network (top-left subfigure) does not reach that value throughout the entire training process. The compression of I
ï¿½
X; Qm[Tâ„“]
ï¿½, for â„“ = 1,..., L, is also faster and more pronounced in deeper architectures.
Thus, both IP phases accelerate as a result of more hidden layers. Furthermore, compression of a preceding
I
ï¿½
X; Qm[Tâ„“âˆ’1]
ï¿½ seems to push I
ï¿½
X; Qm[Tâ„“]
ï¿½ of the next layer to a smaller value.13 We also note that IP values of shallow layers, even in deeper architectures, stay almost unchanged throughout training. These values are roughly
ï¿½
H(X), I(X; Y )
ï¿½, which correspond to
ï¿½
I(X; Tâ„“), I(Tâ„“; Y )
ï¿½ when Tâ„“ = Ï†â„“(X) is a bijection (as a mapping from
Xn to Ï†â„“(Xn) := {Ï†â„“(x) : x âˆˆ Xn}). This fact will be revisited and explained in the Section V.
The interpretation of Fig. 8 in (Fig. 5 therein) synonymizes 'high I
ï¿½
Qm[TL]; Y
ï¿½
' and 'good generalization'.
We refrain from this wording since there is no known rigorous connection between generalization error and I
ï¿½
Qm[TL]; Y
ï¿½(or I(TL; Y ) for that matter).14 If one adopts the IP values prescribed by the IB problem as the figure of merit, then indeed, deeper architectures enhance the corresponding dynamics. However, the implications of the IB theory on how depth affects generalization error or sample complexity remains unclear.
8) Concluding remarks: Several claims from were not discussed here in detail. Our summary focused on the empirical observations of that work. Beyond these, included heuristic arguments about how: (i) compression of representation is causally related to the outstanding generalization performance DNNs enjoy in practice; (ii) learned
13For the true (unquantized) I(X; Tâ„“) terms, â„“ = 1,..., L, their reduction with larger â„“ values is a consequence of the DPI. The DPI, however, does not hold for the quantized mutual information proxies.
14Some connections between other information measures and generalization error are known â€“ see, e.g.,.
24 hidden layers lie on (or very close to) the IB theoretical bound (3) for different Î² values; (iii) the corresponding
'encoder' and 'decoder' maps satisfy the IB self-consistent equations (4); (iv) the effect of depth on the diffusion phase of SGD dynamics; and more. The reader is referred to the original paper for further details.
In sum, the IB theory for DL, proposed a novel perspective on DNNs and their learning dynamics. At its core, the theory aims to summarize each hidden layer into the mutual information pair
ï¿½
I(X; Tâ„“), I(Tâ„“; Y )
ï¿½, â„“ =
1,..., L, and study the systems through that lens. As the true mutual information terms degenerate in deterministic networks, adopted the quantized versions I
ï¿½
X; Qm[TL]
ï¿½ and I
ï¿½
Qm[TL]; Y
ï¿½ as figures of merit in their stead.
While doing so created a gap between the empirical study of and the theoretical IB problem, the evolution of the quantized terms throughout training revealed remarkable empirical trends. These observations were collected to a new information-theoretic paradigm to explain DL, which inspired multiple follow-up works. The next section describes some of these works and how they corroborate or challenge claims made in,.
V. REVISITING THE INFORMATION BOTTLENECK THEORY FOR DEEP LEARNING
Since,, the IB problem and the IP dynamics became sources of interest in DL research. Many works followed up both on the empirical findings and theoretical reasoning in, ; a nonexhastive list includes,, â€“,,. This section focuses on the empirical study conducted in, how quantization misestimates mutual information in deterministic networks (in light of the observations from, ), and the relation between compression and clustering revealed in.
A. Revisiting the Empirical Study
In, an empirical study aiming to test the main claims of was conducted. They focused on the two phases of training in the IP, the relation between compression of I(X; Tâ„“) and generalization, as well as the link between stochasticity in gradients and compression. Through a series of experiments, produced counter examples to all these claims. We note that employed methods similar to for evaluating I(X; Tâ„“) and I(Tâ„“; Y ). As subsequently explained in Sections V-B and V-C, these methods fail to capture the true mutual information values, which are vacuous in deterministic DNNs (i.e., when for fixed parameters, the DNN's output is a deterministic function of the input). Though the authors of were aware of the this issue (see p. 5 and Appendix C therein), they adopted the methodology of in favor of empirical comparability.15
1) Information plane dynamics and the effect of activation function: It was argued in that the fitting and compression phases seen in the IP are inherent to DNN classifiers trained with SGD. On the contrary, found that the IP profile of a DNN strongly depends on the employed nonlinear activation function. Specifically, doublesided saturating nonlinearities like tanh (used in ) or sigmoid yield a compression phase, but linear or single-sided saturating nonlinearities like ReLU(x) := max{0, x} do not compress representations. The experiment showing
15Several methods for computing IP trajectories were employed in. While they mostly used the quantization-based method of (on which we focus herein), they also examined replacing Qm[Tâ„“] with Tâ„“ + N(0, Ïƒ2Idâ„“) (although no Gaussian noise was explicitly injected to the actual activations), as well as estimating mutual information from samples via k nearest neighbor (kNN) and kernel density estimation(KDE) techniques.
25 this compared the IP dynamics of the same network once with tanh nonlinearities and then with ReLU. Two tasks were examined: the synthetic experiment from and MNIST classification. The architecture for the former was the same as in, while the latter used a 784â€“1024â€“20â€“20â€“20â€“10 fully connected architecture. The last layer in both ReLU networks employs sigmoid nonlinearities; all other neurons use ReLU. Fig. 9 (reprinted from [9, Fig. ) shows the IP dynamics of all four models.
First note that top-right subfigure reproduces the IP dynamics from (compare to Fig. 6 herein). The ReLU version of that network, however, does not exhibit compression, except in its last sigmoidal layer. Instead, the mutual information I
ï¿½
X; Qm[tâ„“]
ï¿½ seems to monotonically increase in all ReLU layers. This stands in accordance with the argument of that double-sided saturating nonlinearities can cause compression, while single-sided ones cannot. The same effect is observed for the MNIST network, by comparing its tanh version at the bottom-right with its ReLU version at bottom-left. Similar results were also observed for soft-sign (double-sided saturation) versus soft-plus (single-sided saturation) activations, given in Appendix B of. They concluded that the choice of activation function has a significant effect on IP trajectories. In particular, the compression phase of training is caused by double-sided saturating activations, as opposed to being inherent to the learning dynamics.
2) Relation between compression and generalization: A main argument of is that compression of representation is key for generalization. Specifically, by decreasing I(X; Tâ„“) (while keeping I(Tâ„“; Y ) high), the DNN sheds information about X that is irrelevant for learning Y, which in turn mitigates overfitting and promotes generalization.
To test this claim, first considered deep linear networks and leveraged recent results on generalization dynamics in the student-teacher setup,. In this setup, one neural network ('student') learns to approximate the output of another ('teacher'). The linear student-teacher framework with Gaussian input allows exact computation of both the generalization error and the input mutual information16 for a nontrivial task with interesting structure.
The exact setup is the following. Let X âˆ¼ N(0, 1 dId) be an isotropic d-dimensional Gaussian and set the teacher network output as Y = BâŠ¤
0 X + N0, where B0 âˆˆ Rd is the weight (column) vector and N0 âˆ¼ N(0, Ïƒ2
0) is an independent Gaussian noise. The teacher specifies a stochastic rule PY |X that the student network needs to learn.
Specifically, the student linear DNN is trained on a dataset generated by the teacher. Denoting the layers of the student network by {Aâ„“}L â„“=1, its reproduction of Y is Ë†Y := ALALâˆ’1 Â·Â·Â·A1X. Note that X, Y, Ë†Y and all the internal representation Tâ„“ of the student network are jointly Gaussian. This allows analytic computation of generalization error and mutual information terms for any fixed parameters (i.e., at each epoch) â€“ see Eq. (6)-(7) of.
Leveraging this fact, Fig. 3 of (not shown here) compared the IP dynamics of the student linear network with a single hidden layer to the training and test errors. While the network generalized well, not compression of I(X; T1) was observed. Instead, the linear network qualitatively behaved like a ReLU network, presenting monotonically increasing mutual information trajectories. Building on the study of linear networks in, the authors then matched the size of the student network to the number of samples, causing it to severely overfit the data. Despite now having a large generalization gap, the IP trajectory of the network did not change, still showing monotonically increasing
16To be precise, the computed quantity is I(X; Tâ„“ + Z), for some independent Gaussian noise Z. The addition of Z is needed, as without it I(X; Tâ„“) = âˆ because X is Gaussian and Tâ„“ is a linear deterministic function thereof.
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½â„“ ; ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½â„“ ; ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½; ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½â„“
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½; ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½â„“
Fig. 9 [Fig. 1 from ]: IP dynamics for tanh and ReLU DNN classifiers (left and right, respectively) for the synthetic task from and MNIST (top and bottom, respectively). The last layer in both ReLU networks (bottom row) has sigmoidal activations. While tanh networks exhibit compression of representation, no compression is observed in any of the ReLU layers.
I(X; T1) with epochs [9, Figs. 4A-4B]. This produced an example of two networks with the same IP profile (no compression) but widely different generalization performance.
Similar results were presented for nonlinear networks. The authors of retrained the tanh network from on the synthetic classification task therein, but using only 30% of the data. This network significantly overfitted the data, resulting in a high generalization gap (left panel in Fig. 10, which is reprinted from [9, Fig. 4]). Nonetheless, its IP trajectories exhibit compression of I
ï¿½
X; Qm[Tâ„“]
ï¿½(right panel of Fig. 10). A compression phase also occurs for the original network trained on 85% of the data (Figs. 6 and 9), whose test performance is much better. The main difference between the IP profiles is that the overfitted network has lower I
ï¿½
Qm[Tâ„“]; Y
ï¿½ values, but I
ï¿½
X; Qm[Tâ„“]
ï¿½ compressed in both cases.
Together, the linear and nonlinear examples dissociated compression of I(X; Tâ„“) (as approximated by
I
ï¿½
X; Qm[Tâ„“]
ï¿½
) and generalization: networks that compress may or may not generalize (nonlinear example), and the same applies for networks that do not compress (linear example). This suggest that the connection between compression and generalization, if exists, is not a simple causal relation. Furthermore, based on Fig. 10, a direct
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½; ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½â„“
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½â„“ ; ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½
Training error
Test error
Epochs
Error
Fig. 10 [Fig. 4 from ]: Train/test errors and IP dynamics for a tanh DNN trained on 30% of the synthetic dataset from. The network has high generalization gap, yet exhibits compression of I
ï¿½
X; Qm[Tâ„“]
ï¿½ in the IP. link between I(X; Tâ„“) and generalization that does not involve I(Tâ„“; Y ), seems implausible.
3) Stochastic gradients drive compression: The third claim of revisited by is that the randomness in SGD causes its diffusion phase, which in turn drives compression (see Section IV-B6). According to this rationale, training a DNN with batch gradient decent (BGD), which updates weights using the gradient of the total error across all examples, should not induce diffusion nor result in compression.
The authors of trained the tanh and ReLU networks on the synthetic task from with SGD and BGD.
Fig. 517 therein compares the obtained IP dynamics, showing no noticeable difference between the two training methodologies. Both SGD- and BGD-trained tanh networks present compression, while neither of the ReLU networks does. IP trajectories generated by SGD are shown in Fig. 9; the BGD trajectories, though not presented here, looks very much alike. Interestingly, also examined the gradient's high-to-low SNR phase transition observed in in multiple experiments. They found that it occurs every time, regardless of the employed training method, architecture or nonlinearity type, suggesting it is a general phenomenon inherent to DNN training, though not causally related to compression of representation.
B. Mutual Information Misestimation in Deterministic Networks
As discussed in Section IV-B3, I(X; Tâ„“) and I(Tâ„“; Y ) degenerate in deterministic DNNs with strictly monotone nonlinearities. The network from (also studied in ), whose IP dynamics are shown in Figs. 6, 9 and 10, is deterministic with tanh nonlinearities. Therefore, I(X; Tâ„“) = H(X) and I(Tâ„“; Y ) = I(X; Y ) are constant, independent of the network parameters, under the X âˆ¼ Unif(Xn) model used in these works (see Section IV-B3).
As such, evaluating these information terms via quantization, noise injection, or estimation from samples is illadvised. Yet, their estimates, as computed, e.g., in,, fluctuate with training epochs (that change parameter
17We believe that Figs. 5A and 5B in should be switched to correspond to their captions; compare Fig. 5B and Fig. 1A.
Layer 1
Layer 2
Layer 3
Layer 4
Layer 5 cell size = 0.1 cell size = 0.0001 cell size = 0.001 cell size = 0.01
Epochs
Fig. 11 [Fig. 1 from ]: I
ï¿½
X; Qm[Tâ„“]
ï¿½ vs. epochs for different quantization cell (bin) sizes and the model in, where X is uniformly distributed over a 212-sized empirical dataset. The curves converge to H(X) = ln(212) â‰ˆ 8.3 for small bins. values) presenting IP dynamics. These fluctuations must therefore be a consequence of estimation errors rather than changes in mutual information. Indeed, quantization or noise injection are employed in, as means for performing the measurements, but they are not part of the actual network. As explained next, this creates a mismatch between the measurement model and the system being analyzed.
To simplify discussion, we focus on I(X; Tâ„“) and its quantization-based approximation (similar reasoning applies for I(Tâ„“; Y ) and to measurements via noise injection). Note that after quantization, the mapping from X to Qm[Tâ„“] is no longer injective. This is since (distinct) representations Ï†â„“(x) and Ï†â„“(xâ€²), for x, xâ€² âˆˆ Xn, that lie sufficiently close to one another are mapped to the same value ('quantization cell' or 'bin') under Qm. The distances between representations are captured by the feature map Ï†â„“(Xn), which depends on the networks parameters. As the feature map and the quantization resolution m determine the distribution of Qm[Tâ„“], the dependence of I
ï¿½
X; Qm[Tâ„“]
ï¿½ on the network parameters becomes clear. This results in a parameter-dependent estimate I
ï¿½
X; Qm[Tâ„“]
ï¿½ of the parameter-independent quantity I(X; Tâ„“), which is undesirable. Strictly speaking, there is nothing here to estimate: since X âˆ¼ Unif(Xn) (as assumed in, ) and Ï†â„“ is injective from Xn to Ï†â„“(Xn), the true I(X; Tâ„“) value is log n.
Recalling (24), we expect that I
ï¿½
X; Qm[Tâ„“]
ï¿½
â†’ H(X) = log n as m â†’ 0. For nonnegligible m > 0, the value of I
ï¿½
X; Qm[Tâ„“]
ï¿½ strongly depends on the quantization parameter. However, since no quantization is present in the actual network (see (21)), the value of m is arbitrary and chosen by the user. Thus, the measured I
ï¿½
X; Qm[Tâ„“]
ï¿½ reveal more about the estimator and the dataset than about the true mutual information, which is a global property of the underlying joint distribution. The dependence of I
ï¿½
X; Qm[Tâ„“]
ï¿½ on m is illustrated in Fig. 11 (reprinted from [13, Fig. 1]), showing that widely different profiles can be obtained by changing m. The leftmost subfigure also shows how the quantized mutual information approaches H(X) = log n as m shrinks. Recalling that the dataset size in is n = 212 and taking logarithms to the base of e, we see that for m = 10âˆ’4, I
ï¿½
X; Qm[Tâ„“]
ï¿½
â‰ˆ log 212 â‰ˆ 8.3, for all hidden layers â„“ = 1,..., 5 and across (almost) all epochs of training.
In summary, while I
ï¿½
X; Qm[Tâ„“]
ï¿½ fails to capture the true (vacuous) mutual information value, it encodes nontrivial information about the feature maps. The compression phase observed in is in fact a property of I
ï¿½
X; Qm[Tâ„“]
ï¿½, rather than I(X; Tâ„“), driven by the evolution of internal representations. Notably, for any of the quantization resolutions shown in Fig. 11, at least for the last hidden layer, I
ï¿½
X; Qm[T5]
ï¿½ always undergoes a decrease towards
29 the end of training. This raises the questions of what is the underlying phenomenon inside the internal representation spaces that causes this behavior. As was shown in, the answer turns out to be clustering. The next section focuses on the methodology and the results of that led to this conclusion.
C. Noisy Deep Networks and Relation to Clustering of Representations
While I(X; Tâ„“) is vacuous in deterministic DNNs, the compression phase that its estimate I
ï¿½
X; Qm[Tâ„“]
ï¿½ undergoes during training seems meaningful. To study this phenomenon, developed a rigorous framework for tracking the flow of information in DNNs. Specifically, they proposed an auxiliary 'noisy' DNN setup, under which the map X ï¿½â†’ Tâ„“ is a stochastic parameterized channel, whose parameters are the network's weights and biases. This makes I(X; Tâ„“) over such networks, a meaningful system-dependent quantity. The authors of then proposed a provably accurate estimator of I(X; Tâ„“) and studied its evolution during training. Although not covered in detail herein, additional ways to make the IB non-vacuous (beyond noising the activations) include: adding noise to the weights,, changing the objective, and changing the information measure,.
1) Noisy DNNs: The definition of a noisy DNN replaces Tâ„“ = Ï†â„“(X) (see (21)) with T (Ïƒ) â„“
:= Tâ„“ + Z(Ïƒ) â„“, where ï¿½
Z(Ïƒ) â„“
ï¿½L â„“=1 are independent isotropic dâ„“-dimensional Gaussian vectors of parameter Ïƒ > 0, i.e., Z(Ïƒ) â„“
âˆ¼ NÏƒ :=
N(0, Ïƒ2Id). In other words, i.i.d. Gaussian noise is injected to the output of each hidden neuron. The noise here is intrinsic to the system, i.e, the network is trained with the noisy activation values. This stands in contrast to and, where binning or noise injection were merely a part of the measurement (of mutual information) model.
Intrinsic noise as in ensures that the characteristics of true information measures over the network are tied to the network's dynamics and the representations it is learning. Furthermore, the isotropic noise model relates
I
ï¿½
X; T (Ïƒ) â„“
ï¿½ to I
ï¿½
X; Qm[Tâ„“]
ï¿½ when Ïƒ is of the order of the quantization cell side length. This is important since it is the compression of the latter that was observed in preceding works.
To accredit the noisy DNN framework, empirically showed that it forms a reasonable proxy of deterministic networks used in practice. Namely, when Ïƒ > 0 is relatively small (e.g., of the order of 10âˆ’2 for a tanh network), it was demonstrated that noisy DNNs not only perform similarly to deterministic ones, but also that the representations learned by both systems are closely related.
2) Mutual information estimation: Adopting
ï¿½
I
ï¿½
X; T (Ïƒ) â„“
ï¿½, I
ï¿½
T (Ïƒ) â„“
; Y
ï¿½ï¿½ as the figure of merit in noisy DNNs, one still faces the task of evaluating these mutual information terms. Mutual information is a functional of the joint distribution of the involved random variables. While, Tâ„“ = Ï†â„“(X) + Z(Ïƒ) â„“ and Ï†â„“ is specified by the (known) DNN model, the data-label distribution PX,Y is unknown and we are given only the dataset Dn. Statistical learning theory treats the elements of Dn as i.i.d. samples from PX,Y. Under this paradigm, evaluating the mutual information pair of interest enters the realm of statistical estimation,, â€“. However, mutual information estimation from high-dimensional data is a notoriously difficult. Corresponding error convergence rates (with n) in highdimensional settings are too slow to be useful in practice.
Nevertheless, by exploiting the known distribution of the injected noise, proposed a rate-optimal estimator of I
ï¿½
X; T (Ïƒ) â„“
ï¿½ that scales significantly better with dimension than generic methods (such as those mentioned above).
This was done by developing a forward-pass sampling technique that reduced the estimation of I
ï¿½
X; T (Ïƒ) â„“
ï¿½ to estimating differential entropy under Gaussian noise as studied in (see also,, ). Specifically, the latter considered estimating the differential entropy h(S + Z) = h(P âˆ— NÏƒ) based on 'clean' samples of S âˆ¼ P, where P belongs to a nonparametric distribution class, and knowledge of the distribution of Z âˆ¼ NÏƒ, which is independent of S. Here (P âˆ— Q)(A) :=
ï¿½ ï¿½
1A(x + y) dP(x) dQ(y) is the convolution of two probability measures
P and Q, and 1A is the indicator function of A.
The reduction of mutual information estimation to estimating h(S + Z) uses the decomposition
I
ï¿½
X; T (Ïƒ) â„“
ï¿½
= h(T (Ïƒ) â„“
ï¿½
âˆ’
ï¿½ h
ï¿½
T (Ïƒ) â„“
ï¿½ï¿½X = x
ï¿½ dPX(x), (25) along with the fact that T (Ïƒ) â„“
= Tâ„“ + Z(Ïƒ) â„“, where Tâ„“ = Ïƒ(Aâ„“Ï†â„“âˆ’1(X) + bâ„“), is easily sampled via the forward-pass of the network (see [17, Section IV] and [13, Section III] for more details).18 Building on, the employed estimator for h(P âˆ— NÏƒ) was Ë†h(Sn, Ïƒ) := h( Ë†PSn âˆ— NÏƒ), where Ë†PSn := 1 n
ï¿½n i=1 Î´Si is the empirical distribution of the i.i.d. sample Sn := (S1,..., Sn) of S âˆ¼ P. The estimation risk of Ë†PSn over the class of all compactly supported [17, Theorem 2] or sub-Gaussian [17, Theorem 3] d-dimensional distributions P scales as cdnâˆ’ 1
2, for an explicitly characterized numerical constant c. Matching impossibility results showed that this rate is optimal both in n and d. By composing multiple differential entropy estimators (for h
ï¿½
T (Ïƒ) â„“
ï¿½ and each h
ï¿½
T (Ïƒ) â„“
ï¿½ï¿½X = x
ï¿½, x âˆˆ Dn), an estimator Ë†I(Dn, Ïƒ) of I
ï¿½
X; T (Ïƒ) â„“
ï¿½ was constructed. Its absolute-error estimation risk over, e.g., DNNs with tanh nonlinearities scales as follows.
Proposition 1 (Mutual Information Estimator ) For the described estimation setup, we have sup
PX
E
ï¿½ï¿½ï¿½I(X; T) âˆ’ Ë†IInput
ï¿½
Xn, Ë†h, Ïƒ
ï¿½ï¿½ï¿½ï¿½ â‰¤ 8cdâ„“ + dâ„“ log
ï¿½
1 Ïƒ2
ï¿½
4âˆšn, where c is a constant independent of n, d and PX, which is explicitly characterized in [17, Equation (61)].
Notably, the right-hand side depends exponentially on dimensions, which limits the dimensionality of experiments for which the bound in non-vacuous. This limitation is inherent to the considered estimation problem, as proved that the sample complexity of any good estimator depends exponentially on dâ„“.
3) Empirical study and relation to clustering: The developed toolkit enabled the authors of to accurately track I
ï¿½
X; T (Ïƒ) â„“
ï¿½ during training of (relatively small) noisy DNN classifiers. For the synthetic experiment from with tanh activations, empirically showed that I
ï¿½
X; T (Ïƒ) â„“
ï¿½ indeed undergoes a long-term compression phase(see Fig. 12, which is reprinted from [13, Fig. 5(a)]). To reveal the geometric mechanism driving this phenomenon, they related I
ï¿½
X; T (Ïƒ) â„“
ï¿½ to data transmission over AWGN channels. The mutual information I
ï¿½
X; T (Ïƒ) â„“
ï¿½ can be viewed as the aggregate number of bits (reliably) transmittable over an AWGN channel using an input drawn from the latent representation space. As training progresses, the hidden representations of equilabeled inputs cluster together, becoming increasingly indistinguishable at the channel's output, thereby decreasing I
ï¿½
X; T (Ïƒ) â„“
ï¿½. To test
18Samples for estimating the conditional differential entropy terms h
ï¿½
T (Ïƒ) â„“
ï¿½ï¿½X = x
ï¿½, for x âˆˆ X, are obtained by feeding the network with x multiple times and reading T (Ïƒ) â„“ values corresponding to different noise realizations.
Fig. 12: I
ï¿½
X; T (Ïƒ) â„“
ï¿½ evolution and training/test losses across training for a noisy version of the DNN from. Scatter plots show the input samples as represented by the 5th hidden layer (colors designate classes) at the arrow-marked epochs. this empirically, contrasted I
ï¿½
X; T (Ïƒ) â„“
ï¿½ trajectories with scatter plots of the feature map Ï†â„“(Xn). Remarkably, compression of mutual information and clustering of latent representations clearly corresponded to one another. This can be seen in Fig. 12 by comparing the scatter plots on the right to the arrow-marked epochs in the information flow trajectory on the left.
Next, accounted for the observations from, of compression in deterministic DNNs. It was demonstrated that while the quantization-based estimator employed therein fails to capture the true (constant/infinite) mutual information, it does serve as a measure of clustering. Indeed, for a deterministic network we have
I
ï¿½
X; Qm[Tâ„“]
ï¿½
= H
ï¿½
Qm[Tâ„“]
ï¿½, where Qm partitions the dynamic range (e.g., [âˆ’1, 1]dâ„“ for a tanh layer) into mdâ„“ cells. When hidden representations are spread out, many cells are non-empty, each having some small positive probability mass. Conversely, for clustered representations, the distribution is concentrated on a small number of cells, each with relatively high probability.
Recalling that the uniform distribution maximizes Shannon entropy, we see that reduction in H
ï¿½
Qm[Tâ„“]
ï¿½ corresponds to tighter clusters in the latent representation space.
The results of identified the geometric clustering of representations as the fundamental phenomenon of interest, while elucidating some of the machinery DNNs employ for learning. Leveraging the clustering perspective, also provided evidence that compression and generalization may not be causally related.
Specifically, they constructed DNN examples that generalize better when tight clustering is actively suppressed during training (compare Figs. 5(a) and 5(b) from ). This again showed that the relation between compression of mutual information and generalization is not a simple one, warranting further study. More generally, there seems to be a mismatch between the IB paradigm and common DL practice that mainly employs deterministic NNs, under which information measures of interest tend to degenerate. It remains unclear how to bridge this gap.
VI. SUMMARY AND CONCLUDING REMARKS
This tutorial surveyed the IB problem, from its information-theoretic origins to the recent impact it had on ML research. After setting up the IB problem, we presented its relations to MSSs, discussed operational interpretations, 32 and covered the Gaussian IB setup. Together, these components provide background and context for the recent framing of IB as an objective/model for DNN-based classification. After describing successful applications of the IB framework as an objective for learning classifiers, and generative models, we focused on the IB theory for DL, and the active research area it inspired. The theory is rooted in the idea that DNN classifiers inherently aim to learn representations that are optimal in the IB sense. This novel perspective was combined in with an empirical case-study to make claims about phase transitions in optimization dynamics, computational benefits of deep architectures, and relations between generalization and compressed representations. Backed by some striking empirical observations (though only for a synthetic classification task), the narrative from ignited a series of followup works aiming to test its generality.
We focused here on works that contributed to different aspect of modern IB research. Our starting point is, that revisited the observations from with a thorough empirical analysis. The experiments from were designed to test central aspects of the IB theory for DL; their final conclusion was that the empirical findings from do not hold in general. We then examined theoretical facets applying the (inherently stochastic) framework of IB to deterministic DL systems. Covering contributions from and, caveats in measuring information flows in deterministic DNNs were explained. In a nutshell, key information measures degenerate over deterministic networks, becoming either constant (independent of the network's parameters) or infinite, depending on the modeling of the input feature. Either way, such quantities are vacuous over deterministic networks.
We then described the remedy proposed in to the 'vacuous information measures' issue. Specifically, that work presented an auxiliary stochastic DNN framework over which the considered mutual information terms are meaningful and parameter-dependent. Using this auxiliary model, they demonstrated that compression of I(X; T) over the course of training, is driven by clustering of equilabeled samples in the representation space of T. It was then shown that a similar clustering process occurs during training of deterministic DNN classifiers. Circling back to the original observations of compression (see also ), the authors of demonstrated that the measurement techniques employed therein in fact track clustering of samples. This clarified the geometric phenomena underlying the compression of mutual information during training. Still, many aspects of the IB theory for DL remain puzzling, awaiting further exploration.
REFERENCES
 T. Hastie, R. Tibshirani, and J. Friedman, The elements of statistical learning: data mining, inference, and prediction.
Springer Science
& Business Media, 2009.
 V. N. Vapnik and A. Y. Chervonenkis, "On the uniform convergence of relative frequencies of events to their probabilities," Rep. Academy
Sci. USSR, no. 4, p. 181, 1968.
 M. L. Minsky and S. A. Papert, "Perceptrons," MIT press, p. 248, 1969.
 Y. LeCun, Y. Bengio, and G. Hinton, "Deep learning," Nature, vol. 521, no. 7553, pp. 436â€“444, May 2015.
 N. Tishby and N. Zaslavsky, "Deep learning and the information bottleneck principle," in Proceedings of the Information Theory Workshop(ITW), Jerusalem, Israel, Apr.-May 2015, pp. 1â€“5.
 R. Shwartz-Ziv and N. Tishby, "Opening the black box of deep neural networks via information," 2017, arXiv:1703.00810 [cs.LG].
 A. Achille and S. Soatto, "On the emergence of invariance and disentangling in deep representations," Journal of Machine Learning
Research, vol. 19, pp. 1â€“34, 2018.
 â€”â€”, "Information dropout: Learning optimal representations through noisy computation," IEEE transactions on pattern analysis and machine intelligence, vol. 40, no. 12, pp. 2897â€“2905, Jan. 2018.
 A. M. Saxe, Y. Bansal, J. Dapello, M. Advani, A. Kolchinsky, B. D. Tracey, and D. D. Cox, "On the information bottleneck theory of deep learning," in Proceedings of the International Conference on Learning Representations (ICLR), 2018.
 M. GabriÂ´e, A. Manoel, C. Luneau, J. Barbier, N. Macris, F. Krzakala, and L. ZdeborovÂ´a, "Entropy and mutual information in models of deep neural networks," arXiv preprint arXiv:1805.09785, 2018.
 S. Yu, R. Jenssen, and J. C. Principe, "Understanding convolutional neural network training with information theory," arXiv preprint arXiv:1804.06537, 2018.
 H. Cheng, D. Lian, S. Gao, and Y. Geng, "Evaluating capability of deep neural networks for image classification via information plane," in European Conference on Computer Vision (ECCV-2018), Munich, Germany, September 2018, pp. 168â€“182.
 Z. Goldfeld, E. van den Berg, K. Greenewald, I. Melnyk, N. Nguyen, B. Kingsbury, and Y. Polyanskiy, "Estimating information flow in neural networks," in Proceedings of the International Conference on Machine Learning (ICML-2019), vol. Long Beach, California, USA, Jun. 2019.
 K. WickstrÃ¸m, S. LÃ¸kse, M. Kampffmeyer, S. Yu, J. C. Principe, and R. Jenssen, "Information plane analysis of deep neural networks via matrix-based renyi's entropy and tensor kernels," arXiv preprint arXiv:1909.11396, 2019.
 R. A. Amjad and B. C. Geiger, "Learning representations for neural network-based classification using the information bottleneck principle,"
IEEE Transactions on Pattern Analysis and Machine Intelligence, Apr. 2019.
 N. Tishby, F. C. Pereira, and W. Bialek, "The information bottleneck method," in Proceedings of the Allerton Conference on Communication, Control and Computing, Monticello, Illinois, US, Sep. 1999, pp. 368â€“377.
 Z. Goldfeld, K. Greenewald, Y. Polyanskiy, and J. Weed, "Convergence of smoothed empirical measures with applications to entropy estimation," arXiv preprint arXiv:1905.13576, 2019.
 E. L. Lehmann and H. H. ScheffÂ´e, "Completeness, similar regions, and unbiased estimation: Part i," SankhyÂ¯a: The Indian Journal of Statistics, vol. 10, no. 4, pp. 305â€“340, Nov. 1950.
 S. Kullback and R. A. Leibler, "On information and sufficiency," The annals of mathematical statistics, vol. 22, no. 1, pp. 79â€“86, Mar.
 H. Witsenhausen and A. D. Wyner, "A conditional entropy bound for a pair of discrete random variables," IEEE Trans. Inf. Theory, vol. 21, no. 5, pp. 493â€“501, Sep. 1975.
 P. GÂ´acs and J. KÂ¨orner, "Common information is far less than mutual information," Prob. Contr. Inf. Theory, vol. 2, no. 2, pp. 149â€“162, R. E. Blahut, "Computation of channel capacity and rate-distortion functions," IEEE Trans. Inf. Theory, vol. 18, pp. 460â€“473, Jul. 1972.
 S. Arimoto, "An algorithm for computing the capacity of arbitrary discrete memoryless channels," IEEE Trans. Inf. Theory, vol. 18, pp.
14â€“20, Jan. 1972.
 I. Estella-Aguerri and A. Zaidi, "Distributed variational representation learning," IEEE Transactions on Pattern Analysis and Machine
Intelligence, July 2019.
 G. Chechik, A. Globerson, N. Tishby, and Y. Weiss, "Information bottleneck for Gaussian variables," Journal of Machine Learning
Research, vol. 6, no. Jan., pp. 165â€“188, 2005.
 I. Estella-Aguerri and A. Zaidi, "Distributed information bottleneck method for discrete and gaussian sources," in Proceedings of the International Zurich Seminar on Information and Communication (IZS-2018), Zurich, Switzerland, February 2018, pp. 35â€“39.
 R. Dobrushin and B. Tsybakov, "Information transmission with additional noise," IRE Trans. Inf. Theory, vol. 8, no. 5, pp. 293â€“304, Sep.
 J. Wolf and J. Ziv, "Transmission of noisy information to a noisy receiver with minimum distortion," IEEE Trans. Inf. Theory, vol. 16, no. 4, pp. 406â€“411, Jul. 1970.
 A. Zaidi, I. Estella-Aguerri, and S. Shamai, "On the information bottleneck problems: Models, connections, applications and information theoretic views," Entropy, vol. 22, no. 2, p. 151, February 2020.
 N. Slonim and N. Tishby, "Agglomerative information bottleneck," in Proceedings of Advances in Neural Information Processing Systems(NeurIPS-2000), 2000, pp. 617â€“623.
 A. A. Alemi, I. Fischer, J. V. Dillon, and K. Murphy, "Deep variational information bottleneck," in Proceedings of the International
Conference on Learning Representations (ICLR-2017), Toulon, France, Apr. 2017.
 I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick, S. Mohamed, and A. Lerchner, "Î²-VAE: learning basic visual concepts with a constrained variational framework," in Proceedings of the International Conference on Learning Representations (ICLR-2019), New
Orleans, Louisiana, USA, May 2017.
 Z. Goldfeld, K. Greenewald, Y. Polyanskiy, and Y. Wu, "Differential entropy estimation under gaussian noise," in IEEE International
Conference on the Science of Electrical Engineering (ICSEE-2018), Eilat, Israel, December 2018.
 Z. Goldfeld, K. Greenewald, J. Weed, and Y. Polyanskiy, "Optimality of the plug-in estimator for differential entropy estimation under
Gaussian convolutions," in IEEE International Symposium on Information Theory (ISIT-2019), Paris, France, July 2019.
 D. J. Strouse and D. J. Schwab, "The deterministic information bottleneck," Neural computation, vol. 29, no. 6, pp. 1611â€“1630, Jun. 2017.
 M. Cvitkovic and G. Koliander, "Minimal achievable sufficient statistic learning," in Proceedings of the International Conference on
Machine Learning (ICML-2019), vol. Long Beach, California, USA, Jun. 2019, pp. 1465â€“1474.
 T. M. Cover and J. A. Thomas, Elements of Information Theory, 2nd ed.
New-York: Wiley, 2006.
 B. O. Koopman, "On distributions admitting a sufficient statistic," Transactions of the American Mathematical society, vol. 39, no. 3, pp.
399â€“409, May 1936.
 T. Berger and R. Zamir, "A semi-continuous version of the Berger-Yeung problem," IEEE Trans. Inf. Theory, vol. 45, no. 5, pp. 1520â€“1526, Jul. 1999.
 H. Hotelling, "The most predictable criterion," Journal of Educational Psychology, vol. 26, no. 2, p. 139, Feb. 1935.
 T. Berger, Rate-distortion theory: A mathematical basis for data compression, ser. Information and System Sciences Series.
Englewood
Cliffs, NJ,USA: Prentice-Hall, 1971.
 Y. Polyanskiy and Y. Wu, "Lecture notes on information theory," Lecture Notes 6.441, Department of Electrical Engineering and Computer
Science, MIT, Cambridge, MA, USA, 2012â€“2017.
 T. Berger, "Multiterminal source coding," in Information Theory Approach to Communications, G. Longo, Ed., vol. 229.
CISM Cource and Lecture, 1978, pp. 171â€“231.
 S. Y. Tung, "Multiterminal source coding," Ph.D. dissertation, Cornell University, Ithaca, NY, USA, May 1978.
 D. Slepian and J. Wolf, "Noiseless coding of correlated information sources," IEEE Trans. Inf. Theory, vol. 19, no. 4, pp. 471â€“480, Jul.
 T. A. Courtade and T. Weissman, "Multiterminal source coding under logarithmic loss," IEEE Trans. Inf. Theory, vol. 60, no. 1, pp.
740â€“761, Nov. 2013.
 D. P. Kingma and M. Welling, "Auto-encoding variational bayes," in Proceedings of the International Conference on Learning
Representations (ICLR-2014), Banff, Canada, Apr. 2014.
 N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, "Dropout: A simple way to prevent neural networks from overfitting," Journal of Machine Learning Research, vol. 15, pp. 1929â€“1958, 2014.
 G. Pereyra, G. Tucker, J. Chorowski, Å. Kaiser, and G. Hinton, "Regularizing neural networks by penalizing confident output distributions," in Proceedings of the International Conference on Learning Representations (ICLR-2017), Toulon, France, Apr. 2017.
 I. J. Goodfellow, J. Shlens, and C. Szegedy, "Explaining and harnessing adversarial examples," in Proceedings of the International
Conference on Learning Representations (ICLR-2015), San Diego, California, USA, May 2015.
 N. Carlini and D. Wagner, "Towards evaluating the robustness of neural networks," in Proceedings of the IEEE Symposium on Security and Privacy (SP-2017), San Jose, California, USA, May 2017, pp. 39â€“57.
 V. Mnih, N. Heess, and A. Graves, "Recurrent models of visual attention," in Proceedings of Advances in Neural Information Processing
Systems (NeurIPS-2014), MontrÂ´eal, Canada, Dec. 2014, pp. 2924â€“2932.
 I. CsiszÂ´ar, "On the dimension and entropy of order Î± of the mixture of probability distributions," Acta Mathematica Hungarica, vol. 13, no. 3-4, pp. 245â€“255, Sep. 1962.
 A. El Gamal and Y.-H. Kim, Network Information Theory.
Cambridge University Press, 2011.
 N. Murata, "A statistical study of on-line learning," Online Learning and Neural Networks. Cambridge University Press, Cambridge, UK, pp. 63â€“92, 1998.
 J. Chee and P. Toulis, "Convergence diagnostics for stochastic gradient descent with constant learning rate," in Proceedings of the International Conference of Artificial Intelligence and Statistics (AISTATS-2018), Lanzarote, Canary Islands, April 2018, pp. 1476â€“1485.
 G. F. Montufar, R. Pascanu, K. Cho, and Y. Bengio, "On the number of linear regions of deep neural networks," in Proceedings of Advances in Neural Information Processing Systems (NeurIPS-2014), MontrÂ´eal, Canada, Dec. 2014, pp. 2924â€“2932.
 R. Eldan and O. Shamir, "The power of depth for feedforward neural networks," in Conference on Learning Theory (COLT-2016), New
York City, New York, USA, Jun. 2016, pp. 907â€“940.
 M. Telgarsky, "Benefits of depth in neural networks," in Conference on Learning Theory (COLT-2016), New York City, New York, USA, Jun. 2016, pp. 1517â€“1539.
 T. Poggio, H. Mhaskar, L. Rosasco, B. Miranda, and Q. Liao, "Why and when can deep-but not shallow-networks avoid the curse of dimensionality: a review," International Journal of Automation and Computing, vol. 14, no. 5, pp. 503â€“519, Oct. 2017.
 A. Xu and M. Raginsky, "Information-theoretic analysis of generalization capability of learning algorithms," in Proceedings of Advances in Neural Information Processing Systems (NeurIPS-2017), Long Beach, California, USA, Dec. 2017, pp. 2524â€“2533.
 H. S. A. Kraskov and P. Grassberger, "Estimating mutual information," Phys. rev. E, vol. 69, no. 6, p. 066138, June 2004.
 A. Kolchinsky and B. D. Tracey, "Estimating mixture entropy with pairwise distances," Entropy, vol. 19, no. 7, p. 361, Jul. 2017.
 P. Baldi and K. Hornik, "Neural networks and principal component analysis: Learning from examples without local minima," Neural networks, vol. 2, no. 1, pp. 53â€“58, Jan. 1989.
 H. S. Seung, H. Sompolinsky, and N. Tishby, "Statistical mechanics of learning from examples," Physical review A, vol. 45, no. 8, p.
6056, Apr. 1992.
 M. S. Advani and A. M. Saxe, "High-dimensional dynamics of generalization error in neural networks," arXiv preprint arXiv:1710.03667, A. Achille and S. Soatto, "Where is the information in a deep neural network?" arXiv preprint arXiv:1905.12213, May 2019.
 L. Paninski, "Estimating entropy on m bins given fewer than m samples," IEEE Trans. Inf. Theory, vol. 50, no. 9, pp. 2200â€“2203, Sep.
 G. Valiant and P. Valiant, "Estimating the unseen: improved estimators for entropy and other properties," in Advances in Neural Information
Processing Systems, 2013, pp. 2157â€“2165.
 Y. Han, J. Jiao, and T. Weissman, "Adaptive estimation of Shannon entropy," in IEEE International Symposium on Information Theory(ISIT-2016), Hong Kong, China, Jun. 2015, pp. 1372â€“1376.
 Y. Han, J. Jiao, T. Weissman, and Y. Wu, "Optimal rates of entropy estimation over Lipschitz balls," arXiv preprint arXiv:1711.02141, Nov. 2017.
 M. Noshad, Y. Zeng, and A. O. Hero, "Scalable mutual information estimation using dependence graphs," in Proceedings of the IEEE
International Conference on Acoustics, Speech and Signal Processing (ICASSP-2019), Brighton, UK, May 2019, pp. 2962â€“2966.
 M. I. Belghazi, A. Baratin, S. Rajeshwar, S. Ozair, Y. Bengio, D. Hjelm, and A. Courville, "Mutual information neural estimation," in Proceedings of the International Conference on Machine Learning (ICML-2018), Stockholm, Sweden, Jul. 2018, pp. 530â€“539.
 C. Chung, A. Al-Bashabsheh, H. P. Huang, M. Lim, D. S. H. Tam, and C. Zhao, "Neural entropic estimation: A faster path to mutual information estimation," arXiv preprint arXiv:1905.12957, 2019.
 L. Paninski, "Estimation of entropy and mutual information," Neural Computation, vol. 15, pp. 1191â€“1253, Jun. 2003.
 Z. Goldfeld and K. Kato, "Limit distribution for smooth total variation and Ï‡2-divergence in high dimensions," arXiv preprint arXiv:2002.01013, February 2020.