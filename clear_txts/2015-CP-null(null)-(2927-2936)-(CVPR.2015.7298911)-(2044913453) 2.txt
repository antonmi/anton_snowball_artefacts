Evaluation of Output Embeddings for Fine-Grained Image Classification
Zeynep Akata∗, Scott Reed†, Daniel Walter†, Honglak Lee† and Bernt Schiele∗
∗ Computer Vision and Multimodal Computing
† Computer Science and Engineering Division
Max Planck Institute for Informatics, Saarbrucken, Germany
University of Michigan, Ann Arbor
Abstract
Image classification has advanced significantly in recent years with the availability of large-scale image sets.
However, fine-grained classification remains a major challenge due to the annotation cost of large numbers of finegrained categories.
This project shows that compelling classification performance can be achieved on such categories even without labeled training data.
Given image and class embeddings, we learn a compatibility function such that matching embeddings are assigned a higher score than mismatching ones; zero-shot classification of an image proceeds by finding the label yielding the highest joint compatibility score. We use state-of-the-art image features and focus on different supervised attributes and unsupervised output embeddings either derived from hierarchies or learned from unlabeled text corpora. We establish a substantially improved state-of-the-art on the Animals with Attributes and Caltech-UCSD Birds datasets. Most encouragingly, we demonstrate that purely unsupervised output embeddings (learned from Wikipedia and improved with finegrained text) achieve compelling results, even outperforming the previous supervised state-of-the-art. By combining different output embeddings, we further improve results.
1. Introduction
The image classification problem has been redefined by the emergence of large scale datasets such as ImageNet.
Since deep learning methods dominated recent LargeScale Visual Recognition Challenges (ILSVRC12-14), the attention of the computer vision community has been drawn to Convolutional Neural Networks (CNN). Training
CNNs requires massive amounts of labeled data; but, in fine-grained image collections, where the categories are visually very similar, the data population decreases significantly. We are interested in the most extreme case of learning with a limited amount of labeled data, zero-shot learning, in which no labeled data is available for some classes.
Without labels, we need alternative sources of information that relate object classes. Attributes, which
Figure 1. Structured Joint Embedding leverages images (xi) and labels (yi) by learning parameters W of a function F(xi, yi, W) that measures the compatibility between input (θ(xi)) and output embeddings (ϕ(yi)). It is a general framework that can be applied to any learning problem with more than one modality. describe well-known common characteristics of objects, are an appealing source of information, and they can be easily obtained through crowd-sourcing techniques. However, fine-grained concepts present a special challenge: due to the high degree of similarity among categories, a large number of attributes are required to effectively model these subtle differences. This increases the cost of attribute annotation. One aim of this work is to move towards eliminating the human labeling component from zero-shot learning, e.g. by using alternative sources of information.
On the other hand, large-margin support vector machines(SVM) operate with labeled training images, so a lack of labels limits their use for this task. Inspired by previous work on label embedding and structured SVMs, we propose to use a Structured Joint Embedding (SJE) framework (Fig. 1) that relates input embeddings (i.e. image features) and output embeddings (i.e. side information) through a compatibility function, therefore taking advantage of a structure in the output space. The SJE framework separates the subspace learning problem from the specific input and output features used in a given application. As a general framework, it can be applied to any learning problem where more than one modality is provided for an object.
Our contributions are: (1) We demonstrate that unsuper1 vised class embeddings trained from large unlabeled text corpora are competitive to previously published results that use human supervision.(2) Using the most recent deep architectures as input embeddings, we significantly improve the state-of-the-art (SoA). (3) We extensively evaluate several unsupervised output embeddings for fine-grained classification in a zero-shot setting on three challenging datasets. (4) By combining different output embeddings we obtain best results, surpassing the SoA by a large margin.(5) We propose a novel weakly-supervised Word2Vec variant that improves the accuracy when combined with other output embeddings.
The rest of the paper is organized as follows. Section 2 provides a review of the relevant literature; Sec. 3 details the SJE method; Sec. 4 explains the output embeddings that we analyze; Sec. 5 presents our experimental evaluation; Sec. 6 presents the discussion and our conclusions.
2. Related Work
Learning to classify in the absence of labeled data (zeroshot learning) is a challenging problem, and achieving better-than-chance performance requires structure in the output space. Attributes provide one such space; they relate different classes through well-known and shared characteristics of objects.
Attributes, which are often collected manually, have shown promising results in various applications, i.e. caption generation, face recognition, image retrieval, action recognition and image classification. The main challenge of attribute-based zero-shot learning arises on more challenging fine-grained data collections, in which categories may visually differ only subtly. Therefore, generic attributes fail at modeling small intra-class variance between objects. Improved performance requires a large number of specific attributes which increases the cost of data gathering.
As an alternative to manual annotation, side information can be collected automatically from text corpora. Bag-ofwords is an example where class embeddings correspond to histograms of vocabulary words extracted automatically from unlabeled text. Another example is using taxonomical order of classes as structured output embeddings. Such a taxonomy can be built automatically from a pre-defined ontology such as WordNet. In this case, the distance between nodes is measured using semantic similarity metrics. Finally, distributed text representations learned from large unsupervised text corpora can be employed as structured embeddings. We compare several representatives of these methods(and their combinations) in our evaluation.
Embedding labels in an Euclidean space is an effective tool to model latent relationships between classes.
These relationships can be collected separately from the data, learned from the data or derived from side information.
In order to collect relationships independently of data, compressed sensing uses random projections whereas Error Correcting
Output Codes builds embeddings inspired from information theory. WSABIE uses images with their corresponding labels to learn an embedding of the labels, and CCA maximizes the correlation between two different data modalities. DeViSE employs a ranking formulation for zero-shot learning using images and distributed text representations. The ALE method employs an approximate ranking formulation for the same using images and attributes. ConSe uses the probabilities of a softmaxoutput layer to weigh the semantic vectors of all the classes.
In this work, we use the multiclass objective to learn structured output embeddings obtained from various sources.
Among the closest related work, ALE uses Fisher
Vectors (FV ) as input and binary attributes / hierarchies as output embeddings. Similarly, DeviSe uses
CNN features as input and Word2Vec representations as output embeddings.
In this work, we benefit from both ideas: (1) We use SoA image features, i.e. FV and CNN, (2) among others, we also use attributes and Word2Vec as output embeddings. Our work differs from
 w.r.t. two aspects: (1) We propose and evaluate several output embedding methods specifically built for finegrained classification. (2) We show how some of these output embeddings complement each other for zero-shot learning on general and fine-grained datasets. The reader should be aware of.
3. Structured Joint Embeddings
In this work, we aim to leverage input and output embeddings in a joint framework by learning a compatibility between these embeddings. We are interested in the problem of zero-shot learning for image classification where training and test images belong to two disjoint sets of classes.
Following, given input/output xn ∈ X and yn ∈ Y from S = {(xn, yn), n = 1... N}, Structured Joint Embedding (SJE) learns f : X → Y by minimizing the empirical risk 1
N
�N n=1 ∆(yn, f(xn)) where ∆ : Y × Y → R defines the cost of predicting f(x) when the true label is y.
Here, we use the 0/1 loss.
3.1. Model
We define a compatibility function F : X × Y → R between an input space X and a structured output space Y.
Given a specific input embedding, we derive a prediction by maximizing the compatibility F over SJE as follows: f(x; w) = arg max y∈Y F(x, y; w).
The parameter vector w can be written as a D×E matrix W with D being the input embedding dimension and E being the output embedding dimension. This leads to the bi-linear form of the compatibility function:
F(x, y; W) = θ(x)⊤Wϕ(y).
Here, the input embedding is denoted by θ(x) and the output embedding by ϕ(y). The matrix W is learned by enforcing the correct label to be ranked higher than any of the other labels (Sec. 3.2), i.e. multiclass objective. This formulation is closely related to. Within the label embedding framework, ALE and DeViSe use pairwise ranking objective, WSABIE learns both ϕ(y) and W through ranking, whereas we use multiclass objective. Similarly, use the regression objective and CCA maximizes the correlation of input and output embeddings.
3.2. Parameter Learning
According to the unregularized structured SVM formulation, the objective is:
N
N
� n=1 max y∈Y {0, ℓ(xn, yn, y)}.(2) where the misclassification loss ℓ(xn, yn, y) takes the form:
∆(yn, y) + θ(xn)⊤Wϕ(y) − θ(xn)⊤Wϕ(yn)
For the zero-shot learning scenario, the training and test classes are disjoint. Therefore, we fix ϕ to the output embeddings of training classes and learn W. For prediction, we project a test image onto the W and search for the nearest output embedding vector (using the dot product similarity) that corresponds to one of the test classes.
We use Stochastic Gradient Descent (SGD) for optimization which consists in sampling (xn, yn) at each step and searching for the highest ranked class y.
If arg maxy∈Y ℓ(xn, yn, y) ̸= yn, we update W as follows:
W (t) = W (t−1) + ηtθ(xn)[ϕ(yn) − ϕ(y)]⊤(4) where ηt is the learning step-size used at iteration t. We use a constant step size chosen by cross-validation and we perform regularization through early stopping.
3.3. Learning Combined Embeddings
For some classification tasks, there may be multiple output embeddings available, each capturing a different aspect of the structure of the output space. Each may also have a different signal-to-noise ratio. Since each output embedding possibly offers non-redundant information about the output space, as also shown in, we can learn a better joint embedding by combining them together. We model the resulting compatibility score as
F(x, y; {W}1..K) =
� k αkθ(x)⊤Wkϕk(y)(5) s.t.
� k αk = 1 where W1,..., WK are the joint embedding weight matrices corresponding to the K output embeddings (ϕk). In our experiments, we first train each Wk independently, then perform a grid search over αk on a validation set. Interestingly, we found that the optimal αk for previously-seen classes is often different from the one for unseen classes. Therefore, it is critical to cross-validate αk on the zero-shot setting.
Note that if we take αk = 1/K, ∀k, Equation 5 is equivalent to simply concatenating the ϕk. This corresponds to stacking the Wk into a single matrix W and computing the standard compatibility as in Equation 1. However, such a stacking learns a large W where a high dimensional ϕ biases the final prediction. In contrast, α eliminates the bias, leading to better predictions. Thus, αk can be thought of as the confidence associated with ϕk whose contribution we can control. We show in Sec. 5.2 that finding an appropriate αk can yield improved accuracy compared to any single ϕ.
4. Output Embeddings
In this section, we describe three types of output embeddings: human-annotated attributes, unsupervised word embeddings learned from large text corpora, and hierarchical embeddings derived from WordNet.
4.1. Embedding by Human Annotation: Attributes
Annotating images with class labels is a laborious process when the objects represent fine-grained concepts that are not common in our daily lives. Attributes provide a means to describe such fine-grained concepts. They model shared characteristics of objects such as color and texture which are easily annotated by humans and converted to machine-readable vector format. The set of descriptive attributes may be determined by language experts or by fine-grained object experts. The association between an attribute and a category can be a binary value depicting the presence/absence of an attribute (ϕ0,1 ) or a continuous value that defines the confidence level of an attribute (ϕA ) for each class. We write per-class attributes as: ϕ(y) = [ρy,1,..., ρy,E]⊤ where ρy,i can be {0, 1} or a real number that associates a class with an attribute, y denotes the associated class and E is the number of attributes.
Potentially, ϕA encodes more information than ϕ0,1. For instance, for classes rat, monkey, whale and the attribute big, ϕ0,1 = implies that in terms of size rat = monkey < whale, whereas ϕA = can be interpreted as rat < monkey << whale which is more accurate. We empirically show the benefit of ϕA over ϕ0,1 in Sec. 5.2. In practice, our output embeddings use a per-class vector form, but they can vary in dimensionality (E). For the rest of the section we denote the output embeddings as ϕ for brevity.
4.2. Learning Label Embeddings from Text
In this section, we describe unsupervised and weaklysupervised label embeddings mined from text. With these label embeddings, we can (1) avoid dependence on costly manual annotation of attributes and (2) combine the embeddings with attributes, where available, to achieve better performance.
Word2Vec (ϕW). In Word2Vec, a two-layer neural network is trained to predict a set of target words from a set of context words. Words in the vocabulary are assigned with one-shot encoding so that the first layer acts as a lookup table to retrieve the embedding for any word in the vocabulary. The second layer predicts the target word(s) via hierarchical soft-max. Word2Vec has two main formulations for the target prediction: skip-gram (SG) and continuous bag-of-words (CBOW). In SG, words within a local context window are predicted from the centering word. In
CBOW, the center word of a context window is predicted from the surrounding words. Embeddings are obtained by back-propagating the prediction error gradient over a training set of context windows sampled from the text corpus.
GloVe (ϕG). GloVe incorporates co-occurrence statistics of words that frequently appear together within the document. Intuitively, the co-occurrence statistics encode meaning since semantically similar words such as "ice" and "water" occur together more frequently than semantically dissimilar words such as "ice" and "fashion." The training objective is to learn word vectors such that their dot product equals the co-occurrence probability of these two words. This approach has recently been shown to outperform Word2Vec on the word analogy prediction task.
Weakly-supervised Word2Vec (ϕWws).
The standard
Word2Vec scans the entire document using each word within a sample window as the target for prediction. However, if we know the global context, i.e. the topic of the document, we can use that topic as our target. For instance, in Wikipedia, the entire article is related to the same topic.
Therefore, we can sample our context windows from any location within the article rather than searching for context windows where the topic explicitly appears in the text. We consider this method as a weak form of supervision.
We achieve the best results in our experiments using our novel variant of the CBOW formulation.
Here, we pretrain the first layer weights using standard Word2Vec on
Wikipedia, and fine-tune the second layer weights using a ρjcn = 2 ∗ IC(mscs(u, v)) − (IC(u) + IC(v)) ρlin = 2 ∗ IC(mscs(u, v))
IC(u) + IC(v) ρpath = min p∈pth(u,v)len(p)
Table 1. Notations : mscs (most specific common subsumer), pth (set of paths between two nodes), len (path length), IC (Information Content, defined as the log of the probability of finding a word in a text corpus independent of the hierarchy). negative-sampling objective only on the fine-grained text corpus. These weights correspond to the final output embedding. The negative sampling objective is formulated as follows:
L =
� w,c∈D+ log σ(vT c vw) +
� w′,c∈D− log σ(−vT c vw′) (6) vc =
� i∈context(w) vi/|context(w)| where vw and vw′ are the label embeddings we seek to learn, and vc is the average of word embeddings vi within a context window around word w. D+ consists of context vc and matching targets vw, and D− consists of the same vc and mismatching vw′. To find the vi (which are the columns of the first-layer network weights), we take them from a standard unsupervised Word2Vec model trained on Wikipedia.
During SGD, the vi are fixed and we update each sampled vw and vw′ at each iteration. Intuitively, we seek to maximize the similarity between context and target vectors for matching pairs, and minimize it for mismatching pairs.
Bag-of-Words (ϕB).
BoW builds a "bag" of word frequencies by counting the occurrence of each vocabulary word that appears within a document. It does not preserve the order in which words appear in a document, so it disregards the grammar. We collect Wikipedia articles that correspond to each object class and build a vocabulary of most frequently occurring words. We then build histograms of these words to vectorize our classes.
4.3. Hierarchical Embeddings
Semantic similarity measures how closely related two word senses are according to their meaning. Such a similarity can be estimated by measuring the distance between terms in an ontology. WordNet1, a large-scale hierarchical database of over 100,000 words for English, provides us a means of building our class hierarchy. To measure similarity, we use Jiang-Conrath (ϕjcn), Lin (ϕlin) and path (ϕpath) similarities formulated in Table 1. We denote our whole family of hierarchical embeddings as ϕH. For a more detailed survey, the reader may refer to.
1http://wordnetweb.princeton.edu/
5. Experiments
While our main contribution is a detailed analysis of output embeddings, good image representations are crucial to obtain good classification performance. In Sec. 5.1 we detail datasets, input and output embeddings used in our experiments and in Sec. 5.2 we present our results.
5.1. Experimental Setting
We evaluate SJE on three datasets:
Caltech UCSD
Birds (CUB) and Stanford Dogs (Dogs)2 are finegrained, and Animals With Attributes (AWA) is a standard attribute dataset for zero-shot classification. CUB contains 11,788 images of 200 bird species, Dogs contains
19,501 images of 113 dog breeds and AWA contains 30,475 images of 50 different animals. We use a truly zero-shot setting where the train, val, and test sets belong to mutually exclusive classes. We employ train and val, i.e. disjoint subsets of training set, for cross-validation. We report average per-class top-1 accuracy on the test set. For CUB, we use the same zero-shot split as with 150 classes for the train+val set and 50 disjoint classes for the test set. AWA has a predefined split for 40 train+val and 10 test classes.
For Dogs, we use approximately the same ratio of classes for train+val/test as CUB, i.e. 85 classes for train+val and 28 classes for test. This is the first attempt to perform zeroshot learning on the Dogs dataset.
Input Embeddings. We use Fisher Vectors (FV) and Deep
CNN Features (CNN). FV aggregates per image statistics computed from local image patches into a fixed-length local image descriptor. We extract 128-dim SIFT from regular grids at multiple scales, reduce them to 64-dim using
PCA, build a visual vocabulary with 256 Gaussians and finally reduce the FVs to 4,096. As an alternative, we extract features from a deep convolutional network. Features that are typically obtained from the activations of the fully connected layers have been shown to induce semantic similarities. We resize each image to 224×224 and feed into the network which was pre-trained following the model architecture of either AlexNet or GoogLeNet. For
AlexNet (denoted as CNN) we use the 4,096-dim top-layer hidden unit activations (fc7) as features, and for GoogLeNet(denoted as GOOG) we use the 1,024-dim top-layer pooling units. For both networks, we used the publicly-available
BVLC implementations. We do not perform any taskspecific pre-processing, such as cropping foreground objects or detecting parts.
Output Embeddings. AWA classes have 85 binary and continuous attributes. CUB classes have 312 continuous attributes and the continuous values are thresholded around the mean to obtain binary attributes. The Dogs dataset does
2We use 113 classes that appear in the Federation Cynologique Internationale (FCI) database of dog breeds.
AWA
CUB ϕ0,1 ϕA ϕ0,1 ϕA
Ours
FV (4K)
CNN (4K)
GOOG (1K)
SoA
ALE (64K)
Table 2. Discrete (ϕ0,1) and continuous (ϕA) attributes with SJE vs SoA. For AWA (CUB) achieves 49.4% (27.3%) by combining ϕA and binary hierarchies. not have human-annotated attributes available.
We train Word2Vec (ϕW) and GloVe (ϕG) on the English-language Wikipedia from 13.02.2014. We first pre-process it by replacing the class-names, i.e. blackfooted albatross, with alternative unique names, i.e. scientific name, phoebastrianigripes. We cross-validate the skip-window size and embedding dimensions. For our proposed weakly-supervised Word2Vec (ϕWws), we use the same embedding dimensions as the plain Word2Vec (ϕW).
For BoW, we download the Wikipedia articles that correspond to each class and build the vocabulary by omitting least- and most-frequently occurring words. We crossvalidate the vocabulary size. ϕB is a histogram of the vocabulary words as they appear in the respective document.
For hierarchical embeddings (ϕH), we use the WordNet hierarchy spanning our classes and their ancestors up to the root of the tree. We employ the widely used NLTK library3 for building the hierarchy and measuring the similarity between nodes. Therefore, each ϕH vector is populated with similarity measures of the class to all other classes.
Combination of output embeddings. We explore combinations of five types of output embeddings: supervised attributes ϕA, unsupervised Word2Vec ϕW, GloVe ϕG, BoW ϕB and WordNet-derived similarity embeddings ϕH. We either concatenate (cnc) or combine (cmb) different embeddings. In cnc, for instance in AWA, 85-dim ϕA and 400-dim ϕW would be merged to 485-dim output embeddings. In this case, if we use 1,024-dim GOOG as input embeddings, we learn a single 1,024×485-dim W. In cmb, we first learn 1,024×85-dim WA and 1,024×400-dim WW and then cross-validate the α coefficients to determine the amount each embedding contributes to the final score.
5.2. Experimental Results
In this section, we evaluate several output embeddings on the CUB, AWA and Dogs datasets.
Discrete vs Continuous Attributes. Attribute representations are defined as a vector per class, or a column of the(class × attribute) matrix. These vectors (85-dim for AWA, 312-dim for CUB) can either model the presence/absence(ϕ0,1) or the confidence level (ϕA) of each attribute. We
3http://www.nltk.org/ supervision source ϕ
AWA
CUB
Dogs unsupervised text ϕW
19.6 text ϕG
17.8 text ϕB
WordNet ϕH
24.3 supervised human ϕ0,1
37.8 human ϕATable 3. Summary of zero-shot learning results with SJE w.r.t. supervised and unsupervised output embeddings (Input embeddings:
1K-GOOG). show that continuous attributes indeed encode more semantics than binary attributes by observing a substantial improvement with ϕA over ϕ0,1 with deep features (Tab. 2).
Overall, CNN outperforms FV, while GOOG gives the best performing results; therefore in the following, we comment only on our results obtained using GOOG.
On CUB, i.e. a fine-grained dataset, ϕ0,1 obtains 37.8% accuracy, which is significantly above the SoA (26.9% ).
Moreover, ϕA achieves an impressive 50.1% accuracy; outperforming the SoA by a large margin. We observe the same trend for AWA, which is a benchmark dataset for zeroshot learning. On AWA, ϕ0,1 obtains 52.0% accuracy and ϕA improves the accuracy substantially to 66.7%, significantly outperforming the SoA (48.5% ). To summarize, we have shown that ϕA improves the performance of ϕ0,1 using deep features, which indicates that with ϕA, the SJE method learns a matrix W that better approximates the compatibility of images and side information than ϕ0,1.
Learned Embeddings from Text. As the visual similarity between objects in different classes increases, e.g. in fine-grained datasets, the cost of collecting attributes also increases. Therefore, we aim to extract class similarities automatically from unlabeled online textual resources. We evaluate three methods, Word2Vec (ϕW), GloVe (ϕG) and the historically most commonly-used method BoW (ϕB).
We build ϕW and ϕG on the entire English Wikipedia dump.
Note that the plain Word2Vec was used in ; however, rather than using Word2Vec in an averaging mechanism, we pre-process the Wikipedia as described in Sec 4.2 so that our class names are directly present in the Word2Vec vocabulary. This leads to a significant accuracy improvement.
For ϕB we use a subset of Wikipedia populated only with articles that correspond to our classes.
On CUB (Tab. 3), the best accuracy is observed with ϕW (28.4%) improving the supervised SoA (26.9%, Tab. 2). This is promising and impressive since ϕW does not use any human supervision.
On AWA (Tab. 3), the best accuracy is observed with ϕG (58.8%) followed by ϕW(51.2%), improving the supervised SoA (48.5% ) significantly. On Dogs (Tab. 3), the best accuracy is obtained with ϕB (33.0%). On the other hand, using ϕW (19.6%) and ϕG (17.8%) leads to significantly lower accuracies. Unlike ϕG ϕW ϕW (W) +
B
W
B+W
B
W
B+W ϕWws (B)
FV
CNN
GOOG
Table 4. Comparison of Word2Vec (ϕW) and GloVe (ϕG) learned from a bird specific corpus (B), Wikipedia (W) and their combination (B + W), evaluated on CUB (Input embeddings: 4K-FV, 4K-CNN and 1K-GOOG). birds, different dog breeds belong to the same species and thus they share a common scientific name. As a result, our method of cleanly pre-processing Wikipedia by replacing the occurrences of bird names with a unique scientific name was not possible for Dogs. This may lead to vectors obtained from Wikipedia for dogs that are vulnerable to variation in nomenclature. In summary, our results indicate no winner among ϕW, ϕG and ϕB. These embeddings may be task specific and complement each other. We investigate the complementarity of embeddings in the following sections.
Effect of Text Corpus. For ϕW and ϕG, we analyze the effects of three text corpora (B, W, B+W) with varying size and specificity.
We build our specialized bird corpus (B) by collecting bird-related information from various online resources, i.e. audubon.org, birdweb.org, allaboutbirds.org and BNA4. In combination, this corresponds to 50MB of bird-related text.
We use the English-language Wikipedia from 13.02.2014 as our large and general corpus (W) which is 40GB of text. Finally, we combine B and W to build a large-scale text corpus enriched with bird specific text (B+W). On W and B+W, a small window size (10 for ϕW and 20 for ϕG); on B, a large window size (35 for ϕW and 50 for ϕG) is required. We choose parameters after a grid search. Increased specificity of the text corpus implies semantic consistency throughout the text. Therefore, large context windows capture semantics well in our bird specific (B) corpus. On the other hand, W is organized alphabetically w.r.t. the document title; hence, a large sampling window can include content from another article that is adjacent to the target word alphabetically. Here, small windows capture semantics better by looking at the text locally. We report our results in Tab. 4.
Using ϕG, B+W (26.1%) gives the highest accuracy, followed by W (24.2%). One possible reason is that when the semantic similarity is modeled with cooccurrence statistics, output embeddings become more informative with the increasing corpus size, since the probability of cooccurrence of similar concepts increases.
Using ϕW, the accuracy obtained with B (22.5%) is already higher than the ϕ0,1-based SoA (22.3%), illustrating the benefit of using fine-grained text for fine-grained tasks. Another advantage of using B is that, since it is short, 4http://bna.birds.cornell.edu/bna/
AWA
CUB
DOGS
Top−1 Acc. (in %)
ϕjcn ϕlin ϕpath
Figure 2. Comparison of WordNet similarity measures: ϕjcn, ϕlin and ϕpath. We use ϕH as a general name for hierarchical output embedding. (Input embedding: 1K-GOOG). building ϕW is efficient. Moreover, building ϕW with B does not require any annotation effort. Building ϕW using W (28.4%) gives the highest accuracy, followed by W
+ B (27.5%) which improves the supervised SoA (26.9%).
We speculate that since Word2Vec is a variant of the Feedforward Neural Network Language Model (FNNLM), a deep architecture, it may learn more from negative data than positives. This was also observed for CNN features learned with a large number of unlabeled surrogate classes.
Additionally, we propose a weakly-supervised alternative to Word2Vec framework (ϕWws, Sec. 4.2). The weaksupervision comes from using the specialized B corpus to fine-tune the weights of the network and model the birdrelated information. With ϕWws alone, we obtain 21.0% accuracy. However, when it is combined with ϕW (28.4%), the accuracy improves to 29.7%. Compared to the results in Tab. 4, 29.7% is the highest accuracy obtained using unsupervised embeddings. We regard these results as a very encouraging evidence that Word2Vec representations can indeed be made more discriminative for fine-grained zeroshot learning by integrating a fine-grained text corpus directly to the output embedding learning problem.
Hierarchical Embeddings. The hierarchical organization of concepts typically embodies a fair amount of hidden information about language, such as synonymy, semantic relations, etc. Therefore, semantic relatedness defined by hierarchical distance between classes can form numerical vectors to be used as output embeddings for zero-shot learning. We build ontological relationships between our classes using the WordNet taxonomy. Due to its large size, WordNet encapsulates all of our AWA and Dog classes. For
CUB, the high level bird species, i.e. albatross, appear as synsets in WordNet, but the specific bird names, i.e. blackfooted albatross, are not always present. Therefore we take the hierarchy up to high level bird species as-is and we assume the specific bird classes are all at the bottom of the hierarchy located with the same distance to their immediate ancestors. The WordNet hierarchy contains 319 nodes for CUB (200 classes), 104 nodes for AWA (50 classes) and 163 nodes for Dogs (113 classes). We measure the distance between classes using the similarity measures from Sec 4.1.
While as shown in Fig. 2 different hierarchical similarity measures have very different behaviors on each dataset.
The best performing ϕH obtains 51.2% (Tab. 3) accuracy
AWA
CUB
Dogs ϕA ϕW ϕG ϕB ϕH cnc cmb cnc cmb cnc cmb
✓
✓
✓
✓
✓
✓
✓
✓
✓✓
✓
✓✓
✓
✓Table 5. Attribute ensemble results for all datasets. ϕH: lin for
CUB, path for AWA and Dogs. Top part shows combination results of unsupervised embeddings and bottom part integrates supervised embeddings to the rest (Input embeddings: 1K-GOOG). on AWA which reaches our ϕ0,1 (52.0%) and improves ϕB(44.9%) significantly. On CUB, ϕH obtains 20.6% (Tab. 3) which remain below our ϕ0,1 (37.8%) and approaches ϕB(22.1%). On the other hand, on Dogs ϕH obtains 24.3%(Tab. 3) which is significantly higher than the unsupervised text embeddings ϕW (19.6%) and ϕG (17.8%).
Combining Output Embeddings. In this section, we combine output embeddings obtained through human annotation (ϕA), from text (ϕW,G,B) and from hierarchies (ϕH).5
As a reference, Tab. 3 summarizes the results obtained using one output embedding at a time. Our intuition is that because the different embeddings attempt to encapsulate different information, accuracy should improve when multiple embeddings are combined. We can observe this complementarity either by simple concatenation (cnc) or systematically combining (cmb) output embeddings (Sec.3.3) also known as early/late fusion. For cnc, we perform full
SJE training and cross-validation on the concatenated output embeddings. For cmb, we learn joint embeddings Wk for each output separately (which is trivially parallelized), and find ensemble weights αk via cross-validation. In contrast to the cnc method, no additional joint training is used, although it can improve performance in practice. We observe (Tab. 5) in almost all cases cmb outperforms cnc.
We analyze the combination of unsupervised embeddings (ϕW,G,B,H).
On AWA, ϕG (58.8%, Tab. 3) combined with ϕH (51.2%, Tab. 3), we achieve 60.1% (Tab. 5) which improves the SoA (48.5%, Tab. 2) by a large margin.
On CUB, combining ϕG (24.2%, Tab. 3) with ϕH(20.6%, Tab. 3), we get 29.9% (Tab. 5) and improve the supervised-SoA (26.9%, Tab. 2).
Supporting our initial claim, unsupervised output embeddings obtained from different sources, i.e. text vs hierarchy, seem to be complementary to each other. In some cases, cmb performs worse than cnc; e.g. 28.2% versus 35.1% when using ϕB with ϕH on Dogs. In most other cases cmb performs equivalent or better.
Combining supervised (ϕA) and unsupervised
5We empirically found that the hierarchical embeddings ϕH consistently improved performance when combined or concatenated with other embeddings. Therefore, we report results using ϕH by default.
Figure 3. Highest ranked 5 images for chimpanzee, leopard and seal (AWA) using ϕA, ϕG and ϕG+A. For chimpanzee, ϕA ranks chimpanzees on trees at the top, whereas ϕG models the social nature of the animal ranking a group of chimpanzees highest, ϕG+A synthesizes both aspects. For leopard ϕA puts an emphasis on the head, ϕG seems to place the animal in the wild. In case of seal, ϕA retrieves images related to water, whereas ϕG adds more context by placing seals in the icy natural environment and ϕG+A combines both. supervision method
AWA
CUB
Dogs unsupervised
SJE (best from Tab. 5)
35.1 supervised
SJE (best from Tab. 5)
–
AHLE 
–
Table 6. Summary of best zero-shot learning results with SJE with or without supervision along with SoA. embeddings (ϕW,G,B,H) shows a similar trend. On AWA, combining ϕA (66.7%, Tab. 3) with ϕG and ϕH leads to
73.9% (Tab. 5) which significantly exceeds the SoA (48.5%, Tab. 2). On CUB, combining ϕA with ϕG and ϕH leads to 51.7% (Tab. 5), improving both the results we obtained with ϕA (50.1%, Tab. 3) and the supervised-SoA (26.9%, Tab. 2). We have shown with these experiments that output embeddings obtained through human annotation can also be complemented with unsupervised output embeddings using the SJE framework.
Qualitative Results. Fig. 3 shows top-5 highest ranked images for classes chimpanzee, leopard and seal that are selected from 10 test classes of AWA. We use GOOG as input embeddings and as output embeddings we use supervised ϕA, the best performing unsupervised embedding on
AWA (ϕG), and the combination of the two (ϕG+A). For the class chimpanzee, ϕA emphasizes that chimpanzees live on trees, which is among the list of attributes. On the other hand, ϕG models the social nature of the animal, ranking a group of chimpanzees interacting with each other at the highest.
Indeed this information can easily be retrieved from Wikipedia. ϕG+A synthesizes both aspects. Similarly, for leopard ϕA puts an emphasis on the head where we can observe several of the attributes, i.e. color, spotted, whereas ϕG seems to place the animal in the wild. ϕG+A combines both aspects. In case of class seal, ϕA retrieves images related to water and ranks whales and seals highest, whereas ϕG adds more context by placing seals in the icy natural environment and within groups. Finally, ϕG+A ranks seal-shaped animals on ice, close to water and within groups the highest. We find these qualitative results interesting as they depict how (1) unsupervised embeddings capture nameable semantics about objects and (2) different output embeddings are semantically complementary for zero-shot learning.
6. Conclusion
We evaluated the Structured Joint Embedding (SJE) framework on supervised attributes and unsupervised output embeddings obtained from hierarchies and unlabeled text corpora. We proposed a novel weakly-supervised label embedding technique. By combining multiple output embeddings (cmb), we established a new SoA on AWA(73.9%, Tab. 6) and CUB (51.7%, Tab. 6). Moreover, we showed that unsupervised zero-shot learning with SJE improves the SoA, to 60.1% on AWA and 29.9% on CUB, and obtains 35.1% on Dogs (Tab. 6).
We emphasize the following take-home points: (1) Unsupervised label embeddings learned from text corpora yield compelling zero-shot results, outperforming previous supervised SoA on AWA and CUB (Tab. 2 and 3).
Integrating specialized text corpora helps due to incorporating more fine-grained information to output embeddings(Tab. 4). (3) Combining unsupervised output embeddings improve the zero-shot performance, suggesting that they provide complementary information (Tab. 5).(4) There is still a large gap between the performance of unsupervised output embeddings and human-annotated attributes on AWA and CUB, suggesting that better methods are needed for learning discriminative output embeddings from text. (5) Finally, supporting, encoding continuous nature of attributes significantly improve upon binary attributes for zero-shot classification (Tab. 2).
As future work, we plan to investigate other methods to combine multiple output embeddings and to improve the discriminative power of unsupervised and weaklysupervised label embeddings for fine-grained classification.
Acknowledgments
This work was supported in part by ONR N00014-131-0762, NSF CMMI-1266184, Google Faculty Research
Award, and NSF Graduate Fellowship.
References
 Z. Akata, F. Perronnin, Z. Harchaoui, and C. Schmid. Label embedding for attribute-based classification. In CVPR, 2013.
 Z. Akata, F. Perronnin, Z. Harchaoui, and C. Schmid. Labelembedding for image classification. arXiv:1503.08677, S. Bengio, J. Weston, and D. Grangier. Label embedding trees for large multi-class tasks. In NIPS, 2010. 1, 2
 Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. A neural probabilistic language model. JMLR, 2003. 7
 E. Blanchard, M. Harzallah, H. Bri, P. Kuntz, and R. C. Pauc.
A typology of ontology-based semantic measures. In EMOIINTEROP'05 workshop, at CAiSE'05, 2005. 4
 H. Chen, A. Gallagher, and B. Girod. What's in a name? first names as facial attributes. In CVPR, 2013. 2
 J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. FeiFei. Imagenet: A large-scale hierarchical image database. In
CVPR, 2009. 1
 J. Deng, J. Krause, and L. Fei-Fei. Fine-grained crowdsourcing for fine-grained recognition. In CVPR, June 2013. 1
 T. G. Dietterich and G. Bakiri. Solving multiclass learning problems via error-correcting output codes. JAIR, 1995. 2
 A. Dosovitskiy, J. T. Springenberg, M. Riedmiller, and T. Brox. Discriminative unsupervised feature learning with convolutional neural networks. arXiv:1406.6909, 2014. 7
 M. Douze, A. Ramisa, and C. Schmid. Combining attributes and Fisher vectors for efficient image retrieval. In CVPR, K. Duan, D. Parikh, D. J. Crandall, and K. Grauman. Discovering localized attributes for fine-grained recognition. In
CVPR, 2012. 2
 A. Farhadi, I. Endres, and D. Hoiem. Attribute-centric recognition for cross-category generalization. In CVPR, 2010. 2
 A. Farhadi, I. Endres, D. Hoiem, and D. Forsyth. Describing objects by their attributes. CVPR, 2009. 1
 V. Ferrari and A. Zisserman. Learning visual attributes. In
NIPS, 2007. 1, 2
 A. Frome, G. S. Corrado, J. Shlens, S. Bengio, J. Dean, and T. Mikolov. Devise: A deep visual-semantic embedding model. In NIPS, 2013. 2, 3
 Y. Fu, T. M. Hospedales, T. Xiang, Z. Fu, and S. Gong.
Transductive multi-view embedding for zero-shot recognition and annotation. In ECCV, 2014. 2
 Y. Goldberg and O. Levy. word2vec explained: deriving mikolov et al.'s negative-sampling word-embedding method. arXiv:1402.3722, 2014. 4
 Z. Harris. Distributional structure. Word, 10(23), 1954. 2, 4
 T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning (2nd Ed.). Springer Series in Statistics.
Springer, 2008. 2, 3
 D. Hsu, S. Kakade, J. Langford, and T. Zhang. Multi-label prediction via compressed sensing. In NIPS, 2009. 2
 S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015. 5
 Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell.
Caffe: Convolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093, 2014. 5
 J. J. Jiang and D. W. Conrath. Semantic similarity based on corpus statistics and lexical taxonomy. CoRR, 1997. 2, 4
 P. Kankuekul, A. Kawewong, S. Tangruamsub, and O. Hasegawa. Online incremental attribute-based zero-shot learning. In CVPR, 2012. 2
 A. Khosla, N. Jayadevaprakash, B. Yao, and L. Fei-Fei. Stanford dogs dataset. http://vision.stanford.edu/ aditya86/ImageNetDogs/. 2, 5
 A. Krizhevsky, I. Sutskever, and G. E. Hinton.
Imagenet classification with deep convolutional neural networks. In
NIPS. 2012. 1, 2, 5
 G. Kulkarni, V. Premraj, S. Dhar, S. Li, Y. choi, A. Berg, and T. Berg. Baby talk: understanding and generating simple image descriptions. In CVPR, 2011. 2
 C. Lampert, H. Nickisch, and S. Harmeling. Attribute-based classification for zero-shot visual object categorization. In
TPAMI, 2013. 1, 2, 3, 5
 C. Leacock and M. Chodorow. Filling in a sparse training space for word sense identification, 1994. 2
 Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradientbased learning applied to document recognition. In Proc. of the IEEE, 1998. 1
 D. Lin. An information-theoretic definition of similarity. In
ICML, 1998. 2, 4
 J. Liu, B. Kuipers, and S. Savarese. Recognizing human actions by attributes. In CVPR, 2011. 2
 T. E. J. Mensink, E. Gavves, and C. G. M. Snoek. Costa: Cooccurrence statistics for zero-shot classification. In CVPR, T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In NIPS, 2013. 2, 4, 6
 G. A. Miller. Wordnet: a lexical database for english. CACM, 38:39–41, 1995. 2, 7
 M. Norouzi, T. Mikolov, S. Bengio, Y. Singer, J. Shlens, A. Frome, G. Corrado, and J. Dean. Zero-shot learning by convex combination of semantic embeddings. CoRR, 2013.
 S. Nowozin and C. Lampert. Structured Learning and Prediction in Computer Vision. Foundations and Trends in Computer Graphics and Vision, 2011. 1
 V. Ordonez, G. Kulkarni, and T. Berg. Im2Text: Describing images using 1 million captioned photographs. In NIPS, M. Palatucci, D. Pomerleau, G. Hinton, and T. Mitchell.
Zero-shot learning with semantic output codes.
In NIPS, D. Parikh and K. Grauman. Relative attributes. In ICCV, J. Pennington, R. Socher, and C. D. Manning. Glove: Global vectors for word representation. In EMNLP, 2014. 2, 4
 F. Perronnin and C. Dance. Fisher kernels on visual vocabularies for image categorization. In CVPR, 2007. 2, 5
 P. Resnik. Using information content to evaluate semantic similarity in a taxonomy. In IJCAI, 1995. 2
 M. Rohrbach, M. Stark, and B.Schiele. Evaluating knowledge transfer and zero-shot learning in a large-scale setting.
In CVPR, 2011. 2, 3
 M. Rohrbach, M. Stark, G. Szarvas, I. Gurevych, and B. Schiele. What helps here – and why? Semantic relatedness for knowledge transfer. In CVPR, 2010. 2
 M. Rohrbach, M. Stark, G. Szarvas, and B. Schiele. Combining language sources and robust semantic relatedness for attribute-based knowledge transfer.
Trends and Topics in Computer Vision, 2012. 3, 8
 W. J. Scheirer, N. Kumar, P. N. Belhumeur, and T. E. Boult.
Multi-attribute spaces: Calibration for attribute fusion and similarity search. In CVPR, 2012. 2
 B. Siddiquie, R. Feris, and L. Davis.
Image ranking and retrieval based on multi-attribute queries. In CVPR, 2011. 2
 R. Socher, M. Ganjoo, H. Sridhar, O. Bastani, C. Manning, and A. Ng. Zero-shot learning through cross-modal transfer.
In NIPS, 2013. 3
 C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. arXiv preprint arXiv:1409.4842, 2014. 5
 I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun.
Large margin methods for structured and interdependent output variables. JMLR, 2005. 1, 2, 3
 A. Vedaldi and B. Fulkerson. VLFeat: An open and portable library of computer vision algorithms, 2008. 5
 C. Wah, S. Branson, P. Perona, and S. Belongie. Multiclass recognition and part localization with humans in the loop. In
ICCV, 2011. 5
 P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie, and P. Perona. Caltech-UCSD Birds 200. Technical
Report CNS-TR-2010-001, Caltech, 2010. 2, 3
 J. Weston, S. Bengio, and N. Usunier. Large scale image annotation: Learning to rank with joint word-image embeddings. ECML, 2010. 1, 2, 3
 B. Yao, X. Jiang, A. Khosla, A. L. Lin, L. J. Guibas, and F.F. Li. Human action recognition by learning bases of action attributes and parts. In ICCV, 2011. 2
 X. Yu and Y. Aloimonos. Attribute-based transfer learning for object categorization with zero or one training example.
In ECCV, 2010. 2