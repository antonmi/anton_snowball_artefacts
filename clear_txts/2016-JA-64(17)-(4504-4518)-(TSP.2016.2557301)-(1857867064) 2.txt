Distributed Compressive Sensing: A Deep
Learning Approach
Hamid Palangi, Rabab Ward, Li Deng
Abstract‚ÄîVarious studies that address the compressed sensing problem with
Multiple
Measurement
Vectors(MMVs) have been recently carried. These studies assume the vectors of the different channels to be jointly sparse.
In this paper, we relax this condition. Instead we assume that these sparse vectors depend on each other but that this dependency is unknown. We capture this dependency by computing the conditional probability of each entry in each vector being non-zero, given the "residuals" of all previous vectors. To estimate these probabilities, we propose the use of the Long Short-Term Memory (LSTM), a data driven model for sequence modelling that is deep in time. To calculate the model parameters, we minimize a cross entropy cost function. To reconstruct the sparse vectors at the decoder, we propose a greedy solver that uses the above model to estimate the conditional probabilities.
By performing extensive experiments on two real world datasets, we show that the proposed method significantly outperforms the general MMV solver (the Simultaneous
Orthogonal Matching Pursuit (SOMP)) and a number of the model-based Bayesian methods. The proposed method does not add any complexity to the general compressive sensing encoder. The trained model is used just at the decoder. As the proposed method is a data driven method, it is only applicable when training data is available. In many applications however, training data is indeed available, e.g. in recorded images and videos.
Index Terms‚ÄîCompressive Sensing, Deep Learning, Long Short-Term Memory.
I. INTRODUCTION
C
OMPRESSIVE Sensing (CS),, is an effective approach for acquiring sparse signals where both sensing and compression are performed at the same time. Since there are numerous examples of natural and artificial signals that are sparse in the time, spatial or a transform domain, CS has found numerous applications. These include medical imaging, geophysical data analysis, computational biology, remote sensing and communications.
Copyright (c) 2015 IEEE. Personal use of this material is permitted.
However, permission to use this material for any other purposes must be obtained from the IEEE by sending a request to pubspermissions@ieee.org.
H. Palangi and R. Ward are with the Department of Electrical and Computer Engineering, University of British Columbia, Vancouver, BC, V6T 1Z4 Canada (e-mail: {hamidp,rababw}@ece.ubc.ca)
L. Deng is with Microsoft Research, Redmond, WA 98052 USA(e-mail: {deng}@microsoft.com)
In the general CS framework, instead of acquiring N samples of a signal x ‚àà ‚ÑúN√ó1, M random measurements are acquired where M < N. This is expressed by the underdetermined system of linear equations: y = Œ¶x(1) where y ‚àà ‚ÑúM√ó1 is the known measured vector and Œ¶ ‚àà
‚ÑúM√óN is a random measurement matrix. To uniquely recover x given y and Œ¶, x must be sparse in a given basis Œ®. This means that x = Œ®s(2) where s is K‚àísparse, i.e., s has at most K non-zero elements. The basis Œ® can be complete; i.e., Œ® ‚àà ‚ÑúN√óN, or over-complete; i.e., Œ® ‚àà ‚ÑúN√óN1 where N < N1(compressed sensing for over-complete dictionaries is introduced in ). From (1) and (2): y = As(3) where A = Œ¶Œ®. Since there is only one measurement vector, the above problem is usually called the Single
Measurement Vector (SMV) problem in compressive sensing.
In distributed compressive sensing, also known as the Multiple Measurement Vectors (MMV) problem, a set of L sparse vectors {si}i=1,2,...,L is to be jointly recovered from a set of L measurement vectors {yi}i=1,2,...,L.
Some application areas of MMV include magnetoencephalography, array processing, equalization of sparse communication channels and cognitive radio.
Suppose that the L sparse vectors and the L measurement vectors are arranged as columns of matrices
S = [s1, s2,..., sL] and Y = [y1, y2,..., yL] respectively. In the MMV problem, S is to be reconstructed given Y:
Y = AS
In (4), S is assumed to be jointly sparse, i.e., nonzero entries of each vector occur at the same locations as those of other vectors, which means that the sparse vectors have the same support. Assume that S is jointly sparse. Then, the necessary and sufficient condition to obtain a unique S given Y is :
|supp(S)| < spark(A) ‚àí 1 + rank(S)(5) arXiv:1508.04924v3 [cs.LG] 11 May 2016
2 where |supp(S)| is the number of rows in S with nonzero energy and spark of a given matrix is the smallest possible number of linearly dependent columns of that matrix. spark gives a measure of linear dependency in the system modelled by a given matrix. In the SMV problem, no rank information exists. In the MMV problem, the rank information exists and affects the uniqueness bounds. Generally, solving the MMV problem jointly can lead to better uniqueness guarantees than solving the SMV problem for each vector independently.
In the current MMV literature, a jointly sparse matrix is recovered typically by one of the following methods:
1) greedy methods like Simultaneous Orthogonal
Matching Pursuit (SOMP) which performs non-optimal subset selection, 2) relaxed mixed norm minimization methods, or 3) Bayesian methods like,, where a posterior density function for the values of S is created, assuming a prior belief, e.g., Y is observed and S should be sparse in basis Œ®. The selection of one of the above methods depends on the requirements imposed by the specific application.
A. Problem Statement
The MMV reconstruction methods stated above do not rely on the use of training data. However, for many applications, a large amount of data similar to the data to be compressed by CS is available. Examples are camera recordings of the same environment, images of the same class (e.g., flowers, buildings,....), electroencephalogram(EEG) of different parts of the brain, etc. In this paper, we address the following questions in the MMV problem when training data is available:
1) Can we learn the structure of the sparse vectors in S by a data driven bottom up approach using the already available training data? If yes, then how can we exploit this structure in the MMV problem to design a better reconstruction method?
2) Most of the reconstruction algorithms for the MMV problem rely on the joint sparsity of S.
However, in some practical applications, the sparse vectors in S are not exactly jointly sparse. This can be due to noise or due to sources that create different sparsity patterns. Examples are images of different scenes captured by different cameras, images of different classes, etc. Although S is not jointly sparse, there may exist a possible dependency among the columns of S, however, due to lack of joint sparsity, the above methods will not give satisfactory performance. The question is, can we design the aforementioned data driven method in a way that it captures the dependencies among the sparse vectors in S? The type of such dependencies may not be necessarily that of joint sparsity. And then how can we use the learned dependency structure in the reconstruction algorithm at the decoder?
Please note that we want to address the above questions "without adding any complexity or adaptability" to the encoder. In other words, our aim is not to design an optimal encoder, i.e., optimal sensing matrix Œ¶ or the sparsifying basis Œ®, for the given training data. The encoder would be as simple and general as possible.
This is specially important for applications that use sensors having low power consumption due to a limited battery life. However, the decoder in these cases can be much more complex than the encoder. For example, the decoder can be a powerful data processing machine.
B. Proposed Method
To address the above questions, we propose the use of a two step greedy reconstruction algorithm. In the first step, at each iteration of the reconstruction algorithm, and for each column of S represented as si, we first find the conditional probability of each entry of si being nonzero, given the residuals of all previous sparse vectors(columns) at that iteration. Then we select the most probable entry and add it to the support of si. The definition of the residual matrix at the j‚àíth iteration is Rj = Y ‚àí ASj where Sj is the estimate of the sparse matrix S at the j‚àíth iteration. Therefore in the first step, we find the locations of the non-zero entries. In the second step we find the values of these non-zero entries.
This can be done by solving a least squares problem that finds si given yi and AŒ©i. AŒ©i is a matrix that includes only those atoms (columns) of A that are members of the support of si.
To find the conditional probabilities at each iteration, we propose the use of a Recurrent Neural Network(RNN) with Long Short-Term Memory (LSTM) cells and a softmax layer on top of it. To find the model parameters, we minimize a cross entropy cost function between the conditional probabilities given by the model and the known probabilities in the training data. The details on how to generate the training data and the training data probabilities are explained in subsequent sections.
Please note that this training is done only once. After that, the resulting model is used in the reconstruction algorithm for any test data that has not been observed by the model before. Therefore, the proposed reconstruction algorithm would be almost as fast as the greedy methods.
The block diagram of the proposed method is presented in Fig. 1 and Fig. 2. We will explain these figures in detail in subsequent sections.
To the best of our knowledge, this is the first modelbased method in MMV sparse reconstruction that is 3 ùíì2
LSTM ùíÑ1 ùíì1
LSTM ùíìùêø
LSTM ùíÑùêø‚àí1
‚Ä¶
‚Ä¶
LSTM cells and gates ùíó1 ùíó2 ùíóùêø
ùëº
ùëº
ùëº ùíõ1 ùíõ2 ùíõùêø softmax softmax softmax
‚Ä¶
‚Ä¶
‚Ä¶
ùëÉ(ùíî1|ùíì1)
ùëÉ(ùíî2|ùíì1, ùíì2)
ùëÉ(ùíîùêø|ùíì1, ùíì2, ‚Ä¶, ùíìùêø)
‚Ä¶
‚Ä¶
Œ©1 update
Œ©2 update
Œ©ùêø update
Updating support of ùíîùêø
Least Squares
Least Squares
Least Squares
‚Ä¶ ùíî 1 ùíî 2 ùíî ùêø
Estimation of L-th sparse vector ùíì1 = ùíö1 ‚àí ùë®Œ©1ùíî 1 ùíì2 = ùíö2 ‚àí ùë®Œ©2ùíî 2
‚Ä¶ ùíìùêø = ùíöùêø ‚àí ùë®Œ©ùêøùíî ùêø
Residual vector for L-th sparse vector
Vector of LSTM cells' state
Fig. 1. Block diagram of the proposed method unfolded over channels.
LSTM ùíì(ùë°)
ùëæ3 ùíì(ùë°)
ùëæ4 ùíì(ùë°)
ùëæ2 ùíì(ùë°)
ùëæ1 ùëî(. ) ùúé(. ) ùúé(. )
Input Gate
Output Gate ùúé(. ) Forget Gate
Cell
√ó ùíöùëî(ùë°) ùíä(ùë°) ùíá(ùë°) ùíÑ(ùë° ‚àí 1)
√ó ‚Ñé(. )
√ó ùíÑ(ùë°) ùíê(ùë°) ùíó(ùë°) ùíÑ(ùë° ‚àí 1)
ùëæùëù2
ùëæùëù3
ùëæùëù1 ùíó(ùë° ‚àí 1)
ùëæùëüùëíùëê4
ùüè ùíÉ4 ùíó(ùë° ‚àí 1)
ùüè
ùëæùëüùëíùëê3 ùíÉ3
ùüè ùíÉ1
ùëæùëüùëíùëê1 ùíó(ùë° ‚àí 1)
ùüè ùíÉ2 ùíó(ùë° ‚àí 1)
ùëæùëüùëíùëê2
Fig. 2. Block diagram of the Long Short-Term Memory (LSTM). based on a deep learning bottom up approach. Similar to all deep learning methods, it has the important feature of learning the structure of S from the raw data automatically. Although it is based on a greedy method that selects subsets that are not necessarily optimal, we experimentally show that by using a properly trained model and only one layer of LSTM, the proposed method significantly outperforms well known MMV baselines (e.g., SOMP) as well as the well known Bayesian methods for the MMV problem (e.g., Multitask Bayesian Compressive Sensing (MT-BCS) and Sparse Bayesian
Learning for temporally correlated sources (T-SBL) ).
We show this on two real world datasets.
We emphasize that the computations carried at the encoder mainly include multiplication by a random matrix.
The extra computations are only needed at the decoder.
Therefore an important feature of compressive sensing(low power encoding) is preserved.
C. Related Work
Exploiting data structures besides sparsity for compressive sensing has been extensively studied in the literature,,,,,,,,,,,. In, it has been theoretically shown that using signal models that exploit these structures will result in a decrease in the number of measurements. In, a thorough review on CS methods that exploit the structure present in the sparse signal or in the measurements is presented. In, a Bayesian framework for CS is presented. This framework uses a prior information about the sparsity of s to provide a posterior density function for the entries of s (assuming y is observed). It then uses a Relevance Vector Machine(RVM) to estimate the entries of the sparse vector.
This method is called Bayesian Compressive Sensing(BCS). In, a Bayesian framework is presented for the MMV problem. It assumes that the L "tasks" in the MMV problem in (4), are not statistically independent.
By imposing a shared prior on the L tasks, an empirical method is presented to estimate the hyperparameters and extensions of RVM are used for the inference step.
This method is known as Multitask Compressive Sensing(MT-BCS). In, it is experimentally shown that the MT-BCS outperforms the method that applies Orthogonal Matching Pursuit (OMP) on each task, the Simultaneous Orthogonal Matching Pursuit (SOMP) method which is a straightforward extension of OMP for the MMV problem, and the method that applies BCS for each task. In, the Sparse Bayesian Learning (SBL), is used to solve the MMV problem. It was shown that the global minimum of the proposed method is always the sparsest one. The authors in, address the MMV problem when the entries in each row of S are correlated. An algorithm based on SBL is proposed and it is shown that the proposed algorithm outperforms the mixed norm (‚Ñì1,2) optimization as well as the method proposed in. The proposed method is called TSBL. In, a greedy algorithm aided by a neural network is proposed to address the SMV problem in(3). The neural network parameters are calculated by solving a regression problem and are used to select the appropriate column of A at each iteration of OMP. The main modification to OMP is replacing the correlation step with a neural network. They experimentally show that the proposed method outperforms OMP and ‚Ñì1 optimization. This method is called Neural Network
OMP (NNOMP). In, an extension of with a hierarchical Deep Stacking Netowork (DSN) is proposed for the MMV problem. "The joint sparsity of S is an important assumption in the proposed method". To train the DSN model, the Restricted Boltzmann Machine(RBM) is used to pre-train DSN and then fine tuning is performed. It has been experimentally shown that this method outperforms SOMP and ‚Ñì1,2 in the MMV problem. The proposed methods are called Nonlinear
Weighted SOMP (NWSOMP) for the one layer model and DSN-WSOMP for the multilayer model. In, a feedforward neural network is used to solve the SMV problem as a regression task. Similar to (if we assume that we have only one sparse vector in ), a pre-training phase followed by a fine tuning is used. For pre-training, the authors have used Stacked Denoising
Auto-encoder (SDA). Please note that an RBM with
Gaussian visible units and binary hidden units (i.e., the one used in ) has the same energy function as an auto-encoder with sigmoid hidden units and real valued observations. Therefore the extension of to the MMV problem will give similar performance as that of. In, a reconstruction method is proposed for sparse signals whose sparsity patterns change slowly with time. The main idea is to replace Compressive
Sensing (CS) on the observation y with CS on the Least Squares (LS) residuals. LS residuals are calculated using the previous estimation of the support. In, a reconstruction method is proposed to recover sparse signals with a sparsity pattern that slowly changes over time. The main idea is to use Sparse Bayesian Learning(SBL) framework. Similar to SBL, a set of hyperparameters are defined to control the sparsity of signals.
The main difference is that the prior for each coefficient also involves the coefficients of the adjacent temporal observations. In, a CS algorithm is proposed for time-varying sparse signals based on the least-absolute shrinkage and selection operator (Lasso). A dynamic
Lasso algorithm is proposed for the signals with timevarying amplitudes and support.
The rest of the paper is organized as follows: In section II, the basics of Recurrent Neural Networks(RNN) with Long Short-Term Memory (LSTM) cells are briefly explained. The proposed method and the learning algorithm are presented in section III. Experimental results on two real world datasets are presented in section
IV. Conclusions and future work directions are discussed in section V. Details of the final gradient expressions for the learning section of the proposed method are presented in Appendix A.
II. RNN WITH LSTM CELLS
The RNN is a type of deep neural networks, that are "deep" in the temporal dimension. It has been used extensively in time sequence modelling,,,,,,,,. If we look at the sparse vectors (columns) in S as a sequence, the main idea of using RNN for the MMV problem is to predict the sparsity patterns over different sparse vectors in S.
Although RNN performs sequence modelling in a principled manner, it is generally difficult to learn the long term dependency within the sequence due to the vanishing gradients problem. One of the effective solutions for this problem in RNNs is to employ memory cells instead of neurons that is originally proposed in as Long Short-Term Memory (LSTM). It is further developed in and by adding forget gate and peephole connections to the architecture.
We use the architecture of LSTM illustrated in Fig.
2 for the proposed sequence modelling method for the MMV problem. In this figure, i(t), f(t), o(t), c(t) are input gate, forget gate, output gate and cell state vector respectively, Wp1, Wp2 and Wp3 are peephole connections, Wi, Wreci and bi, i = 1, 2, 3, 4 are input connections, recurrent connections and bias values, 5 respectively, g(¬∑) and h(¬∑) are tanh(¬∑) function and œÉ(¬∑) is the sigmoid function. We use this architecture to find v for each channel and then use the proposed method in Fig. 1 to find the entries that have a higher probability of being non-zero. Considering Fig. 2, the forward pass for LSTM model is as follows: yg(t) = g(W4r(t) + Wrec4v(t ‚àí 1) + b4) i(t) = œÉ(W3r(t) + Wrec3v(t ‚àí 1) + Wp3c(t ‚àí 1) + b3) f(t) = œÉ(W2r(t) + Wrec2v(t ‚àí 1) + Wp2c(t ‚àí 1) + b2) c(t) = f(t) ‚ó¶ c(t ‚àí 1) + i(t) ‚ó¶ yg(t) o(t) = œÉ(W1r(t) + Wrec1v(t ‚àí 1) + Wp1c(t) + b1) v(t) = o(t) ‚ó¶ h(c(t))(6) where ‚ó¶ denotes the Hadamard (element-wise) product.
Summary of notations used in Fig. 2 is as follows:
‚Ä¢ "t": Stands for the time index in the sequence.
For example, if we have 4 residual vectors of four different channels, we can show them as r(t), t =
‚Ä¢ "1": is a scalar
‚Ä¢ "Wreci, i = 1, 2, 3, 4": Recurrent weight matrices of dimension ncell √ó ncell where ncell is the number of cells in LSTM.
‚Ä¢ "Wi, i = 1, 2, 3, 4": Input weight matrices of dimension M √ó ncell where M is the number of random measurements in compressive sensing.
These matrices map the residual vectors to feature space.
‚Ä¢ "bi, i = 1, 2, 3, 4": Bias vectors of size ncell √ó 1.
‚Ä¢ "Wpi, i = 1, 2, 3": Peephole connections of dimension ncell √ó ncell.
‚Ä¢ "v(t), t = 1, 2,..., L": Output of the cells. Vector of size ncell √ó 1. L is the number of channels in the MMV problem.
‚Ä¢ "i(t), o(t), yg(t), t = 1, 2,..., L": Input gates, output gates and inputs before gating respectively.
Vector of size ncell √ó 1.
‚Ä¢ "g(¬∑) and h(¬∑)": tanh(¬∑) function.
‚Ä¢ "œÉ(¬∑)": Sigmoid function.
III. PROPOSED METHOD
A. High Level Picture
The summary of the proposed method is presented in Fig. 1. We initialize the residual vector, r, for each channel by the measurement vector, y, of that channel.
These residual vectors serve as the input to the LSTM model that captures features of the residual vectors using input weight matrices (W1,W2,W3,W4) as well as the dependency among the residual vectors using recurrent weight matrices (Wrec1,Wrec2,Wrec3,Wrec4) and the central memory unit shown in Fig. 2. A transformation matrix U is then used to transform, v ‚àà ‚Ñúncell√ó1, the output of each memory cell after gating, into the sparse vectors space, i.e., z ‚àà ‚ÑúN√ó1. "ncell" is the number of cells in the LSTM model. Then a softmax layer is used for each channel to find the probability of each entry of each sparse vector being non-zero. For example, for channel 1, the j-th output of the softmax layer is:
P(s1(j)|r1) = ez(j)
ÔøΩN k=1 ez(k)
Then for each channel, the entry with the maximum probability value is selected and added to the support set of that channel. After that, given the new support set, the following least squares problem is solved to find an estimate of the sparse vector for the j-th channel:
ÀÜsj = argmin sj
‚à•yj ‚àí AŒ©jsj‚à•2
Using ÀÜsj, the new residual value for the j-th channel is calculated as follows: rj = yj ‚àí AŒ©jÀÜsj
This residual serves as the input to the LSTM model at the next iteration of the algorithm. The stopping criteria for the algorithm is when the residual values are small enough or when it has performed N iterations where N is the dimension of the sparse vector. Since we have used
LSTM cells for the proposed method, we call it LSTMCS algorithm. The pseudo-code of the proposed method is presented in Algorithm 1.
Algorithm 1 Distributed Compressive Sensing using
Long Short-Term Memory (LSTM-CS)
Inputs: CS measurement matrix A ‚àà ‚ÑúM√óN; matrix of measurements Y ‚àà
‚ÑúM√óL; minimum ‚Ñì2 norm of residual matrix "resMin" as stopping criterion;
Trained "lstm" model
Output: Matrix of sparse vectors ÀÜS ‚àà ‚ÑúN√óL
Initialization: ÀÜS = 0; j = 1; i = 1; Œ© = ‚àÖ; R = Y.
1: procedure LSTM-CS(A,Y, lstm)
2: while i ‚â§ N or ‚à•R‚à•2 ‚â§ resMin do
3: i ‚Üê i + 1
4: for j = 1 ‚Üí L do
R(:, j)i ‚Üê
R(:,j)i‚àí1 max(|R(:,j)i‚àí1|)
6: vj ‚Üê lstm(R(:, j)i, vj‚àí1, cj‚àí1)
‚ñ∑ LSTM
7: zj ‚Üê Uvj
8: c ‚Üê softmax(zj)
9: idx ‚Üê Support(max(c))
Œ©i ‚Üê Œ©i‚àí1 ‚à™ idx
ÀÜSŒ©i(:, j) ‚Üê (AŒ©i)‚Ä†Y(:, j)
‚ñ∑ Least Squares
ÀÜSŒ©C i (:, j) ‚Üê 0
R(:, j)i ‚Üê Y(:, j) ‚àí AŒ©i ÀÜSŒ©i(:, j)
14: end for
15: end while
16: end procedure
We continue by explaining how the training data is prepared from off-line dataset and then we present the details of the learning method. Please note that all the computations explained in the subsequent two sections are performed only once and they do not affect the run time of the proposed solver in Fig. 1. It is almost as fast as greedy algorithms in sparse reconstruction.
B. Training Data Generation
The main idea of the proposed method is to look at the sparse reconstruction problem as a two step task: a classification as the first step and a subsequent least squares as the second step. In the classification step, the aim is to find the atom of the dictionary, i.e., the column of A, that is most relevant to the given residual of the current channel and the residuals of the previous channels. Therefore we need a set of residual vectors and their corresponding sparse vectors for supervised training. Since the training data and A are given, we can imitate the steps explained in the previous section to generate the residuals. This means that, given a sparse vector s with k non-zero entries, we calculate y using(3). Then we find the entry that has the maximum value in s and set it to zero. Assume that the index of this entry is k0. This gives us a new sparse vector with k ‚àí1 non-zero entries. Then we calculate the residual vector from: r = y ‚àí ak0s(k0)
Where ak0 is the k0-th column of A and s(k0) is the k0-th entry of s. It is obvious that this residual value is because of not having the remaining k ‚àí 1 non-zero entries of s. From these remaining k‚àí1 non-zero entries, the second largest value of s has the main contribution to r in (10). Therefore, we use r to predict the location of the second largest value of s. Assume that the index of the second largest value of s is k1. We define s0 as a one hot vector that has value 1 at k1-th entry and zero at other entries. Therefore, the training pair is (r, s0).
Now we set the k1-th entry of s to zero. This gives us a new sparse vector with k ‚àí 2 non-zero entries. Then we calculate the new residual vector from: r = y ‚àí [ak0, ak1][s(k0), s(k1)]T
We use the residual in (11) to predict the location of the third largest value in s. Assume that the index of the third largest value of s is k2. We define s0 as a one hot vector that has value 1 at k2-th entry and zero at other entries. Therefore, the new training pair is (r, s0).
The above procedure is continued upto the point that s does not have any non-zero entry. Then the same procedure is used for the next training sample. This gives us training samples for one channel. Then the same procedure is used for the next channel in S. Since the number of non-zero entries, k, is not known in advance, we assume a maximum number of non-zero entries per channel for training data generation.
C. Learning Method
To calculate the parameters of the proposed model, i.e., W1, W2, W3, W4, Wrec1, Wrec2, Wrec3, Wrec4, Wp1, Wp2, Wp3, b1, b2, b3, b4 in Fig. 2 and transformation matrix U in Fig.1, we minimize a cross entropy cost function over the training data. Assuming s is the output vector of the softmax layer given by the model in Fig. 1 (output of the softmax layer is represented as conditional probabilities in Fig. 1) and s0 is the one hot vector explained in the previous section, the following optimization problem is solved:
L(Œõ) = min
Œõ
ÔøΩ
ÔøΩ
ÔøΩ nB
ÔøΩ i=1
Bsize
ÔøΩ r=1
L
ÔøΩ œÑ=1
N
ÔøΩ j=1
Lr,i,œÑ,j(Œõ)
ÔøΩ
ÔøΩ
ÔøΩ
Lr,i,œÑ,j(Œõ) = ‚àís0,r,i,œÑ(j)log(sr,i,œÑ(j))(12) where nB is the number of mini-batches in the training data, Bsize is the number of training data pairs, (r, s0), in each mini-batch, L is the number of channels in the MMV problem, i.e., number of columns of S, and N is the length of vector s and s0. Œõ denotes the collection of the model parameters that includes W1, W2, W3, W4, Wrec1, Wrec2, Wrec3, Wrec4, Wp1, Wp2, Wp3, b1, b2, b3 and b4 in Fig. 2 and U in Fig. 1.
To solve the optimization problem in (12), we use
Backpropagation through time (BPTT) with Nesterov method. The update equations for parameter Œõ at epoch k are as follows:
‚ñ≥Œõk = Œõk ‚àí Œõk‚àí1
‚ñ≥Œõk = ¬µk‚àí1‚ñ≥Œõk‚àí1 ‚àí œµk‚àí1‚àáL(Œõk‚àí1 + ¬µk‚àí1‚ñ≥Œõk‚àí1)(13) where ‚àáL(¬∑) is the gradient of the cost function in (12), œµ is the learning rate and ¬µk is a momentum parameter determined by the scheduling scheme used for training.
Above equations are equivalent to Nesterov method in. To see why, please refer to appendix A.1 of where the Nesterov method is derived as a momentum method. The gradient of the cost function, ‚àáL(Œõ), is:
‚àáL(Œõ) = nB
ÔøΩ i=1
Bsize
ÔøΩ r=1
L
ÔøΩ œÑ=1
N
ÔøΩ j=1
‚àÇLr,i,œÑ,j(Œõ)
‚àÇŒõ
ÔøΩ
ÔøΩÔøΩ
ÔøΩ one large update
As it is obvious from (14), since we have unfolded the LSTM over channels in S, we fold it back when we want to calculate gradients over the whole sequence of channels.
‚àÇLr,i,œÑ,j(Œõ)
‚àÇŒõ in (14) and error signals for different parameters of the proposed model that are necessary for training are presented in Appendix A. Due to lack of space, we omit the presentation of full derivation of the gradients.
We have used mini-batch training to accelerate training and one large update instead of incremental updates during back propagation through time. To resolve the gradient explosion problem we have used gradient
Algorithm 2 Training the proposed model for Distributed Compressive Sensing
Inputs: Fixed step size "œµ", Scheduling for "¬µ", Gradient clip threshold
"thG", Maximum number of Epochs "nEpoch", Total number of training pairs in each mini-batch "Bsize", Number of channels for the MMV problem "L".
Outputs: LSTM-CS trained model for distributed compressive sensing "Œõ".
Initialization: Set all parameters in Œõ to small random numbers, i = 0, k = 1. procedure LSTM-CS(Œõ) while i ‚â§ nEpoch do for "first minibatch" ‚Üí "last minibatch" do r ‚Üê 1 while r ‚â§ Bsize do
Compute ÔøΩL œÑ=1
‚àÇLr,œÑ
‚àÇŒõk
‚ñ∑ use (23) to (54) in appendix A r ‚Üê r + 1 end while
Compute ‚àáL(Œõk) ‚Üê "sum above terms over r" if ‚àáL(Œõk) > thG then
‚àáL(Œõk) ‚Üê thG
‚ñ∑ For each entry of the gradient matrix ‚àáL(Œõk) end if Compute ‚ñ≥Œõk
‚ñ∑ use (13)
Update: Œõk ‚Üê ‚ñ≥Œõk + Œõk‚àí1 k ‚Üê k + 1 end for i ‚Üê i + 1 end while end procedure clipping. To accelerate the convergence, we have used
Nesterov method and found it effective in training the proposed model for the MMV problem.
We have used a simple yet effective scheduling for ¬µk in (13), in the first and last 10% of all parameter updates ¬µk = 0.9 and for the other 80% of all parameter updates ¬µk = 0.995. We have used a fixed step size for training
LSTM. Please note that since we are using mini-batch training, all parameters are updated for each mini-batch in (14).
A summary of training method for LSTM-CS is presented in Algorithm 2.
Although the training method and derivatives in Appendix A are presented for all parameters in LSTM, in the implementation,we have removed peephole connections and forget gates. Since length of each sequence, i.e., the number of columns in S, is known in advance, we set state of each cell to zero in the beginning of a new sequence. Therefore, forget gates are not a great help here. Also, as long as the order of columns in S is kept, the precise timing in the sequence is not of great concern, therefore, peephole connections are not that important as well. Removing peephole connections and forget gate will also help to have less training time, i.e., less number of parameters need to be tuned during training.
IV. EXPERIMENTAL RESULTS AND DISCUSSION
We have performed the experiments on two real world datasets, the first is the MNIST dataset of handwritten digits and the second is three different classes of images from natural image dataset of Microsoft Research in Cambridge.
In this section, we would like to answer the following questions: (i) How is the performance of different reconstruction algorithms for the MMV problem, including the proposed method, when different channels, i.e., different columns in S, have different sparsity patterns? (ii) Does the proposed method perform well enough when there is correlation among different sparse vectors? E.g., when sparse vectors are DCT or Wavelet transform of different blocks of an image? (iii) How fast is the proposed method compared to other reconstruction algorithms for the MMV problem? (iv) How robust is the proposed method to noise?
For all the results presented in this section, the reconstruction error is defined as:
NMSE = ‚à•ÀÜS ‚àí S‚à•
‚à•S‚à•(15) where S is the actual sparse matrix and ÀÜS is the recovered sparse matrix from random measurements by the reconstruction algorithm. The machine used to perform the experiments has an Intel(R) Core(TM) i7 CPU with clock 2.93 GHz and with 16 GB RAM.
A. MNIST Dataset
MNIST is a dataset of handwritten digits where the images of the digits are normalized in size and centred so that we have fixed size images. The task is to simultaneously encode 4 images each of size 24√ó24, i.e., we have
4 channels and L = 4 in (4). The encoder is a typical compressive sensing encoder, i.e., a randomly generated matrix A. We have normalized each column of A to have unit norm. Since the images are already sparse, i.e., have a few number of non-zero pixels, no transform, Œ® in (2), is used. To simulate the measurement noise, we have added a Gaussian noise with standard deviation
0.005 to the measurement matrix Y in (4). This results in measurements with signal to noise ratio (SNR) of approximately 46dB. We have divided each image into four 12 √ó 12 blocks. This means that the length of each sparse vector is N = 144. We have taken 50% random measurements from each sparse vector, i.e., M = 72. After receiving and reconstructing all blocks at the decoder, we compute the reconstruction error defined in (15) for the full image. We have randomly selected
10 images for each digit from the set {0, 1, 2, 3}, i.e., 40 images in total for the test. This means that the first column of S is an image of digit 0, the second column is an image of digit 1, the third column is an image of digit 2 and the fourth column is an image of digit 3. Test images are represented in Fig. 3.
Fig. 3. Randomly selected images for test from MNIST dataset. The first channel encodes digit zero, the second channel encodes digit one and so on.
We have compared the performance of the proposed reconstruction algorithm (LSTM-CS) with 7 reconstruction methods for the MMV problem. These methods are:
‚Ä¢ Simultaneous
Orthogonal
Matching
Pursuit(SOMP) which is a well known baseline for the MMV problem.
‚Ä¢ Bayesian Compressive Sensing (BCS) applied independently on each channel. For the BCS method we set the initial noise variance of i-th channel to the value suggested by the authors, i.e., std(yi)2/100 where i ‚àà {1, 2, 3, 4} and std(.) calculates the standard deviation. We set the threshold for stopping the algorithm to 10‚àí8.
‚Ä¢ Multitask Compressive Sensing (MT-BCS) which takes into account the statistical dependency of different channels. For MT-BCS we set the parameters of the Gamma prior on noise variance to a = 100/0.1 and b = 1 which are the values suggested by the authors. We set the stopping threshold to 10‚àí8 as well.
‚Ä¢ Sparse Bayesian Learning for Temporally correlated sources (T-SBL) which exploits correlation among different sources in the MMV problem. For
T-SBL, we used the default values proposed by the authors.
‚Ä¢ Nonlinear Weighted SOMP (NWSOMP) which solves a regression problem to help the SOMP algorithm with prior knowledge from training data.
For NWSOMP, during training, we used one layer, 512 neurons and 25 epochs of parameters update.
‚Ä¢ Compressive Sensing on Least Squares Residual(LSCS) where no explicit joint sparsity assumption is made in the design of the method. For
LSCS, we used sigma0 = cc‚àó(1/3)‚àósqrt(Sav/m) suggested by the authors where m is the number of measurements and Sav = 16 as suggested by the author. We tried a range of different values of cc and got the best results with cc = 0.1. We also
Number of non-zero entries in the sparse vector
NMSE
SOMP
LSTM-CS,ncell=512
MT-BCS
T-SBL
NWSOMP
LSTM-CS,ncell=512,x4 TrData
LSCS
PCSBL-GAMP
BCS
Number of non-zero entries in the sparse vector
NMSE
SOMP
LSTM-CS,ncell=512
MT-BCS
T-SBL
NWSOMP
LSTM-CS,ncell=512,x4 TrData
LSCS
PCSBL-GAMP
Fig. 4. Comparison of different MMV reconstruction algorithms for
MNIST dataset. Bottom figure is the same as top figure without results of BCS algorithm to make the difference among different algorithms more visible. In this experiment M = 72 and N = 144. set sigsys = 1, siginit = 3 and lambdap = 4 as suggested by the author.
‚Ä¢ The method proposed in, and referred to as PCSBL-GAMP where sparse Bayesian learning is used to design the method and no explicit joint sparsity assumption is made. For PCSBL-GAMP, we used beta = 1, Pattern = 2 because we need the coupling among the sparse vectors, i.e., left and right coupling, maximum number of iterations equal to maxiter = 400, and C = 1e0 as suggested by the authors for the noisy case.
For LSTM-CS, during training, we used one layer, 512 cells and 25 epochs of parameter updates. We used only 200 images for the training set. The training set does not include any of the 40 images used for test.
To monitor and prevent overfitting, we used 3 images per channel as the validation set and we used early stopping if necessary. Please note that the images used for validation were not used in the training set or in the test set. Results are presented in Fig. 4.
In Fig. 4, the vertical axis is the NMSE defined in(15) and horizontal axis is the number of non-zero entries in the sparse vector. The number of measurements, M, is fixed to 72. Each point on the curves in Fig. 4 is the average of NMSE over 40 reconstructed test images at the decoder.
For the MNIST dataset, we observe from Fig. 4 that
LSTM-CS significantly outperforms the reconstruction
Original
50% Measurements
SOMP
MT-BCS
T-SBL
NWSOMP
LSTM-CS
Original
50% Measurements
SOMP
MT-BCS
T-SBL
NWSOMP
LSTM-CS
Original
50% Measurements
SOMP
MT-BCS
T-SBL
NWSOMP
LSTM-CS
Original
50% Measurements
SOMP
MT-BCS
T-SBL
NWSOMP
LSTM-CS
LS‚àíCS
PCSBL‚àíGAMP
LS‚àíCS
PCSBL‚àíGAMP
LS‚àíCS
PCSBL‚àíGAMP
LS‚àíCS
PCSBL‚àíGAMP
Channel 2
Channel 1
Channel 3
Channel 4
Reconstruction Algorithms:
Fig. 5. Reconstructed images using different MMV reconstruction algorithms for 4 images of the MNIST dataset. First row are original images, S, second row are measurement matrices, Y, third row are reconstructed images using LS-CS, fourth row are reconstructed images using
SOMP, fifth row using PCSBL-GAMP, sixth row using MT-BCS, seventh row using T-SBL, eighth row using NWSOMP and the last row are reconstructed images using the proposed LSTM-CS method. algorithms for the MMV problem discussed in this paper.
One important reason for this is that existing MMV solvers rely on the joint sparsity in S, while the proposed method does not rely on this assumption. Another reason is that the structure of each sparse vector is effectively captured by LSTM. The reconstructed images using different MMV reconstruction algorithms for 4 test images are presented in Fig. 5. An interesting observation from
Fig. 5 is that the accuracy of reconstruction depends on the complexity of the sparsity pattern. For example when the sparsity pattern is simple, e.g., image of digit 1 in Fig. 5, all the algorithms perform well. But when the sparsity pattern is more complex, e.g., image of digit
0 in Fig. 5, then their reconstruction accuracy degrades significantly.
We have repeated the experiments on the MNIST dataset with 25% random measurements, i.e., M = 36.
The results are presented in Fig. 6. We trained 4 different
LSTM models for this experiment. The first one is the same model used for previous experiment (m = 72).
In the second model, we increased the number of cells in the LSTM model from 512 to 1024. In the third and fourth models, we used 2 times and 4 times more training data respectively. The rest of the experiments' settings was similar to the settings described before. As observed from these results, by investing more on training a good
LSTM model, LSTM-CS method performs better.
All the results presented so far are for noisy measurements where an additive Gaussian noise with standard deviation 0.005 is used (SNR ‚âÉ 46dB). To evaluate the stability of the proposed LSTM-CS method to noise, and compare it with other methods discussed in this paper, an experiment was performed using the following range of noise standard deviations: œÉ = {0.5, 0.2, 0.1, 0.05, 0.01, 0.005}(16) where œÉ is the standard deviation of noise. This approximately corresponds to:
SNR = {6 dB, 14 dB, 20 dB, 26 dB, 40 dB, 46 dB}
We used the same experimental settings explained above.
Results are presented in Fig. 7.
As observed from the results, in very noisy environment, i.e., SNR = 6 dB, performance of MT-BCS, LSCS and PCSBL-GAMP degrades significantly while
T-SBL, NWSOMP and LSTM-CS (proposed in this paper) methods show less severe degradation. In very low noise environment, i.e., SNR = 46 dB, performance of LSTM-CS, trained with just 512 cells and 200 training images, is better than other methods. In medium noise environment, i.e., SNR = 20 dB and SNR = 26 dB, performance of LSTM-CS, T-SBL and PCSBL-GAMP are close (although LSTM-CS is slightly better). Please note that the performance of LSTM-CS can be further improved by using a better architecture (e.g., more
Signal to Noise Ratio (dB)
NMSE
SOMP
NWSOMP
LSTM-CS
MT-BCS
T-SBL
PCSBL-GAMP
LSCS
BCS(a) Results for all Methods.
Signal to Noise Ratio (dB)
NMSE
SOMP
NWSOMP
LSTM-CS
MT-BCS
T-SBL
PCSBL-GAMP
LSCS(b) Results without BCS method for a more clear visibility.
Fig. 7. Reconstruction performance of the methods discussed in the paper for different noise levels.
Number of non-zero entries in the sparse vector
NMSE
SOMP
T-SBL
LSTM-CS
MT-BCS
NWSOMP
BCS
LSTM-CS,ncell=1024
LSTM-CS,ncell=1024,x2 TrData
LSTM-CS,ncell=1024,x4 TrData
PCSBL-GAMP
LSCS
Number of non-zero entries in the sparse vector
NMSE
SOMP
T-SBL
LSTM-CS
MT-BCS
NWSOMP
LSTM-CS,ncell=1024
LSTM-CS,ncell=1024,x2 TrData
LSTM-CS,ncell=1024,x4 TrData
PCSBL-GAMP
LSCS
Fig. 6. Comparison of different MMV reconstruction algorithms for
MNIST dataset. Bottom figure is the same as top figure without results of BCS algorithm to make the difference among different algorithms more visible. In this experiment M = 36 and N = 144. cells, more training data or more layers) as explained previously.
To present the phase transition diagram of solvers, we used a simple LSTM-CS solver that uses 512 cells and just 200 training images. The performance was evaluated over the following values of m n where n is the number of entries in each sparse vector and m is the number of measurements per channel: m n = {0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50}
For this experiment, we randomly selected 50 images per channel from MNIST dataset. Since we have L = 4 channels, and each image is of size 24 √ó 24, and each image has 4 blocks of 12√ó12 pixels, in total we will have
50√ó4√ó4 = 800 sparse vectors. Considering Fig. 4 of the paper, NMSE of most solvers is about 0.6. Therefore we set the following as the condition for perfect recovery: if more than 90% of test images are reconstructed with an
NMSE of 0.6 or less, count that test image as perfectly recovered. We did this for each m n in (18). Results are presented in Fig. 8. Results presented in Fig.8 shows the reconstruction performance improvement when LSTMCS method is used.
We also present the performance of LSTM-CS for different number of random measurements. We used the set of random measurements in (18) with n = 144.
We used an LSTM with 512 cells and 400 training images. The settings for all other methods was similar to the one described before. Results are presented in Fig.
9. As observed from Fig. 9, using LSTM-CS method improves the reconstruction performance compared to other methods discussed in this paper.
B. Natural Images Dataset
For experiments on natural images we used the MSR
Cambridge dataset. Ten randomly selected test images belonging to three classes of this dataset are used for experiments. The images are shown in Fig. 10. We have used 64 √ó 64 images. Each image is divided into
8√ó8 blocks. After reconstructing all blocks of an image in the decoder, the NMSE for the reconstructed image
Number of Random Measurements (m)
NMSE
SOMP
NWSOMP
LSTM-CS
MT-BCS
T-SBL
LSCS
PCSBL-GAMP
BCS(a) Results for all Methods.
Number of Random Measurements (m)
NMSE
SOMP
NWSOMP
LSTM-CS
MT-BCS
T-SBL
LSCS
PCSBL-GAMP(b) Results without BCS method for a more clear visibility.
Fig. 9.
Comparison of different MMV reconstruction algorithms for different number of random measurements for MNIST dataset. In this experiment n = 144.
1 m/n k/m
SOMP
LSTM-CS
PCSBL-GAMP
BCS
T-SBL
NWSOMP
MT-BCS
LSCS
Fig. 8.
Phase transition diagram for different methods on MNIST dataset where 90% perfect recovery is considered. Assuming a perfect recovery condition of NMSE ‚â§ 0.6 for this dataset. "n" is the number of entries in each sparse vector, "m" is the number of random measurements and "k" is the number of non-zero entries in each sparse vector. is calculated. The task is to simultaneously encode 4 blocks (L = 4) of an image and reconstruct them in the decoder. This means that S in (4) has 4 columns each one having N = 64 entries. We used 50% measurements, i.e., Y in (4) have 4 columns each one having M = 32 entries.
We have compared the performance of the proposed algorithm, LSTM-CS, with SOMP, T-SBL, MT-BCS and NWSOMP. We have not included results of applying
BCS per channel due its weak performance compared to other methods (this is shown in the experiments for
MNIST dataset). We have used the same setting as the settings for the MNIST dataset for different methods which is explained in the previous section. The only differences here are: (i) For each class of images, we have used just 55 images for training set and 5 images for validation set which do not include any of 10 images used for test. (ii) We have used 15 epochs for training
LSTM-CS which is enough for this dataset, compared to 25 epochs for the MNIST dataset. The experiments were performed for two popular transforms, DCT and Wavelet, for all aforementioned reconstruction algorithms. For the wavelet transform we used Haar wavelet transform with 3 levels of decomposition. Results for
DCT transform are presented in Fig. 11. Results for wavelet transform are presented in Fig. 12.
To conclude the experiments section, the CPU time for different reconstruction algorithms for the MMV problem discussed in this paper are presented in Fig. 13.
Each point on the curves in Fig. 13 is the time spent to reconstruct each sparse vector averaged over all the 8√ó8 blocks in 10 test images. We observe from this figure that the proposed algorithm is almost as fast as greedy algorithms. Please note that there is a faster version of T-SBL that is known as TMSBL. It will improve the CPU time of T-SBL but it is still slower than other reconstruction methods.
V. CONCLUSIONS AND FUTURE WORK
This paper presents a method to reconstruct sparse vectors for the MMV problem. The proposed method learns the structure of sparse vectors and does not rely on the commonly used joint sparsity assumption. Through experiments on two real world datasets, we showed that the proposed method outperforms the general MMV baseline SOMP as well as a number of Bayesian model based methods for the MMV problem. Please note that we have not used multiple layers of LSTM or Fig. 10. Randomly selected natural images from three different classes used for test. The first row are "buildings", the second row are "cows" and the third row are "flowers". the advanced deep learning methods for training, e.g., regularization using drop out which can improve the performance of LSTM-CS. This paper is a proof of concept that deep learning methods and specifically sequence modelling methods, e.g., LSTM, can improve the performance of the MMV solvers significantly. This is specially the case when the sparsity patterns are more complicated than that of obtained by the DCT or Wavelet transforms. We showed this on the MNIST dataset.
Please note that if collecting training samples is expensive or enough training samples are not available, using other sparse reconstruction methods is recommended.
Our future work includes: 1) Extending the LSTM-CS to bidirectional LSTM-CS. 2) Extending the proposed method to non-linear distributed compressive sensing.
3) Using the proposed method for video compressive sensing where there is correlation amongst the video frames, and compressive sensing of EEG signals where there is correlation amongst the different EEG channels.
VI. ACKNOWLEDGEMENT
We want to thank the authors of,,, and for making the code of their work available.
This was important in performing comparisons. For reproducibility of the results, please contact the authors for the MATLAB codes of the proposed LSTM-CS method.
We also want to thank WestGrid and Compute Canada
Calcul Canada for providing computational resources for part of this work.
APPENDIX A
EXPRESSIONS FOR THE GRADIENTS
In this appendix we present the final gradient expressions that are necessary to use for training the proposed model for the MMV problem. Due to lack of space, we omit the presentation of full derivations of these gradients.
Starting with the cost function in(12), we use the Nesterov method described in(13) to update
LSTM-CS model parameters.
Here, Œõ is one of the weight matrices or bias vectors
{W1, W2, W3, W4, Wrec1, Wrec2, Wrec3, Wrec4, Wp1, Wp2, Wp3, b1, b2, b3, b4} in the LSTM-CS architecture. The general format of the gradient of the cost function, ‚àáL(Œõ), is the same as (14). To calculate
‚àÇLr,i,œÑ (Œõ)
‚àÇŒõ from (12) we have:
‚àÇLr,i,œÑ(Œõ)
‚àÇŒõ
= ‚àí
N
ÔøΩ j=1 s0,r,i,œÑ(j)‚àÇlog(sr,i,œÑ(j))
‚àÇŒõ
After a straightforward derivation of derivatives we will have:
‚àÇLr,i,œÑ(Œõ)
‚àÇŒõ
= (Œ≤sr,i,œÑ ‚àí s0,r,i,œÑ)‚àÇzœÑ
‚àÇŒõ(20) where zœÑ is the vector z for œÑ-th channel in Fig. 1 and Œ≤ is a scalar defined as: Œ≤ =
N
ÔøΩ j=1 s0,r,i,œÑ(j)
Since during training data generation we have generated one hot vectors for s0, Œ≤ always equals to 1. Since we are looking at different channels as a sequence, for a more clear presentation we show any vector corresponding to t-th channel with (t) instead of index œÑ. For example, zœÑ is represented by z(t).
Since z(t) = Uv(t) we have:
‚àÇz(t)
‚àÇŒõ
= UT ‚àÇv(t)
‚àÇŒõ
Combining (20), (21) and (22) we will have:
‚àÇLr,i,t(Œõ)
‚àÇŒõ
= UT (sr,i(t) ‚àí s0,r,i(t))‚àÇv(t)
‚àÇŒõ
Starting from "t = L"-th channel, we define e(t) as: e(t) = UT (sr,i(t) ‚àí s0,r,i(t))
Number of non-zero entries in the sparse vector
NMSE
SOMP
MT-BCS
T-SBL
LSTM-CS
NWSOMP
LSCS
PCSBL-GAMP
Number of non-zero entries in the sparse vector
NMSE
SOMP
MT-BCS
T-SBL
LSTM-CS
NWSOMP
LSCS
PCSBL-GAMP
Number of non-zero entries in the sparse vector
NMSE
SOMP
MT-BCS
T-SBL
LSTM-CS
NWSOMP
PCSBL-GAMP
LSCS
Fig. 11.
Comparison of different MMV reconstruction algorithms for natural image dataset using DCT transform and just one layer for LSTM model in LSTM-CS. Image classes from top to bottom respectively: buildings, cows and flowers.
The expressions for the gradients for different parameters of LSTM-CS model are presented in the subsequent sections. We omit the subscripts r and i for simplicity of presentation. Please note that the final value of the gradient is sum of gradient values over the mini-batch samples and number of channels as represented by summations in (14).
A. Output Weights U
‚àÇLt
‚àÇU = (s(t) ‚àí s0(t)).v(t)T
B. Output Gate
For recurrent connections we have:
‚àÇLt
‚àÇWrec1
= Œ¥rec1(t).v(t ‚àí 1)T
Number of non-zero entries in the sparse vector
NMSE
SOMP
MT-BCS
T-SBL
LSTM-CS
NWSOMP
PCSBL-GAMP
LSCS
Number of non-zero entries in the sparse vector
NMSE
SOMP
MT-BCS
T-SBL
LSTM-CS
NWSOMP
LSCS
PCSBL-GAMP
Number of non-zero entries in the sparse vector
NMSE
SOMP
MT-BCS
T-SBL
LSTM-CS
NWSOMP
LSCS
PCSBL-GAMP
Fig. 12.
Comparison of different MMV reconstruction algorithms for natural image dataset using Wavelet transform and just one layer for LSTM model in LSTM-CS. Image classes from top to bottom respectively: buildings, cows and flowers. where Œ¥rec1(t) = o(t) ‚ó¶ (1 ‚àí o(t)) ‚ó¶ h(c(t)) ‚ó¶ e(t)
For input connections, W1, and peephole connections, Wp1, we will have:
‚àÇLt
‚àÇW1
= Œ¥rec1(t).r(t)T
‚àÇLt
‚àÇWp1
= Œ¥rec1(t).c(t)T
The derivative for output gate bias values will be:
‚àÇLt
‚àÇb1
= Œ¥rec1(t)
Number of non-zero entries in each sparse vector
CPU Time (sec)
SOMP
NWSOMP
MT-BCS
T-SBL
LSTM-CS
LS-CS
PCSBL-GAMP
Number of non-zero entries in each sparse vector
CPU Time (sec)
SOMP
NWSOMP
MT-BCS
LSTM-CS
PCSBL-GAMP
Fig. 13. CPU time for different MMV reconstruction algorithms. These times are for the experiment using DCT transform for 10 test images from the building class. The bottom figure is the same as top figure but without T-SBL and LS-CS to make the difference among different methods more clear.
C. Input Gate
For the recurrent connections we have:
‚àÇLt
‚àÇWrec3
= diag(Œ¥rec3(t)). ‚àÇc(t)
‚àÇWrec3(31) where Œ¥rec3(t) = (1 ‚àí h(c(t))) ‚ó¶ (1 + h(c(t))) ‚ó¶ o(t) ‚ó¶ e(t)
‚àÇc(t)
‚àÇWrec3
= diag(f(t)).‚àÇc(t ‚àí 1)
‚àÇWrec3
+ bi(t).v(t ‚àí 1)T bi(t) = yg(t) ‚ó¶ i(t) ‚ó¶ (1 ‚àí i(t))
For the input connections we will have the following:
‚àÇLt
‚àÇW3
= diag(Œ¥rec3(t)).‚àÇc(t)
‚àÇW3(33) where ‚àÇc(t)
‚àÇW3
= diag(f(t)).‚àÇc(t ‚àí 1)
‚àÇW3
+ bi(t).r(t)T
For the peephole connections we will have:
‚àÇLt
‚àÇWp3
= diag(Œ¥rec3 y(t)). ‚àÇc(t)
‚àÇWp3(35) where ‚àÇc(t)
‚àÇWp3
= diag(f(t)).‚àÇc(t ‚àí 1)
‚àÇWp3
+bi(t).c(t‚àí1)T (36)
For bias values, b3, we will have:
‚àÇLt
‚àÇb3
= diag(Œ¥rec3(t)).‚àÇc(t)
‚àÇb3(37) where ‚àÇc(t)
‚àÇb3
= diag(f(t)).‚àÇc(t ‚àí 1)
‚àÇb3
+ bi(t)
D. Forget Gate
For the recurrent connections we will have:
‚àÇLt
‚àÇWrec2
= diag(Œ¥rec2(t)). ‚àÇc(t)
‚àÇWrec2(39) where Œ¥rec2(t) = (1 ‚àí h(c(t))) ‚ó¶ (1 + h(c(t))) ‚ó¶ o(t) ‚ó¶ e(t)
‚àÇc(t)
‚àÇWrec2
= diag(f(t)).‚àÇc(t ‚àí 1)
‚àÇWrec2
+ bf(t).v(t ‚àí 1)T bf(t) = c(t ‚àí 1) ‚ó¶ f(t) ‚ó¶ (1 ‚àí f(t))
For input connections to forget gate we will have:
‚àÇLt
‚àÇW2
= diag(Œ¥rec2(t)).‚àÇc(t)
‚àÇW2(41) where ‚àÇc(t)
‚àÇW2
= diag(f(t)).‚àÇc(t ‚àí 1)
‚àÇW2
+ bf(t).r(t)T
For peephole connections we have:
‚àÇLt
‚àÇWp2
= diag(Œ¥rec2(t)). ‚àÇc(t)
‚àÇWp2(43) where ‚àÇc(t)
‚àÇWp2
= diag(f(t)).‚àÇc(t ‚àí 1)
‚àÇWp2
+bf(t).c(t‚àí1)T (44)
For forget gate's bias values we will have:
‚àÇLt
‚àÇb2
= diag(Œ¥rec2(t)).‚àÇc(t)
‚àÇb2(45) where ‚àÇc(t)
‚àÇb2
= diag(f(t)).‚àÇc(t ‚àí 1)
‚àÇb3
+ bf(t)
E. Input without Gating (yg(t))
For recurrent connections we will have:
‚àÇLt
‚àÇWrec4
= diag(Œ¥rec4(t)). ‚àÇc(t)
‚àÇWrec4(47) where Œ¥rec4(t) = (1 ‚àí h(c(t))) ‚ó¶ (1 + h(c(t))) ‚ó¶ o(t) ‚ó¶ e(t)
‚àÇc(t)
‚àÇWrec4
= diag(f(t)).‚àÇc(t ‚àí 1)
‚àÇWrec4
+ bg(t).v(t ‚àí 1)T bg(t) = i(t) ‚ó¶ (1 ‚àí yg(t)) ‚ó¶ (1 + yg(t))
For input connections we have:
‚àÇLt
‚àÇW4
= diag(Œ¥rec4(t)).‚àÇc(t)
‚àÇW4(49) where ‚àÇc(t)
‚àÇW4
= diag(f(t)).‚àÇc(t ‚àí 1)
‚àÇW4
+ bg(t).r(t)T
For bias values we will have:
‚àÇLt
‚àÇb4
= diag(Œ¥rec4(t)).‚àÇc(t)
‚àÇb4(51) where ‚àÇc(t)
‚àÇb4
= diag(f(t)).‚àÇc(t ‚àí 1)
‚àÇb4
+ bg(t)
F. Error signal backpropagation
Error signals are back propagated through time using following equations: Œ¥rec1(t ‚àí 1) = [o(t ‚àí 1) ‚ó¶ (1 ‚àí o(t ‚àí 1)) ‚ó¶ h(c(t ‚àí 1))]
‚ó¶ [WT rec1.Œ¥rec1(t) + e(t ‚àí 1)](53) Œ¥reci(t ‚àí 1) = [(1 ‚àí h(c(t ‚àí 1))) ‚ó¶ (1 + h(c(t ‚àí 1)))
‚ó¶ o(t ‚àí 1)] ‚ó¶ [WT reci.Œ¥reci(t) + e(t ‚àí 1)], for i ‚àà {2, 3, 4}
REFERENCES
 S. Hochreiter and J. Schmidhuber, "Long short-term memory,"
Neural Comput., vol. 9, no. 8, pp. 1735‚Äì1780, Nov. 1997.
 D. Donoho, "Compressed sensing," IEEE Transactions on Information Theory, vol. 52, no. 4, pp. 1289 ‚Äì1306, april 2006.
 E. Candes, J. Romberg, and T. Tao, "Stable signal recovery from incomplete and inaccurate measurements," Communications on
Pure and Applied Mathematics, vol. 59, no. 8, pp. 1207‚Äì1223, R. Baraniuk, "Compressive sensing [lecture notes]," IEEE Signal
Processing Magazine, vol. 24, no. 4, pp. 118 ‚Äì121, july 2007.
 E. J. Candes, Y. C. Eldar, D. Needell, and P. Randall, "Compressed sensing with coherent and redundant dictionaries," Applied and Computational Harmonic Analysis, vol. 31, no. 1, pp.
59 ‚Äì 73, 2011.
 M. Duarte and Y. Eldar, "Structured compressed sensing: From theory to applications," IEEE Transactions on Signal Processing, vol. 59, no. 9, pp. 4053 ‚Äì4085, sept. 2011.
 M. Davies and Y. Eldar, "Rank awareness in joint sparse recovery," IEEE Transactions on Information Theory, vol. 58, no. 2, pp. 1135 ‚Äì1146, Feb. 2012.
 Y. Eldar and H. Rauhut, "Average case analysis of multichannel sparse recovery using convex relaxation," IEEE Transactions on
Information Theory, vol. 56, no. 1, pp. 505 ‚Äì519, Jan. 2010.
 J. Tropp, A. Gilbert, and M. Strauss, "Algorithms for simultaneous sparse approximation. part I: Greedy pursuit," Signal
Processing, vol. 86, no. 3, pp. 572 ‚Äì 588, 2006.
 J. Tropp, "Algorithms for simultaneous sparse approximation. part II: Convex relaxation," Signal Processing, vol. 86, no. 3, pp. 589 ‚Äì 602, 2006.
 D. P. Wipf and B. D. Rao, "An empirical bayesian strategy for solving the simultaneous sparse approximation problem," IEEE
Transactions on Signal Processing, vol. 55, no. 7, pp. 3704‚Äì3716, S. Ji, D. Dunson, and L. Carin, "Multitask compressive sensing,"
IEEE Transactions on Signal Processing, vol. 57, no. 1, pp. 92‚Äì
 Z. Zhang and B. D. Rao, "Sparse signal recovery with temporally correlated source vectors using sparse bayesian learning," IEEE
Journal of Selected Topics in Signal Processing, vol. 5, no. 5, pp. 912‚Äì926, 2011.
 R. Baraniuk, V. Cevher, M. Duarte, and C. Hegde, "Model-based compressive sensing," IEEE Transactions on Information Theory, vol. 56, no. 4, pp. 1982‚Äì2001, April 2010.
 S. Ji, Y. Xue, and L. Carin, "Bayesian compressive sensing,"
IEEE Transactions on Signal Processing, vol. 56, no. 6, pp.
2346‚Äì2356, 2008.
 D. Merhej, C. Diab, M. Khalil, and R. Prost, "Embedding prior knowledge within compressed sensing by neural networks," IEEE
Transactions on Neural Networks, vol. 22, no. 10, pp. 1638 ‚Äì
1649, oct. 2011.
 H. Palangi, R. Ward, and L. Deng, "Using deep stacking network to improve structured compressed sensing with multiple measurement vectors," in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2013.
 A. Mousavi, A. B. Patel, and R. G. Baraniuk, "A deep learning approach to structured signal recovery,"
2015, http://arxiv.org/abs/1508.04065.
 N. Vaswani, "Ls-cs-residual (ls-cs): Compressive sensing on least squares residual," IEEE Transactions on Signal Processing, vol. 58, no. 8, pp. 4108‚Äì4120, 2010.
 J. Fang, Y. Shen, and H. Li, "Pattern coupled sparse bayesian learning for recovery of time varying sparse signals," in 19th
International Conference on Digital Signal Processing (DSP), 2014, pp. 705‚Äì709.
 D. Angelosante, G. Giannakis, and E. Grossi, "Compressed sensing of time-varying signals," in 16th International Conference on
Digital Signal Processing, 2009, pp. 1‚Äì8.
 M. E. Tipping, "Sparse bayesian learning and the relevance vector machine," J. Mach. Learn. Res., vol. 1, pp. 211‚Äì244, Sep. 2001.
 A. C. Faul and M. E. Tipping, "Analysis of sparse bayesian learning," in Neural Information Processing Systems (NIPS) 14.
MIT Press, 2001, pp. 383‚Äì389.
 L. Deng, D. Yu, and J. Platt, "Scalable stacking and learning for building deep architectures," in Proc. ICASSP, march 2012, pp.
2133 ‚Äì2136.
 G. E. Hinton, "Training products of experts by minimizing contrastive divergence," Neural Computation, vol. 14, no. 8, pp.
1771‚Äì1800, Aug. 2002.
 P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol, "Extracting and composing robust features with denoising autoencoders," ICML, pp. 1096‚Äì1103, 2008.
 P. Vincent, "A connection between score matching and denoising autoencoders," Neural Comput., vol. 23, no. 7, pp. 1661‚Äì1674, Jul. 2011.
 G. Hinton, L. Deng, D. Yu, G. E. Dahl, A. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath, and B. Kingsbury, "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups," IEEE
Signal Processing Magazine, vol. 29, no. 6, pp. 82‚Äì97, November
 G. Dahl, D. Yu, L. Deng, and A. Acero, "Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition," IEEE Transactions on Audio, Speech, and Language
Processing, vol. 20, no. 1, pp. 30 ‚Äì42, jan. 2012.
 J. L. Elman, "Finding structure in time," Cognitive Science, vol. 14, no. 2, pp. 179‚Äì211, 1990.
 A. J. Robinson, "An application of recurrent nets to phone probability estimation," IEEE Transactions on Neural Networks, vol. 5, no. 2, pp. 298‚Äì305, August 1994.
 L. Deng, K. Hassanein, and M. Elmasry, "Analysis of the correlation structure for a neural predictive model with application to speech recognition," Neural Networks, vol. 7, no. 2, pp. 331‚Äì339, T. Mikolov, M. Karafi¬¥at, L. Burget, J. Cernock`y, and S. Khudanpur, "Recurrent neural network based language model." in Proc.
INTERSPEECH, Makuhari, Japan, September 2010, pp. 1045‚Äì
 A. Graves, "Sequence transduction with recurrent neural networks," in Representation Learning Workshp, ICML, 2012.
 Y. Bengio, N. Boulanger-Lewandowski, and R. Pascanu, "Advances in optimizing recurrent networks," in Proc. ICASSP, Vancouver, Canada, May 2013.
 G. Mesnil, X. He, L. Deng, and Y. Bengio, "Investigation of recurrent-neural-network architectures and learning methods for spoken language understanding," in Proc. INTERSPEECH, Lyon, France, August 2013.
 H. Palangi, L. Deng, and R. Ward, "Recurrent deep-stacking networks for sequence classification," in Signal and Information
Processing (ChinaSIP), 2014 IEEE China Summit International
Conference on, July 2014, pp. 510‚Äì514.
 H. Palangi, L. Deng, and R. K. Ward, "Learning input and recurrent weight matrices in echo state networks," in NIPS Workshop on Deep Learning, December 2013. [Online].
Available: http://research.microsoft.com/apps/pubs/default.aspx? id=204701
 F. A. Gers, J. Schmidhuber, and F. Cummins, "Learning to forget:
Continual prediction with lstm," Neural Computation, vol. 12, pp.
2451‚Äì2471, 1999.
 F. A. Gers, N. N. Schraudolph, and J. Schmidhuber, "Learning precise timing with lstm recurrent networks," J. Mach. Learn.
Res., vol. 3, pp. 115‚Äì143, Mar. 2003.
 Y. Nesterov, "A method of solving a convex programming problem with convergence rate o (1/k2)," Soviet Mathematics
Doklady, vol. 27, pp. 372‚Äì376, 1983.
 I. Sutskever, J. Martens, G. E. Dahl, and G. E. Hinton, "On the importance of initialization and momentum in deep learning," in ICML (3)'13, 2013, pp. 1139‚Äì1147.
 Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner, "Gradient-based learning applied to document recognition," Proceedings of the IEEE, vol. 86, no. 11, pp. 2278‚Äì2324, Nov 1998. [Online].
Available: http://yann.lecun.com/exdb/mnist/
 [Online]. Available: http://research.microsoft.com/en-us/projects/ objectclassrecognition/
 J. Fang, L. Zhang, and H. Li, "Two-dimensional pattern-coupled sparse bayesian learning via generalized approximate message passing," 2015. [Online]. Available: http://arxiv.org/abs/1505.
Hamid Palangi (S'12) is a Ph.D. Candidate in the Electrical and Computer Engineering Department at the University of British
Columbia (UBC), Canada. Before joining
UBC in Jan. 2012, he received his M.Sc. degree in 2010 from Sharif University of Technology, Iran and B.Sc. degree in 2007 from Shahid Rajaee University, Iran, both in Electrical Engineering. Since Jan. 2012, he has been a member of Image and Signal
Processing Lab at UBC. His main research interests are Machine Learning, Deep Learning and Neural Networks, Linear Inverse Problems and Compressive Sensing, with applications in Natural Language and Image data.
Rabab Ward is a Professor Emeritus in the Electrical and Computer Engineering Department at the University of British Columbia(UBC), Canada.. Her research interests are mainly in the areas of signal, image and video processing. She has made contributions in the areas of signal detection, image encoding, image recognition, restoration and enhancement, and their applications to multimedia and medical imaging, face recognition, infant cry signals and brain computer interfaces.
She has published around 500 refereed journal and conference papers and holds six patents related to cable television, picture monitoring, measurement and noise reduction. She is a Fellow of the Royal Society of Canada, the IEEE, the Canadian Academy of Engineers and the Engineering Institute of Canada. She has received many top awards such as the "Society Award of the IEEE Signal Processing Society, the Career Achievement Award of CUFA BC,The Paradigm Shifter Award from The Society for Canadian Women in Science and Technology and British Columbia's APEGBC top engineering award "The RA
McLachlan Memorial Award" and UBC Killam Research Prize and Killam Senior Mentoring Award. She is presently the President of the IEEE Signal Processing Society. She was the General Chair of IEEE
ICIP 2000 and Co-Chair of IEEE ICASSP 2013.
Li Deng received a Ph.D. from the University of Wisconsin-Madison. He was an assistant and then tenured full professor at the University of Waterloo, Ontario, Canada during 1989-1999. Immediately afterward he joined Microsoft Research, Redmond, USA as a Principal Researcher, where he currently directs the R & D of its Deep Learning
Technology Center he founded in early 2014.
Dr. Dengs current activities are centered on business-critical applications involving big data analytics, natural language text, semantic modeling, speech, image, and multimodal signals. Outside his main responsibilities, Dr. Dengs research interests lie in solving fundamental problems of machine learning, artificial and human intelligence, cognitive and neural computation with their biological connections, and multimodal signal/information processing. In addition to over 70 granted patents and over 300 scientific publications in leading journals and conferences, Dr. Deng has authored or co-authored 5 books including 2 latest books: Deep Learning: Methods and Applications (NOW Publishers, 2014) and Automatic Speech Recognition: A Deep-Learning Approach(Springer, 2015), both with English and Chinese editions. Dr. Deng is a Fellow of the IEEE, the Acoustical Society of America, and the ISCA.
He served on the Board of Governors of the IEEE Signal Processing
Society. More recently, he was the Editor-In-Chief for the IEEE Signal
Processing Magazine and for the IEEE/ACM Transactions on Audio, Speech, and Language Processing; he also served as a general chair of ICASSP and area chair of NIPS. Dr. Dengs technical work in industryscale deep learning and AI has impacted various areas of information processing, especially Microsoft speech products and text- and bigdata related products/services. His work helped initiate the resurgence of (deep) neural networks in the modern big-data, big-compute era, and has been recognized by several awards, including the 2013 IEEE
SPS Best Paper Award and the 2015 IEEE SPS Technical Achievement
Award for outstanding contributions to deep learning and to automatic speech recognition.