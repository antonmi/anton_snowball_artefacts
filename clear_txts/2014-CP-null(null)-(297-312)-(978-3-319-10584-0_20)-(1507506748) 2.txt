Simultaneous Detection and Segmentation
Bharath Hariharan1, Pablo Arbel´aez1,2, Ross Girshick1, and Jitendra Malik1
1 University of California, Berkeley
2 Universidad de los Andes, Colombia
{bharath2,arbelaez,rbg,malik}@eecs.berkeley.edu
Abstract. We aim to detect all instances of a category in an image and, for each instance, mark the pixels that belong to it. We call this task Simultaneous Detection and Segmentation (SDS). Unlike classical bounding box detection, SDS requires a segmentation and not just a box. Unlike classical semantic segmentation, we require individual object instances.
We build on recent work that uses convolutional neural networks to classify category-independent region proposals (R-CNN ), introducing a novel architecture tailored for SDS. We then use category-specific, topdown figure-ground predictions to refine our bottom-up proposals. We show a 7 point boost (16% relative) over our baselines on SDS, a 5 point boost (10% relative) over state-of-the-art on semantic segmentation, and state-of-the-art performance in object detection. Finally, we provide diagnostic tools that unpack performance and provide directions for future work.
Keywords: detection, segmentation, convolutional networks.
Introduction
Object recognition comes in many flavors, two of the most popular being object detection and semantic segmentation. Starting with face detection, the task in object detection is to mark out bounding boxes around each object of a particular category in an image. In this task, a predicted bounding box is considered a true positive if it overlaps by more than 50% with a ground truth box, and different algorithms are compared based on their precision and recall. Object detection systems strive to find every instance of the category and estimate the spatial extent of each. However, the detected objects are very coarsely localized using just bounding boxes.
In contrast, semantic segmentation requires one to assign a category label to all pixels in an image. The MSRC dataset was one of the first publicly available benchmarks geared towards this task. Later, the standard metric used to evaluate algorithms in this task converged on pixel IU (intersection over union): for each category, this metric computes the intersection over union of the predicted pixels and ground truth pixels over the entire dataset. This task deals with "stuff" categories (such as grass, sky, road) and "thing" categories (such as cow, person, car) interchangeably. For things, this means that there is no notion
D. Fleet et al. (Eds.): ECCV 2014, Part VII, LNCS 8695, pp. 297–312, 2014. c
⃝ Springer International Publishing Switzerland 2014
B. Hariharan et al. of object instances. A typical semantic segmentation algorithm might accurately mark out the dog pixels in the image, but would provide no indication of how many dogs there are, or of the precise spatial extent of any one particular dog.
These two tasks have continued to this day and were part of the PASCAL
VOC challenge. Although often treated as separate problems, we believe the distinction between them is artificial. For the "thing" categories, we can think of a unified task: detect all instances of a category in an image and, for each instance, correctly mark the pixels that belong to it. Compared to the bounding boxes output by an object detection system or the pixel-level category labels output by a semantic segmentation system, this task demands a richer, and potentially more useful, output. Our aim in this paper is to improve performance on this task, which we call Simultaneous Detection and Segmentation(SDS).
The SDS algorithm we propose has the following steps (Figure 1):
1. Proposal Generation: We start with category-independent bottom-up object proposals. Because we are interested in producing segmentations and not just bounding boxes, we need region proposals. We use MCG to generate
2000 region candidates per image. We consider each region candidate as a putative object hypothesis.
2. Feature Extraction: We use a convolutional neural network to extract features on each region. We extract features from both the bounding box of the region as well as from the region foreground. This follows work by Girshick et al. (R-CNN) who achieved competitive semantic segmentation results and dramatically improved the state-of-the-art in object detection by using
CNNs to classify region proposals. We consider several ways of training the CNNs. We find that, compared to using the same CNN for both inputs (image windows and region masks), using separate networks where each network is finetuned for its respective role dramatically improves performance. We improve performance further by training both networks jointly, resulting in a feature extractor that is trained end-to-end for the SDS task.
3. Region Classification: We train an SVM on top of the CNN features to assign a score for each category to each candidate.
4. Region Refinement: We do non-maximum suppression (NMS) on the scored candidates. Then we use the features from the CNN to produce category-specific coarse mask predictions to refine the surviving candidates.
Combining this mask with the original region candidates provides a further boost.
Since this task is not a standard one, we need to decide on evaluation metrics.
The metric we suggest in this paper is an extension to the bounding box detection metric. It has been proposed earlier. Given an image, we expect the algorithm to produce a set of object hypotheses, where each hypothesis comes with a predicted segmentation and a score. A hypothesis is correct if its segmentation overlaps with the segmentation of a ground truth instance by more than
50%. As in the classical bounding box task, we penalize duplicates. With this labeling, we compute a precision recall (PR) curve, and the average precision
Simultaneous Detection and Segmentation(AP), which is the area under the curve. We call the AP computed in this way
APr, to distinguish it from the traditional bounding box AP, which we call APb(the superscripts r and b correspond to region and bounding box respectively).
APr measures the accuracy of segmentation, and also requires the algorithm to get each instance separately and completely. Our pipeline achieves an APr of 49.5% while at the same time improving APb from 51.0% (R-CNN) to 53.0%.
One can argue that the 50% threshold is itself artificial. For instance if we want to count the number of people in a crowd, we do not need to know their accurate segmentations. On the contrary, in a graphics application that seeks to matte an object into a scene, we might want extremely accurate segmentations.
Thus the threshold at which we regard a detection as a true positive depends on the application. In general, we want algorithms that do well under a variety of thresholds. As the threshold varies, the PR curve traces out a PR surface. We can use the volume under this PR surface as a metric. We call this metric APr vol and APb vol respectively. APr vol has the attractive property that an APr vol of 1 implies we can perfectly detect and precisely segment all objects. Our pipeline gets an APr vol of 41.4%. We improve APb vol from 41.9% (R-CNN) to 44.2%.
We also find that our pipeline furthers the state-of-the-art in the classic
PASCAL VOC semantic segmentation task, from 47.9% to 52.6%. Last but not the least, following work in object detection, we also provide a set of diagnostic tools for analyzing common error modes in the SDS task. Our algorithm, the benchmark and all diagnostic tools are publicly available at http://www.eecs.berkeley.edu/Research/Projects/CS/vision/shape/sds.
����
���
�������
���
��������
����
���������
����������
��������
����������
�������
��������������
�������
����������
Fig. 1. Overview of our pipeline. Our algorithm is based on classifying region proposals using features extracted from both the bounding box of the region and the region foreground with a jointly trained CNN. A final refinement step improves segmentation.
Related Work
For semantic segmentation, several researchers have tried to use activations from off-the-shelf object detectors to guide the segmentation process. Yang et al. use object detections from the deformable parts model to segment the image, pasting figure-ground masks and reasoning about their relative depth ordering.
B. Hariharan et al.
Arbel´aez et al. use poselet detections as features to score region candidates, in addition to appearance-based cues. Ladicky et al. use object detections as higher order potentials in a CRF-based segmentation system: all pixels in the foreground of a detected object are encouraged to share the category label of the detection. In addition, their system is allowed to switch off these potentials by assigning a true/false label to each detection. This system was extended by Boix et al. who added a global, image-level node in the CRF to reason about the categories present in the image, and by Kim et al. who added relationships between objects. In more recent work, Tighe et al. use exemplar object detectors to segment out the scene as well as individual instances.
There has also been work on localizing detections better using segmentation.
Parkhi et al. use color models from predefined rectangles on cat and dog faces to do GrabCut and improve the predicted bounding box. Dai and Hoiem generalize this to all categories and use instance and category appearance models to improve detection. These approaches do well when the objects are coherent in color or texture. This is not true of many categories such as people, where each object can be made of multiple regions of different appearance. An alternative to doing segmentation post facto is to use segmentation to generate object proposals which are then classified. The proposals may be used as just bounding boxes or as region proposals. These proposals incorporate both the consistency of appearance in an object as well as the possibility of having multiple disparate regions for each object. State-of-the-art detection systems and segmentation systems are now based on these methods.
In many of these approaches, segmentation is used only to localize the detections better. Other authors have explored using segmentation as a stronger cue. Fidler et al. use the output of a state-of-the-art semantic segmentation approach to score detections better. Mottaghi uses detectors based on non-rectangular patches to both detect and segment objects.
The approaches above were typically built on features such as SIFT or HOG. Recently the computer vision community has shifted towards using convolutional neural networks (CNNs). CNNs have their roots in the Neocognitron proposed by Fukushima. Trained with the back-propagation algorithm, LeCun showed that they could be used for handwritten zip code recognition. They have since been used in a variety of tasks, including detection and semantic segmentation. Krizhevsky et al. showed a large increase in performance by using CNNs for classification in the ILSVRC challenge.
Donahue et al. showed that Krizhevsky's architecture could be used as a generic feature extractor that did well across a wide variety of tasks. Girshick et al. build on this and finetune Krizhevsky's architecture for detection to nearly double the state-of-the-art performance. They use a simple pipeline, using
CNNs to classify bounding box proposals from. Our algorithm builds on this system, and on high quality region proposals from.
Simultaneous Detection and Segmentation
Our Approach
Proposal Generation
A large number of methods to generate proposals have been proposed in the literature. The methods differ on the type of outputs they produce (boxes vs segments) and the metrics they do well on. Since we are interested in the APr metric, we care about segments, and not just boxes. Keeping our task in mind, we use candidates from MCG for this paper. This approach significantly outperforms all competing approaches on the object level Jaccard index metric, which measures the average best overlap achieved by a candidate for a ground truth object. In our experiments we find that simply switching to MCG from
Selective Search improves APb slightly (by 0.7 points), justifying this choice.
We use the proposals from MCG as is. MCG starts by computing a segmentation hierarchy at multiple image resolutions, which are then fused into a single multiscale hierarchy at the finest scale. Then candidates are produced by combinatorially grouping regions from all the single scale hierarchies and from the multiscale hierarchy. The candidates are ranked based on simple features such as size and location, shape and contour strength.
Feature Extraction
We start from the R-CNN object detector proposed by Girshick et al. and adapt it to the SDS task. Girshick et al. train a CNN on ImageNet Classification and then finetune the network on the PASCAL detection set. For finetuning they took bounding boxes from Selective Search, padded them, cropped them and warped them to a square and fed them to the network. Bounding boxes that overlap with the ground truth by more than 50% were taken as positives and other boxes as negatives. The class label for each positive box was taken to be the class of the ground truth box that overlaps the most with the box.
The network thus learned to predict if the bounding box overlaps highly with a ground truth bounding box. We are working with MCG instead of Selective
Search, so we train a similar object detection network, finetuned using bounding boxes of MCG regions instead of Selective Search boxes.
At test time, to extract features from a bounding box, Girshick et al. pad and crop the box, warp it to a square and pass it through the network, and extract features from one of the later layers, which is then fed into an SVM. In this paper we will use the penultimate fully connected layer.
For the SDS task, we can now use this network finetuned for detection to extract feature vectors from MCG bounding boxes. However these feature vectors do not contain any information about the actual region foreground, and so will be ill-equipped to decide if the region overlaps highly with a ground truth segmentation or not. To get around this, we start with the idea used by Girshick et al. for their experiment on semantic segmentation: we extract a second set of features from the region by feeding it the cropped, warped box, but with
B. Hariharan et al. the background of the region masked out (with the mean image.) Concatenating these two feature vectors together gives us the feature vector we use. (In their experiments Girshick et al. found both sets of features to be useful.) This method of extracting features out of the region is the simplest way of extending the object detection system to the SDS task and forms our baseline. We call this feature extractor A.
The network we are using above has been finetuned to classify bounding boxes, so its use in extracting features from the region foreground is suboptimal. Several neurons in the network may be focussing on context in the background, which will be unavailable when the network is fed the region foreground. This suggests that we should use a different network to extract the second set of features: one that is finetuned on the kinds of inputs that it is going to see. We therefore finetune another network (starting again from the net trained on ImageNet) which is fed as input cropped, padded bounding boxes of MCG regions with the background masked out. Because this region sees the actual foreground, we can actually train it to predict region overlap instead, which is what we care about. Therefore we change the labeling of the MCG regions to be based on segmentation overlap of the region with a ground truth region (instead of overlap with bounding box). We call this feature extractor B.
The previous strategy is still suboptimal, because the two networks have been trained in isolation, while at test time the two feature sets are going to be combined and fed to the classifier. This suggests that one should train the networks jointly. We formalize this intuition as follows. We create a neural network with the architecture shown in Figure 2. This architecture is a single network with two pathways. The first pathway operates on the cropped bounding box of the region (the "box" pathway) while the second pathway operates on the cropped bounding box with the background masked (the "region" pathway). The two pathways are disjoint except at the very final classifier layer, which concatenates the features from both pathways. Both these pathways individually have the same architecture as that of Krizhevsky et al. Note that both A and B can be seen as instantiations of this architecture, but with different sets of weights.
A uses the same network parameters for both pathways. For B, the box pathway gets its weights from a network finetuned separately using bounding box overlap, while the region pathway gets its parameters from a network finetuned separately using region overlap.
Instead of using the same network in both pathways or training the two pathways in isolation, we now propose to train it as a whole directly. We use segmentation overlap as above. We initialize the box pathway with the network finetuned on boxes and the region pathway with the network finetuned on regions, and then finetune the entire network. At test time, we discard the final classification layer and use the output of the penultimate layer, which concatenates the features from the two pathways. We call this feature extractor C.
Simultaneous Detection and Segmentation
�������
�������
�������
�������
�������
�����
�����
�������
�������
�������
�������
�������
�����
�����
�������
����������
Fig. 2. Left: The region with its bounding box. Right: The architecture that we train for C. The top pathway operates on cropped boxes and the bottom pathway operates on region foregrounds.
Region Classification
We use the features from the previous step to train a linear SVM. We first train an initial SVM using ground truth as positives and regions overlapping ground truth by less than 20% as negative. Then we re-estimate the positive set: for each ground truth we pick the highest scoring MCG candidate that overlaps by more than 50%. Ground truth regions for which no such candidate exists (very few in number) are discarded. We then retrain the classifier using this new positive set. This training procedure corresponds to a multiple instance learning problem where each ground truth defines a positive bag of regions that overlap with it by more than 50%, and each negative region is its own bag. We found this training to work better than using just the ground truth as positives.
At test time we use the region classifiers to score each region. Because there may be multiple overlapping regions, we do a strict non-max suppression using a region overlap threshold of 0. This is because while the bounding box of two objects can in fact overlap, their pixel support in the image typically shouldn't.
Post NMS, we work with only the top 20,000 detections for each category (over the whole dataset) and discard the rest for computational reasons. We confirmed that this reduction in detections has no effect on the APr metric.
Region Refinement
We take each of the remaining regions and refine its support. This is necessary because our region candidates have been created by a purely bottom-up, class agnostic process. Since the candidate generation has not made use of categoryspecific shape information, it is prone to both undershooting (i.e. missing some part of the object) and overshooting (i.e. including extraneous stuff).
We first learn to predict a coarse, top-down figure-ground mask for each region. To do this, we take the bounding box of each predicted region, pad it as for feature extraction, and then discretize the resulting box into a 10 × 10 grid. For each grid cell we train a logistic regression classifier to predict the probability that the grid cell belongs to the foreground. The features we use are the features extracted from the CNN, together with the figure-ground mask of the region
B. Hariharan et al.
Fig. 3. Some examples of region refinement. We show in order the image, the original region, the coarse 10 × 10 mask, the coarse mask projected to superpixels, the output of the final classifier on superpixels and the final region after thresholding. Refinement uses top-down category specific information to fill in the body of the train and the cat and remove the road from the car. discretized to the same 10 × 10 grid. The classifiers are trained on regions from the training set that overlap by more than 70% with a ground truth region.
This coarse figure-ground mask makes a top-down prediction about the shape of the object but does not necessarily respect the bottom-up contours. In addition, because of its coarse nature it cannot do a good job of modeling thin structures like aircraft wings or structures that move around. This information needs to come from the bottom-up region candidate. Hence we train a second stage to combine this coarse mask with the region candidate. We project the coarse mask to superpixels by assigning to each superpixel the average value of the coarse mask in the superpixel. Then we classify each superpixel, using as features this projected value in the superpixel and a 0 or 1 encoding if the superpixel belongs to the original region candidate. Figure 3 illustrates this refinement.
Experiments and Results
We use the segmentation annotations from SBD to train and evaluate. We train all systems on PASCAL VOC 2012 train. For all training and finetuning of the network we use the recently released Caffe framework.
Results on APr and APr vol
Table 1 and Table 2 show results on the APr and the APr vol metrics respectively on PASCAL VOC 2012 val (ground truth segmentations are not available for test). We compute APr vol by averaging the APr obtained for 9 thresholds.
1. O2P uses features and regions from Carreira et al., which is the state-ofthe-art in semantic segmentation. We train region classifiers on these features and do NMS to get detections. This baseline gets a mean APr of 25.2% and a mean APr vol of 23.4%.
Simultaneous Detection and Segmentation
2. A is our most naive feature extractor. It uses MCG candidates and features from the bounding box and region foreground, using a single CNN finetuned using box overlaps. It achieves a mean APr of 42.9% and a mean APr vol of 37.0%, a large jump over O2P. This mirrors gains in object detection observed by Girshick et al., although since O2P is not designed for this task the comparison is somewhat unfair.
3. B is the result of finetuning a separate network exclusively on region foregrounds with labels defined by region overlap. This gives a large jump of the APr metric (of about 4 percentage points) and a smaller but significant jump on the APr vol metric of about 2.5 percentage points.
4. C is the result of training a single large network with two pathways. There is a clear gain over using two isolated networks: on both metrics we gain about 0.7 percentage points.
5. C+ref is the result of refining the masks of the regions obtained from C.
We again gain 2 points in the APr metric and 1.2 percentage points in the APr vol metric. This large jump indicates that while MCG candidates we start from are very high quality, there is still a lot to be gained from refining the regions in a category specific manner.
A paired sample t-test indicates that each of the above improvements are statistically significant at the 0.05 significance level.
The left part of Figure 5 plots the improvement in mean APr over A as we vary the threshold at which a detection is considered correct. Each of our improvements increases APr across all thresholds, indicating that we haven't overfit to a particular regime.
Clearly we get significant gains over both our naive baseline as well as O2P.
However, prior approaches that reason about segmentation together with detection might do better on the APr metric. To see if this is the case, we compare to the SegDPM work of Fidler et al.. SegDPM combined DPMs with
O2P and achieved a 9 point boost over DPMs in classical object detection.
For this method, only the bounding boxes are available publicly, and for some boxes the algorithm may choose not to have associated segments. We therefore compute an upper bound of its performance by taking each detection, considering all MCG regions whose bounding box overlaps with the detection by more than 70%, and selecting the region which best overlaps a ground truth.
Since SegDPM detections are only available on PASCAL VOC2010 val, we restrict our evaluations only to this set. Our upper bound on SegDPM has a mean APr of 31.3, whereas C+ref achieves a mean APr of 50.3.
Producing Diagnostic Information
Inspired by, we created tools for figuring out error modes and avenues for improvement for the SDS task. As in, we evaluate the impact of error modes by measuring the improvement in APr if the error mode was corrected. For localization, we assign labels to detections under two thresholds: the usual strict
B. Hariharan et al.
Table 1. Results on APr on VOC2012 val. All numbers are %.
O2P
A
B
C
C+ref aeroplane
68.4 bicycle
49.4 bird
52.1 boat
32.8 bottle
33.0 bus
67.8 car
53.6 cat
73.9 chair
19.9 cow
43.7 diningtable
25.7 dog
60.6 horse
55.9 motorbike
58.9 person
56.7 pottedplant
28.5 sheep
55.6 sofa
32.1 train
64.7 tvmonitor
Mean
Table 2. Results on APr vol on VOC2012 val. All numbers are %.
O2P
A
B
C
C+ref aeroplane
52.3 bicycle
42.6 bird
42.2 boat
28.6 bottle
28.6 bus
58.0 car
45.4 cat
58.9 chair
19.7 cow
37.1 diningtable
22.8 dog
49.5 horse
42.9 motorbike
45.9 person
48.5 pottedplant
25.5 sheep
44.5 sofa
30.2 train
52.6 tvmonitor
Mean
Simultaneous Detection and Segmentation
307 threshold of 0.5 and a more lenient threshold of 0.1 (note that this is a threshold on region overlap). Detections that count as true positives under the lenient threshold but as false positives under the strict threshold are considered mislocalizations. Duplicate detections are also considered mislocalizations. We then consider the performance if either a) all mislocalized instances were removed, or b) all mislocalized instances were correctly localized and duplicates removed.
Figure 4 shows how the PR curve for the APr benchmark changes if mislocalizations are corrected or removed for two categories. For the person category, removing mislocalizations brings precision up to essentially 100%, indicating that mislocalization is the predominant source of false positives. Correcting the mislocalizations provides a huge jump in recall. For the cat category the improvement provided by better localization is much less, indicating that there are still some false positives arising from misclassifications.
We can do this analysis for all categories. The average improvement in APr by fixing mislocalization is a measure of the impact of mislocalization on performance. We can also measure impact in this way for other error modes: for instance, false positives on objects of other similar categories, or on background.(For defining similar and non-similar categories, we divide object categories into
"animals", "transport" and "indoor" groups.) The left subfigure in Figure 6 shows the result of such an analysis on our best system (C+ref). The dark blue bar shows the APr improvement if we remove mislocalized detections and the light blue bar shows the improvement if we correct them. The other two bars show the improvement from removing confusion with similar categories and background. Mislocalization has a huge impact: it sets us back by about
16 percentage points. Compared to that confusion with similar categories or background is virtually non-existent.
We can measure the impact of mislocalization on the other algorithms in Table 1 as well, as shown in Table 3. It also shows the upper bound APr achievable when all mislocalization is fixed. Improvements in the feature extractor improve the upper bound (indicating fewer misclassifications) but also reduce the gap due to mislocalization (indicating better localization). Refinement doesn't change the upper bound and only improves localization, as expected.
To get a better handle on what one needs to do to improve localization, we considered two statistics. For each detection and a ground truth, instead of just taking the overlap (i.e. intersection over union), we can compute the pixel precision (fraction of the region that lies inside the ground truth) and pixel recall(fraction of the ground truth that lies inside the region). It can be shown that having both a pixel precision > 67% and a pixel recall > 67% is guaranteed to give an overlap of greater than 50%. We assign detection labels using pixel precision or pixel recall using a threshold of 67% and compute the respective
AP. Comparing these two numbers then gives us a window into the kind of localization errors: a low pixel precision AP indicates that the error mode is overshooting the region and predicting extraneous background pixels, while a low pixel recall AP indicates that the error mode is undershooting the region and missing out some ground truth pixels.
B. Hariharan et al.
The second half of Figure 6 shows the difference between pixel precision AP(APpp) and pixel recall AP (APpr). Bars to the left indicate higher pixel recall
AP, while bars to the right indicate higher pixel precision AP. For some categories such as person and bird we tend to miss ground truth pixels, whereas for others such as bicycle we tend to leak into the background.
Recall
Precision
C+ref
No misloc
Corr misloc
Recall
Precision
C+ref
No misloc
Corr misloc
Fig. 4. PR on person(left) and cat(right). Blue is C+ref. Green is if an oracle removes mislocalized predictions, and red is if the oracle corrects our mislocalizations.
−1
Overlap Threshold
Change in APr (percentage points)
B
C
C+ref
−4
−2
Overlap Threshold
Change in APb (percentage points)
R−CNN−MCG
A
B
C
Fig. 5. Left: Improvement in mean APr over A due to our 3 variants for a variety of overlap thresholds. We get improvements for all overlap thresholds. Right: A similar plot for APb. Improvements are relative to R-CNN with Selective Search proposals.
As the threshold becomes stricter, the better localization of our approach is apparent.
Results on APb and APb vol
Comparison with prior work is easier on the classical bounding box and segmentation metrics. It also helps us evaluate if handling the SDS task also improves performance on the individual tasks. To compare on APb, we retrain our final region classifiers for the bounding box detection task. This is because the ranking of regions based on bounding box overlap is different from that based on
Simultaneous Detection and Segmentation
B
S
L
Improvement in APr (percentage points)
−0.4
−0.2
0.4 aeroplane bicycle bird boat bottle bus car cat chair cow diningtable dog horse motorbike person pottedplant sheep sofa train tvmonitor
APpp−APpr
Fig. 6. Left: Impact of the three kinds of false positives on mean APr. L : mislocalization, B : detection on background, and S : misfirings on similar categories. Right:
Disambiguating between two kinds of mislocalizations. Bars to the left mean that we frequently overshoot the ground truth, while bars to the right mean that we undershoot.
Table 3. Maximum achievable APr (assuming perfect localization) and loss in APr due to mislocalization for all systems
A
B
C
C+ref
AP Upper bound
Loss due to mislocalization
15.8 segmentation overlap. As in, we use ground truth boxes as positive, and MCG boxes overlapping by less than 50% as negative. At test time we do not do any region refinement.
We add two baselines: R-CNN is the system of Girshick et al. taken as is, and R-CNN-MCG is R-CNN on boxes from MCG instead of Selective Search. Note that neither of these baselines uses features from the region foreground.
Table 4 shows the mean APb and APb vol. We get improvements over R-CNN on both APb and APb vol, with improvements on the latter metric being somewhat larger. The right half of Figure 5 shows the variation in APb as we vary the overlap threshold for counting something as correct. We plot the improvement in APb over vanilla R-CNN. We do worse than R-CNN for low thresholds, but are much better for higher thresholds. This is also true to some extent for RCNN-MCG, so this is partly a property of MCG, and partly a consequence of our algorithm's improved localization. Interestingly, C does worse than B. We posit that this is because now the entire network has been finetuned for SDS.
Finally we evaluated C on PASCAL VOC 2012 test. Our mean APb of 50.7 is an improvement over the R-CNN mean APb of 49.6 (both without bounding box regression), and much better than other systems, such as SegDPM (40.7).
B. Hariharan et al.
Table 4. Results on APb and APb vol on VOC12 val. All numbers are %.
R-CNN R-CNN-MCG
A
B
C
Mean APb
Mean APb vol
Results on Pixel IU
For the semantic segmentation task, we convert the output of our final system(C+ref) into a pixel-level category labeling using the simple pasting scheme proposed by Carreira et al.. We cross validate the hyperparameters of this pasting step on the VOC11 segmentation Val set. The results are in Table 5. We compare to O2P and R-CNN which are the current state-of-the-art on this task. We advance the state-of-the-art by about 5 points, or 10% relative.
To conclude, our pipeline achieves good results on the SDS task while improving state-of-the-art in object detection and semantic segmentation. Figure 7 shows examples of the output of our system.
Table 5. Results on Pixel IU. All numbers are %.
O2P R-CNN C+ref
Mean Pixel IU (VOC2011 Test)
Mean Pixel IU (VOC2012 Test)Fig. 7. Top detections: 3 persons, 2 bikes, diningtable, sheep, chair, cat. We can handle uncommon pose and clutter and are able to resolve individual instances.
Acknowledgments. This work was supported by ONR MURI N000141010933, a Google Research Grant and a Microsoft Research fellowship. We thank the NVIDIA Corporation for providing GPUs through their academic program.
Simultaneous Detection and Segmentation
References
1. Arbel´aez, P., Pont-Tuset, J., Barron, J., Marques, F., Malik, J.: Multiscale combinatorial grouping. In: CVPR (2014)
2. Arbel´aez, P., Hariharan, B., Gu, C., Gupta, S., Malik, J.: Semantic segmentation using regions and parts. In: CVPR (2012)
3. Boix, X., Gonfaus, J.M., van de Weijer, J., Bagdanov, A.D., Serrat, J., Gonz`alez, J.: Harmony potentials. IJCV 96(1) (2012)
4. Bourdev, L., Maji, S., Brox, T., Malik, J.: Detecting people using mutually consistent poselet activations. In: Daniilidis, K., Maragos, P., Paragios, N. (eds.) ECCV 2010, Part VI. LNCS, vol. 6316, pp. 168–181. Springer, Heidelberg (2010)
5. Carreira, J., Caseiro, R., Batista, J., Sminchisescu, C.: Semantic segmentation with second-order pooling. In: Fitzgibbon, A., Lazebnik, S., Perona, P., Sato, Y., Schmid, C. (eds.) ECCV 2012, Part VII. LNCS, vol. 7578, pp. 430–443. Springer, Heidelberg (2012)
6. Carreira, J., Sminchisescu, C.: Constrained parametric min-cuts for automatic object segmentation. In: CVPR (2010)
7. Dai, Q., Hoiem, D.: Learning to localize detected objects. In: CVPR (2012)
8. Dalal, N., Triggs, B.: Histograms of oriented gradients for human detection. In:
CVPR (2005)
9. Deng, J., Berg, A., Satheesh, S., Su, H., Khosla, A., Fei-Fei, L.: ImageNet Large
Scale Visual Recognition Competition 2012 (ILSVRC 2012) (2012), http://www.image-net.org/challenges/LSVRC/2012/
10. Donahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E., Darrell, T.:
Decaf: A deep convolutional activation feature for generic visual recognition. arXiv preprint arXiv:1310.1531 (2013)
11. Everingham, M., Van Gool, L., Williams, C.K.I., Winn, J., Zisserman, A.: The Pascal Visual Object Classes (VOC) Challenge. IJCV 88(2) (2010)
12. Farabet, C., Couprie, C., Najman, L., LeCun, Y.: Learning hierarchical features for scene labeling. TPAMI 35(8) (2013)
13. Felzenszwalb, P.F., Girshick, R.B., McAllester, D., Ramanan, D.: Object detection with discriminatively trained part-based models. TPAMI 32(9) (2010)
14. Fidler, S., Mottaghi, R., Yuille, A., Urtasun, R.: Bottom-up segmentation for topdown detection. In: CVPR (2013)
15. Fukushima, K.: Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. Biological Cybernetics
16. Girshick, R., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for accurate object detection and semantic segmentation. In: CVPR (2014)
17. Hariharan, B., Arbelaez, P., Bourdev, L., Maji, S., Malik, J.: Semantic contours from inverse detectors. In: ICCV (2011)
18. Hoiem, D., Chodpathumwan, Y., Dai, Q.: Diagnosing error in object detectors. In:
Fitzgibbon, A., Lazebnik, S., Perona, P., Sato, Y., Schmid, C. (eds.) ECCV 2012, Part III. LNCS, vol. 7574, pp. 340–353. Springer, Heidelberg (2012)
19. Jia, Y.: Caffe: An open source convolutional architecture for fast feature embedding(2013), http://caffe.berkeleyvision.org/
20. Kim, B.-S., Sun, M., Kohli, P., Savarese, S.: Relating things and stuff by high-order potential modeling. In: Fusiello, A., Murino, V., Cucchiara, R. (eds.) ECCV 2012
Ws/Demos, Part III. LNCS, vol. 7585, pp. 293–304. Springer, Heidelberg (2012)
B. Hariharan et al.
21. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep convolutional neural networks. In: NIPS (2012)
22. Ladick´y, L., Sturgess, P., Alahari, K., Russell, C., Torr, P.H.S.: What, where and how many? Combining object detectors and CRFs. In: Daniilidis, K., Maragos, P., Paragios, N. (eds.) ECCV 2010, Part IV. LNCS, vol. 6314, pp. 424–437. Springer, Heidelberg (2010)
23. LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W., Jackel, L.D.: Backpropagation applied to handwritten zip code recognition. Neural
Computation 1(4) (1989)
24. Lowe, D.G.: Distinctive image features from scale-invariant keypoints. IJCV 60(2)
25. Mottaghi, R.: Augmenting deformable part models with irregular-shaped object patches. In: CVPR (2012)
26. Parkhi, O.M., Vedaldi, A., Jawahar, C., Zisserman, A.: The truth about cats and dogs. In: ICCV (2011)
27. van de Sande, K.E., Uijlings, J.R., Gevers, T., Smeulders, A.W.: Segmentation as selective search for object recognition. In: ICCV (2011)
28. Sermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., LeCun, Y.: Overfeat:
Integrated recognition, localization and detection using convolutional networks. In:
ICLR (2014)
29. Sermanet, P., Kavukcuoglu, K., Chintala, S., LeCun, Y.: Pedestrian detection with unsupervised multi-stage feature learning. In: CVPR (2013)
30. Shotton, J., Winn, J.M., Rother, C., Criminisi, A.: TextonBoost: Joint appearance, shape and context modeling for multi-class object recognition and segmentation.
In: Leonardis, A., Bischof, H., Pinz, A. (eds.) ECCV 2006, Part I. LNCS, vol. 3951, pp. 1–15. Springer, Heidelberg (2006)
31. Tighe, J., Niethammer, M., Lazebnik, S.: Scene parsing with object instances and occlusion handling. In: ECCV (2010)
32. Yang, Y., Hallman, S., Ramanan, D., Fowlkes, C.C.: Layered object models for image segmentation. TPAMI 34(9) (2012)