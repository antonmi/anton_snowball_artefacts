Transitive Transfer Learning
Ben Tan
Department of Computer
Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, China btan@cse.ust.hk
Yangqiu Song
Department of Computer
Science, University of Illinois at Urbana-Champaign, USA yqsong@illinois.edu
Erheng Zhong
Personalization Sciences, Yahoo Labs, Sunnyvale, USA erheng@yahoo-inc.com
Qiang Yang
Department of Computer
Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, China qyang@cse.ust.hk
ABSTRACT
Transfer learning, which leverages knowledge from source domains to enhance learning ability in a target domain, has been proven effective in various applications. A major limitation of transfer learning is that the source and target domains should be directly related. If there is little overlap between the two domains, performing knowledge transfer between these domains will not be effective. Inspired by human transitive inference and learning ability, whereby two seemingly unrelated concepts can be connected by series of intermediate bridges using auxiliary concepts, in this paper we study a novel learning problem: Transitive Transfer Learning (abbreviated to TTL). TTL is aimed at breaking the large domain distances and transferring knowledge even when the source and target domains share few factors directly. For example, when the source and target domains are text and images respectively, TTL can use some annotated images as the intermediate domain to bridge them. To solve the TTL problem, we propose a framework wherein we first select one or more domains to act as a bridge between the source and target domains to enable transfer learning, and then perform the transferring of knowledge via this bridge. Extensive empirical evidence shows that the framework yields state-of-the-art classification accuracies on several classification data sets.
Categories and Subject Descriptors
H.2.8 [Database Management]: Database Applications - Data Mining
General Terms
Machine Learning
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org.
KDD '15, August 11 - 14, 2015, NSW, Australia
Copyright 2015 ACM. ISBN 978-1-4503-3664-2/15/08...$15.00
DOI: http://dx.doi.org/10.1145/2783258.2783295.
Figure 1: An pictorial illustration of the transitive transfer learning problem. In TTL, the source and target domains have few common factors, but they can be connected by intermediate domains through some underlying factors.
Keywords
Transfer Learning, Transitive Transfer Learning, Nonnegative Matrix Tri-factorizations
INTRODUCTION
Transfer learning, which aims to borrow knowledge from source domains to help the learning in a target domain, has been established as one of the most important machine learning paradigms.
Various algorithms have been widely used and proven effective in many applications, for example, classification, reinforcement learning and recommendation systems, and so on. A critical requirement for successful transfer learning is that the source and target domains should be related. This relation can be in the form of related instances, features or models. If no direct relation can be found, forcibly transferring knowledge will not work. In the worst case, it could lead to having no improvement, or even worse performance, in the target domain. This is one of the major limitations of traditional transfer learning. However, as human beings, we naturally have the ability to carry out inference and learning via transitivity. This ability helps humans connect many concepts and transfer knowledge between two seemingly unrelated concepts by introducing a few intermediate concepts as a bridge.
For example, after taking a class in elementary computer science, 1155 we may find it easier to transfer the knowledge to theoretical computer science if we have taken an applied algorithm design course in between, since the algorithm course may involve both concepts in programming and theory. Likewise, having learned some basic math, we may find it impossible to directly take a course in convex optimization. However, this becomes feasible when we take an intermediate course in linear algebra and probability. The linear algebra and probability course serves as the intermediate domain for knowledge transfer.
Human ability to conduct transitive inference and learning inspires us to study a novel learning problem known as Transitive
Transfer Learning (TTL). As illustrated in Figure 1, in TTL, the source and target domains have few common factors, but they can be connected by intermediate domains through some underlining factors. We expect TTL to have wide practical applications. For example, when the source domain is composed of text documents and the target domain contains image data, they share no overlap feature spaces, knowledge learned in text documents can hardly be transferred to images. However, TTL can introduce some annotated images to learn a feature mapping between these two different feature spaces and have a smooth knowledge transfer. In other applications, such as text sentiment classification, all the data have the same feature space, but two group of data may have large distribution gap, TTL can introduce some auxiliary intermediate data to form a transitive knowledge transfer structure with which we can obtain a more versatile sentiment classification system.
In this paper, we propose a learning framework for the TTL problem. The framework is composed of two steps. The first step is to find an appropriate domain to bridge the given source and target domains. The second step is to do effective knowledge transfer among all domains. In the first step, we propose a probability model to select appropriate domains that is able to draw the source and target domains closer, based on domain characteristics such as domain difficulty and pairwise closeness. As data from different domains are collected from different data sources, each pair of domains may have distribution shift. In the second step, considering both of the domain relationship and distribution shift, we propose a transfer learning algorithm that allows to learn overlap features among domains and propagate label information through them. A high-level description of the TTL framework is summarized in Table 1. We give a formal definition of the TTL problem in Section 2, and describe the technical details of these two steps in Sections 3 and 4 respectively.
PROBLEM DEFINITION
In the problem, we have labeled source domain data S={(xs i, yi)}ns i=1, unlabeled target domain data T ={xt i}nt i=1, and k unlabeled intermediate domains Dj = {x dj i } nj i=1, j = 1,..., k, x∗ ∈
Rm∗ is a m∗ dimensional feature vector. The data from different domains could have different dimensions. S and T have a large distribution gap, thus directly transferring knowledge between them may cause a substantial performance loss in the target domain.
The TTL framework is aimed at finding intermediate domain(s) to bridge S and T, and minimizing the performance loss in T.
Formally, given a domain distribution gap measure g(·, ·), the first step is to find an intermediate domain that satisfies g(S, T |Di)
< g(S, T ). The second step performs transfer learning from the source domain S to target domain T via intermediate domain Di; this is implemented via learning two feature clustering functions psd(S, Di) and pdt(Di, T ), such that the distribution gap of data on common feature clusters selected by psd(S, Di) and pdt(Di, T ) are further reduced. The label information in the source domain is Table 1: The TTL Framework
Input: The source,target and candidate intermediate domains
Step 1: Intermediate domain selection (see Section 3)
Step 2: Transitive knowledge transfer (see Section 4)
Output: Prediction results in the target domain propagated to the intermediate and target data on the selected common feature clusters.
INTERMEDIATE DOMAIN SELECTION
Intermediate domain selection is problem specific, different problems may have different strategies. For example, when the source domain is composed of text data and the target domain is image data, one can crawl some annotated images from Flickr as the intermediate domain data. In other problems when there are multiple candidate intermediate domains, one should propose some selection algorithms according to domain properties. In this paper, we propose an algorithm for text sentiment classification problem as an example. As studied by previous research, domain difficulty and domain distance are two major factors that affect the transfer learning performance between two domains. On one hand, intuitively, if the source domain is less difficult than the intermediate and target domains, the model learned from the source data is highly predictive and is very likely to achieve high performance on the intermediate and target domains as well. On the other hand, if the intermediate domain is able to draw closer the source and target domains than their original distance, then the knowledge transfer process between the source and target domains will have less information loss. Hence, in this paper, we introduce domain complexity and A-distance to estimate domain difficulty and pairwise domain distance respectively. We summarize these measures as follows:
• Domain complexity: domain difficulty measure is problem specific, as different problem may have different feature types.
In this paper, we choose domain complexity to measure the difficulty. The domain complexity is calculated as the percentage of long tail features that have low frequency.
These long tail features bring in long tail feature distribution and significant feature diversity, thus make automatic machine learning difficult. We calculate the domain complexity as follows: cplx(D) = |{x|c(x) < t × n}| m
For non-negative features, c(x) is the number of instances whose feature x is larger than zero. |{x|c(x) < t × n}| is the number of features that appear in less than t×n instances.
In this paper, we measure the domain complexity as the percentage of long tail features that appear in less than 10% instances. For continuous features, we can measure their relative entropy as domain difficulty.
• A-distance: The A-distance estimates the distribution difference of two sets of data samples that are drawn from two probability distributions. Practically, given two sets of domain data Di and Dj, we can calculate the A-distance as follows: disA(Di, Dj) = 2(1 − 2 min h∈H error(h|Di, Dj)), Table 2: Domain characteristic features feature description cplx_src (c1) source domain complexity cplx_inter (c2) intermediate domain complexity cplx_tar (c3) target domain complexity dissi
A (c4) a_distance between source and intermediate disst
A(c5) a_distance between source and target disit
A (c6) a_distance between intermediate and target
H is a hypothesis space, h is the optimal proxy classifier that discriminates data points from different domains. In this paper, we first assign the source data positive labels, and target data negative ones, then use logistic regression as the proxy classifier to estimate the error(h|Di, Dj)) in A-distance.
In, the authors have been proven that the prediction error of the target domain is bounded by the error of the source domain, the A-distance and some other constant factor.
Given a triple tr = {S, D, T }, we can extract six features as described in Table 2. The first three features summarize individual in-domain characteristics, the last three features capture the pairwise cross domain distances. These features together affect the success probability of a transfer learning algorithm. However, it is impossible to design a universal domain selection criteria, as different problems may have different preferences (weights) on these features. To model the success probability of the introduced intermediate domain, we propose the following logistic function: f(tr) = δ(β0 +
� i=1 βici), (3) where δ(x) =
1+e−x. We estimate the parameters β = {β0, · · ·, β6} to maximize the log likelihood defined as:
L(β) = t
� i=1 l(i) log f(tri) + (1 − l(i)) log(1 − f(tri)), (4) l(i) is a binary label, indicating whether the intermediate domain in the ith triple is able to bridge the source and target domains.
We get the label by the following strategy. We perform a semisupervised label propagation algorithm with input S and T, and obtain a prediction accuracy accst on the target domain. We also perform the same algorithm with input {S, D, T }, and obtain another accuracy accsit on the target domain. If accsit > accst, we set l(i) = 1, otherwise, l(i) = 0. The label is determined by both the domain characteristics and the propagation model. A sophisticated model may accept more intermediate domains than a simple model. In this paper, we prefer to use a simple model such as KNN that are able to provide us strictly fitted candidates.
We transform the intermediate domain selection problem to a probability estimation problem. A candidate intermediate domain with high f(tr) is more likely to be selected.
TRANSITIVE KNOWLEDGE TRANSFER
In the first step, an intermediate domain that can bridge the source and target domains has been selected, however, there is still distribution shift among these domains. Thus, in the second step of the TTL framework, we propose a novel transfer learning algorithm that considers both of the transitive relationship and distribution shift among all the domains. The algorithm is based on nonnegative matrix tri-factorization that can perform feature clustering and label propagation simultaneously, so we first give some background knowledge.
Non-negative Matrix Tri-factorization
Non-negative Matrix Tri-factorization (NMTF) is a popular and effective technique for data clustering and classification. In
NMTF, the feature-instance matrix is decomposed into three submatrices. In general, given a feature-instance matrix X ∈ Rm×n, m is the number of dimensions, n is the number of instances. One can obtain the factorized sub-matrices by solving the optimization problem as follows: arg minF,A,GT
L = ||X − FAGT ||, (5) where || · || denotes the Frobenius norm of matrix.
The matrix F ∈ Rm×p indicates the information of feature clusters and p is the number of hidden feature clusters. The element
Fi,j indicates the probability that the ith feature belongs to the jth feature cluster.
The matrix G ∈ Rc×n is the instance cluster assignment matrix and c is the number of instance clusters. If the largest element of the ith row is located in the jth column, it means that the ith instance belongs to the jth instance cluster. In the classification problem, each instance cluster can be regarded as a label class.
A ∈ Rp×c is the association matrix. c is the number of instance clusters or label classes, for the binary classification problem c = 2.
The element Ai,j is the probability that the ith feature cluster is associated with the jth instance cluster.
NMTF for Transfer Learning
NMTF is also used as a basic technique for transfer learning algorithms. Given the source and target domains S and T, Xs and Xt are their feature-instance matrices respectively, one can decompose these two matrices simultaneously, and allow the decomposed matrices share some cross-domain information (sub-matrices). Formally, given two related domains S and T, their feature-instance matrices can be decomposed simultaneously as follows:
LST = ||Xs − FsAsGs|| + ||Xt − FtAtGt||
=
����Xs − [F 1, F 2 s ]
�
A1
A2 s
�
GT s
���� +
����Xt − [F 1, F 2 t ]
�
A1
A2 t
�
GT t
����, (6) where F 1 ∈ Rm×p1
+ and A1 ∈ Rp1×c
+ contain the common factors shared by the source and target domains. F 2 s, F 2 t ∈ Rm×p2
+ and A2 s, A2 t ∈ Rp2×n
+ contain domain-specific information. They are not shared by domains. p1, p2 are two parameters that indicate the number of hidden feature clusters. Gs ∈ Rn×c is the label class matrix and generated from the instance labels {yi|i = 1, · · ·, n} of the source domain S. If the ith instance belongs to jth class, then the (i, j) element in Gs equals to 1, otherwise, it equals to 0.
Gs is a constant matrix and keeps unchanged during the factorization process. Gt is the label class matrix of the target domain. Its elements are variables that we want to learn by the matrix decomposition.
From Eq. (6), we can notice that the label information of the source domain is propagated to the target domain through the shared common factors F1 and A1.
The TTL Transfer Learning Algorithm
As shown in Figure 1, the source, intermediate and target domains have a transitive relationship. In other words, the intermediate domain bridges the source and target domains, but has different common factors to them respectively. Hence, to capture these properties, we propose a coupled NMTF algorithm. The proposed
Figure 2: An illustration of the proposed transfer learning algorithm in the TTL framework.
The algorithm learns two coupled feature representations by feature clustering, and then propagates the label information from the source to the target domain through the intermediate domain on the coupled feature representation. transfer learning algorithm is illustrated in Figure 2, and written in Eq. (7)
L = ||Xs − FsAsGT s || + ||XI − FIAIGT
I ||+
||XI − F
′
IA
′
IGT
I || + ||Xt − FtAtGT t ||
=
����Xs − [ ˆF 1, ˆF 2 s ]
� ˆA1
ˆA2 s
�
GT s
���� +
����XI − [ ˆF 1, ˆF 2
I ]
� ˆA1
ˆA2
I
�
GT
I
���� +
����XI − [ ˜F 1, ˜F 2
I ]
� ˜A1
˜A2
I
�
GT
I
���� +
����Xt − [ ˜F 1, ˜F 2 t ]
� ˜A1
˜A2 t
�
GT t
����.
From the above equation, we can see that the first two terms (
||Xs − FsAsGT s || + ||XI − FIAIGT
I ||) refer to the first feature clustering and label propagation between the source and intermediate domains in Figure 2, the last two terms refer to the second feature clustering and label propagation between the intermediate and target domains. In Eq. (7), it is worth noting that we decompose
XI twice with different decomposition matrices, since XI shares different knowledge with Xs and Xt respectively. At the same time, we couple these two decomposition processes together by the label matrix GI. It is reasonable that the instances in the intermediate domain should have the same labels in different decomposition processes. Moreover, if we solve the matrix decomposition by iterative algorithms, in every iteration, each decomposition process is able to consider the feedbacks from the other decomposition.
If these two processes are separately solved, the first decomposition process will not consider the results from the second one, and may suffer from the bias problem. In the experiment, we find that the coupled strategy achieves better performance than separated decomposition.
Overall, the proposed learning algorithm fits the transitive relationship among domains. The label information in the source domain is transferred through ˆF1 and ˆA1 to the intermediate domain, and affects the learning results of GI. The knowledge on class labels incorporated with GI from the intermediate domain is further transferred to the target domain through ˜F1 and ˜A1.
As we discussed in Section 4.1, the decomposed matrix F contains the information on hidden feature clusters, indicating the distribution of features on each hidden cluster. Therefore, the summation of each column of F has to be equal to one. The label matrix
G indicates the label distribution of each instance. Thus, the summation of each row of G has to be equal to one. Considering these matrix constrains, we obtain the final optimization objective function for the proposed learning algorithm: arg minFs,As,FI,AI,GI,F ′
I,A′
I,Ft,At,Gt
L s.t.
�m i=1 ˆF 1(i, j) = 1, �m i=1 ˆF 2 s (i, j) = 1, �m i=1 ˆF 2
I (i, j) = 1, �m i=1 ˜F 1(i, j) = 1, �m i=1 ˜F 2
I (i, j) = 1, �m i=1 ˜F 2 t (i, j) = 1, �c j=1 GI(i, j) = 1
�c j=1 Gt(i, j) = 1.
Since the objective function in Eq. (8) is non-convex, it is intractable to obtain the global optimal solution. Therefore, we develop an alternating optimization algorithm to achieve the local optimal solution. We first show the updating rules of matrices ˜F 1, ˜F 2
I, ˜F 2 t, and Gt. We summarize the notations of matrix multiplications in Table 3, and show the updating rules as follows:
˜F 1(i, j) = ˜F 1(i, j) ×
�
[ ˜
M1
I+ ˜
M1 t ](i,j)
[ ˜
T 1
I + ˜
T 1 t ](i,j), ˜F 2
I (i, j) = ˜F 2
I (i, j) ×
�
˜
M2
I(i,j)
˜
T 2
I (s,t), ˜F 2 t (i, j) = ˜F 2 t (i, j) ×
�
˜
M2 t (i,j)
˜
T 2 t (s,t), Gt(i, j) = Gt(i, j) ×
�
[XT t FtAt](i,j)
[GtAT t F T t FtAt](i,j).
From Eq. (8), after the matrices are updated, the constrained matrices have to be normalized as:
˜F 1(i, j) =
˜
F 1(i,j)
�m i=1 ˜
F 1(i,j), ˜F 2
I (i, j) =
˜
F 2
I (i,j)
�m i=1 ˜
F 2
I (i,j), ˜F 2 t (i, j) =
˜
F 2 t (i,j)
�m i=1 ˜
F 2 t (i,j), Gt(i, j) =
Gt(i,j)
�c j=1 Gt(i,j).
The updating rules and normalization methods for other submatrices are similar and are shown in the Appendix.
We need not update Gs, which contains the ground-truth label information.
We give the procedure of the proposed learning algorithm in Algorithm 1. As shown in Eq. (7) and the Appendix section, the updating rule for GI is constrained by FI, F ′
I, AI and A′
I. In addition, the sub-matrices ˆF 1, ˆA1 and, ˜F 1, ˜A1 are constrained by Xs, Gs and Xt, Gt respectively. Therefore, the updating rule of Gt is transitively constrained by Xs, Gs and, the discriminative information in the source domain is transitively transferred to the target domain.
The updating processes of Fs, FI, F
′
I and Ft refer to the feature clusterings in Figure 2. The updating processes of GI and Gt refer to the label propagations in Figure 2.
We analyze the convergence property of Eq. (9) with normalization rules in Eq. (10). We first analyze the convergence of ˜F 1 with the rest of the parameters fixed. By using the properties of trace operation and frobenius norm ||X||2 = tr(XT X) = tr(XXT ), we re-formulate the objective function Eq. (8) as a Lagrangian function and keep the terms related to ˜F 1:
Table 3: Notations of matrix multiplications
ˆ
M1
I = XIGI ˆA1T
ˆ
M2
I = XIGI ˆA2T
I
ˆ
M1 t = XtGt ˆA1T
ˆ
M2 t = XtGt ˆA2T t
ˆ
NI = ˆF 1 ˆA1GT
I + ˆF 2
I ˆA2
IGT
I
ˆ
Nt = ˆF 1 ˆA1GT t + ˆF 2 t ˆA2 tGT t
ˆT 1
I = ˆ
NIGI ˆA1T
ˆT 1 t = ˆ
NtGt ˆA1T
ˆT 2
I = ˆ
NIGI ˆA2T
I
ˆT 2 t = ˆ
NtGt ˆA2T t
Ft = [ ˆF1 ˆF 2 t ]
At = [ ˆA1 ˆA2 t]
Algorithm 1 The TTL Transfer Learning Algorithm
1: Input: Source, target, intermediate domains S, T and D, the parameters p, and the number of iterations Itermax.
2: Initialize the matrices Fs, As, FI, AI, GI, Ft, At, Gt.
3: while iter < Itermax do
Update the sub-matrices of Fs, As, FI, AI, Ft, At and label matrices GI, Gt according to the updating rules given in Eq. (9) and Eq. (12) of the Appendix section.
Normalize the sub-matrices of Fs, FI, Ft, and label matrices GI, Gt according to the normalization rules given in Eq. (10) and Eq. (13) of the Appendix.
6: end while
7: Output: the predicted results of Gt.
L( ˜F 1) = tr(−2XT
I ˜F 1 ˜A1GT
I + 2GI ˜A1T ˜F 1T ˜
NI)
+tr(−2XT t ˜F 1 ˜A1GT t + 2GT ˜A1T ˜F 1T ˜
Nt)
+tr[λ( ˜F 1T 1m1T m ˜F 1 − 21p1T m ˜F 1)], (11) where λ ∈ Rp×p is a diagonal matrix. 1m and 1p are all-ones vectors with dimension m and p respectively.
LEMMA 1. Using the update rule in Eq. (9) and normalization rules in Eq. (10), the loss function in Eq. (11) will monotonously decrease.
The proof of Lemma 1 is shown in the Appendix. The convergence of other terms can be proven in the same way. According to the convergence analysis on the update rules and the multiplicative update rules, each update step in Algorithm 1 will not increase
Eq. (8). The objective has a lower bounded by zero. The convergence of the proposed transfer learning algorithm is proven.
EXPERIMENTS
In this section, we perform three tests. The first test is designed to analyze how the intermediate domain and model parameters affect the performance of the TTL framework, and to evaluate the convergence rate empirically. This is done by conducting experiments on six synthetic text classification tasks generated from the 20Newsgroups data set 1.
The second test is designed to evaluate the TTL framework when the source and target domain data have completely different structures. The experiments are conducted on the text-to-image data set.
The intermediate domains for all tasks in the data set are crawled from Flicker.
1http://qwone.com/~jason/20Newsgroups/
Finally, the third test is designed to test the efficiency of the intermediate domain selection algorithm and the transfer learning algorithm in the framework. The experiments are conducted on some text sentiment classification tasks 2. The data from different domains have the same feature space but different distribution. Moreover, there are many candidate intermediate domains for each pair of source and target domains.
Baseline methods
In the synthetic text classification and sentiment classification tasks, all the data have the same feature space. We compare the proposed framework with three baseline methods to verify the effectiveness.
The first baseline is SVM, which is a classical supervised learning algorithm. We use the linear kernel of SVM with the implementation in LibLinear3. The second one is the triplex transfer learning(TriplexTL) algorithm, which is a state-of-the-art transfer learning method implemented with NMTF. The other transfer learning algorithm is LatentMap, which is also a state-of-the-art transfer learning algorithm. It draws the joint distribution of two domains closer by mapping the data to a low dimensional latent space. The three baseline methods are tested under two different settings. The first one is direct-transfer. We train the learners based on the labeled data in the source domain and test them directly on the data in the target domain. We use subscript ST to indicate the methods under this setting in the following experiments, for example, TriplexTLST and LMST. The second setting is a 2-stage transfer learning process. We first apply TriplexTL/LM between the source and the intermediate domain to predict the intermediate domain labels, and then again apply TriplexTL/LM between the intermediate domain and the target domain. The major difference between this naive transitive transfer learning strategy and the proposed transfer learning algorithm is that no iterative feature clustering and label propagation is performed. We use subscript SIT to represent methods under this setting, for instance, TriplexTLSIT and LMSIT.
In the text-to-image data set, the data have different feature spaces.
The above mentioned baselines cannot handle these data. Hence, we compare TTL with two heterogeneous transfer learning (HTL) algorithms.
The first baseline is co-transfer. It models the problem as a coupled Markov chain with restart. The transition probabilities of the Markov chain is construdture by using the intra-relationship based on affinity metric among data in the source and target domains, and the inter-relationship between the source and target domains based on co-occurrence information of the intermediate do2http://www.cs.jhu.edu/~mdredze/datasets/ sentiment/
3http://www.csie.ntu.edu.tw/~cjlin/ liblinear/
Figure 3: The problem setting of the 20Newsgroup data set. main. The second one is HTLIC 4. It learns a new target feature representation by using data from the source, intermediate and target domain data via the collective matrix factorization technique.
A SVM classifier is then learned on the new target feature representation.
All methods in the experiments are performed ten times, and we report their average performances and variances.
Synthetic text classification tasks
20Newsgroups Data Set
The 20Newsgroups is a hierarchical text collection, containing some top categories like 'comp', 'sci', 'rec' and 'talk'. Each category has some sub-categories, such as 'sci.crypt' and 'sci.med'.
We use four main categories to generate six tasks, in each of which two top categories are chosen for generating binary categorization.
With a hierarchical structure, for each category, all of the subcategories are then organized into three parts, where each part has different subcategories and is of a different distribution. Therefore, they can be treated as the source, intermediate and target domains, respectively. To generate the transitive transfer learning setting, we divide the vocabularies into two separated subsets Set A and Set B.
Then, we set the term frequencies of words in Set A of the source domain to zero. Similarly, we set the term frequencies of words in Set B of the target domain to zero. Therefore, the source and target domains have no overlapping words. The problem setting on this data set is illustrated in Figure 3, where the blocks with texture indicate that the features have values. We can see that the source and target domains have no shared features, but they have shared features with the intermediate domain, respectively. Apparently, the intermediate domains here can bridge the generated source and target domains. We give a detailed description of the six tasks in Table 4. The feature dimensions in these tasks range from 2405 to
5984. The number of instances in these tasks are around 7000.
Performance on synthetic tasks
In experiments, we compare the proposed framework with the baseline methods on six text classification tasks.
The text classification tasks are very challenging. The source and target domains have no overlapping features. The SVM classifiers trained with labeled source data have almost no discriminative ability on the target data. From the results in Table 5, we can see that the SVMST classifiers obtain a very bad performance.
Likewise, the source classifiers can barely be adapted for the target domain data. Hence, TriplexTLST and LMST obtain bad performance also, but better than SVMST. The naive transfer learn4http://www.cse.ust.hk/~yinz/htl4ic.zip
# of labeled data
TTL
TriplexTLIT(a) vary # of labeled data
# of removed feature : d
Accuracy
TTL
TriplexTLSIT(b) remove d features
Figure 4: Performance with different intermediate domains. ing algorithms, TriplexTLSIT and LMSIT, achieve relative good performance, because they use the intermediate domain data as a bridge to perform a 2-stage knowledge transfer. The proposed TTL framework achieves the best performance. This can be ascribed to the reason that TTL not only bridges the source and target domains by using the intermediate domain data, but also has iterative feature clustering and label propagation loops where the knowledge provided by the source domain can be deeply reshaped and reorganized to be exploited for the target domain.
Performance with different intermediate domains
The intermediate domain plays an important role in bridging the source and target domains. Hence, we also conduct some experiments on the "comp-vs-talk" task to test the proposed TTL framework when 1) the amount of labeled intermediate data increases;
2) the connection between the source/target and the intermediate domains becomes weaker.
In the first setting, we compare TTL with TriplexTLIT that transfers knowledge from labeled intermediate domain data to the target data. We vary the amount of labeled intermediate data from 50 to
400. We randomly sample the labeled intermediate domain data ten times, and show the average performance and variance in Figure 4(a). From the results, we can see that the performance of TTL is better than TriplexTLIT when the amount of labeled intermediate domain data is small. However, when there is a large amount of labeled intermediate data, the performance of TriplexTLIT is better. The results are reasonable, because when we have large amount of data that are near and adaptable to the target data, we need not seek help from domains that are far away.
In the second setting, some overlap features in the intermediate domain are removed. We compare the TTL framework with
TriplexTLSIT. In each comparison experiment, we randomly remove d features ten times, and show the average performance and its variance in Figure 4(b). From the results we can see that the performance decreases as features are removed. The reason is that the connection between the intermediate and source/target domain becomes weaker when more features are removed.
Model Analysis
In the Appendix, we have theoretically proven the convergence of the transfer learning algorithm in the TTL framework. Here we test the convergence rate. We conduct an experiment on "compvs-talk" task, and set the number of iterations to 100. We show the objective value of Eq. (8) as the dashed line in Figure 5(a), and see that after around five to ten iterations, the objective value experiences almost no change. Similarly, we show the classification accuracy of the target domain of each iteration as the solid line in Table 4: Dataset Description
Task
Source
Intermediate
Target rec-vs-comp autos : misc baseball : mac hockey : windows rec-vs-talk autos : guns motorcycles : mideast hockey : misc rec-vs-sci autos : electronics motorcycles : med hockey : space sci-vs-comp electronics : graphics med : misc space : windows sci-vs-talk crypt : guns electronics : mideast med : misc comp-vs-talk graphics : guns misc : mideast windows : politic
Table 5: Accuracy (%) on the synthetic text classification tasks
SVMST
TriplexTLST
TriplexTLSIT
LMST
LMSIT
TTL rec-vs-comp
53.04 ± 1.87
56.74 ± 4.95
52.23 ± 2.97
55.34 ± 3.75
57.91 ± 3.27 rec-vs-talk
59.41 ± 9.74
61.67 ± 7.93
60.11 ± 7.22
60.97 ± 6.53
68.77 ± 1.61 rec-vs-sci
51.64 ± 1.44
51.95 ± 1.70
50.89 ± 2.13
51.23 ± 1.56
51.95 ± 0.98 sci-vs-comp
52.14 ± 2.65
55.93 ± 2.39
53.26 ± 2.95
55.29 ± 2.76
56.26 ± 2.14 sci-vs-talk
51.57 ± 2.07
52.80 ± 1.66
50.98 ± 2.14
52.69 ± 1.73
53.15 ± 1.53 comp-vs-talk
60.90 ± 9.35
64.08 ± 10.19
61.34 ± 9.73
64.58 ± 9.67
72.22 ± 3.20
Iteration
Objective Value
Accuracy(a) Convergence
The parameter p in TTL
Accuracy(b) Varying the value of p
Figure 5: Convergence analysis and model parameter analysis.
Figure 5(a). The results show that there is no change in the performance after 60-80 iterations. The convergence trends on other tasks are similar.
We also analyze the model parameter p. We vary p from 5 to
100 to test how it affects the classification performance. The experiments are also conducted on "comp-vs-talk" task. The results are shown in Figure 5(b), from which we can see that the algorithm achieves better performance when p is between 20 and 40. For different tasks, we can use ten-fold cross validation to choose the value. In this paper, we simply set p to be 30 in the experiments.
Text-to-image classification tasks
NUS-WISE data set
The NUS-WISE data set for heterogeneous transfer learning problem is generated by. It contains 45 text-to-image tasks. Each task is composed of 1200 text documents, 600 images, and 1600 co-occurred text-image pairs. The data in each task are about two different categories, such as "boat" and "flower". Therefore, we can do binary classification for each task. There are 10 categories in the data set, including "bird", "boat", "flower", "food", "rock", "sun", "tower", "toy", "tree" and "car". The text vocabulary size is 500. Each text data is represented by a 500 dimensional bagof-word vector. For image data, we extract SIFT features and represent each image in a 512 dimensional feature vector. In this
No. of labeled target data
Avg. Accuracy
TTL co−transfer
HTLIC
SVM(a) Average performance
Task
Classification Accuracy
TTL co−transfer
HTLIC
SVM(b) Detailed performance
Figure 6: The classification accuracy on the tasks of the textto-image data set. data set, our task is to transfer knowledge from source text documents to images through co-occurred text-image pairs.
Performance on text-to-image tasks
As HTLIC needs some labeled target domain data to train the SVM classifier, in the text-to-image tasks, we assume all the source domain data and a few target domain data are labeled. We vary the amount of labeled data in the target domain from 5 to 25, and show the average classification accuracies of all the tasks in Fig. 6(a), from which we can see that the performance of each algorithm increases when more labeled target data are used. We can also find that SVM achieves the worst performance, since it considers no auxiliary information. HTLIC and co-transfer achieve better performance than SVM, since they successfully leverage some knowledge from the source domain by using the intermediate domain data. The proposed TTL framework obtains the best performance.
The reason is that TTL takes the distribution shift between three domains into account and explicitly exploits the transitively shared knowledge for label propagation from the source to the target domain.
We also report the detailed results on each individual task with
25 labeled target domain data. The classification accuracies and variances on each task are shown in Fig. 6(b). The x-axis indicates
1161 the task and the y-axis represents the classification accuracy. We sort the tasks by the performance of the proposed TTL framework in ascend order. From the results, we can find that TTL is superior to other algorithms on most tasks and is always at the top. In addition, TTL is more stable than other algorithms.
Sentiment classification tasks
Sentiment Classification Data set
The sentiment classification data set used in our experiment consist of Amazon product reviews on 12 different categories, including "Apparel", "Books", "Camera_&_photo", "DVD", "Electronics", "Health_&_personal_care", "Kitchen_&_housewares", "Music", "Sports_&_outdoors", "Toys_&_games" and "Video". Each product review consists of review text and a sentiment label. The data from different domains have different distributions. For example, reviews in "Kitchen_&_housewares" may have adjectives such as "malfunctioning", "reliable" and "sturdy". However, reviews in the "DVD" domain may have "thrilling", "horrific" and "hilarious". In this data set, the data within each domain are balanced. One half of the data are positive reviews and the other half are negative. The data size in each domain ranges from 2,000 to
20,000. The vocabulary size for each domain is around 20,000. We randomly sample around 2,000 instances for each domain. From the 12 domains, we can generate P 3
12=1,320 triples, such as <"Apparel", "Books", "Camera_&_photo"> where "Apparel", "Books" and "Camera_&_photo" are the source, intermediate and target domains respectively. We conduct experiments on all the 1320 triple to evaluate the performance of the proposed intermediate domain selection algorithm. We also conduct experiments on triples that are selected by the intermediate domain selection algorithm to test the proposed transfer learning algorithm in the TTL framework.
Intermediate Domain Selection
In order to evaluate the proposed intermediate domain selection algorithm, we propagate labels from the labeled source domain data to the unlabeled target domain data, and evaluate the prediction accuracy accst on the target domain data. We also propagate labels from the labeled source domain data to the unlabeled intermediate and target domain data by the same algorithm, and evaluate the prediction accuracy accsit on the target domain data. In the experiment, we use semi-supervised learning with RBF kernel to do label propagation. If accsit > t × accst, (t > 1.0), it means that the intermediate domain data are able to bridge the source and target domain, and we assign a positive label to the triple. Otherwise, we assign a negative label. In the experiment, we set t = 1.03, and get 102 positive labels among 1,320 triples.
We then randomly split all the triples into two parts, each part contains the same number of positive and negative triples. The first part is used to train the intermediate domain selection algorithm, the second part is for testing. Since the data are unbalanced, we randomly sampled some negative triples to form a balanced data set. We do the random sampling ten times. Each time, we use 10fold cross validation to assess the performance of the intermediate domain selection algorithm on the first part. The average accuracy is 0.845 ± 0.034.
Performance on Sentiment Classification Tasks
We also test the proposed transfer learning algorithm in the TTL framework on some triples selected by the intermediate domain selection algorithm with high confidence from the second part. We learn the selection model on the training triples and select 10 triples with highest confidence from the testing triple set. The selected triples are listed in Table 6. Some results are interesting and explainable. For example, "video" domain is able to bridge the "music" and "apparel" domains. Intuitively, most music review words are about sound such as rhythm and melody. Most apparel reviews may talk about the appearance like the color. The video reviews contain both the vocal and visual aspects, and are able to draw the music and apparel domains close.
From the results in Table 6, we can see that TriplexST has almost the same results as SVMST. The direct transfer learning algorithm here achieves no performance improvement. This is because the source and target domains have large distribution gap. TTL and TriplexSIT are better than TriplexST. We can also see that TTL always achieves the best performance.
RELATED WORKS
We discuss two categories of research related to transitive transfer learning: transfer learning and multi-task learning.
Transfer Learning solves the lack of class label problem in the target domain by "borrowing" supervised knowledge from related source domains. There are mainly two typical types of algorithms. The first one is instance based knowledge transfer, which selects or adapts the weights of the relevant data from source domains for the target domain. The second one is feature based knowledge transfer, that transforms both source and target data into a common feature space where data follow similar distributions. More recently, multi-source transfer learning performs transfer learning with multiple source domains. For instance, the work in extends TrAdaboost by adding a wrapper boosting framework on weighting each source domain. Different from previous transfer learning, transitive transfer learning does not assume that the source domain and the target domain should be related.
That means, transitive learning can be more general and more useful when the existing labeled and related source domains are not adequate enough to improve the target domain.
Multi-task Learning algorithms simultaneously learn several tasks together and mutually enhance the classification results of each task. It assumes that different tasks share some natural "compact" representations, such as the information reflected by shared data clusters or subspaces. In practice, for example, classifiers for different tasks can be designed to share some global parameters or even a global classifier. More recently, approaches that learn the relationships between pairwise tasks are also being developed. However, these methods require reasonably large amounts of labeled data for each task to learn the relationship.
In contrast, transitive transfer learning works even when both intermediate and target domains are unlabeled. It only assumes that the source domain should have sufficient labeling information to transfer. The intermediate domain serves as a bridge between source and target domains. Even if the intermediate domain is not labeled, the classification information passed from the source domain still contributes to the final classification task through the latent factors learnt in the learning process.
CONCLUSIONS AND FUTURE WORK
In this paper, we study a new problem, transitive transfer learning (TTL), which transfers knowledge from a source domain to an indirectly related target domain with the help of some intermediate domains. We propose a TTL framework to solve the problem. The framework first selects one or more intermediate domains to bridge the given source and target domains, and then performs knowledge transfer along this bridge by capturing overlap hidden features among them. The experiments are conducted on three data
Table 6: Accuracy (%) on the Sentiment classification tasks
Source
Intermediate
Target
SV MST
TriplexST
TriplexSIT
TTL music video apperal
78.57 ± 1.84
78.51 ± 1.24
79.21 ± 1.47 health_&_personal_care baby books
74.15 ± 1.20
74.26 ± 1.21
75.38 ± 1.51 dvd toys_&_games apparel
80.10 ± 1.46
81.11 ± 1.46
83.57 ± 1.34 music toys_&_games baby
76.64 ± 1.52
77.64 ± 1.46
81.22 ± 1.37 books camera_&_photo apparel
80.38 ± 1.34
80.98 ± 1.21
82.74 ± 1.04 sports_&_outdoors video books
72.25 ± 1.58
73.00 ± 1.67
76.01 ± 1.05 video baby camera_&_photo
78.43 ± 1.06
79.42 ± 1.03
81.07 ± 1.06 dvd kitchen_&_housewares baby
78.01 ± 1.13
81.11 ± 1.05
81.42 ± 1.03 electronics baby toys_&_games
81.60 ± 1.63
81.95 ± 1.49
82.12 ± 0.09 electronics baby kitchen_&_housewares
83.52 ± 1.02
84.50 ± 1.06
85.63 ± 1.04 sets, showing that the proposed framework achieves state-of-theart performance. The convergence of the proposed TTL framework has also been theoretically and experimentally proven.
Future Work As a new learning problem, it raises several issues for further exploration in the future. For example, when the source and target need a string of domains to build a connection, how to find the string of intermediate domains to enable max transfer is a valuable research problem. In addition, extending the algorithm to multiple source domains may be an interesting way to generalize transitive transfer learning to be more powerful.
ACKNOWLEDGMENTS
We thank the support of China National 973 project 2014CB340304 and Hong Kong RGC Projects 621013, 620 812, and 621211. We also thank Yin Zhu, Lili Zhao, Zhongqi Lu, Kaixiang Mo and Ying
Wei for discussion.
REFERENCES
 R. K. Ando and T. Zhang. A framework for learning predictive structures from multiple tasks and unlabeled data.
JMLR, 6:1817–1853, Dec. 2005.
 J. Baxter. A bayesian/information theoretic model of learning to learn viamultiple task sampling. Machine
Learning, 28(1):7–39, 1997.
 S. Ben-David, J. Blitzer, K. Crammer, F. Pereira, et al.
Analysis of representations for domain adaptation. NIPS, S. Ben-David, J. Gehrke, and R. Schuller. A theoretical framework for learning from a pool of disparate data sources.
In KDD, pages 443–449, 2002.
 S. Ben-David and R. Schuller. Exploiting task relatedness for mulitple task learning. In COLT, pages 567–580, 2003.
 J. Blitzer, M. Dredze, and F. Pereira. Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. In ACL, Prague, Czech Republic, P. E. Bryant and T. Trabasso. Transitive inferences and memory in young children. Nature, 1971.
 O. Chapelle, P. Shivaswamy, S. Vadrevu, K. Weinberger, Y. Zhang, and B. Tseng. Multi-task learning for boosting with application to web search ranking. In KDD, pages
1189–1198, 2010.
 W. Dai, Q. Yang, G.-R. Xue, and Y. Yu. Boosting for transfer learning. In ICML, pages 193–200, 2007.
 C. Ding, T. Li, W. Peng, and H. Park. Orthogonal nonnegative matrix t-factorizations for clustering. In KDD, pages 126–135. ACM, 2006.
 T. Evgeniou and M. Pontil. Regularized multi–task learning.
In KDD, pages 109–117, 2004.
 Z. Kang, K. Grauman, and F. Sha. Learning with whom to share in multi-task feature learning. In ICML, pages
521–528, 2011.
 D. D. Lee and H. S. Seung. Algorithms for non-negative matrix factorization. In NIPS, pages 556–562. MIT Press, C.-K. Lin, Y.-Y. Lee, C.-H. Yu, and H.-H. Chen. Exploring ensemble of models in taxonomy-based cross-domain sentiment classification. CIKM '14, pages 1279–1288, New
York, NY, USA, 2014. ACM.
 M. Long, J. Wang, G. Ding, W. Cheng, X. Zhang, and W. Wang. Dual transfer learning. In SDM, pages 540–551.
SIAM, 2012.
 D. G. Lowe. Distinctive image features from scale-invariant keypoints. IJCV, 60:91–110, 2004.
 M. Ng, Q. Wu, and Y. Ye. Co-transfer learning using coupled markov chains with restart. 2013.
 S. J. Pan and Q. Yang. A survey on transfer learning. TKDE, 22(10):1345–1359, October 2010.
 W. Pan, N. N. Liu, E. W. Xiang, and Q. Yang. Transfer learning to predict missing ratings via heterogeneous user feedbacks. In IJCAI, pages 2318–2323, 2011.
 N. Ponomareva and M. Thelwall. Biographies or blenders:
Which resource is best for cross-domain sentiment analysis?
In CICLing, pages 488–499. Springer, 2012.
 M. T. Rosenstein, Z. Marx, L. P. Kaelbling, and T. G.
Dietterich. To transfer or not to transfer. In NIPS 2005
Workshop on Transfer Learning, volume 898, 2005.
 B. Tan, E. Zhong, M. Ng, and Q. Yang. Mixed-transfer: transfer learning over mixed graphs. In SDM, 2014.
 B. Tan, E. Zhong, W. Xiang, and Q. Yang. Multi-transfer:
Transfer learning with multiple views and multiple sources.
In SDM, 2013.
 M. E. Taylor and P. Stone. Transfer learning for reinforcement learning domains: A survey. JMLR, 10:1633–1685, Dec. 2009.
 S. Xie, W. Fan, J. Peng, O. Verscheure, and J. Ren. Latent space domain transfer between high dimensional overlapping distributions. In WWW, pages 91–100, 2009.
 Y. Yao and G. Doretto. Boosting for transfer learning with multiple sources. In CVPR, pages 1855–1862, 2010.
 Y. Zhang and D.-Y. Yeung. A convex formulation for learning task relationships in multi-task learning. In UAI, Y. Zhang and D.-Y. Yeung. Multi-task boosting by exploiting task relationships. In ECML/PKDD, pages 697–710, 2012.
 E. Zhong, W. Fan, Q. Yang, O. Verscheure, and J. Ren. Cross validation framework to choose amongst models and datasets for transfer learning. In ECML/PKDD, pages 547–562, 2010.
 X. Zhu. Semi-supervised learning literature survey. 2005.
 Y. Zhu, Y. Chen, Z. Lu, S. J. Pan, G.-R. Xue, Y. Yu, and Q. Yang. Heterogeneous transfer learning for image classification. In AAAI, 2011.
 F. Zhuang, P. Luo, C. Du, Q. He, and Z. Shi. Triplex transfer learning: exploiting both shared and distinct concepts for text classification. In WSDM, pages 425–434, 2013.
Appendix
Table 7: Notations of matrix multiplications
ˆ
M1 s = XsGs ˆA1T
ˆ
M2 s = XsGs ˆA2T s
ˆ
M1
I = XIGI ˆA1T
ˆ
M2
I = XIGI ˆA2T
I
ˆ
Ns = ˆF 1 ˆA1GT s + ˆF 2 s ˆA2 sGT s
ˆT 1 s = ˆ
NsGs ˆA1T
ˆ
NI = ˆF 1 ˆA1GT
I + ˆF 2
I ˆA2
IGT
I
ˆT 2 s = ˆ
NsGs ˆA2T s
ˆT 1
I = ˆ
NT GT ˆA1T
ˆT 2
I = ˆ
NIGI ˆA2T
I
FI = [ ˆF1 ˆF 2
I ]
AI = [ ˆA1 ˆA2
I]
F ′
I = [ ˜F1 ˜F 2
I ]
A′
I = [ ˜A1 ˜A2
I]
We summarize some other matrix multiplication notations in Table 7, and give the update rules for ˆFs, ˆAs, ˆFI and ˆAI as follow:
ˆF 1(i, j) = ˆF 1(i, j) ×
�
[ ˆ
M1s+ ˆ
M1
I](i,j)
[ ˆ
T 1 s + ˆ
T 1
I ](i,j), ˆF 2 s (i, j) = ˆF 2 s (i, j) ×
�
ˆ
M2s(i,j)
ˆ
T 2 s (i,j), ˆF 2
I (i, j) = ˆF 2
I (i, j) ×
�
ˆ
M2
I(i,j)
ˆ
T 2
I (i,j), ˆA1(i, j) = ˆA1(i, j) ×
�
[ ˆ
F 1T (XsGs+XIGI)](i,j)
[ ˆ
F 1T ( ˆ
NsGs+ ˆ
NIGI)](i,j), ˆA2 s(i, j) = ˆA2 s(i, j) ×
�
[ ˆ
F 2T s
XsGs](i,j)
[ ˆ
F 2T s
ˆ
NsGs](i,j), ˆA2
I(i, j) = ˆA2
I(i, j) ×
�
[ ˆ
F 2T
I
XIGI](i,j)
[ ˆ
F 2T
I
ˆ
NIGI](i,j), GI(i, j) = GI(i, j) ×
�
[XT
I F ′
IA′
I+XT
I FIAI](i,j)
[GIA′T
I
F ′T
I
F ′
IA′
I+GIAT
I F T
I FIAI](i,j)
The normalization methods for ˆFs and ˆFI are:
ˆFs(i, j) =
ˆ
Fs(i,j)
�m i=1 ˆ
Fs(i,j), ˆFI(i, j) =
ˆ
FI(i,j)
�m i=1 ˆ
FI(i,j), Convergence Analysis
We first analyze the convergence of ˆF 1 with the rest parameters are fixed. By using the properties of trace operation and frobenius norm ||X||2 = tr(XT X) = tr(XXT ), we re-formulate the objective function Eq. (8) as a Lagrangian function and keep the terms related to ˆF 1:
L( ˆF 1) = tr(−2XT s ˆF 1 ˆA1GT s + 2Gs ˆA1T ˆF 1T ˆ
Ns)
+tr(−2XT
I ˆF 1 ˆA1GT
I + 2GI ˆA1T ˆF 1T ˆ
NI)
+tr[λ( ˆF 1T 1m1T m ˆF 1 − 21p1T m ˆF 1)], (14) where λ ∈ Rp×p is a diagonal matrix. 1m and 1p are all-ones vectors with dimension ms and p respectively. The differential of Eq. (14) is:
∂L( ˆ
F 1)
∂ ˆ
F 1
= tr(−2XsGs ˆA1T + 2 ˆ
NsGs ˆA1T )
+tr(−2XIYt ˆA1T + 2 ˆ
NtGt ˆA1T )
+21m(1T m ˆF 1 − 1T p )λ, Then, we obtain the temporary updating rule:
ˆF 1(i, j) = ˆF 1(i, j) ×
�
[XsGs ˆ
A1T +XIGI ˆ
A1T +1m1T p λ](i,j)
[ ˆ
NsGs ˆ
A1T + ˆ
NIGI ˆ
A1T +1m1T m ˆ
F 1λ](i,j), As proved in, the temporary update rule in Eq. (16) is able to monotonously decrease the Eq. (14). Therefore, there is still one variable λ that needs further calculation. Considering the constrains in Eq. (8), we find that λ is used to satisfy the conditions that the summation of each column of ˆF 1 has to be equal to one. We use the the normalization method in Eq. (13) to normalize ˆF 1. The method satisfies the condition regardless of λ. After that, 1m1T p λ is equal to 1m1T m ˆF 1λ. By getting rid of the terms that contain λ, we get the final update rule in Eq. (12) that is approximately equal to Eq. (16) in terms of convergence, since both 1m1T p λ and 1m1T m ˆF 1λ are constants. Using update rule in Eq. (12) will also monotonously decrease the value of Eq. (14).
We can use similar methodology to analyze the convergence of the update rules and normalization methods for other terms in Eq. (8).
According to the Multiplicative Update Rules in, using the update rules in Eq. (9) and Eq. (12) and using the normalization methods in Eq. (10) and Eq. (13), the value of the objective function in Eq. (8) will not increase. The objective function has a zero lower bound. The convergence of Algorithm 1 is guaranteed.