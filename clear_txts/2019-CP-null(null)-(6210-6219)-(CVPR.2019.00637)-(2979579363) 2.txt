Unsupervised Embedding Learning via Invariant and Spreading
Instance Feature
Mang Ye†
Xu Zhang‡
Pong C. Yuen†
Shih-Fu Chang‡
† Hong Kong Baptist University, Hong Kong
‡ Columbia University, New York
{mangye,pcyuen}@comp.hkbu.edu.hk, {xu.zhang,sc250}@columbia.edu
Abstract
This paper studies the unsupervised embedding learning problem, which requires an effective similarity measurement between samples in low-dimensional embedding space. Motivated by the positive concentrated and negative separated properties observed from category-wise supervised learning, we propose to utilize the instance-wise supervision to approximate these properties, which aims at learning data augmentation invariant and instance spreadout features. To achieve this goal, we propose a novel instance based softmax embedding method, which directly optimizes the 'real' instance features on top of the softmax function. It achieves significantly faster learning speed and higher accuracy than all existing methods. The proposed method performs well for both seen and unseen testing categories with cosine similarity. It also achieves competitive performance even without pre-trained network over samples from fine-grained categories.
1. Introduction
Deep embedding learning is a fundamental task in computer vision, which aims at learning a feature embedding that has the following properties: 1) positive concentrated, the embedding features of samples belonging to the same category are close to each other ; 2) negative separated, the embedding features of samples belonging to different categories are separated as much as possible.
Supervised embedding learning methods have been studied to achieve such objectives and demonstrate impressive capabilities in various vision tasks. However, annotated data needed for supervised methods might be difficult to obtain. Collecting enough annotated data for different tasks requires costly human efforts and special domain expertise. To address this issue, this paper tackles the unsupervised embedding learning problem (a.k.a. unsupervised metric learning in ), which aims at learning discriminative embedding features without human annotated labels.
CNN
Input Images
Output Features
Figure 1: Illustration of our basic idea. The features of the same instance under different data augmentations should be invariant, while features of different image instances should be separated.
Unsupervised embedding learning usually requires that the similarity between learned embedding features is consistent with the visual similarity or category relations of input images. In comparison, general unsupervised feature learning usually aims at learning a good "intermediate" feature representation from unlabelled data. The learned feature is then generalized to different tasks by using a small set of labelled training data from the target task to fine-tune models (e.g., linear classifier, object detector, etc.) for the target task. However, the learned feature representation may not preserve visual similarity and its performance drops dramatically for similarity based tasks, e.g. nearest neighbor search.
The main challenge of unsupervised embedding learning is to discover visual similarity or weak category information from unlabelled samples. Iscen et al. proposed to mine hard positive and negative samples on manifolds. However, its performance heavily relies on the quality of the initialized feature representation for label mining, which limits the applicability for general tasks. In this paper, we propose to utilize the instance-wise supervision to approximate the positive concentrated and negative separated properties mentioned earlier. The learning process only relies on instance-wise relationship and does not rely on relations between pre-defined categories, so it can be well generalized to samples of arbitrary categories that have not been seen before (unseen testing categories).
For positive concentration: it is usually infeasible to mine reliable positive information with randomly initialized network. Therefore, we apply a random data augmentation(e.g., transformation, scaling) to each image instance and use the augmented image as a positive sample. In other words, features of each image instance under different data augmentations should be invariant. For negative separation: since unlabelled data are usually highly imbalanced, the number of negative samples for each image instance is much larger than that of positive samples. Therefore, a small batch of randomly selected instances can be approximately treated as negative samples for each instance.
With such assumption, we try to separate each instance from all the other sampled instances within the batch, resulting in a spread-out property. It is clear that such assumption may not always hold, and each batch may contain a few false negatives. However, through our extensive experiments, we observe that the spread-out property effectively improves the discriminability. In summary, our main idea is to learn a discriminative instance feature, which preserves data augmentation invariant and spread-out properties for unsupervised embedding learning, as shown in Fig. 1.
To achieve these goals, we introduce a novel instance feature-based softmax embedding method. Existing softmax embedding is usually built on classifier weights or memorized features, which has limited efficiency and discriminability. We propose to explicitly optimize the feature embedding by directly using the inner products of instance features on top of softmax function, leading to significant performance and efficiency gains. The softmax function mines hard negative samples and takes full advantage of relationships among all sampled instances to improve the performance. The number of instance is significantly larger than the number of categories, so we introduce a Siamese network training strategy.
We transform the multi-class classification problem to a binary classification problem and use maximum likelihood estimation for optimization.
The main contributions can be summarized as follows:
• We propose a novel instance feature-based softmax embedding method to learn data augmentation invariant and instance spread-out features. It achieves significantly faster learning speed and higher accuracy than all the competing methods.
• We show that both the data augmentation invariant and instance spread-out properties are important for instance-wise unsupervised embedding learning. They help capture apparent visual similarity between samples and generalizes well on unseen testing categories.
• The proposed method achieves the state-of-the-art performances over other unsupervised learning methods on comprehensive image classification and embedding learning experiments.
2. Related Work
General Unsupervised Feature Learning.
Unsupervised feature learning has been widely studied in literature. Existing works can be roughly categorized into three categories : 1) generative models, this approach aims at learning a parameterized mapping between images and predefined noise signals, which constrains the distribution between raw data and noises. Bolztmann Machines(RBMs), Auto-encoders and generative adversarial network (GAN) are widely studied. 2) Estimating Between-image Labels, it usually estimates between-image labels using the clustering technique
 or kNN-based methods, which provide label information. Then label information and feature learning process are iteratively updated. 3) Self-supervised Learning, this approach designs pretext tasks/signals to generate
"pseudo-labels" and then formulate it as a prediction task to learn the feature representations. The pretext task could be the context information of local patches, the position of randomly rearranged patches, the missing pixels of an image or the color information from gray-scale images. Some attempts also use video information to provide weak supervision to learn feature representations.
As we discussed in Section 1, general unsupervised feature learning usually aims at learning a good "intermediate" feature representation that can be well generalized to other tasks. The intermediate feature representation may not preserve visual similar property. In comparison, unsupervised embedding learning requires additional visual similarity property of the learned features.
Deep Embedding Learning. Deep embedding learning usually learns an embedding function by minimizing the intra-class variation and maximizing the inter-class variation. Most of them are designed on top of pairwise or triplet relationships. In particular, several sampling strategies are widely investigated to improve the performance, such as hard mining, semihard mining, smart mining and so on. In comparison, softmax embedding achieves competitive performance without sampling requirement.
Supervised learning has achieved superior performance on various tasks, but they still rely on enough annotated data.
Unsupervised Embedding Learning. According to the evaluation protocol, it can be categorized into two cases, 1) the testing categories are the same with the training categories (seen testing categories), and 2) the testing categories are not overlapped with the training categories (unseen testing categories). The latter setting is more challenging. Without category-wise labels, Iscen et al. proposed to mine hard positive and negative samples on manifolds, and then train the feature embedding with triplet loss.
However, it heavily relies on the initialized representation for label mining.
3. Proposed Method
Our goal is to learn a feature embedding network fθ(·) from a set of unlabelled images X = {x1, x2, · · ·, xn}. fθ(·) maps the input image xi into a low-dimensional embedding feature fθ(xi) ∈ Rd, where d is the feature dimension. For simplicity, the feature representation fθ(xi) of an image instance is represented by fi, and we assume that all the features are ℓ2 normalized, i.e. ∥fi∥2 = 1. A good feature embedding should satisfy: 1) the embedding features of visual similar images are close to each other; 2) the embedding features of dissimilar image instances are separated.
Without category-wise labels, we utilize the instancewise supervision to approximate the positive concentrated and negative seperated properties. In particular, the embedding features of the same instance under different data augmentations should be invariant, while the features of different instances should be spread-out. In the rest of this section, we first review two existing instance-wise feature learning methods, and then propose a much more efficient and discriminative instance feature-based softmax embedding. Finally, we will give a detailed rationale analysis and introduce our training strategy with Siamese network.
3.1. Instance-wise Softmax Embedding
Softmax Embedding with Classifier Weights. Exemplar
CNN treats each image as a distinct class.
Following the conventional classifier training, it defines a matrix
W = [w1, w2, · · ·, wn]T ∈ Rn×d, where the j-th column wj is called the corresponding classifier weight for the j-th instance. Exemplar CNN ensures that image instance under different image transformations can be correctly classified into its original instance with the learned weight. Based on
Softmax function, the probability of sample xj being recognized as the i-th instance can be represented as
P(i|xj) = exp(wT i fj)
�n k=1 exp(wT k fj).
At each step, the network pulls sample feature fi towards its corresponding weight wi, and pushes it away from the classifier weights wk of other instances. However, classifier weights prevent explicitly comparison over features, which results in limited efficiency and discriminability.
Softmax Embedding with Memory Bank. To improve the inferior efficiency, Wu et al. propose to set up a memory bank to store the instance features fi calculated in the previous step. The feature stored in the memory bank is denoted as vi, which serves as the classifier weight for the corresponding instance in the following step. Therefore, the probability of sample xj being recognized as the i-th instance can be written as
P(i|xj) = exp(vT i fj/τ)
�n k=1 exp(vT k fj/τ), (2) where τ is the temperature parameter controlling the concentration level of the sample distribution. vT i fj measures the cosine similarity between the feature fj and the i-th memorized feature vi. For instance xi at each step, the network pulls its feature fi towards its corresponding memorized vector vi, and pushes it away from the memorized vectors of other instances. Due to efficiency issue, the memorized feature vi corresponding to instance xi is only updated in the iteration which takes xi as input. In other words, the memorized feature vi is only updated once per epoch. However, the network itself is updated in each iteration. Comparing the real-time instance feature fi with the outdated memorized feature vi would cumber the training process. Thus, the memory bank scheme is still inefficient.
A straightforward idea to improve the efficiency is directly optimizing over feature itself, i.e. replacing the weight {wi} or memory {vi} with fi. However, it is implausible due to two reasons: 1) Considering the probability
P(i|xi) of recognizing xi to itself, since f T i fi=1, i.e. the feature and "pseudo classifier weight" (the feature itself) are always perfectly aligned, optimizing the network will not provide any positive concentrated property; 2) It's impractical to calculate the feature of all the samples (fk, k = 1,..., n) on-the-fly in order to calculate the denominator in Eq. (2), especially for large-scale instance number dataset.
3.2. Softmax Embedding on 'Real' Instance Feature
To address above issues, we propose a softmax embedding variant for unsupervised embedding learning, which directly optimizes the real instance feature rather than classifier weights or memory bank. To achieve the goal that features of the same instance under different data augmentations are invariant, while the features of different instances are spread-out, we propose to consider 1) both the original image and its augmented image, 2) a small batch of randomly selected samples instead of the full dataset.
For each iteration, we randomly sample m instances from the dataset.
To simplify the notation, without loss of generality, the selected samples are denoted by
{x1, x2, · · ·, xm}. For each instance, a random data augmentation operation T(·) is applied to slightly modify the original image. The augmented sample T(xi) is denoted by
ˆxi, and its embedding feature fθ(ˆxi) is denoted by ˆfi. Instead of considering the instance feature learning as a multiclass classification problem, we solve it as binary classification problem via maximum likelihood estimation (MLE). In particular, for instance xi, the augmented sample ˆxi should be classified into instance i, and other instances xj, j ̸= i shouldn't be classified into instance i. The probability of ˆxi being recognized as instance i is defined by
P(i|ˆxi) = exp(f T i ˆfi/τ)
�m k=1 exp(f T k ˆfi/τ)
CNN
FC
L2 Norm
FC
L2 Norm
CNN
Low-dim
Low-dim
Embedding Space f1 f2 f3 f1 f2 f3 𝐱1 𝐱2 𝐱3
𝐱1
𝐱2
𝐱3
Data
Augmentation
Share Weights
Figure 2: The framework of the proposed unsupervised learning method with Siamese network. The input images are projected into low-dimensional normalized embedding features with the CNN backbone. Image features of the same image instance with different data augmentations are invariant, while embedding features of different image instances are spread-out.
On the other hand, the probability of xj being recognized as instance i is defined by
P(i|xj) = exp(f T i fj/τ)
�m k=1 exp(f T k fj/τ), j ̸= i
Correspondingly, the probability of xj not being recognized as instance i is 1 − P(i|xj).
Assuming different instances being recognized as instance i are independent, the joint probability of ˆxi being recognized as instance i and xj, j ̸= i not being classified into instance i is Pi = P(i|ˆxi)
� j̸=i(1 − P(i|xj))
The negative log likelihood is given by
Ji = − log P(i|ˆxi) −
� j̸=i log(1 − P(i|xj))
We solve this problem by minimizing the sum of the negative log likelihood over all the instances within the batch, which is denoted by
J = −
� i log P(i|ˆxi) −
� i
� j̸=i log(1 − P(i|xj)).
3.3. Rationale Analysis
This section gives a detailed rationale analysis about why minimizing Eq. (6) could achieve the augmentation invariant and instance spread-out feature. Minimizing Eq. (6) can be viewed as maximizing Eq. (3) and minimizing Eq. (4).
Considering Eq. (3), it can be rewritten as
P(i|ˆxi) = exp(f T i ˆfi/τ) exp(f T i ˆfi/τ) + � k̸=i exp(f T k ˆfi/τ)
Maximizing Eq. (3) requires maximizing exp(f T i ˆfi/τ) and minimizing exp(f T k ˆfi/τ), k ̸= i. Since all the features are ℓ2 normalized, maximizing exp(f T i ˆfi/τ) requires increasing the inner product (cosine similarity) between fi and ˆfi, resulting in a feature that is invariant to data augmentation.
On the other hand, minimizing exp(f T k ˆfi/τ) ensures ˆfi and other instances {fk} are separated. Considering all the instances within the batch, the instances are forced to be separated from each other, resulting in the spread-out property.
Similarly, Eq. (4) can be rewritten as, P(i|xj) = exp(f T i fj/τ) exp(f T j fj/τ) + � k̸=j exp(f T k fj/τ), Note that the inner product f T j fj is 1 and the value of τ is generally small (say 0.1 in the experiment). Therefore, exp(f T j fj/τ) generally determines the value of the whole denominator. Minimizing Eq. (4) means that exp(f T i fj/τ) should be minimized, which aims at separating fj from fi.
Thus, it further enhances the spread-out property.
3.4. Training with Siamese Network
We proposed a Siamese network to implement the proposed algorithm as shown in Fig. 2. At each iteration, m randomly selected image instances are fed into in the first branch, and the corresponding augmented samples are fed into the second branch. Note that data augmentation is also be used in the first branch to enrich the training samples. For implementation, each sample has one randomly augmented positive sample and 2N − 2 negative samples to compute Eq. (7), where N is the batch size. The proposed training strategy greatly reduces the computational cost. Meanwhile, this training strategy also takes full advantage of relationships among all instances sampled in a mini-batch. Theoretically, we could also use a multibranch network by considering multiple augmented images for each instance in the batch.
Methods kNN
RandomCNN
DeepCluster (10) 
DeepCluster (1000) 
Exemplar 
NPSoftmax 
NCE 
Triplet
Triplet (Hard)
Ours
Table 1: kNN accuracy (%) on CIFAR-10 dataset.
4. Experimental Results
We have conducted the experiments with two different settings to evaluate the proposed method1. The first setting is that the training and testing sets share the same categories(seen testing category). This protocol is widely adopted for general unsupervised feature learning. The second setting is that the training and testing sets do not share any common categories (unseen testing category). This setting is usually used for supervised embedding learning. Following, we don't use any semantic label in the training set. The latter setting is more challenging than the former setting and it could apparently demonstrate the quality of learned features on unseen categories.
4.1. Experiments on Seen Testing Categories
We follow the experimental settings in to conduct the experiments on CIFAR-10 and STL-10 datasets, where training and testing set share the same categories.
Specifically, ResNet18 network is adopted as the backbone and the output embedding feature dimension is set to
128. The initial learning rate is set to 0.03, and it is decayed by 0.1 and 0.01 at 120 and 160 epoch. The network is trained for 200 epochs. The temperature parameter τ is set to 0.1. The algorithm is implemented on PyTorch with SGD optimizer with momentum. The weight decay parameter is 5×10−4 and momentum is 0.9. The training batch size is set to 128 for all competing methods on both datasets. Four kinds of data augmentation methods (RandomResizedCrop, RandomGrayscale, ColorJitter, RandomHorizontalFlip) in PyTorch with default parameters are adopted.
Following, we adopt weighted kNN classifier to evaluate the performance. Given a test sample, we retrieve its top-k (k = 200) nearest neighbors based on cosine similarity, then apply weighted voting to predict its label.
CIFAR-10 Dataset
CIFAR-10 datset contains 50K training images and 10K testing images from the same ten classes. The image size are 32 × 32. Five methods are included for comparison: DeepCluster with different cluster numbers, Exem1Code is available at https://github.com/mangye16/
Unsupervised_Embedding_Learning
Training Epochs
90 kNN Accuracy (%)
Ours
DeepCluster 
NCE 
Exemplar 
Figure 3: Evaluation of the training efficiency on CIFAR-10 dataset. kNN accuracy (%) at each epoch is reported, demonstrating the learning speed of different methods. plar CNN, NPSoftmax, NCE and Triplet loss with and without hard mining. Triplet (hard) is the online hard negative sample within each batch for training, and the margin parameter is set to 0.5. DeepCluster and NCE represent the state-of-the-art unsupervised feature learning methods. The results are shown in Table 1.
Classification Accuracy. Table 1 demonstrates that our proposed method achieves the best performance (83.6%) with kNN classifier.
DeepCluster performs well in learning good "intermediate" features with large-scale unlabelled data, but the performance with kNN classification drops dramatically. Meanwhile, it is also quite sensitive to cluster numbers, which is unsuitable for different tasks.
Compared to Exemplar CNN which uses the classifier weights for training, the proposed method outperforms it by 9.1%. Compared to NPSoftmax and NCE, which use memorized feature for optimizing, the proposed method outperform by 2.8% and 3.2% respectively. The performance improvement is clear due to the idea of directly performing optimization over feature itself. Compared to triplet loss, the proposed method also outperforms it by a clear margin. The superiority is due to the hard mining nature in Softmax function.
Efficiency. We plot the learning curves of the competing methods at different epochs in Fig. 3. The proposed method takes only 2 epochs to get a kNN accuracy of 60% while takes 25 epochs and takes 45 epochs to reach the same accuracy. It is obvious that our learning speed is much faster than the competitors. The efficiency is guaranteed by directly optimization on instance features rather than classifier weights or memory bank.
STL-10 Dataset
STL-10 dataset is an image recognition dataset with colored images of size 96 × 96, which is widely used in unsupervised learning. Specifically, this dataset is originally designed with three splits: 1) train, 5K labelled images in ten
Methods
Training
Linear kNN
RandomCNN
None22.4 k-MeansNet∗ 
105KHMP∗ 
105KSatck∗ 
105KExemplar∗ 
105KNPSoftmax 
5K
NCE 
5K
DeepCluster(100) 
5K
Ours
5K
Ours
105K
Table 2: Classification accuracy (%) with linear classifier and kNN classifier on STL-10 dataset.
∗Results are taken from, the baseline network is different. classes for training, 2) test, 8K images from the same ten classes for testing, 3) unlabelled, 100K unlabelled images which share similar distribution with labelled data for unsupervised learning. We follow the same experimental setting as CIFAR-10 dataset and report classification accuracy(%) with both Linear Classifier (Linear) and kNN classier(kNN) in Table 2. Linear classifier means training a SVM classifier on the learned features and the labels of training samples. The classifier is used to predict the label of test samples. We implement NPSoftmax, NCE and DeepCluster (cluster number 100) under the same settings with their released code. By default, we only use 5K training images without using labels for training. The performances of some state-of-the-art unsupervised methods(k-MeansNet, HMP, Satck and Exemplar ) are also reported. Those results are taken from.
As shown in Table 2, when only using 5K training images for learning, the proposed method achieves the best accuracy with both classifiers (kNN: 74.1%, Linear: 69.5%), which are much better than NCE and DeepCluster under the same evaluation protocol. Note that kNN measures the similarity directly with the learned features and Linear requires additional classifier learning with the labelled training data. When 105K images are used for training, the proposed method also achieves the best performance for both kNN classifier and linear classifier. In particular, the kNN accuracy is 74.1% for 5K training images, and it increases to 81.6% for full 105K training images. The classification accuracy with linear classifier also increases from 69.5% to 77.9%. This experiment verifies that the proposed method can benefit from more training samples.
4.2. Experiments on Unseen Testing Categories
This section evaluates the discriminability of the learned feature embedding when the semantic categories of training samples and testing samples are not overlapped. We follow the experimental settings described in to conduct experiments on CUB200-2011(CUB200), Stanford Online Product (Product) and Car196 datasets. No semantic label is used for training. Caltech-UCSD Birds
Methods
R@1
R@2
R@4
R@8
NMI
Initial (FC)
Supervised Learning
Lifted 
Clustering 
Triplet+ 
Smart+ 
Unsupervised Learning
Cyclic 
Exemplar 
NCE 
DeepCluster 
MOM 
Ours
Table 3: Results (%) on CUB200 dataset.
Methods
R@1
R@10
R@100
NMI
Initial (FC)
Exemplar 
NCE 
DeepCluster 
MOM 
Ours
Table 4: Results (%) on Product dataset.
200 (CUB200) is a fine-grained bird dataset. Following, the first 100 categories with 5,864 images are used for training, while the other 100 categories with 5,924 images are used for testing. Stanford Online Product (Product)
 is a large-scale fine-grained product dataset. Similarly, 11,318 categories with totally 59,551 images are used for training, while the other 11,316 categories with 60,502 images are used for testing. Cars (Car196) dataset is a fine-grained car category dataset. The first 98 categories with 8,054 images are used for training, while the other 98 categories with 8,131 images are used for testing.
Implementation Details. We implement the proposed method on PyTorch. The pre-trained Inception-V1 on
ImageNet is used as the backbone network following existing methods. A 128-dim fully connected layer with ℓ2 normalization is added after the pool5 layer as the feature embedding layer. All the input images are firstly resized to 256 × 256. For data augmentation, the images are randomly cropped at size 227×227 with random horizontal flipping following. Since the pre-trained network performs well on CUB200 dataset, we randomly select the augmented instance and its corresponding nearest instance as positive. In testing phase, a single center-cropped image is adopted for fine-grained recognition as in. We adopt the SGD optimizer with 0.9 momentum. The initial learning rate is set to 0.001 without decay. The temperature parameter τ is set to 0.1. The training batch size is set to 64.
Evaluation Metrics. Following existing works on supervised deep embedding learning, the retrieval performance and clustering quality of the testing set are evaluated. Cosine similarity is adopted for similarity meaMethods
R@1
R@2
R@4
R@8
NMI
Initial (FC)
Exemplar 
NCE 
DeepCluster 
MOM 
Ours
Table 5: Results (%) on Car196 dataset. surement. Given a query image from the testing set, R@K measures the probability of any correct matching (with same category label) occurs in the top-k retrieved ranking list. The average score is reported for all testings samples. Normalized Mutual Information (NMI) is utilized to measure the clustering performance of the testing set.
Comparison to State-of-the-arts. The results of all the competing methods on three datasets are listed in Table 3, 4 and 5, respectively. MOM is the only method that claims for unsupervised metric learning. We implement the other three state-of-the-art unsupervised methods (Exemplar, NCE and DeepCluster ) on three datasets with their released code under the same setting for fair comparison. Note that these methods are originally evaluated for general unsupervised feature learning, where the training and testing set share the same categories. We also list some results of supervised learning (originate from
 ) on CUB200 dataset as shown in Table 3.
Generally, the instance-wise feature learning methods(NCE, Examplar, Ours) outperform non-instancewise feature learning methods (DeepCluster, MOM
 ), especially on Car196 and Product datasets, which indicates instance-wise feature learning methods have good generalization ability on unseen testing categories. Among all the instance-wise feature learning methods, the proposed method is the clear winner, which also verifies the effectiveness of directly optimizing over feature itself. Moreover, the proposed unsupervised learning method is even competitive to some supervised learning methods on CUB200 dataset.
Qualitative Result. Some retrieved examples with cosine similarity on CUB200 dataset at different training epochs are shown in Fig. 4. The proposed algorithm can iteratively improve the quality of the learned feature and retrieve more correct images.
Although there are some wrongly retrieved samples from other categories, most of the top retrieved samples are visually similar to the query.
Training from Scratch. We also evaluate the performance using a network (ResNet18) without pre-training.
The results on the large-scale Product dataset are shown in Table 6. The proposed method is also a clear winner. Interestingly, MOM fails in this experiment. The main reason is that the feature from randomly initialized network provides limited information for label mining. Therefore, MOM cannot estimate reliable labels for training.
Methods
R@1
R@10
R@100
NMI
Random
Exemplar 
NCE 
MOM 
Ours
Table 6: Results (%) on Product dataset using network without pre-trained parameters.
4.3. Ablation Study
The proposed method imposes two important properties for instance feature learning: data augmentation invariant and instance spread-out. We conduct ablation study to show the effectiveness of each property on CIFAR-10 dataset.
Strategy
Full w/o R w/o G w/o C w/o F kNN Acc (%)
Table 7: Effects of each data augmentation operation on CIFAR10 dataset. 'w/o': Without. 'R': RandomResizedCrop, 'G': RandomGrayscale, 'C': ColorJitter, 'F': RandomHorizontalFlip.
Strategy
Full
No DA
Hard
Easy kNN Acc (%)
Table 8: Different sampling strategies on CIFAR-10 dataset.
To show the importance of data augmentation invariant property, we firstly evaluate the performance by removing each of the operation respectively from the data augmentation set. The results are shown in Table 7. We observe that all listed operations contribute to the remarkable performance gain achieved by the proposed algorithm. In particular, RandomResizedCrop contributes the most. We also evaluate the performance without data augmentation (No
DA) in Table 8, and it shows that performance drops significantly from 83.6% to 37.4%. It is because when training without data augmentation, the network does not create any positive concentration property. The features of visually similar images are falsely separated.
To show the importance of spread-out property, we evaluated two different strategies to choose negative samples:
1) selecting the top 50% instance features that are similar to query instance as negative (hard negative); 2) selecting the bottom 50% instance features that are similar to query instance as negative (easy negative). The results are shown as "Hard" and "Easy" in Table 8. The performance drops dramatically when only using the easy negative. In comparison, the performance almost remains the same as the full model when only using hard negative. It shows that separating hard negative instances helps to improve the discriminability of the learned embedding.
4.4. Understanding of the Learned Embedding
We calculate the cosine similarity between the query feature and its 5NN features from the same category (Positive)
Query
Epoch 0
Epoch 1
Epoch 2
Figure 4: 4NN retrieval results of some example queries on CUB200-2011 dataset. The positive (negative) retrieved results are framed in green (red). The similarity is measured with cosine similarity.
Positive
Negative
Positive
Negative(a) Random Network
Positive
Negative
Positive
Negative(c) Exemplar 
Positive
Negative
Positive
Negative(b) NCE 
Positive
Negative
Positive
Negative(d) Ours
Figure 5: The cosine similarity distributions on CIFAR-10 as well as 5NN features from different categories (Negative). The distributions of the cosine similarity of different methods are shown in Fig. 5. A more separable distribution indicates a better feature embedding. It shows that the proposed method performs best to separate positive and negative samples. We could also observe that our learned feature preserves the best spread-out property.
It is interesting to show how the learned instance-wise feature helps the category label prediction. We report the cosine similarity distribution based on other category definitions (attributes in ) instead of semantic label in Fig. 6.
The distribution clearly shows that the proposed method also performs well to separate other attributes, which demonstrates the generalization ability of the learned feature.
5. Conclusion
In this paper, we propose to address the unsupervised embedding learning problem by learning a data augmentation invariant and instance spread-out feature. In particular, we propose a novel instance feature based softmax embedding trained with Siamese network, which explicitPositive
Negative
Positive
Negative(a) Attribute "animals vs artifacts"
Positive
Negative
Positive
Negative(b) Attribute "big vs small shape animal"
Figure 6: The cosine similarity distributions of randomly initialized network (left column) and our learned model (right column) with different attributes on CIFAR-10. ly pulls the features of the same instance under different data augmentations close and pushes the features of different instances away. Comprehensive experiments show that directly optimizing over instance feature leads to significant performance and efficiency gains. We empirically show that the spread-out property is particularly important and it helps capture the visual similarity among samples.
Acknowledgement
This work is partially supported by Research Grants
Council (RGC/HKBU12200518), Hong Kong. This work is partially supported by the United States Air Force Research
Laboratory (AFRL) and the Defense Advanced Research
Projects Agency (DARPA) under Contract No. FA8750-16C-0166. Any opinions, findings and conclusions or recommendations expressed in this material are solely the responsibility of the authors and does not necessarily represent the official views of AFRL, DARPA, or the U.S. Government.
References
 Pulkit Agrawal, Joao Carreira, and Jitendra Malik. Learning to see by moving. In ICCV, pages 37–45, 2015. 2
 Liefeng Bo, Xiaofeng Ren, and Dieter Fox. Unsupervised feature learning for rgb-d based object recognition. In Experimental Robotics, pages 387–402. Springer, 2013. 6
 Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised learning of visual features. In ECCV, pages 132–149, 2018. 1, 2, 5, Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In
AISTATS, pages 215–223, 2011. 5
 Adam Coates and Andrew Y Ng. Selecting receptive fields in deep networks. In NIPS, pages 2528–2536, 2011. 6
 Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by context prediction. In
ICCV, pages 1422–1430, 2015. 1, 2
 Jeff Donahue, Philipp Kr¨ahenb¨uhl, and Trevor Darrell. Adversarial feature learning. arXiv preprint arXiv:1605.09782, Alexey Dosovitskiy, Philipp Fischer, Jost Tobias Springenberg, Martin Riedmiller, and Thomas Brox. Discriminative unsupervised feature learning with exemplar convolutional neural networks. PAMI, 38(9):1734–1747, 2016. 2, 3, 5, 6, Alexey Dosovitskiy, Jost Tobias Springenberg, Martin Riedmiller, and Thomas Brox. Discriminative unsupervised feature learning with convolutional neural networks. In NIPS, pages 766–774, 2014. 2
 Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Olivier Mastropietro, Alex Lamb, Martin Arjovsky, and Aaron
Courville. Adversarially learned inference. arXiv preprint arXiv:1606.00704, 2016. 2
 Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, pages
2672–2680, 2014. 2
 Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an invariant mapping. In CVPR, Ben Harwood, BG Kumar, Gustavo Carneiro, Ian Reid, Tom
Drummond, et al. Smart mining for deep metric learning. In
ICCV, pages 2821–2829, 2017. 2, 6
 Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Delving deep into rectifiers: Surpassing human-level performance on imagenet classification.
In ICCV, pages 1026–
 Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition.
In CVPR, pages 770–778, 2016. 5
 Alexander Hermans, Lucas Beyer, and Bastian Leibe. In defense of the triplet loss for person re-identification. arXiv preprint arXiv:1703.07737, 2017. 2, 5
 Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015. 3
 Shota Horiguchi, Daiki Ikami, and Kiyoharu Aizawa. Significance of softmax-based features in comparison to distance metric learning-based features. arXiv preprint arXiv:1712.10151, 2017. 2
 Chen Huang, Chen Change Loy, and Xiaoou Tang. Unsupervised learning of discriminative attributes and visual representations. In CVPR, pages 5175–5184, 2016. 8
 Fu Jie Huang, Y-Lan Boureau, Yann LeCun, et al. Unsupervised learning of invariant feature hierarchies with applications to object recognition. In CVPR, pages 1–8, 2007. 2
 Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, and Ondrej
Chum. Mining on manifolds: Metric learning without labels.
 Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei.
3d object representations for fine-grained categorization. In
ICCVW, pages 554–561, 2013. 6
 Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009. 5, 8
 Honglak Lee, Roger Grosse, Rajesh Ranganath, and Andrew Y Ng. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. In
ICML, pages 609–616, 2009. 2
 Dong Li, Wei-Chih Hung, Jia-Bin Huang, Shengjin Wang, Narendra Ahuja, and Ming-Hsuan Yang. Unsupervised visual representation learning by graph-based consistent constraints. In ECCV, pages 678–694, 2016. 6
 Renjie Liao, Alex Schwing, Richard Zemel, and Raquel Urtasun. Learning deep parsimonious representations. In NIPS, pages 5076–5084, 2016. 1, 2
 Tongliang Liu and Dacheng Tao. Classification with noisy labels by importance reweighting. IEEE TPAMI, 38(3):447–
 Chaochao Lu and Xiaoou Tang. Surpassing human-level face verification performance on lfw with gaussianface. In AAAI, pages 3811–3819, 2015. 1
 R Manmatha, Chao-Yuan Wu, Alexander J Smola, and Philipp Kr¨ahenb¨uhl. Sampling matters in deep embedding learning. In ICCV, pages 2859–2867, 2017. 2
 Yair Movshovitz-Attias, Alexander Toshev, Thomas K Leung, Sergey Ioffe, and Saurabh Singh. No fuss distance metric learning using proxies. In ICCV, pages 360–368, 2017.
 Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In ECCV, pages 69–84, 2016. 1, 2
 Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio
Savarese. Deep metric learning via lifted structured feature embedding. In CVPR, pages 4004–4012, 2016. 1, 2, 4, 5, 6, Edouard
Oyallon, Eugene
Belilovsky, and Sergey
Zagoruyko.
Scaling the scattering transform:
Deep hybrid networks. In ICCV, pages 5619–5628, 2017. 6
 Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor
Darrell, and Alexei A Efros.
Context encoders: Feature learning by inpainting. In CVPR, pages 2536–2544, 2016.
 Florian Schroff, Dmitry Kalenichenko, and James Philbin.
Facenet: A unified embedding for face recognition and clustering. In CVPR, pages 815–823, 2015. 2
 Hinrich Sch¨utze, Christopher D Manning, and Prabhakar
Raghavan. Introduction to information retrieval, volume 39.
Cambridge University Press, 2008. 7
 Kihyuk Sohn. Improved deep metric learning with multiclass n-pair loss objective. In NIPS, pages 1857–1865, 2016.
 Hyun Oh Song, Stefanie Jegelka, Vivek Rathod, and Kevin
Murphy. Deep metric learning via facility location. In CVPR, pages 2206–2214, 2017. 6
 Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In CVPR, pages 1–9, 2015. 6
 Yichuan Tang, Ruslan Salakhutdinov, and Geoffrey Hinton.
Robust boltzmann machines for recognition and denoising.
In CVPR, pages 2264–2271, 2012. 2
 Daniel Tarlow, Kevin Swersky, Laurent Charlin, Ilya
Sutskever, and Rich Zemel. Stochastic k-neighborhood selection for supervised and unsupervised learning. In ICML, pages 199–207, 2013. 2
 Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and composing robust features with denoising autoencoders. In ICML, pages 1096–
 Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd birds-200-2011 dataset. 2011. 6
 Xiaolong Wang and Abhinav Gupta. Unsupervised learning of visual representations using videos. In ICCV, pages 2794–
 Yandong Wen, Kaipeng Zhang, Zhifeng Li, and Yu Qiao. A discriminative feature learning approach for deep face recognition. In ECCV, pages 499–515, 2016. 2
 Zhirong Wu, Yuanjun Xiong, X Yu Stella, and Dahua Lin.
Unsupervised feature learning via non-parametric instance discrimination. In CVPR, pages 3733–3742, 2018. 1, 2, 3, 5, Tong Xiao, Hongsheng Li, Wanli Ouyang, and Xiaogang
Wang. Learning deep feature representations with domain guided dropout for person re-identification. In CVPR, pages
1249–1258, 2016. 2
 Mang Ye, Xiangyuan Lan, and Pong C. Yuen. Robust anchor embedding for unsupervised video person re-identification in the wild. In ECCV, pages 170–186, 2018. 1
 Mang Ye, Jiawei Li, Andy J Ma, Liang Zheng, and Pong C.
Yuen. Dynamic graph co-matching for unsupervised videobased person re-identification. In IEEE Transactions on Image Processing (TIP), 2019. 2
 Mang Ye, Andy J Ma, Liang Zheng, Jiawei Li, and Pong C.
Yuen. Dynamic label graph matching for unsupervised video re-identification. In ICCV, pages 5142–5150, 2017. 1
 Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In ECCV, pages 649–666, 2016. 2
 Xu Zhang, X Yu Felix, Sanjiv Kumar, and Shih-Fu Chang.
Learning spread-out local feature descriptors.
In ICCV, pages 4605–4613, 2017. 1, 2
 Xuan Zhang, Hao Luo, Xing Fan, Weilai Xiang, Yixiao Sun, Qiqi Xiao, Wei Jiang, Chi Zhang, and Jian Sun.
Alignedreid: Surpassing human-level performance in person reidentification. arXiv preprint arXiv:1711.08184, 2017. 1
 J Zhao, M Mathieu, R Goroshin, and Y Lecun.
Stacked what-where auto-encoders. arXiv preprint arXiv:1506.02351. 6