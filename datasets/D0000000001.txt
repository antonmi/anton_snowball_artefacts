Representation Learning: A Review and New
Perspectives
Yoshua Bengio, Aaron Courville, and Pascal Vincent
Department of computer science and operations research, U. Montreal
!
Abstract—
The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and joint training of deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep architectures. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.
Index Terms—Deep learning, representation learning, feature learning, unsupervised learning, Boltzmann Machine, RBM, auto-encoder, neural network
INTRODUCTION
The performance of machine learning methods is heavily dependent on the choice of data representation (or features) on which they are applied. For that reason, much of the actual effort in deploying machine learning algorithms goes into the design of preprocessing pipelines and data transformations that result in a representation of the data that can support effective machine learning. Such feature engineering is important but labor-intensive and highlights the weakness of current learning algorithms: their inability to extract and organize the discriminative information from the data. Feature engineering is a way to take advantage of human ingenuity and prior knowledge to compensate for that weakness. In order to expand the scope and ease of applicability of machine learning, it would be highly desirable to make learning algorithms less dependent on feature engineering, so that novel applications could be constructed faster, and more importantly, to make progress towards Artificial Intelligence (AI). An AI must fundamentally understand the world around us, and we argue that this can only be achieved if it can learn to identify and disentangle the underlying explanatory factors hidden in the observed milieu of low-level sensory data.
This paper is about feature learning, or representation learning, i.e., learning transformations of the data that make it easier to extract useful information when building classifiers or other predictors. In the case of probabilistic models, a good representation is often one that captures the posterior distribution of the underlying explanatory factors for the observed input.
Among the various ways of learning representations, this paper focuses on deep learning methods: those that are formed by the composition of multiple non-linear transformations of the data, with the goal of yielding more abstract – and ultimately more useful – representations. Here we survey this rapidly developing area with special emphasis on recent progress. We consider some of the fundamental questions that have been driving research in this area. Specifically, what makes one representation better than another? Given an example, how should we compute its representation, i.e. perform feature extraction? Also, what are appropriate objectives for learning good representations? In the course of dealing with these issues we review some of the most popular models in the field and place them in a context of the field as a whole.
WHY SHOULD WE CARE ABOUT LEARNING
REPRESENTATIONS?
Representation learning has become a field in itself in the machine learning community, with regular workshops at the leading conferences such as NIPS and ICML, sometimes under the header of Deep Learning or Feature Learning. Although depth is an important part of the story, many other priors are interesting and can be conveniently captured by a learner when the learning problem is cast as one of learning a representation, as discussed in the next section. The rapid increase in scientific activity on representation learning has been accompanied and nourished (in a virtuous circle) by a remarkable string of empirical successes both in academia and in industry. In this section, we briefly highlight some of these high points.
Speech Recognition and Signal Processing
Speech was one of the early applications of neural networks, in particular convolutional (or time-delay) neural networks 1.
The recent revival of interest in neural networks, deep learning, and representation learning has had a strong impact in the area of speech recognition, with breakthrough results (Dahl et al., 2010; Seide et al., 2011; Mohamed et al., 2012; Dahl et al., 2012) obtained by several academics as well as researchers at industrial labs taking over the task of bringing these algorithms to a larger scale and into products. For example, Microsoft has released in 2012 a new version of their MAVIS (Microsoft
Audio Video Indexing Service) speech system based on deep learning (Seide et al., 2011). These authors managed to reduce
1. See Bengio (1993) for a review of early work in this area. arXiv:1206.5538v2 [cs.LG] 18 Oct 2012
2 the word error rate on four major benchmarks by about 30%(e.g. from 27.4% to 18.5% on RT03S) compared to state-ofthe-art models based on Gaussian mixtures for the acoustic modeling and trained on the same amount of data (309 hours of speech). The relative improvement in error rate obtained by Dahl et al. (2012) on a smaller large-vocabulary speech recognition benchmark (Bing mobile business search dataset, with 40 hours of speech) is between 16% and 23%.
Representation-learning algorithms (based on recurrent neural networks) have also been applied to music, substantially beating the state-of-the-art in polyphonic transcription (Boulanger-Lewandowski et al., 2012), with a relative error improvement of between 5% and 30% on a standard benchmark of four different datasets.
Object Recognition
The beginnings of deep learning in 2006 have focused on the MNIST digit image classification problem (Hinton et al., 2006a; Bengio et al., 2007), breaking the supremacy of SVMs(1.4% error) on this dataset2. The latest records are still held by deep networks: Ciresan et al. (2012) currently claims the title of state-of-the-art for the unconstrained version of the task(e.g., using a convolutional architecture), with 0.27% error, and Rifai et al. (2011c) is state-of-the-art for the knowledgefree version of MNIST, with 0.81% error.
In the last few years, deep learning has moved from digits to object recognition in natural images, and the latest breakthrough has been achieved on the ImageNet dataset3 bringing down the state-of-the-art error rate from 26.1% to
15.3% (Krizhevsky et al., 2012).
Natural Language Processing
Besides speech recognition, there are many other Natural
Language Processing applications of representation learning algorithms. The idea of distributed representation for symbolic data was introduced by
Hinton (1986), and first developed in the context of statistical language modeling by Bengio et al. (2003)4. They are all based on learning a distributed representation for each word, also called a word embedding.
Combining this idea with a convolutional architecture, Collobert et al. (2011) developed the SENNA system5 that shares representations across the tasks of language modeling, part-ofspeech tagging, chunking, named entity recognition, semantic role labeling and syntactic parsing. SENNA approaches or surpasses the state-of-the-art on these tasks but is much faster than traditional predictors and requires only 3500 lines of C code to perform its predictions.
The neural net language model was also improved by adding recurrence to the hidden layers (Mikolov et al., 2011), allowing it to beat the state-of-the-art (smoothed n-gram models) not only in terms of perplexity (exponential of the average negative log-likelihood of predicting the right next word, going down from 140 to 102) but also in terms of 2. for the knowledge-free version of the task, where no image-specific prior is used, such as image deformations or convolutions
3. The 1000-class ImageNet benchmark, whose results are detailed here: http://www.image-net.org/challenges/LSVRC/2012/results.html
4. See this review of neural net language models (Bengio, 2008).
5. downloadable from http://ml.nec-labs.com/senna/ word error rate in speech recognition (since the language model is an important component of a speech recognition system), decreasing it from 17.2% (KN5 baseline) or 16.9%(discriminative language model) to 14.4% on the Wall Street
Journal benchmark task. Similar models have been applied in statistical machine translation (Schwenk et al., 2012), improving the BLEU score by almost 2 points. Recursive autoencoders (which generalize recurrent networks) have also been used to beat the state-of-the-art in full sentence paraphrase detection (Socher et al., 2011a) almost doubling the F1 score for paraphrase detection. Representation learning can also be used to perform word sense disambiguation (Bordes et al., 2012), bringing up the accuracy from 67.8% to 70.2% on the subset of Senseval-3 where the system could be applied(with subject-verb-object sentences). Finally, it has also been successfully used to surpass the state-of-the-art in sentiment analysis (Glorot et al., 2011b; Socher et al., 2011b).
Multi-Task and Transfer Learning, Domain Adaptation
Transfer learning is the ability of a learning algorithm to exploit commonalities between different learning tasks in order to share statistical strength, and transfer knowledge across tasks. As discussed below, we hypothesize that representation learning algorithms have an advantage for such tasks because they learn representations that capture underlying factors, a subset of which may be relevant for each particular task, as illustrated in Figure 1. This hypothesis seems confirmed by a number of empirical results showing the strengths of representation learning algorithms in transfer learning scenarios. raw input x task 1 output y1 task 3 output y3 task 2 output y2
Task%A%
Task%B%
Task%C%
%output%
%input%
%shared% subsets%of% factors%
Fig. 1.
Illustration of a representation-learning model which discovers explanatory factors (middle hidden layer, in red), some of which explain the input (semi-supervised setting), and some of which explain the target for each task. Because these subsets overlap, sharing of statistical strength allows gains in generalization.
Most impressive are the two transfer learning challenges held in 2011 and won by representation learning algorithms.
First, the Transfer Learning Challenge, presented at an ICML
2011 workshop of the same name, was won using unsupervised layer-wise pre-training (Bengio, 2011; Mesnil et al., 2011). A second Transfer Learning Challenge was held the same year and won by Goodfellow et al. (2011). Results were presented at NIPS 2011's Challenges in Learning Hierarchical
Models Workshop. Other examples of the successful application of representation learning in fields related to transfer
3 learning include domain adaptation, where the target remains the same but the input distribution changes (Glorot et al., 2011b; Chen et al., 2012). Of course, the case of jointly predicting outputs for many tasks or classes, i.e., performing multi-task learning also enhances the advantage of representation learning algorithms, e.g. as in Krizhevsky et al. (2012);
Collobert et al. (2011).
WHAT MAKES A REPRESENTATION GOOD?
Priors for Representation Learning in AI
In Bengio and LeCun (2007), one of us introduced the notion of AI-tasks, which are challenging for current machine learning algorithms, and involve complex but highly structured dependencies. One reason why explicitly dealing with representations is interesting is because they can be convenient to express many general priors about the world around us, i.e., priors that are not task-specific but would be likely to be useful for a learning machine to solve AI-tasks. Examples of such general-purpose priors are the following:
• Smoothness: we want to learn functions f s.t. x ≈ y generally implies f(x) ≈ f(y). This is the most basic prior and is present in most machine learning, but is insufficient to get around the curse of dimensionality, as discussed in Section 3.2 below.
• Multiple explanatory factors: the data generating distribution is generated by different underlying factors, and for the most part what one learns about one factor generalizes in many configurations of the other factors.
The objective to recover or at least disentangle these underlying factors of variation is discussed in Section 3.5.
This assumption is behind the idea of distributed representations, discussed in Section 3.3 below.
• A hierarchical organization of explanatory factors: the concepts that are useful at describing the world around us can be defined in terms of other concepts, in a hierarchy, with more abstract concepts higher in the hierarchy, being defined in terms of less abstract ones. This is the assumption exploited by having deep representations, elaborated in Section 3.4 below.
• Semi-supervised learning: in the context where we have input variables X and target variables Y we may want to predict, a subset of the factors that explain X's distribution explain a great deal of Y, given X. Hence representations that are useful for P(X) tend to be useful when learning P(Y |X), allowing sharing of statistical strength between the unsupervised and supervised learning tasks, as discussed in Section 4.
• Shared factors across tasks: in the context where we have many Y 's of interest or many learning tasks in general, tasks (e.g., the corresponding P(Y |X, task)) are explained by factors that are shared with other tasks, allowing sharing of statistical strengths across tasks, as discussed in the previous section (Multi-Task and Transfer Learning, Domain Adaptation).
• Manifolds: probability mass concentrates near regions that have a much smaller dimensionality than the original space where the data lives. This is explicitly exploited in some of the auto-encoder algorithms and other manifoldinspired algorithms described respectively in Sections 7.2 and 8.
• Natural clustering: different values of categorical variables such as object classes6 are associated with separate manifolds. More precisely, the local variations on the manifold tend to preserve the value of a category, and a linear interpolation between examples of different classes in general involves going through a low density region, i.e., P(X|Y = i) for different i tend to be well separated and not overlap much. For example, this is exploited in the Manifold Tangent Classifier discussed in Section 8.3. This hypothesis is consistent with the idea that humans have named categories and classes because of such statistical structure (discovered by their brain and propagated by their culture), and machine learning tasks often involves predicting such categorical variables.
• Temporal and spatial coherence: this is similar to the cluster assumption but concerns sequences of observations; consecutive or spatially nearby observations tend to be associated with the same value of relevant categorical concepts, or result in a small move on the surface of the high-density manifold. More generally, different factors change at different temporal and spatial scales, and many categorical concepts of interest change slowly. When attempting to capture such categorical variables, this prior can be enforced by making the associated representations slowly changing, i.e., penalizing changes in values over time or space. This prior was introduced in Becker and Hinton (1992) and is discussed in Section 11.3.
• Sparsity: for any given observation x, only a small fraction of the possible factors are relevant. In terms of representation, this could be represented by features that are often zero (as initially proposed by Olshausen and Field (1996)), or by the fact that most of the extracted features are insensitive to small variations of x. This can be achieved with certain forms of priors on latent variables (peaked at 0), or by using a non-linearity whose value is often flat at 0 (i.e., 0 and with a 0 derivative), or simply by penalizing the magnitude of the Jacobian matrix (of derivatives) of the function mapping input to representation. This is discussed in Sections 6.1.3 and 7.2.
We can view many of the above priors as ways to help the learner discover and disentangle some of the underlying (and a priori unknown) factors of variation that the data may reveal.
This idea is pursued further in Sections 3.5 and 11.4.
Smoothness and the Curse of Dimensionality
For AI-tasks, such as computer vision and natural language understanding, it seems hopeless to rely only on simple parametric models (such as linear models) because they cannot capture enough of the complexity of interest. Conversely, machine learning researchers have sought flexibility in local7 non-parametric learners such as kernel machines with
6. it is often the case that the Y of interest is a category
7. local in the sense that the value of the learned function at x depends mostly on training examples x(t)'s close to x
4 a fixed generic local-response kernel (such as the Gaussian kernel). Unfortunately, as argued at length by Bengio and Monperrus (2005); Bengio et al. (2006a); Bengio and LeCun(2007); Bengio (2009); Bengio et al. (2010), most of these algorithms only exploit the principle of local generalization, i.e., the assumption that the target function (to be learned) is smooth enough, so they rely on examples to explicitly map out the wrinkles of the target function. Generalization is mostly achieved by a form of local interpolation between neighboring training examples. Although smoothness can be a useful assumption, it is insufficient to deal with the curse of dimensionality, because the number of such wrinkles (ups and downs of the target function) may grow exponentially with the number of relevant interacting factors, when the data are represented in raw input space. We advocate learning algorithms that are flexible and non-parametric8 but do not rely exclusively on the smoothness assumption. Instead, we propose to incorporate generic priors such as those enumerated above into representation-learning algorithms. Smoothnessbased learners (such as kernel machines) and linear models can still be useful on top of such learned representations. In fact, the combination of learning a representation and kernel machine is equivalent to learning the kernel, i.e., the feature space. Kernel machines are useful, but they depend on a prior definition of a suitable similarity metric, or a feature space in which naive similarity metrics suffice. We would like to use the data, along with very generic priors, to discover those features, or equivalently, a similarity function.
Distributed representations
Good representations are expressive, meaning that a reasonably-sized learned representation can capture a huge number of possible input configurations. A simple counting argument helps us to assess the expressiveness of a model producing a representation: how many parameters does it require compared to the number of input regions (or configurations) it can distinguish? A one-hot representations, such as the result of traditional clustering algorithms, a Gaussian mixture model, a nearest-neighbor algorithm, a decision tree, or a Gaussian SVM all require O(N) parameters (and/or
O(N) examples) to distinguish O(N) input regions. One could naively believe that in order to define O(N) input regions one cannot do better. However, RBMs, sparse coding, autoencoders or multi-layer neural networks can all represent up to
O(2k) input regions using only O(N) parameters (with k the number of non-zero elements in a sparse representation, and k = N in non-sparse RBMs and other dense representations).
These are all distributed representations (where k elements can independently be varied, e.g., they are not mutually exclusive) or sparse (distributed representations where only a few of the elements can be varied at a time). The generalization of clustering to distributed representations is multi-clustering, where either several clusterings take place in parallel or the 8. We understand non-parametric as including all learning algorithms whose capacity can be increased appropriately as the amount of data and its complexity demands it, e.g. including mixture models and neural networks where the number of parameters is a data-selected hyper-parameter. same clustering is applied on different parts of the input, such as in the very popular hierarchical feature extraction for object recognition based on a histogram of cluster categories detected in different patches of an image (Lazebnik et al., 2006; Coates and Ng, 2011a). The exponential gain from distributed or sparse representations is discussed further in section 3.2 (and Figure 3.2) of Bengio (2009). It comes about because each parameter (e.g. the parameters of one of the units in a sparse code, or one of the units in a Restricted
Boltzmann Machine) can be re-used in many examples that are not simply near neighbors of each other, whereas with local generalization, different regions in input space are basically associated with their own private set of parameters, e.g., as in decision trees, nearest-neighbors, Gaussian SVMs, etc. In a distributed representation, an exponentially large number of possible subsets of features or hidden units can be activated in response to a given input. In a single-layer model, each feature is typically associated with a preferred input direction, corresponding to a hyperplane in input space, and the code or representation associated with that input is precisely the pattern of activation (which features respond to the input, and how much). This is in contrast with a non-distributed representation such as the one learned by most clustering algorithms, e.g., k-means, in which the representation of a given input vector is a one-hot code identifying which one of a small number of cluster centroids best represents the input 9.
Depth and abstraction
Depth is a key aspect to representation learning strategies we consider in this paper. As we will discuss, deep architectures are often challenging to train effectively and this has been the subject of much recent research and progress. However, despite these challenges, they carry two significant advantages that motivate our long-term interest in discovering successful training strategies for deep architectures. These advantages are: (1) deep architectures promote the re-use of features, and(2) deep architectures can potentially lead to progressively more abstract features at higher layers of representations(more removed from the data).
Feature re-use. The notion of re-use, which explains the power of distributed representations, is also at the heart of the theoretical advantages behind deep learning, i.e., constructing multiple levels of representation or learning a hierarchy of features. The depth of a circuit is the length of the longest path from an input node of the circuit to an output node of the circuit. The crucial property of a deep circuit is that its number of paths, i.e., ways to re-use different parts, can grow exponentially with its depth. Formally, one can change the depth of a given circuit by changing the definition of what
9. As discussed in (Bengio, 2009), things are only slightly better when allowing continuous-valued membership values, e.g., in ordinary mixture models (with separate parameters for each mixture component), but the difference in representational power is still exponential (Montufar and Morton, 2012). The situation may also seem better with a decision tree, where each given input is associated with a one-hot code over the tree leaves, which deterministically selects associated ancestors (the path from root to node).
Unfortunately, the number of different regions represented (equal to the number of leaves of the tree) still only grows linearly with the number of parameters used to specify it (Bengio and Delalleau, 2011).
5 each node can compute, but only by a constant factor. The typical computations we allow in each node include: weighted sum, product, artificial neuron model (such as a monotone nonlinearity on top of an affine transformation), computation of a kernel, or logic gates. Theoretical results clearly show families of functions where a deep representation can be exponentially more efficient than one that is insufficiently deep (H˚astad, 1986; H˚astad and Goldmann, 1991; Bengio et al., 2006a;
Bengio and LeCun, 2007; Bengio and Delalleau, 2011). If the same family of functions can be represented with fewer parameters (or more precisely with a smaller VC-dimension, learning theory would suggest that it can be learned with fewer examples, yielding improvements in both computational efficiency (less nodes to visit) and statistical efficiency (less parameters to learn, and re-use of these parameters over many different kinds of inputs).
Abstraction and invariance. Deep architectures can lead to abstract representations because more abstract concepts can often be constructed in terms of less abstract ones. In some cases, such as in the convolutional neural network (LeCun et al., 1998b), we build this abstraction in explicitly via a pooling mechanism (see section 11.2). More abstract concepts are generally invariant to most local changes of the input. That makes the representations that capture these concepts generally highly non-linear functions of the raw input. This is obviously true of categorical concepts, where more abstract representations detect categories that cover more varied phenomena (e.g. larger manifolds with more wrinkles) and thus they potentially have greater predictive power. Abstraction can also appear in high-level continuous-valued attributes that are only sensitive to some very specific types of changes in the input. Learning these sorts of invariant features has been a long-standing goal in pattern recognition.
Disentangling Factors of Variation
Beyond being distributed and invariant, we would like our representations to disentangle the factors of variation. Different explanatory factors of the data tend to change independently of each other in the input distribution, and only a few at a time tend to change when one considers a sequence of consecutive real-world inputs.
Complex data arise from the rich interaction of many sources. These factors interact in a complex web that can complicate AI-related tasks such as object classification. For example, an image is composed of the interaction between one or more light sources, the object shapes and the material properties of the various surfaces present in the image. Shadows from objects in the scene can fall on each other in complex patterns, creating the illusion of object boundaries where there are none and dramatically effect the perceived object shape.
How can we cope with these complex interactions? How can we disentangle the objects and their shadows? Ultimately, we believe the approach we adopt for overcoming these challenges must leverage the data itself, using vast quantities of unlabeled examples, to learn representations that separate the various explanatory sources. Doing so should give rise to a representation significantly more robust to the complex and richly structured variations extant in natural data sources for
AI-related tasks.
It is important to distinguish between the related but distinct goals of learning invariant features and learning to disentangle explanatory factors. The central difference is the preservation of information. Invariant features, by definition, have reduced sensitivity in the direction of invariance. This is the goal of building features that are insensitive to variation in the data that are uninformative to the task at hand. Unfortunately, it is often difficult to determine a priori which set of features will ultimately be relevant to the task at hand. Further, as is often the case in the context of deep learning methods, the feature set being trained may be destined to be used in multiple tasks that may have distinct subsets of relevant features. Considerations such as these lead us to the conclusion that the most robust approach to feature learning is to disentangle as many factors as possible, discarding as little information about the data as is practical. If some form of dimensionality reduction is desirable, then we hypothesize that the local directions of variation least represented in the training data should be first to be pruned out (as in PCA, for example, which does it globally instead of around each example).
What are good criteria for learning representations?
One of the challenges of representation learning that distinguishes it from other machine learning tasks such as classification is the difficulty in establishing a clear objective, or target for training. In the case of classification, the objective is (at least conceptually) obvious, we want to minimize the number of misclassifications on the training dataset. In the case of representation learning, our objective is far-removed from the ultimate objective, which is typically learning a classifier or some other predictor. Our problem is reminiscent of the credit assignment problem encountered in reinforcement learning. We have proposed that a good representation is one that disentangles the underlying factors of variation, but how do we translate that into appropriate training criteria? Is it even necessary to do anything but maximize likelihood under a good model or can we introduce priors such as those enumerated above (possibly data-dependent ones) that help the representation better do this disentangling? This question remains clearly open but is discussed in more detail in Sections 3.5 and 11.4.
BUILDING DEEP REPRESENTATIONS
In 2006, a breakthrough in feature learning and deep learning was initiated by Geoff Hinton and quickly followed up in the same year (Hinton et al., 2006a; Bengio et al., 2007;
Ranzato et al., 2007). It has been extensively reviewed and discussed in Bengio (2009). A central idea, referred to as greedy layerwise unsupervised pre-training, was to learn a hierarchy of features one level at a time, using unsupervised feature learning to learn a new transformation at each level to be composed with the previously learned transformations; essentially, each iteration of unsupervised feature learning adds one layer of weights to a deep neural network. Finally, the set
6 of layers could be combined to initialize a deep supervised predictor, such as a neural network classifier, or a deep generative model, such as a Deep Boltzmann Machine (Salakhutdinov and Hinton, 2009).
This paper is mostly about feature learning algorithms that can be used to form deep architectures. In particular, it was empirically observed that layerwise stacking of feature extraction often yielded better representations, e.g., in terms of classification error (Larochelle et al., 2009; Erhan et al., 2010b), quality of the samples generated by a probabilistic model (Salakhutdinov and Hinton, 2009) or in terms of the invariance properties of the learned features (Goodfellow et al., 2009). Whereas this section focuses on the idea of stacking single-layer models, Section 10 follows up with a discussion on joint training of all the layers.
The greedy layerwise unsupervised pre-training procedure (Hinton et al., 2006a; Bengio et al., 2007; Bengio, 2009) is based on training each layer with an unsupervised representation learning algorithm, taking the features produced at the previous level as input for the next level. It is then straightforward to use the resulting deep feature extraction either as input to a standard supervised machine learning predictor (such as an SVM) or as initialization for a deep supervised neural network (e.g., by appending a logistic regression layer or purely supervised layers of a multi-layer neural network). The layerwise procedure can also be applied in a purely supervised setting, called the greedy layerwise supervised pre-training (Bengio et al., 2007). For example, after the first one-hidden-layer MLP is trained, its output layer is discarded and another one-hidden-layer MLP can be stacked on top of it, etc. Although results reported in Bengio et al.(2007) were not as good as for unsupervised pre-training, they were nonetheless better than without pre-training at all.
Alternatively, the outputs of the previous layer can be fed as extra inputs for the next layer, as successfully done in Yu et al.
Whereas combining single layers into a supervised model is straightforward, it is less clear how layers pre-trained by unsupervised learning should be combined to form a better unsupervised model. We cover here some of the approaches to do so, but no clear winner emerges and much work has to be done to validate existing proposals or improve them.
The first proposal was to stack pre-trained RBMs into a Deep Belief Network (Hinton et al., 2006a) or DBN, where the top layer is interpreted as an RBM and the lower layers as a directed sigmoid belief network. However, it is not clear how to approximate maximum likelihood training to further optimize this generative model. One option is the wake-sleep algorithm (Hinton et al., 2006a) but more work should be done to assess the efficiency of this procedure in terms of improving the generative model.
The second approach that has been put forward is to combine the RBM parameters into a Deep Boltzmann Machine(DBM), by basically halving the RBM weights to obtain the DBM weights (Salakhutdinov and Hinton, 2009). The DBM can then be trained by approximate maximum likelihood as discussed in more details later (Section 10.2). This joint training has brought substantial improvements, both in terms of likelihood and in terms of classification performance of the resulting deep feature learner (Salakhutdinov and Hinton, Another early approach was to stack RBMs or autoencoders into a deep auto-encoder (Hinton and Salakhutdinov, 2006). If we have a series of encoder-decoder pairs(f (i)(·), g(i)(·)), then the overall encoder is the composition of the encoders, f (N)(... f (2)(f (1)(·))), and the overall decoder is its "transpose" (often with transposed weight matrices as well), g(1)(g(2)(... f (N)(·))). The deep auto-encoder (or its regularized version, as discussed in Section 7.2) can then be jointly trained, with all the parameters optimized with respect to a common training criterion. More work on this avenue clearly needs to be done, and it was probably avoided by fear of the challenges in training deep feedforward networks, discussed in the Section 10 along with very encouraging recent results.
Yet another recently proposed approach to training deep architectures(Ngiam et al., 2011) is to consider the iterative construction of a free energy function (i.e., with no explicit latent variables, except possibly for a top-level layer of hidden units) for a deep architecture as the composition of transformations associated with lower layers, followed by toplevel hidden units. The question is then how to train a model defined by an arbitrary parametrized (free) energy function.
Ngiam et al. (2011) have used Hybrid Monte Carlo (Neal, 1993), but other options include contrastive divergence (Hinton et al., 2006b), score matching (Hyv¨arinen, 2005a; Hyv¨arinen, 2008), denoising score matching (Kingma and LeCun, 2010;
Vincent, 2011), and noise-contrastive estimation (Gutmann and Hyvarinen, 2010).
SINGLE-LAYER LEARNING MODULES
Within the community of researchers interested in representation learning, there has developed two broad parallel lines of inquiry: one rooted in probabilistic graphical models and one rooted in neural networks. Fundamentally, the difference between these two paradigms is whether the layered architecture of a deep learning model is to be interpreted as describing a probabilistic graphical model or as describing a computation graph. In short, are hidden units considered latent random variables or as computational nodes?
To date, the dichotomy between these two paradigms has remained in the background, perhaps because they appear to have more characteristics in common than separating them.
We suggest that this is likely a function of the fact that much recent progress in both of these areas has focused on singlelayer greedy learning modules and the similarities between the types of single-layer models that have been explored: mainly, the restricted Boltzmann machine (RBM) on the probabilistic side, and the auto-encoder variants on the neural network side. Indeed, as shown by one of us (Vincent, 2011) and others (Swersky et al., 2011), in the case of the restricted
Boltzmann machine, training the model via an inductive principle known as score matching (Hyv¨arinen, 2005b) (to be discussed in sec. 6.4.3) is essentially identical to a regularized reconstruction objective of an auto-encoder. Another strong
7 link between pairs of models on both sides of this divide is when the computational graph for computing representation in the neural network model corresponds exactly to the computational graph that corresponds to inference in the probabilistic model, and this happens to also correspond to the structure of graphical model itself.
The connection between these two paradigms becomes more tenuous when we consider deeper models where, in the case of a probabilistic model, exact inference typically becomes intractable. In the case of deep models, the computational graph diverges from the structure of the model. For example, in the case of a deep Boltzmann machine, unrolling variational(approximate) inference into a computational graph results in a recurrent graph structure. We have performed preliminary exploration (Savard, 2011) of deterministic variants of deep auto-encoders whose computational graph is similar to that of a deep Boltzmann machine (in fact very close to the meanfield variational approximations associated with the Boltzmann machine), and that is one interesting intermediate point to explore (between the deterministic approaches and the graphical model approaches).
In the next few sections we will review the major developments in single-layer training modules used to support feature learning and particularly deep learning. We divide these sections between (Section 6) the probabilistic models, with inference and training schemes that directly parametrize the generative – or decoding – pathway and (Section 7) the typically neural network-based models that directly parametrize the encoding pathway. Interestingly, some models, like Predictive Sparse Decomposition (PSD) (Kavukcuoglu et al., 2008) inherit both properties, and will also be discussed (Section 7.2.4). We then present a different view of representation learning, based on the associated geometry and the manifold assumption, in Section 8.
Before we do this, we consider an unsupervised single-layer representation learning algorithm that spans all three views(probabilistic, auto-encoder, and manifold learning) discussed here.
Principal Components Analysis
We will use probably the oldest feature extraction algorithm, principal components analysis (PCA) (Pearson, 1901;
Hotelling, 1933), to illustrate the probabilistic, auto-encoder and manifold views of representation-learning. PCA learns a linear transformation h = f(x) = W T x + b of input x ∈ Rdx, where the columns of dx × dh matrix W form an orthogonal basis for the dh orthogonal directions of greatest variance in the training data. The result is dh features (the components of representation h) that are decorrelated. The three interpretations of PCA are the following: a) it is related to probabilistic models (Section 6) such as probabilistic PCA, factor analysis and the traditional multivariate Gaussian distribution (the leading eigenvectors of the covariance matrix are the principal components); b) the representation it learns is essentially the same as that learned by a basic linear auto-encoder (Section 7.2); and c) it can be viewed as a simple linear form of linear manifold learning (Section 8), i.e., characterizing a lower-dimensional region in input space near which the data density is peaked. Thus, PCA may be in the back of the reader's mind as a common thread relating these various viewpoints. Unfortunately the expressive power of linear features is very limited: they cannot be stacked to form deeper, more abstract representations since the composition of linear operations yields another linear operation. Here, we focus on recent algorithms that have been developed to extract non-linear features, which can be stacked in the construction of deep networks, although some authors simply insert a nonlinearity between learned single-layer linear projections (Le et al., 2011c; Chen et al., 2012).
Another rich family of feature extraction techniques that this review does not cover in any detail due to space constraints is Independent Component Analysis or ICA (Jutten and Herault, 1991; Comon, 1994; Bell and Sejnowski, 1997). Instead, we refer the reader to Hyv¨arinen et al. (2001a); Hyv¨arinen et al.(2009). Note that, while in the simplest case (complete, noisefree) ICA yields linear features, in the more general case it can be equated with a linear generative model with nonGaussian independent latent variables, similar to sparse coding(section 6.1.3), which result in non-linear features. Therefore, ICA and its variants like Independent and Topographic
ICA (Hyv¨arinen et al., 2001b) can and have been used to build deep networks (Le et al., 2010, 2011c): see section 11.2. The notion of obtaining independent components also appears similar to our stated goal of disentangling underlying explanatory factors through deep networks. However, for complex realworld distributions, it is doubtful that the relationship between truly independent underlying factors and the observed highdimensional data can be adequately characterized by a linear transformation.
PROBABILISTIC MODELS
From the probabilistic modeling perspective, the question of feature learning can be interpreted as an attempt to recover a parsimonious set of latent random variables that describe a distribution over the observed data. We can express any probabilistic model over the joint space of the latent variables, h, and observed or visible variables x, (associated with the data) as p(x, h). Feature values are conceived as the result of an inference process to determine the probability distribution of the latent variables given the data, i.e. p(h | x), often referred to as the posterior probability. Learning is conceived in term of estimating a set of model parameters that (locally) maximizes the likelihood of the training data with respect to the distribution over these latent variables. The probabilistic graphical model formalism gives us two possible modeling paradigms in which we can consider the question of inferring latent variables: directed and undirected graphical models. The key distinguishing factor between these paradigms is the nature of their parametrization of the joint distribution p(x, h). The choice of directed versus undirected model has a major impact on the nature and computational costs of the algorithmic approach to both inference and learning.
Directed Graphical Models
Directed latent factor models are parametrized through a decomposition of the joint distribution, p(x, h) = p(x | h)p(h), involving a prior p(h), and a likelihood p(x
| h) that
8 describes the observed data x in terms of the latent factors h. Unsupervised feature learning models that can be interpreted with this decomposition include: Principal Components
Analysis (PCA) (Roweis, 1997; Tipping and Bishop, 1999), sparse coding (Olshausen and Field, 1996), sigmoid belief networks (Neal, 1992) and the newly introduced spike-andslab sparse coding model (Goodfellow et al., 2011).
Explaining Away
In the context of latent factor models, the form of the directed model often leads to one important property, namely explaining away: a priori independent causes of an event can become non-independent given the observation of the event.
Latent factor models can generally be interpreted as latent cause models, where the h activations cause the observed x.
This renders the a priori independent h to be non-independent.
As a consequence, recovering the posterior distribution of h, p(h | x) (which we use as a basis for feature representation), is often computationally challenging and can be entirely intractable, especially when h is discrete.
A classic example that illustrates the phenomenon is to imagine you are on vacation away from home and you receive a phone call from the company that installed the security system at your house. They tell you that the alarm has been activated. You begin worrying your home has been burglarized, but then you hear on the radio that a minor earthquake has been reported in the area of your home. If you happen to know from prior experience that earthquakes sometimes cause your home alarm system to activate, then suddenly you relax, confident that your home has very likely not been burglarized.
The example illustrates how the observation, alarm activation, rendered two otherwise entirely independent causes, burglarized and earthquake, to become dependent – in this case, the dependency is one of mutual exclusivity. Since both burglarized and earthquake are very rare events and both can cause alarm activation, the observation of one explains away the other. The example demonstrates not only how observations can render causes to be statistically dependent, but also the utility of explaining away. It gives rise to a parsimonious prediction of the unseen or latent events from the observations.
Returning to latent factor models, despite the computational obstacles we face when attempting to recover the posterior over h, explaining away promises to provide a parsimonious p(h | x), which can be an extremely useful characteristic of a feature encoding scheme. If one thinks of a representation as being composed of various feature detectors and estimated attributes of the observed input, it is useful to allow the different features to compete and collaborate with each other to explain the input. This is naturally achieved with directed graphical models, but can also be achieved with undirected models (see Section 6.2) such as Boltzmann machines if there are lateral connections between the corresponding units or corresponding interaction terms in the energy function that defines the probability model.
Probabilistic Interpretation of PCA
While PCA was not originally cast as probabilistic model, it possesses a natural probabilistic interpretation (Roweis, 1997;
Tipping and Bishop, 1999) that casts PCA as factor analysis: p(h)
=
N(h; 0, σ2 hI) p(x | h)
=
N(x; Wh + µx, σ2 xI), (1) where x ∈ Rdx, h ∈ Rdh, N(v; µ, Σ) is the multivariate normal density of v with mean µ and covariance Σ, and columns of W span the same space as leading dh principal components, but are not constrained to be orthonormal.
Sparse Coding
As in the case of PCA, sparse coding has both a probabilistic and non-probabilistic interpretation. Sparse coding also relates a latent representation h (either a vector of random variables or a feature vector, depending on the interpretation) to the data x through a linear mapping W, which we refer to as the dictionary. The difference between sparse coding and PCA is that sparse coding includes a penalty to ensure a sparse activation of h is used to encode each input x.
Specifically, from a non-probabilistic perspective, sparse coding can be seen as recovering the code or feature vector associated with a new input x via: h∗ = f(x) = argmin h
∥x − Wh∥2
2 + λ∥h∥1, Learning the dictionary W can be accomplished by optimizing the following training criterion with respect to W:
JSC =
� t
∥x(t) − Wh∗(t)∥2
2, (3) where the x(t) is the input vector for example t and h∗(t) are the corresponding sparse codes determined by Eq. 2. W is usually constrained to have unit-norm columns (because one can arbitrarily exchange scaling of column i with scaling of h(t) i, such a constraint is necessary for the L1 penalty to have any effect).
The probabilistic interpretation of sparse coding differs from that of PCA, in that instead of a Gaussian prior on the latent random variable h, we use a sparsity inducing Laplace prior(corresponding to an L1 penalty): p(h)
= dh
� i λ exp(−λ|hi|) p(x | h)
=
N(x; Wh + µx, σ2 xI).
In the case of sparse coding, because we will seek a sparse representation (i.e., one with many features set to exactly zero), we will be interested in recovering the MAP (maximum a posteriori value of h: i.e. h∗ = argmaxh p(h | x) rather than its expected value E[ [h] |x]. Under this interpretation, dictionary learning proceeds as maximizing the likelihood of the data given these MAP values of h∗: argmaxW
� t p(x(t) | h∗(t)) subject to the norm constraint on W. Note that this parameter learning scheme, subject to the MAP values of the latent h, is not standard practice in the probabilistic graphical model literature. Typically the likelihood of the data p(x) =
� h p(x | h)p(h) is maximized directly. In the presence of latent variables, expectation maximization (Dempster et al., 1977) is employed where the parameters are optimized with respect to the marginal likelihood, i.e., summing or integrating the joint log-likelihood over the values of the latent variables
9 under their posterior P(h | x), rather than considering only the MAP values of h. The theoretical properties of this form of parameter learning are not yet well understood but seem to work well in practice (e.g. k-Means vs Gaussian mixture models and Viterbi training for HMMs). Note also that the interpretation of sparse coding as a MAP estimation can be questioned (Gribonval, 2011), because even though the interpretation of the L1 penalty as a log-prior is a possible interpretation, there can be other Bayesian interpretations compatible with the training criterion.
Sparse coding is an excellent example of the power of explaining away. The Laplace distribution (equivalently, the L1 penalty) over the latent h acts to resolve a sparse and parsimonious representation of the input. Even with a very overcomplete dictionary with many redundant bases, the MAP inference process used in sparse coding to find h∗ can pick out the most appropriate bases and zero the others, despite them having a high degree of correlation with the input. This property arises naturally in directed graphical models such as sparse coding and is entirely owing to the explaining away effect. It is not seen in commonly used undirected probabilistic models such as the RBM, nor is it seen in parametric feature encoding methods such as auto-encoders. The trade-off is that, compared to methods such as RBMs and auto-encoders, inference in sparse coding involves an extra inner-loop of optimization to find h∗ with a corresponding increase in the computational cost of feature extraction. Compared to autoencoders and RBMs, the code in sparse coding is a free variable for each example, and in that sense the implicit encoder is non-parametric.
One might expect that the parsimony of the sparse coding representation and its explaining away effect would be advantageous and indeed it seems to be the case. Coates and Ng (2011a) demonstrated with the CIFAR-10 object classification task (Krizhevsky and Hinton, 2009) with a patchbase feature extraction pipeline, that in the regime with few(< 1000) labeled training examples per class, the sparse coding representation significantly outperformed other highly competitive encoding schemes. Possibly because of these properties, and because of the very computationally efficient algorithms that have been proposed for it (in comparison with the general case of inference in the presence of explaining away), sparse coding enjoys considerable popularity as a feature learning and encoding paradigm. There are numerous examples of its successful application as a feature representation scheme, including natural image modeling (Raina et al., 2007; Kavukcuoglu et al., 2008; Coates and Ng, 2011a;
Yu et al., 2011), audio classification (Grosse et al., 2007), natural language processing (Bagnell and Bradley, 2009), as well as being a very successful model of the early visual cortex (Olshausen and Field, 1997). Sparsity criteria can also be generalized successfully to yield groups of features that prefer to all be zero, but if one or a few of them are active then the penalty for activating others in the group is small. Different group sparsity patterns can incorporate different forms of prior knowledge (Kavukcuoglu et al., 2009; Jenatton et al., 2009;
Bach et al., 2011; Gregor et al., 2011).
Spike-and-Slab Sparse Coding. Spike-and-slab sparse coding (S3C) is one example of a promising variation on sparse coding for feature learning (Goodfellow et al., 2012). The S3C model possesses a set of latent binary spike variables together with a a set of latent real-valued slab variables. The activation of the spike variables dictate the sparsity pattern.
S3C has been applied to the CIFAR-10 and CIFAR-100 object classification tasks (Krizhevsky and Hinton, 2009), and shows the same pattern as sparse coding of superior performance in the regime of relatively few (< 1000) labeled examples per class (Goodfellow et al., 2012). In fact, in both the CIFAR100 dataset (with 500 examples per class) and the CIFAR10 dataset (when the number of examples is reduced to a similar range), the S3C representation actually outperforms sparse coding representations. This advantage was revealed clearly with S3C winning the NIPS'2011 Transfer Learning
Challenge (Goodfellow et al., 2011).
Undirected Graphical Models
Undirected graphical models, also called Markov random fields (MRFs), parametrize the joint p(x, h) through a factorization in terms of unnormalized non-negative clique potentials: p(x, h) = 1
Zθ
� i ψi(x)
� j ηj(h)
� k νk(x, h)(5) where ψi(x), ηj(h) and νk(x, h) are the clique potentials describing the interactions between the visible elements, between the hidden variables, and those interaction between the visible and hidden variables respectively. The partition function Zθ ensures that the distribution is normalized. Within the context of unsupervised feature learning, we generally see a particular form of Markov random field called a Boltzmann distribution with clique potentials constrained to be positive: p(x, h) = 1
Zθ exp (−Eθ(x, h)), (6) where Eθ(x, h) is the energy function and contains the interactions described by the MRF clique potentials and θ are the model parameters that characterize these interactions.
A
Boltzmann machine is defined as a network of symmetrically-coupled binary random variables or units.
These stochastic units can be divided into two groups: (1) the visible units x ∈ {0, 1}dx that represent the data, and (2) the hidden or latent units h ∈ {0, 1}dh that mediate dependencies between the visible units through their mutual interactions. The pattern of interaction is specified through the energy function:
EBM θ(x, h) = −1
2xT Ux − 1
2hT V h − xT Wh − bT x − dT h, (7) where θ
=
{U, V, W, b, d} are the model parameters which respectively encode the visible-to-visible interactions, the hidden-to-hidden interactions, the visible-to-hidden interactions, the visible self-connections, and the hidden self-connections (also known as biases). To avoid overparametrization, the diagonals of U and V are set to zero.
The Boltzmann machine energy function specifies the probability distribution over the joint space [x, h], via the Boltzmann distribution, Eq. 6, with the partition function Zθ given by:
Zθ = x1=1
� x1=0
· · · xdx =1
� xdx =0 h1=1
� h1=0
· · · hdh =1
� hdh =0 exp
�
−EBM θ(x, h; θ)
�
This joint probability distribution gives rise to the set of conditional distributions of the form:
P(hi | x, h\i) = sigmoid
�
�� j
Wjixj +
� i′̸=i
Vii′hi′ + di
�
�
P(xj | h, x\j) = sigmoid
�
�� i
Wjixj +
� j′̸=j
Ujj′xj′ + bj
�
�.
In general, inference in the Boltzmann machine is intractable.
For example, computing the conditional probability of hi given the visibles, P(hi | x), requires marginalizing over the rest of the hiddens, which implies evaluating a sum with 2dh−1 terms:
P(hi | x) = h1=1
� h1=0
· · · hi−1=1
� hi−1=0 hi+1=1
� hi+1=0
· · · hdh =1
� hdh =0
P(h | x)
However with some judicious choices in the pattern of interactions between the visible and hidden units, more tractable subsets of the model family are possible, as we discuss next.
Restricted Boltzmann Machines
The restricted Boltzmann machine (RBM) is likely the most popular subclass of Boltzmann machine (Smolensky, 1986).
It is defined by restricting the interactions in the Boltzmann energy function, in Eq. 7, to only those between h and x, i.e.
ERBM θ is EBM θ with U = 0 and V = 0. As such, the RBM can be said to form a bipartite graph with the visibles and the hiddens forming two layers of vertices in the graph (and no connection between units of the same layer). With this restriction, the RBM possesses the useful property that the conditional distribution over the hidden units factorizes given the visibles:
P(h | x) =
� i
P(hi | x)
P(hi = 1 | x) = sigmoid
�� j
Wjixj + di
�
Likewise, the conditional distribution over the visible units given the hiddens also factorizes:
P(x | h) =
� j
P(xj | h)
P(xj = 1 | h) = sigmoid
�� i
Wjihi + bj
�
This conditional factorization property of the RBM immediately implies that most inferences we would like to make are readily tractable. For example, the RBM feature representation is taken to be the set of posterior marginals P(hi | x), which, given the conditional independence described in Eq. 12, are immediately available. Note that this is in stark contrast to the situation with popular directed graphical models for unsupervised feature extraction, where computing the posterior probability is intractable.
Importantly, the tractability of the RBM does not extend to its partition function, which still involves summing an exponential number of terms. It does imply however that we can limit the number of terms to min{2dx, 2dh}. Usually this is still an unmanageable number of terms and therefore we must resort to approximate methods to deal with its estimation.
It is difficult to overstate the impact the RBM has had to the fields of unsupervised feature learning and deep learning.
It has been used in a truly impressive variety of applications, including fMRI image classification (Schmah et al., 2009), motion and spatial transformations (Taylor and Hinton, 2009; Memisevic and Hinton, 2010), collaborative filtering(Salakhutdinov et al., 2007) and natural image modeling(Ranzato and Hinton, 2010; Courville et al., 2011b).
Generalizations of the RBM to Real-valued data
Important progress has been made in the last few years in defining generalizations of the RBM that better capture realvalued data, in particular real-valued image data, by better modeling the conditional covariance of the input pixels. The standard RBM, as discussed above, is defined with both binary visible variables v ∈ {0, 1} and binary latent variables h ∈
{0, 1}. The tractability of inference and learning in the RBM has inspired many authors to extend it, via modifications of its energy function, to model other kinds of data distributions. In particular, there has been multiple attempts to develop RBMtype models of real-valued data, where x ∈ Rdx. The most straightforward approach to modeling real-valued observations within the RBM framework is the so-called Gaussian RBM(GRBM) where the only change in the RBM energy function is to the visible units biases, by adding a bias term that is quadratic in the visible units x. While it probably remains the most popular way to model real-valued data within the RBM framework, Ranzato and Hinton (2010) suggest that the GRBM has proved to be a somewhat unsatisfactory model of natural images. The trained features typically do not represent sharp edges that occur at object boundaries and lead to latent representations that are not particularly useful features for classification tasks. Ranzato and Hinton (2010) argue that the failure of the GRBM to adequately capture the statistical structure of natural images stems from the exclusive use of the model capacity to capture the conditional mean at the expense of the conditional covariance. Natural images, they argue, are chiefly characterized by the covariance of the pixel values, not by their absolute values. This point is supported by the common use of preprocessing methods that standardize the global scaling of the pixel values across images in a dataset or across the pixel values within each image.
These kinds of concerns about the ability of the GRBM to model natural image data has lead to the development of alternative RBM-based models that each attempt to take on this objective of better modeling non-diagonal conditional covariances. (Ranzato and Hinton, 2010) introduced the mean and covariance RBM (mcRBM). Like the GRBM, the mcRBM is a 2-layer Boltzmann machine that explicitly models the visible units as Gaussian distributed quantities. However unlike the GRBM, the mcRBM uses its hidden layer to independently parametrize both the mean and covariance of the data through two sets of hidden units. The mcRBM is a combination of the covariance RBM (cRBM) (Ranzato et al., 2010a), that models the conditional covariance, with the GRBM that captures the 11 conditional mean. While the GRBM has shown considerable potential as the basis of a highly successful phoneme recognition system (Dahl et al., 2010), it seems that due to difficulties in training the mcRBM, the model has been largely superseded by the mPoT model. The mPoT model (mean-product of Student's T-distributions model)(Ranzato et al., 2010b) is a combination of the GRBM and the product of Student's Tdistributions model (Welling et al., 2003). It is an energy-based model where the conditional distribution over the visible units conditioned on the hidden variables is a multivariate Gaussian(non-diagonal covariance) and the complementary conditional distribution over the hidden variables given the visibles are a set of independent Gamma distributions.
The PoT model has recently been generalized to the mPoT model (Ranzato et al., 2010b) to include nonzero Gaussian means by the addition of GRBM-like hidden units, similarly to how the mcRBM generalizes the cRBM. The mPoT model has been used to synthesize large-scale natural images (Ranzato et al., 2010b) that show large-scale features and shadowing structure. It has been used to model natural textures (Kivinen and Williams, 2012) in a tiled-convolution configuration (see section 11.2).
Another recently introduced RBM-based model with the objective of having the hidden units encode both the mean and covariance information is the spike-and-slab Restricted
Boltzmann Machine (ssRBM) (Courville et al., 2011a,b).
The ssRBM is defined as having both a real-valued "slab" variable and a binary "spike" variable associated with each unit in the hidden layer. The ssRBM has been demonstrated as a feature learning and extraction scheme in the context of CIFAR-10 object classification (Krizhevsky and Hinton, 2009) from natural images and has performed well in the role (Courville et al., 2011a,b). When trained convolutionally(see Section 11.2) on full CIFAR-10 natural images, the model demonstrated the ability to generate natural image samples that seem to capture the broad statistical structure of natural images better than previous parametric generative models, as illustrated with the samples of Figure 2.
The mcRBM, mPoT and ssRBM each set out to model real-valued data such that the hidden units encode not only the conditional mean of the data but also its conditional covariance. Other than differences in the training schemes, the most significant difference between these models is how they encode their conditional covariance. While the mcRBM and the mPoT use the activation of the hidden units to enforce constraints on the covariance of x, the ssRBM uses the hidden unit to pinch the precision matrix along the direction specified by the corresponding weight vector. These two ways of modeling conditional covariance diverge when the dimensionality of the hidden layer is significantly different from that of the input. In the over-complete setting, sparse activation with the ssRBM parametrization permits variance only in the select directions of the sparsely activated hidden units. This is a property the ssRBM shares with sparse coding models (Olshausen and Field, 1997; Grosse et al., 2007). On the other hand, in the case of the mPoT or mcRBM, an over-complete set of constraints on the covariance implies that capturing arbitrary covariance along a particular direction of the input requires
Fig. 2. (Top) Samples from a convolutionally trained µ-ssRBM, see details in Courville et al. (2011b). (Bottom) The images in the CIFAR-10 training set closest (L2 distance with contrast normalized training images) to the corresponding model samples.
The model does not appear to be capturing the natural image statistical structure by overfitting particular examples from the dataset. decreasing potentially all constraints with positive projection in that direction. This perspective would suggest that the mPoT and mcRBM do not appear to be well suited to provide a sparse representation in the overcomplete setting.
RBM parameter estimation
In this section we discuss several algorithms for training the restricted Boltzmann machine. Many of the methods we discuss are applicable to more general undirected graphical models, but are particularly practical in the RBM setting.
Freund and Haussler (1994) proposed a learning algorithm for harmoniums (RBMs) based on projection pursuit (Friedman and Stuetzle, 1981). Contrastive Divergence (Hinton, 1999;
Hinton et al., 2006a) has been used most often to train
RBMs, and many recent papers use Stochastic Maximum
Likelihood (Younes, 1999; Tieleman, 2008).
As discussed in Sec. 6.1, in training probabilistic models parameters are typically adapted in order to maximize the likelihood of the training data (or equivalently the log-likelihood, or its penalized version, which adds a regularization term).
With T training examples, the log likelihood is given by:
T
� t=1 log P(x(t); θ) =
T
� t=1 log
� h∈{0,1}dh
P(x(t), h; θ).
One straightforward way we can consider maximizing this quantity is to take small steps uphill, following the loglikelihood gradient, to find a local maximum of the likelihood.
For any Boltzmann machine, the gradient of the log-likelihood
12 of the data is given by:
∂
∂θi
T
� t=1 log p(x(t))
=
−
T
� t=1
Ep(h|x(t))
� ∂
∂θi EBM θ(x(t), h)
�
T
� t=1
Ep(x,h)
� ∂
∂θi EBM θ(x, h)
�, (15) where we have the expectations with respect to p(h(t) | x(t)) in the "clamped" condition (also called the positive phase), and over the full joint p(x, h) in the "unclamped" condition(also called the negative phase). Intuitively, the gradient acts to locally move the model distribution (the negative phase distribution) toward the data distribution (positive phase distribution), by pushing down the energy of (h, x(t)) pairs (for h ∼ P(h|x(t))) while pushing up the energy of (h, x) pairs(for (h, x) ∼ P(h, x)) until the two forces are in equilibrium, at which point the sufficient statistics (gradient of the energy function) have equal expectations with x sampled from the training distribution or with x sampled from the model.
The RBM conditional independence properties imply that the expectation in the positive phase of Eq. 15 is readily tractable. The negative phase term – arising from the partition function's contribution to the log-likelihood gradient – is more problematic because the computation of the expectation over the joint is not tractable. The various ways of dealing with the partition function's contribution to the gradient have brought about a number of different training algorithms, many trying to approximate the log-likelihood gradient.
To approximate the expectation of the joint distribution in the negative phase contribution to the gradient, it is natural to again consider exploiting the conditional independence of the RBM in order to specify a Monte Carlo approximation of the expectation over the joint:
Ep(x,h)
� ∂
∂θi ERBM θ(x, h)
�
≈ 1
L
L
� l=1
∂
∂θi ERBM θ(˜x(l), ˜h(l)), (16) with the samples (˜x(l), ˜h(l)) drawn by a block Gibbs MCMC(Markov chain Monte Carlo) sampling scheme from the model distribution:
˜x(l)
∼
P(x | ˜h(l−1))
˜h(l)
∼
P(h | ˜x(l)).
Naively, for each gradient update step, one would start a Gibbs sampling chain, wait until the chain converges to the equilibrium distribution and then draw a sufficient number of samples to approximate the expected gradient with respect to the model (joint) distribution in Eq. 16. Then restart the process for the next step of approximate gradient ascent on the log-likelihood. This procedure has the obvious flaw that waiting for the Gibbs chain to "burn-in" and reach equilibrium anew for each gradient update cannot form the basis of a practical training algorithm. Contrastive Divergence (Hinton, 1999; Hinton et al., 2006a), Stochastic Maximum Likelihood (Younes, 1999; Tieleman, 2008) and fast-weights persistent contrastive divergence or FPCD (Tieleman and Hinton, 2009) are all examples of algorithms that attempt sidestep the need to burn-in the negative phase Markov chain.
Contrastive Divergence:
Contrastive divergence (CD) estimation (Hinton, 1999; Hinton et al., 2006a) uses a biased estimate of the gradient in Eq. 15 by approximating the negative phase expectation with a very short Gibbs chain (often just one step) initialized at the training data used in the positive phase. This initialization is chosen to reduce the variance of the negative expectation based on samples from the short running Gibbs sampler. The intuition is that, while the samples drawn from very short
Gibbs chains may be a heavily biased (and poor) representation of the model distribution, they are at least moving in the direction of the model distribution relative to the data distribution represented by the positive phase training data.
Consequently, they may combine to produce a good estimate of the gradient, or direction of progress. Much has been written about the properties and alternative interpretations of CD, e.g.
Carreira-Perpi˜nan and Hinton (2005); Yuille (2005); Bengio and Delalleau (2009); Sutskever and Tieleman (2010).
Stochastic Maximum Likelihood:
The Stochastic Maximum Likelihood (SML) algorithm (also known as persistent contrastive divergence or PCD) (Younes, 1999; Tieleman, 2008) is an alternative way to sidestep an extended burn-in of the negative phase Gibbs sampler. At each gradient update, rather than initializing the Gibbs chain at the positive phase sample as in CD, SML initializes the chain at the last state of the chain used for the previous update. In other words, SML uses a continually running Gibbs chain (or often a number of Gibbs chains run in parallel) from which samples are drawn to estimate the negative phase expectation.
Despite the model parameters changing between updates, these changes should be small enough that only a few steps of Gibbs(in practice, often one step is used) are required to maintain samples from the equilibrium distribution of the Gibbs chain, i.e. the model distribution.
One aspect of SML that has received considerable recent attention is that it relies on the Gibbs chain to have reasonably good mixing properties for learning to succeed. Typically, as learning progresses and the weights of the RBM grow, the ergodicity of the Gibbs sample begins to break down10. If the learning rate ϵ associated with gradient ascent θ ← θ + ϵˆg(with E[ˆg] ≈ ∂ log pθ(x)
∂θ
) is not reduced to compensate, then the Gibbs sampler will diverge from the model distribution and learning will fail. There have been a number of attempts made to address the failure of Gibbs chain mixing in the context of SML. Desjardins et al. (2010); Cho et al. (2010);
Salakhutdinov (2010b,a) have all considered various forms of tempered transitions to improve the mixing rate of the negative phase Gibbs chain.
Tieleman and Hinton (2009) have proposed quite a different approach to addressing potential mixing problems of SML with their fast-weights persistent contrastive divergence
10. When weights become large, the estimated distribution is more peaky, and the chain takes very long time to mix, to move from mode to mode, so that practically the gradient estimator can be very poor. This is a serious chicken-and-egg problem because if sampling is not effective, nor is the training procedure, which may seem to stall.(FPCD), and it has also been exploited to train Deep Boltzmann Machines (Salakhutdinov, 2010a) and construct a pure sampling algorithm for RBMs (Breuleux et al., 2011). FPCD builds on the surprising but robust tendency of Gibbs chains to mix better during SML learning than when the model parameters are fixed. The phenomenon is rooted in the form of the likelihood gradient itself (Eq. 15). The samples drawn from the SML Gibbs chain are used in the negative phase of the gradient, which implies that the learning update will slightly increase the energy (decrease the probability) of those samples, making the region in the neighborhood of those samples less likely to be resampled and therefore making it more likely that the samples will move somewhere else (typically going near another mode). Rather than drawing samples from the distribution of the current model (with parameters θ), FPCD exaggerates this effect by drawing samples from a local perturbation of the model with parameters θ∗ and an update specified by: θ∗ t+1 = (1 − η)θt+1 + ηθ∗ t + ϵ∗ ∂
∂θi
� T
� t=1 log p(x(t))
�, (17) where ϵ∗ is the relatively large fast-weight learning rate(ϵ∗ > ϵ) and 0 < η < 1 (but near 1) is a forgetting factor that keeps the perturbed model close to the current model.
Unlike tempering, FPCD does not converge to the model distribution as ϵ and ϵ∗ go to 0, and further work is necessary to characterize the nature of its approximation to the model distribution. Nevertheless, FPCD is a popular and apparently effective means of drawing approximate samples from the model distribution that faithfully represent its diversity, at the price of sometimes generating spurious samples in between two modes (because the fast weights roughly correspond to a smoothed view of the current model's energy function). It has been applied in a variety of applications (Tieleman and Hinton, 2009; Ranzato et al., 2011; Kivinen and Williams, 2012) and it has been transformed into a sampling algorithm (Breuleux et al., 2011) that also shares this fast mixing property with herding (Welling, 2009), for the same reason, i.e., introducing negative correlations between consecutive samples of the chain in order to promote faster mixing.
Pseudolikelihood, Ratio-matching and other Inductive Principles
While CD, SML and FPCD are by far the most popular methods for training RBMs and RBM-based models, all of these methods are perhaps most naturally described as offering different approximations to maximum likelihood training. There exist other inductive principles that are alternatives to maximum likelihood that can also be used to train RBMs. In particular, these include pseudo-likelihood (Besag, 1975) and ratiomatching (Hyv¨arinen, 2007). Both of these inductive principles attempt to avoid explicitly dealing with the partition function, and their asymptotic efficiency has been analyzed (Marlin and de Freitas, 2011). Pseudo-likelihood seeks to maximize the product of all one-dimensional conditional distributions of the form P(xd|x\d), while ratio-matching can be interpreted as an extension of score matching (Hyv¨arinen, 2005a) to discrete data types. Both methods amount to weighted differences of the gradient of the RBM free energy11 evaluated at a data point and at all neighboring points within a hamming ball of radius
1. One drawback of these methods is that the computation of the statistics for all neighbors of each training data point require a significant computational overhead, scaling linearly with the dimensionality of the input, nd. CD, SML and FPCD have no such issue. Marlin et al. (2010) provides an excellent survey of these methods and their relation to CD and SML.
They also empirically compared all of these methods on a range of classification, reconstruction and density modeling tasks and found that, in general, SML provided the best combination of overall performance and computational tractability.
However, in a later study, the same authors (Swersky et al., 2011) found denoising score matching (Kingma and LeCun, 2010; Vincent, 2011) to be a competitive inductive principle both in terms of classification performance (with respect to
SML) and in terms of computational efficiency (with respect to analytically obtained score matching). Note that denoising score matching is a special case of the denoising auto-encoder training criterion (Section 7.2.2) when the reconstruction error residual equals a gradient, i.e., the score function associated with an energy function, as shown in (Vincent, 2011).
In the spirit of the Boltzmann machine update rule (Eq. 15) several other principles have been proposed to train energybased models. One approach is noise-contrastive estimation (Gutmann and Hyvarinen, 2010), in which the training criterion is transformed into a probabilistic classification problem: distinguish between (positive) training examples and(negative) noise samples generated by a broad distribution(such as the Gaussian). Another family of approaches, more in the spirit of Contrastive Divergence, relies on distinguishing positive examples (of the training distribution) and negative examples obtained by slight perturbations of the positive examples (Collobert and Weston, 2008; Bordes et al., 2012;
Weston et al., 2010). This apparently simple principle has been used successfully to train a model on huge quantities of data to map images and queries in the same space for Google's image search (Weston et al., 2010).
DIRECT ENCODING: LEARNING A PARAMETRIC MAP FROM INPUT TO REPRESENTATION
Within the framework of probabilistic models adopted in Section 6, the learned representation is always associated with latent variables, specifically with their posterior distribution given an observed input x. Unfortunately, the posterior distribution of latent variables given inputs tends to become very complicated and intractable if the model has more than a couple of interconnected layers, whether in the directed or undirected graphical model frameworks. It then becomes necessary to resort to sampling or approximate inference techniques, and to pay the associated computational and approximation error price. This is in addition to the difficulties raised by the intractable partition function in undirected graphical
11. The free energy F(x; θ) is defined in relation to the marginal likelihood of the data: F(x; θ) = − log P(x) − log Zθ and in the case of the RBM is tractable.
14 models. Moreover a posterior distribution over latent variables is not yet a simple usable feature vector that can for example be fed to a classifier. So actual feature values are typically derived from that distribution, taking the latent variable's expectation (as is typically done with RBMs), their marginal probability, or finding their most likely value (as in sparse coding). If we are to extract stable deterministic numerical feature values in the end anyway, an alternative (apparently) non-probabilistic feature learning paradigm that focuses on carrying out this part of the computation, very efficiently, is that of auto-encoders and other directly parametrized feature or representation functions. The commonality between these methods is that they learn a direct encoding, i.e., parametric map from inputs to their representation, The regularized auto-encoders are described in the next section, and are concerned with the case where the encoding function that computes the representation is associated with a decoding function that maps back to input space. In sections 8.1 and 11.3, we consider some direct encoding methods that do not require a decoder and a reconstruction error, such as semi-supervised embedding (Weston et al., 2008) and slow feature analysis (Wiskott and Sejnowski, 2002).
Auto-Encoders
Whereas probabilistic models sometimes define intermediate variables whose posterior can then be interpreted as a representation, in the auto-encoder framework (LeCun, 1987; Bourlard and Kamp, 1988; Hinton and Zemel, 1994), one starts by explicitly defining a feature-extracting function in a specific parametrized closed form. This function, that we will denote fθ, is called the encoder and will allow the straightforward and efficient computation of a feature vector h = fθ(x) from an input x. For each example x(t) from a data set
{x(1),..., x(T )}, we define h(t) = fθ(x(t))(18) where h(t) is the feature-vector or representation or code computed from x(t). Another closed form parametrized function gθ, called the decoder, maps from feature space back into input space, producing a reconstruction r = gθ(h). Whereas probabilistic models are defined from an explicit probability function and are trained to maximize (often approximately) the data likelihood (or a proxy), auto-encoders are parametrized through their encoder and decoder and are trained using a different training principle. The set of parameters θ of the encoder and decoder are learned simultaneously on the task of reconstructing as well as possible the original input, i.e. attempting to incur the lowest possible reconstruction error
L(x, r) – a measure of the discrepancy between x and its reconstruction – on average over a training set. Note how the main objective is to make reconstruction error low on the training examples, and by generalization, where the probability is high under the unknown data-generating distribution. For the minimization of reconstruction error to capture the structure of the data-generating distribution, it is therefore important that something in the training criterion or the parametrization prevents the auto-encoder from learning the identity function, which would yield zero reconstruction error everywhere. This is achieved through various means in the different forms of auto-encoders, as described below in more detail, and we call these regularized auto-encoders. A particular form of regularization consists in constraining the code to have a low dimension, and this is what the classical auto-encoder or PCA do.
In summary, basic auto-encoder training consists in finding a value of parameter vector θ minimizing reconstruction error
JDAE(θ)
=
� t
L(x(t), gθ(fθ(x(t))))(19) where x(t) is a training example. This minimization is usually carried out by stochastic gradient descent as in the training of Multi-Layer-Perceptrons (MLPs). Since auto-encoders were primarily developed as MLPs predicting their input, the most commonly used forms for the encoder and decoder are affine mappings, optionally followed by a non-linearity: fθ(x)
= sf(b + Wx)(20) gθ(h)
= sg(d + W ′h)(21) where sf and sg are the encoder and decoder activation functions (typically the element-wise sigmoid or hyperbolic tangent non-linearity, or the identity function if staying linear).
The set of parameters of such a model is θ = {W, b, W ′, d} where b and d are called encoder and decoder bias vectors, and W and W ′ are the encoder and decoder weight matrices.
The choice of sg and L depends largely on the input domain range. and nature, and are usually chosen so that L returns a negative log-likelihood for the observed value of x. A natural choice for an unbounded domain is a linear decoder with a squared reconstruction error, i.e. sg(a) = a and L(x, r) =
∥x − r∥2. If inputs are bounded between 0 and 1 however, ensuring a similarly-bounded reconstruction can be achieved by using sg = sigmoid. In addition if the inputs are of a binary nature, a binary cross-entropy loss12 is sometimes used.
In the case of a linear auto-encoder (linear encoder and decoder) with squared reconstruction error, the basic autoencoder objective in Equation 19 is known to learn the same subspace13 as PCA. This is also true when using a sigmoid nonlinearity in the encoder (Bourlard and Kamp, 1988), but not if the weights W and W ′ are tied (W ′ = W T ).
Similarly, Le et al. (2011b) recently showed that adding a regularization term of the form � i
� j s3(Wjxi) to a linear auto-encoder with tied weights, where s3 is a nonlinear convex function, yields an efficient algorithm for learning linear ICA.
If both encoder and decoder use a sigmoid non-linearity, then fθ(x) and gθ(h) have the exact same form as the conditionals P(h | v) and P(v | h) of binary RBMs (see Section
6.2.1). This similarity motivated an initial study (Bengio et al., 2007) of the possibility of replacing RBMs with auto-encoders as the basic pre-training strategy for building deep networks, as well as the comparative analysis of auto-encoder reconstruction error gradient and contrastive divergence updates (Bengio and Delalleau, 2009).
12. L(x, r) = − �dx i=1 xi log(ri) + (1 − ri) log(1 − ri)
13. Contrary to traditional PCA loading factors, but similarly to the parameters learned by probabilistic PCA, the weight vectors learned by such an auto-encoder are not constrained to form an orthonormal basis, nor to have a meaningful ordering. They will however span the same subspace.
One notable difference in the parametrization is that RBMs use a single weight matrix, which follows naturally from their energy function, whereas the auto-encoder framework allows for a different matrix in the encoder and decoder. In practice however, weight-tying in which one defines W ′
= W T may be (and is most often) used, rendering the parametrizations identical. The usual training procedures however differ greatly between the two approaches. A practical advantage of training auto-encoder variants is that they define a simple tractable optimization objective that can be used to monitor progress.
Regularized Auto-Encoders
Traditionally, auto-encoders, like PCA, were primarily seen as a dimensionality reduction technique and thus used a bottleneck, i.e. dh < dx. But successful uses of sparse coding and RBM approaches tend to favour learning over-complete representations, i.e. dh > dx. This can render the autoencoding problem too simple (e.g. simply duplicating the input in the features may allow perfect reconstruction without having extracted more meaningful features). Thus alternative ways to "constrain" the representation, other than constraining its dimensionality, have been investigated. We broadly refer to these alternatives as "regularized" auto-encoders. The effect of a bottleneck or of these regularization terms is that the auto-encoder cannot reconstruct well everything, it is trained to reconstruct well the training examples and generalization means that reconstruction error is also small on test examples.
An interesting justification (Ranzato et al., 2008) for the sparsity penalty (or any penalty that restricts in a soft way the volume of hidden configurations easily accessible by the learner) is that it acts in spirit like the partition function of RBMs, by making sure that only few input configurations can have a low reconstruction error.
Alternatively, one can view the objective of the regularization applied to an auto-encoder is to make the representation as "constant" (insensitive) as possible with respect to changes in input. This view immediately justifies two variants of regularized auto-encoders described below: contractive autoencoders reduce the number of effective degrees of freedom of the representation (around each point) by making the encoder contractive, i.e., making the derivative of the encoder small(thus making the hidden units saturate), while the denoising auto-encoder makes the whole mapping "robust", i.e., insensitive to small random perturbations, or contractive, making sure that the reconstruction cannot be good when moving in most directions around a training example.
Sparse Auto-Encoders
The earliest use of single-layer auto-encoders for building deep architectures by stacking them (Bengio et al., 2007) considered the idea of tying the encoder weights and decoder weights to restrict capacity as well as the idea of introducing a form of sparsity regularization (Ranzato et al., 2007).
Several ways of introducing sparsity in the representation learned by auto-encoders have then been proposed, some by penalizing the hidden unit biases (making these additive offset parameters more negative) (Ranzato et al., 2007; Lee et al., 2008; Goodfellow et al., 2009; Larochelle and Bengio, 2008) and some by directly penalizing the output of the hidden unit activations (making them closer to their saturating value at
0) (Ranzato et al., 2008; Le et al., 2011a; Zou et al., 2011).
Note that penalizing the bias runs the danger that the weights could compensate for the bias, which could hurt the numerical optimization of parameters. When directly penalizing the hidden unit outputs, several variants can be found in the literature, but no clear comparative analysis has been published to evaluate which one works better. Although the L1 penalty(i.e., simply the sum of output elements hj in the case of sigmoid non-linearity) would seem the most natural (because of its use in sparse coding), it is used in few papers involving sparse auto-encoders. A close cousin of the L1 penalty is the Student-t penalty (log(1+h2 j)), originally proposed for sparse coding (Olshausen and Field, 1997). Several papers penalize the average output ¯hj (e.g. over a minibatch), and instead of pushing it to 0, encourage it to approach a fixed target, either through a mean-square error penalty, or maybe more sensibly (because hj behaves like a probability), a KullbackLiebler divergence with respect to the binomial distribution with probability ρ: −ρ log ¯hj − (1 − ρ) log(1 − ¯hj)+constant, e.g., with ρ = 0.05.
Denoising Auto-Encoders
Vincent et al. (2008, 2010) proposed altering the training objective in Equation 19 from mere reconstruction to that of denoising an artificially corrupted input, i.e. learning to reconstruct the clean input from a corrupted version. Learning the identity is no longer enough: the learner must capture the structure of the input distribution in order to optimally undo the effect of the corruption process, with the reconstruction essentially being a nearby but higher density point than the corrupted input. Figure 3 illustrates that the denoising autoencoder is learning a reconstruction function that corresponds to a vector field pointing towards high-density regions (the manifold where examples concentrate).
Corrupted input
Corrupted input prior:&examples&concentrate& near&a&lower&dimensional&
"manifold"&& original input
Fig. 3.
When the data concentrate near a lower-dimensional manifold, the corruption vector is most of the time almost orthogonal to the manifold, and the reconstruction function learns to denoise, map from low-probability configurations (corrupted inputs) to high-probability ones (original inputs), creating a kind of vector field aligned with the score (derivative of the estimated density).
Formally, the objective optimized by such a Denoising
Auto-Encoder (DAE) is:
JDAE
=
� t
Eq(˜x|x(t))
�
L(x(t), gθ(fθ(˜x)))
�(22) where Eq(˜x|x(t)) [·] denotes the expectation over corrupted examples ˜x drawn from corruption process q(˜x|x(t)). In practice this is optimized by stochastic gradient descent, where the stochastic gradient is estimated by drawing one or a few corrupted versions of x(t) each time x(t) is considered. Corruptions considered in Vincent et al. (2010) include additive isotropic Gaussian noise, salt and pepper noise for gray-scale images, and masking noise (salt or pepper only). Qualitatively better features are reported, resulting in improved classification performance, compared to basic auto-encoders, and similar or better than that obtained with RBMs. Chen et al. (2012) show that a simpler alternative with a closed form solution can be obtained when restricting to a linear auto-encoder and have successfully applied it to domain adaptation.
The analysis in Vincent (2011) relates the denoising autoencoder criterion to energy-based probabilistic models: denoising auto-encoders basically learn in r(˜x) − ˜x a vector pointing in the direction of the estimated score i.e., ∂ log p(˜x)
∂˜x, as illustrated in Figure 3. In the special case of linear reconstruction and squared error, Vincent (2011) shows that
DAE training amounts to learning an energy-based model, whose energy function is very close to that of a GRBM, using a regularized variant of the score matching parameter estimation technique (Hyv¨arinen, 2005a; Hyv¨arinen, 2008;
Kingma and LeCun, 2010) termed denoising score matching (Vincent, 2011). Previously, Swersky (2010) had shown that training GRBMs with score matching was equivalent to training a regular (non-denoising) auto-encoder with an additional regularization term, while, following up on the theoretical results in Vincent (2011), Swersky et al. (2011) showed the practical advantage of the denoising criterion to implement score matching efficiently.
Contractive Auto-Encoders
Contractive Auto-Encoders (CAE) proposed by Rifai et al.(2011a) follow up on Denoising Auto-Encoders (DAE) and share a similar motivation of learning robust representations.
CAEs achieve this by adding an analytic contractive penalty term to the basic auto-encoder of Equation 19. This term is the Frobenius norm of the encoder's Jacobian, and results in penalizing the sensitivity of learned features to infinitesimal changes of the input.
Let J(x) =
∂fθ
∂x (x) the Jacobian matrix of the encoder evaluated at x. The CAE's training objective is the following:
JCAE
=
� t
L(x(t), gθ(fθ(x(t)))) + λ
���J(x(t))
���
F(23) where λ is a hyper-parameter controlling the strength of the regularization.
For an affine sigmoid encoder, the contractive penalty term is easy to compute:
Jj(x)
= fθ(x)j(1 − fθ(x)j)Wj
���J(x(t))
���
=
� j(fθ(x)j(1 − fθ(x)j))2∥Wj∥2
There are at least three notable differences with DAEs, which may be partly responsible for the better performance that
CAE features seem to empirically demonstrate: a) the sensitivity of the features is penalized14 directly rather than the sensitivity of the reconstruction; b) penalty is analytic rather than stochastic: an efficiently computable expression replaces what might otherwise require dx corrupted samples to size up(i.e. the sensitivity in dx directions); c) a hyper-parameter λ allows a fine control of the trade-off between reconstruction and robustness (while the two are mingled in a DAE). Note however that there is a tight connection between the DAE and the CAE: as shown in (Bengio et al., 2012b) a DAE with small corruption noise can be seen (through a Taylor expansion) as a type of contractive auto-encoder where the contractive penalty is on the whole reconstruction function rather than just on the encoder15.
A potential disadvantage of the CAE's analytic penalty is that it amounts to only encouraging robustness to infinitesimal changes of the input. This is remedied by a further extension proposed in Rifai et al. (2011b) and termed CAE+H, that penalizes all higher order derivatives, in an efficient stochastic manner, by adding a third term that encourages J(x) and J(x + ϵ) to be close:
JCAE+H
=
� t
L(x(t), gθ(x(t))) + λ
���J(x(t))
���
F
+γEϵ
�
∥J(x) − J(x + ϵ)∥2
F
�(25) where ϵ ∼ N(0, σ2I), and γ is the associated regularization strength hyper-parameter. As for the DAE, the training criterion is optimized by stochastic gradient descent, whereby the expectation is approximated by drawing several corrupted versions of x(t).
Note that the DAE and CAE have been successfully used to win the final phase of the Unsupervised and Transfer
Learning Challenge (Mesnil et al., 2011). Note also that the representation learned by the CAE tends to be saturated rather than sparse, i.e., most of the hidden units are near the extremes of their range (e.g. 0 or 1), and their derivative
∂hi(x)
∂x is tiny. The non-saturated units are few and sensitive to the inputs, with their associated filters (hidden unit weight vector) together forming a basis explaining the local changes around x, as discussed in Section 8.2. Another way to get saturated (i.e. nearly binary) units (for the purpose of hashing) is semantic hashing (Salakhutdinov and Hinton, 2007).
Predictive Sparse Decomposition
Sparse coding (Olshausen and Field, 1997) may be viewed as a kind of auto-encoder that uses a linear decoder with a squared reconstruction error, but whose non-parametric encoder fθ performs the comparatively non-trivial and relatively costly minimization of Equation. 2, which entails an iterative optimization.
A practically successful variant of sparse coding and auto-encoders, named Predictive Sparse Decomposition or 14. i.e., the robustness of the representation is encouraged.
15. but note that in the CAE, the decoder weights are tied to the encoder weights, to avoid degenerate solutions, and this should also make the decoder contractive.
PSD (Kavukcuoglu et al., 2008) replaces that costly and highly non-linear encoding step by a fast non-iterative approximation during recognition (computing the learned features).
PSD has been applied to object recognition in images and video (Kavukcuoglu et al., 2009, 2010; Jarrett et al., 2009;
Farabet et al., 2011), but also to audio (Henaff et al., 2011), mostly within the framework of multi-stage convolutional and hierarchical architectures (see Section 11.2). The main idea can be summarized by the following equation for the training criterion, which is simultaneously optimized with respect to the hidden codes (representation) h(t) and with respect to the parameters (W, α):
JPSD =
� t λ∥h(t)∥1 + ∥x(t) − Wh(t)∥2
2 + ∥h(t) − fα(x(t))∥2
2 (26) where x(t) is the input vector for example t, h(t) is the optimized hidden code for that example, and fα(·) is the encoding function, the simplest variant being fα(x(t)) = tanh(b + W T x(t))(27) where the encoding weights are the transpose of the decoding weights, but many other variants have been proposed, including the use of a shrinkage operation instead of the hyperbolic tangent (Kavukcuoglu et al., 2010). Note how the L1 penalty on h tends to make them sparse, and notice that it is the same criterion as sparse coding with dictionary learning(Eq. 3) except for the additional constraint that one should be able to approximate the sparse codes h with a parametrized encoder fα(x). One can thus view PSD as an approximation to sparse coding, where we obtain a fast approximate encoding process as a side effect of training. In practice, once PSD is trained, object representations used to feed a classifier are computed from fα(x), which is very fast, and can then be further optimized (since the encoder can be viewed as one stage or one layer of a trainable multi-stage system such as a feedforward neural network).
PSD can also be seen as a kind of auto-encoder (there is an encoder fα(·) and a decoder W) where, instead of being tied to the output of the encoder, the codes h are given some freedom that can help to further improve reconstruction. One can also view the encoding penalty added on top of sparse coding as a kind of regularizer that forces the sparse codes to be nearly computable by a smooth and efficient encoder. This is in contrast with the codes obtained by complete optimization of the sparse coding criterion, which are highly non-smooth or even non-differentiable, a problem that motivated other approaches to smooth the inferred codes of sparse coding (Bagnell and Bradley, 2009), so a sparse coding stage could be jointly optimized along with following stages of a deep architecture.
REPRESENTATION
LEARNING
AS
MANIFOLD LEARNING
Another important perspective on representation learning is based on the geometric notion of manifold. Its premise is the manifold hypothesis (Cayton, 2005; Narayanan and Mitter, 2010), according to which real-world data presented in high dimensional spaces are expected to concentrate in the vicinity of a manifold M of much lower dimensionality dM, embedded in high dimensional input space Rdx. This can be a potentially powerful prior for representation learning for AI tasks. As soon as there is a notion of "representation" then one can think of a manifold by considering the variations in input space, which are captured by or reflected (by corresponding changes) in the learned representation. To first approximation, some directions are well preserved (they are the tangent directions of the manifold) while others aren't (they are directions orthogonal to the manifolds). With this perspective, the primary unsupervised learning task is then seen as modeling the structure of the datasupporting manifold16. The associated representation being learned corresponds to an intrinsic coordinate system on the embedded manifold. The archetypal manifold modeling algorithm is, not surprisingly, also the archetypal low dimensional representation learning algorithm: Principal Component Analysis. PCA models a linear manifold. It was initially devised by
Pearson (1901) precisely with the objective of finding the closest linear manifold (specifically a line or a plane) to a cloud of data points. The principal components, i.e. the representation fθ(x) that PCA yields for an input point x, uniquely locates its projection on that manifold: it corresponds to intrinsic coordinates on the manifold. Data manifold for complex real world domains are however expected to be strongly nonlinear. Their modeling is sometimes approached as patchworks of locally linear tangent spaces (Vincent and Bengio, 2003;
Brand, 2003). The large majority of algorithms built on this geometric perspective adopt a non-parametric approach, based on a training set nearest neighbor graph (Sch¨olkopf et al., 1998; Roweis and Saul, 2000; Tenenbaum et al., 2000;
Brand, 2003; Belkin and Niyogi, 2003; Donoho and Grimes, 2003; Weinberger and Saul, 2004; Hinton and Roweis, 2003; van der Maaten and Hinton, 2008). In these non-parametric approaches, each high-dimensional training point has its own set of free low-dimensional embedding coordinates, which are optimized so that certain properties of the neighborhood graph computed in original high dimensional input space are best preserved. These methods however do not directly learn a parametrized feature extraction function fθ(x) applicable to new test points17, which seriously limits their use as feature extractors, except in a transductive setting. Comparatively few non-linear manifold learning methods have been proposed, that learn a parametric map that can directly compute a representation for new points; we will focus on these.
Learning a parametric mapping based on a neighborhood graph
The non-parametric manifold learning algorithms we just mentioned are all based on a training set neighborhood graph, typically derived from pairwise Euclidean distances between training points. Some of them are not too difficult to modify from non-parametric to instead learn a parametric mapping fθ, 16. What is meant by data manifold is actually a loosely defined notion: data points need not strictly lie on it, but the probability density is expected to fall off sharply as one moves away from the "manifold" (which may actually be constituted of several possibly disconnected manifolds with different intrinsic dimensionality).
17. For several of these techniques, representations for new points can be computed using the Nystr¨om approximation as has been proposed as an extension in (Bengio et al., 2004), but this remains cumbersome and computationally expensive.
18 that will be applicable to new points. The principle is simple: instead of having free low-dimensional embedding coordinate
"parameters" for each training point, these coordinates are now obtained through an explicitly parametrized function on inputspace coordinates, whose parameters are to be learned. The same optimization objective as in the non-parametric version can be minimized, through gradient descent: now instead of gradient descent updates on the embedding coordinates, gradients are backpropagated further to the parameters of that mapping function. Thus a parametric version of the very successful non-parametric manifold embedding algorithm tSNE (van der Maaten and Hinton, 2008) has been proposed in (van der Maaten, 2009), and could be directly applied to learning a direct parametric encoding.
Another interesting approach, that learns a direct encoding while taking into account the manifold hypothesis through a neighborhood graph is Semi-Supervised Embedding (Weston et al., 2008). Here a deep parametrized neural network architecture simultaneously learns a manifold embedding and a classifier. While optimizing the supervised classification cost, the training criterion also uses training set neighbors of each training example to encourage intermediate layers of representation to be invariant when changing the training example for a neighbor.
The more reduced and tightly controlled number of free parameters in such methods, compared to their pure nonparametric counterparts, forces the models to generalize the manifold shape non-locally (Bengio et al., 2006b), which, provided that generalization is valid, can translate into better features and final performance (van der Maaten and Hinton, Yet basing the modeling of manifolds on training set neighborhood relationships might be risky statistically in high dimensional spaces (sparsely populated due to the curse of dimensionality) as e.g. most Euclidean nearest neighbors risk having too little in common semantically. The neareset neighbor graph is simply not enough densely populated to map out satisfyingly the wrinkles of the target manifold. It can also become problematic computationally to consider all pairs of data points18, which scales quadratically with training set size.
Learning a non-linear manifold through a coding scheme
We now turn to manifold interpretations of learning techniques that are not based on training set neighbor searches. Let us begin with PCA, seen as an encoding scheme. In PCA, the same basis vectors are used to project any input point x. The sensitivity of the extracted components (the code) to input changes in the direction of these vectors is the same regardless of position x. The tangent space is the same everywhere along the linear manifold. By contrast, for a non-linear manifold, the tangent space is expected to change directions as we move, as illustrated by the tangent plane in Figure 6. In non-linear representation-learning algorithms it is convenient to think about the local variations in the representation as the input
18. Even if pairs are picked stochastically, many must be considered before obtaining one that weighs significantly on the optimization objective. x is varied on the manifold, i.e., as we move from a highprobability example to a very close one in input space. As we will discuss below, the first derivative of the mapping from input to representation (the encoder) therefore specifies the shape of the manifold (its tangent plane) around an example x lying on it. If the density was really concentrated on the manifold, and the encoder had captured that, we would find the derivatives to be non-zero only in the directions spanned by the tangent plane.
Let us consider sparse-coding in this light: parameter matrix
W may be interpreted as a dictionary of input directions from which a different subset will be picked to model the local tangent space at an x on the manifold. That subset corresponds to the active, i.e. non-zero, features for input x. Note that nonzero component hi will be sensitive to small changes of the input in the direction of the associated weight vector W:,i, whereas inactive features are more likely to be stuck at 0 until a significant displacement has taken place in input space.
The Local Coordinate Coding (LCC) algorithm (Yu et al., 2009) is very similar to sparse coding, but is explicitly derived from a manifold perspective. Using the same notation as that of sparse-coding in Equation 2, LCC replaces regularization term ∥h(t)∥1 = � j |h(t) j | yielding objective
JLCC =
� t
�
∥x(t) − Wh(t)∥2
2 + λ
� j
|h(t) j |∥W:,j − x(t)∥1+p
�
This is identical to sparse-coding when p = −1, but with larger p it encourages the active anchor points for x(t) (i.e. the codebook vectors W:,j with non-negligible |h(t) j | that are combined to reconstruct x(t)) to be not too far from x(t), hence the local aspect of the algorithm. An important theoretical contribution of Yu et al. (2009) is to show that that any Lipschitz-smooth function φ : M → R defined on a smooth nonlinear manifold M embedded in Rdx, can be well approximated by a globally linear function with respect to the resulting coding scheme (i.e. linear in h), where the accuracy of the approximation and required number dh of anchor points depend on dM rather than dx. This result has been further extended with the use of local tangent directions (Yu and Zhang, 2010), as well as to multiple layers (Lin et al., 2010).
Let us now consider the efficient non-iterative "feedforward" encoders fθ, used by PSD and the auto-encoders reviewed in Section 7.2, that are in the form of Equation
20 or 27.The computed representation for x will be only significantly sensitive to input space directions associated with non-saturated hidden units (see e.g. Eq. 24 for the Jacobian of a sigmoid layer). These directions to which the representation is significantly sensitive, like in the case of PCA or sparse coding, may be viewed as spanning the tangent space of the manifold at training point x.
Rifai et al. (2011a) empirically analyze in this light the singular value spectrum of the Jacobian (derivatives of representation vector with respect to input vector) of a trained
CAE. Here the SVD provides an ordered orthonormal basis of most sensitive directions. The spectrum is sharply decreasing, indicating a relatively small number of significantly sensitive directions. This is taken as empirical evidence that the 1"
MNIST"
Input"Point"
Tangents"
Fig. 4.
The tangent vectors to the high-density manifold as estimated by a Contractive Auto-Encoder (Rifai et al., 2011a).
The original input is shown on the top left. Each tangent vector(images on right side of first row) corresponds to a plausible additive deformation of the original input, as illustrated on the second row, where a bit of the 3rd singular vector is added to the original, to form a translated and deformed image. Unlike in PCA, the tangent vectors are different for different inputs, because the estimated manifold is highly non-linear.
CAE indeed modeled the tangent space of a low-dimensional manifold. The leading singular vectors form a basis for the tangent plane of the estimated manifold, as illustrated in Figure 4. The CAE criterion is believed to achieve this thanks to its two opposing terms: the isotropic contractive penalty, that encourages the representation to be equally insensitive to changes in any input directions, and the reconstruction term, that pushes different training points (in particular neighbors) to have a different representation (so they may be reconstructed accurately), thus counteracting the isotropic contractive pressure only in directions tangent to the manifold.
Note that analyzing learned representations through the lens of the spectrum of the Jacobian and relating it to the notion of tangent space of a manifold is feasible, whenever the mapping is differentiable, and regardless of how it was learned, whether as direct encoding (as in auto-encoder variants), or derived from latent variable inference (as in sparse coding or RBMs). Exact low dimensional manifold models (like PCA) would yield non-zero singular values associated to directions along the manifold, and exact zeros for directions orthogonal to the manifold. But in smooth models like the contractive auto-encoder or the RBM we will instead have large versus relatively small singular values (as opposed to non-zero versus exactly zero).
Leveraging the modeled tangent spaces
The local tangent space, at a point along the manifold, can be thought of capturing locally valid transformations that were prominent in the training data. For example Rifai et al.(2011c) examine the tangent directions extracted with an SVD of the Jacobian of CAEs trained on digits, images, or textdocument data: they appear to correspond to small translations or rotations for images or digits, and to substitutions of words within a same theme for documents. Such very local transformations along a data manifold are not expected to change class identity. To build their Manifold Tangent
Classifier (MTC), Rifai et al. (2011c) then apply techniques such as tangent distance (Simard et al., 1993) and tangent propagation (Simard et al., 1992), that were initially developed to build classifiers that are insensitive to input deformations provided as prior domain knowledge. Now these techniques are applied using the local leading tangent directions extracted by a CAE, i.e. not using any prior domain knowledge (except the broad prior about the existence of a manifold). This approach set a new record for MNIST digit classification among prior-knowledge free approaches19.
CONNECTIONS
BETWEEN PROBABILISTIC
AND DIRECT ENCODING MODELS
The standard likelihood framework for probabilistic models decomposes the training criterion for models with parameters θ in two parts: the log-likelihood log P(x|θ) (or log P(x|h, θ) with latent variables h), and the prior log P(θ)(or log P(h|θ) + log P(θ) with latent variables).
PSD: a probabilistic interpretation
In the case of the PSD algorithm, a connection can be made between the above standard probabilistic view and the direct encoding computation graph. In this view, the probabilistic model of PSD is the same directed generative model P(x|h) of sparse coding (Section 6.1.3), which only accounts for the decoder. The encoder is viewed as an approximate inference mechanism used to guess P(h|x) and initialize a MAP iterative inference (where the sparse prior P(h) is taken into account). However, note that in PSD, the encoder is trained jointly with the decoder, rather than simply taking the end result of iterative inference as a target to approximate. An interesting view20 to integrate this fact is that the encoder is a parametric approximation for the MAP solution of a variational lower bound on the joint log-likelihood. When
MAP learning is viewed as a special case of variational learning (where the approximation of the joint log-likelihood is with a dirac distribution located at the MAP solution), the variational recipe tells us to simultaneously improve the likelihood (reduce reconstruction error) and improve the variational approximation (reduce the discrepancy between the encoder output and the latent variable value). Hence PSD is an interesting case of representation learning algorithm that sits at the intersection of probabilistic models (with latent variables) and direct encoding methods (which directly parametrize the mapping from input to representation). RBMs also sit at the intersection because their particular parametrization includes an explicit mapping from input to representation, thanks to the restricted connectivity between hidden units. However, this nice property does not extend to their natural deep generalizations, i.e., Deep Boltzmann Machines, discussed in Section 10.2.
Regularized
Auto-Encoders
Capture
Local
Statistics of the Density
Can we also say something about the probabilistic interpretation of regularized auto-encoders, including sparse
19. It yielded 0.81% error rate using the full MNIST training set, with no prior deformations, and no convolution.
20. suggested by Ian Goodfellow, personal communication
20 auto-encoders, denoising auto-encoders, and contractive autoencoders? Their training criterion does not fit the standard likelihood framework because this would require a kind of prior(e.g. we want a sparse or contractive or robust representation) that is data-dependent.
An interesting hypothesis emerges to answer that question, out of recent theoretical results (Vincent, 2011; Bengio et al., 2012b): their training criterion, instead of being a form of maximum likelihood, corresponds to a different inductive principle, such as score matching. The score matching connection is discussed in Section 7.2.2 and has been shown for a particular parametrization of Denoising Auto-Encoder and equivalent Gaussian RBM (Vincent, 2011). The work in Bengio et al. (2012b) generalizes this idea to a broader class of parametrizations (arbitrary encoders and decoders), and shows that by regularizing the auto-encoder so that it be contractive (which is the case not only of contractive autoencoders but also of denoising and sparse ones), one obtains that the reconstruction function and its derivative estimate local statistics of the underlying data-generative distribution, such as the local mean (the mean in a small ball around each point), the local covariance, and the first and second derivatives of the estimated density. This view can actually be exploited to successfully sample from auto-encoders, as shown in Rifai et al. (2012); Bengio et al. (2012b). The proposed sampling algorithms are MCMCs similar to Langevin MCMC, using not just the estimated first derivative of the density but also the estimated second derivative, so as to stay close to manifolds near which the density concentrates. x" r(x)" x1" x2" x3"
Fig. 5. The reconstruction function r(x) (in green) learned by an autoencoder on a 1-dimensional input with high capacity, minimizing reconstruction error at the training examples xi (with in r(xi) in red) while trying to be as constant as possible otherwise. The dotted line is the identity reconstruction (which might be obtained without the regularizer). The blue arrows shows the vector field of r(x) − x pointing towards high density peaks as estimated by the model, and estimating the score (logdensity derivative).
This interpretation connects well with the geometric perspective introduced in Section 8. The regularization effects(e.g., due to a sparsity regularizer, a contractive regularizer, or the denoising criterion) asks the learned representation to be as insensitive as possible to the input, while minimizing reconstruction error on the training examples forces the representation to contain just enough information to distinguish them. The solution is that variations along the high-density manifolds are preserved while other variations are compressed.
It means that the reconstruction function should be as constant as possible while reproducing training examples, i.e., that points near a training example should be mapped to that training example, as illustrated in Figure 5. This also means that the reconstruction function should map an input towards the nearest point manifold, i.e., the difference between reconstruction and input is a vector aligned with the estimated score (the derivative of the log-density with respect to the input). When the score is zero (on the manifold), we have to look towards the second derivative of the log-density or of the energy (and the first derivative of the reconstruction function).
The directions of smallest second derivatives of the log-density are those where the density remains high (where the first derivative remains close to 0) and correspond to moving on the manifold.
Fig. 6.
Illustration of the sampling procedure for regularized auto-encoders (Rifai et al., 2012; Bengio et al., 2012b): Each
MCMC step consists in adding noise δ mostly in the directions of the estimated manifold tangent plane and projecting back towards the manifold (high-density regions) by performing a reconstruction step.
As illustrated in Figure 6, the basic idea of the autoencoder sampling algorithms in Rifai et al. (2012); Bengio et al. (2012b) is to make MCMC moves where one (a) moves toward the manifold by following the density gradient(i.e., applying a reconstruction) and (b) adds noise in the directions of the leading singular vectors of the reconstruction(or encoder) Jacobian, corresponding to those associated with smallest second derivative of the log-density.
Learning Approximate Inference
Let us now consider from closer how a representation is computed in probabilistic models with latent variables, when iterative inference is required. There is a computation graph(possibly with random number generation in some of the nodes, in the case of MCMC) that maps inputs to representation, and in the case of deterministic inference (e.g., MAP inference or variational inference), that function could be optimized directly. This is a way to generalize PSD that has been explored in recent work on probabilistic models at the intersection of inference and learning (Bagnell and Bradley, 2009; Gregor and LeCun, 2010b; Grubb and Bagnell, 2010;
Salakhutdinov and Larochelle, 2010; Stoyanov et al., 2011;
Eisner, 2012), where a central idea is that instead of using a generic inference mechanism, one can use one that is learned and is more efficient, taking advantage of the specifics of the type of data on which it is applied.
Sampling Challenges
A troubling challenge with many probabilistic models with latent variables like most Boltzmann machine variants is that good MCMC sampling is required as part of the learning procedure, but that sampling becomes extremely inefficient (or unreliable) as training progresses because the modes of the learned distribution become sharper, making mixing between modes very slow. Whereas initially during training a learner assigns mass almost uniformly, as training progresses, its entropy decreases, approaching the entropy of the target distribution as more examples and more computation are provided. According to our Manifold and Natural Clustering priors of Section 3.1, the target distribution has sharp modes (manifolds) separated by extremely low density areas. Mixing then becomes more difficult because MCMC methods, by their very nature, tend to make small steps to nearby high-probability configurations.
This is illustrated in Figure 7.
1"
Fig. 7. Top: early during training, MCMC mixes easily because the estimated distribution has high entropy and puts enough mass everywhere for small-steps movements (MCMC) to go from mode to mode. Bottom: later on, training relying on good mixing can stall because estimated modes are separated by long low-density deserts.
Bengio et al. (2012a) suggest that deep representations could help mixing between such well separated modes, based on both theoretical arguments and on empirical evidence. The idea is that if higher-level representations disentangle better the underlying abstract factors, then small steps in this abstract space (e.g., swapping from one category to another) can easily be done by MCMC. The high-level representations can then be mapped back to the input space in order to obtain inputlevel samples, as in the Deep Belief Networks (DBN) sampling algorithm (Hinton et al., 2006a). This has been demonstrated both with DBNs and with the newly proposed algorithm for sampling from contracting and denoising auto-encoders (Rifai et al., 2012; Bengio et al., 2012b). This observation alone does not suffice to solve the problem of training a DBN or a DBM, but it may provide a crucial ingredient, and it makes it possible to consider successfully sampling from deep models trained by procedures that do not require an MCMC, like the stacked regularized auto-encoders used in Rifai et al. (2012).
Evaluating and Monitoring Performance
It is always possible to evaluate a feature learning algorithm in terms of its usefulness with respect to a particular task (e.g. object classification), with a predictor that is fed or initialized with the learned features. In practice, we do this by saving the features learned (e.g. at regular intervals during training, to perform early stopping) and training a cheap classifier on top (such as a linear classifier). However, training the final classifier can be a substantial computational overhead (e.g., supervised fine-tuning a deep neural network takes usually more training iterations than the feature learning itself), so we may want to avoid having to train a classifier for every training iteration of the unsupervised learner and every hyper-parameter setting. More importantly this may give an incomplete evaluation of the features (what would happen for other tasks?). All these issues motivate the use of methods to monitor and evaluate purely unsupervised performance. This is rather easy with all the auto-encoder variants (with some caution outlined below) and rather difficult with the undirected graphical models such as the RBM and Boltzmann machines.
For auto-encoder and sparse coding variants, test set reconstruction error can readily be computed, but by itself may be misleading because larger capacity (e.g., more features, more training time) tends to systematically lead to lower reconstruction error, even on the test set. Hence it cannot be used reliably for selecting most hyper-parameters. On the other hand, denoising reconstruction error is clearly immune to this problem, so that solves the problem for denoising auto-encoders. It is not clear and remains to be demonstrated empirically or theoretically if this may also be true of the training criteria for sparse auto-encoders and contractive autoencoders.
For RBMs and some (not too deep) Boltzmann machines, one option is the use of Annealed Importance Sampling (Murray and Salakhutdinov, 2009) in order to estimate the partition function (and thus the test log-likelihood). Note that this estimator can have high variance and that it becomes less reliable(variance becomes too large) as the model becomes more interesting, with larger weights, more non-linearity, sharper modes and a sharper probability density function (see our previous discussion in Section 9.4). Another interesting and recently proposed option for RBMs is to track the partition function during training (Desjardins et al., 2011), which could be useful for early stopping and reducing the cost of ordinary
AIS. For toy RBMs (e.g., 25 hidden units or less, or 25 inputs or less), the exact log-likelihood can also be computed analytically, and this can be a good way to debug and verify some properties of interest.
GLOBAL TRAINING OF DEEP MODELS
One of the most interesting challenges raised by deep architectures is: how should we jointly train all the levels? In the previous section we have only discussed how single-layer models could be combined to form a deep model with a joint training criterion. Here we consider joint training of all the levels and the difficulties that may arise. This follows up on
Section 4, where we focused on how to combine single-layer learners into deep architectures.
On the Challenge of Training Deep Architectures
Higher-level abstraction means more non-linearity. It means that two nearby configurations of the input may have to be interpreted very differently because a few surface details change the underlying semantics, whereas most other changes in the surface details would not change the underlying semantics. The representations associated with input manifolds may be complex because these functions may have to unfold and distort input manifolds that generally have complicated shapes into spaces where distributions are much simpler, where relations between factors are simpler, maybe even linear or involving many (conditional) independencies. Our expectation is that modeling the joint distribution between high-level abstractions and concepts should be much easier in the sense of requiring much less data to learn. The hard part is learning the representation. The price to pay may be that these more abstract representation functions (mapping input to representation) involve a more challenging training task, because of the added non-linearity.
It is only since 2006 that researchers have seriously investigated ways to train deep architectures, to the exception of the convolutional networks (LeCun et al., 1998b). The first realization was that unsupervised or supervised layer-wise training was easier, and that this could be taken advantage of by stacking single-layer models into deeper ones, as discussed at length in Section 4.
It is interesting to ask why does the layerwise unsupervised pre-training procedure sometimes help a supervised learner (Erhan et al., 2010b). There seems to be a more general principle at play 21 of guiding the training of intermediate representations, which may be easier than trying to learn it all in one go. This is nicely related with the curriculum learning idea (Bengio et al., 2009) that it may be much easier to learn simpler concepts first and then build higher-level ones on top of simpler ones. This is also coherent with the success of several deep learning algorithms that provide some such guidance for intermediate representations, like SemiSupervised Embedding (Weston et al., 2008).
The question of why unsupervised pre-training could be helpful was extensively studied (Erhan et al., 2010b), trying to dissect the answer into a regularization effect and an optimization effect. The regularization effect is clear from the experiments where the stacked RBMs or denoising autoencoders are used to initialize a supervised classification neural network (Erhan et al., 2010b). It may simply come from the use of unsupervised learning to bias the learning dynamics and initialize it in the basin of attraction of a "good" local minimum (of the training criterion), where "good" is in terms of generalization error. The underlying hypothesis exploited by this procedure is that some of the features or latent factors that are good at capturing the leading variations in the input distribution are also good at capturing the variations in the target output random variables of interest (e.g., classes). The optimization effect is more difficult to tease out because the top two layers of a deep neural net can just overfit the training
21. First communicated to us by Leon Bottou set whether the lower layers compute useful features or not, but there are several indications that optimizing the lower levels with respect to a supervised training criterion is challenging.
One such indication is that changing the numerical conditions of the optimization procedure can have a profound impact on the joint training of a deep architecture, for example changing the initialization range and changing the type of non-linearity used (Glorot and Bengio, 2010), much more so than with shallow architectures. One hypothesis to explain some of the difficulty in the optimization of deep architectures is centered on the singular values of the Jacobian matrix associated with the transformation from the features at one level into the features at the next level (Glorot and Bengio, 2010). If these singular values are all small (less than 1), then the mapping is contractive in every direction and gradients would vanish when propagated backwards through many layers. This is a problem already discussed for recurrent neural networks (Bengio et al., 1994), which can be seen as very deep networks with shared parameters at each layer, when unfolded in time. This optimization difficulty has motivated the exploration of second-order methods for deep architectures and recurrent networks, in particular Hessian-free secondorder methods (Martens, 2010; Martens and Sutskever, 2011).
Unsupervised pre-training has also been proposed to help training recurrent networks and temporal RBMs (Sutskever et al., 2009), i.e., at each time step there is a local signal to guide the discovery of good features to capture in the state variables: model with the current state (as hidden units) the joint distribution of the previous state and the current input. Natural gradient (Amari, 1998) methods that can be applied to networks with millions of parameters (i.e. with good scaling properties) have also been proposed (Le Roux et al., 2008b). Cho et al. (2011) proposes to use adaptive learning rates for RBM training, along with a novel and interesting idea for a gradient estimator that takes into account the invariance of the model to flipping hidden unit bits and inverting signs of corresponding weight vectors. At least one study indicates that the choice of initialization (to make the Jacobian of each layer closer to 1 across all its singular values) could substantially reduce the training difficulty of deep networks (Glorot and Bengio, 2010) and this is coherent with the success of the initialization procedure of Echo State
Networks (Jaeger, 2007), as recently studied by Sutskever(2012). There are also several experimental results (Glorot and Bengio, 2010; Glorot et al., 2011a; Nair and Hinton, 2010) showing that the choice of hidden units non-linearity could influence both training and generalization performance, with particularly interesting results obtained with sparse rectifying units (Jarrett et al., 2009; Nair and Hinton, 2010; Glorot et al., 2011a; Krizhevsky et al., 2012). Another promising idea to improve the conditioning of neural network training is to nullify the average value and slope of each hidden unit output (Raiko et al., 2012), and possibly locally normalize magnitude as well (Jarrett et al., 2009). The debate still rages between using online methods such as stochastic gradient descent and using second-order methods on large minibatches(of several thousand examples) (Martens, 2010; Le et al., 2011a), with a variant of stochastic gradient descent recently
23 winning an optimization challenge 22.
Finally, several recent results exploiting large quantities of labeled data suggest that with proper initialization and choice of non-linearity, very deep purely supervised networks can be trained successfully without any layerwise pre-training (Ciresan et al., 2010; Glorot et al., 2011a;
Krizhevsky et al., 2012). Researchers report than in such conditions, layerwise unsupervised pre-training brought little or no improvement over pure supervised learning from scratch when training for long enough. This reinforces the hypothesis that unsupervised pre-training acts as a prior, which may be less necessary when very large quantities of labeled data are available, but begs the question of why this had not been discovered earlier. The latest results reported in this respect (Krizhevsky et al., 2012) are particularly interesting because they allowed to drastically reduce the error rate of object recognition on a benchmark (the 1000-class ImageNet task) where many more traditional computer vision approaches had been evaluated(http://www.image-net.org/challenges/LSVRC/2012/results.html).
The main techniques that allowed this success include the following: efficient GPU training allowing one to train longer (more than 100 million visits of examples), an aspect first reported by Lee et al. (2009a); Ciresan et al. (2010), large number of labeled examples, artificially transformed examples (see Section 11.1), a large number of tasks (1000 or 10000 classes for ImageNet), convolutional architecture with max-pooling (see section 11 for these latter two techniques), rectifying non-linearities(discussed above), careful initialization (discussed above), careful parameter update and adaptive learning rate heuristics, layerwise feature normalization (across features), and a new dropout trick based on injecting strong binary multiplicative noise on hidden units. This trick is similar to the binary noise injection used at each layer of a stack of denoising auto-encoder.
Future work is hopefully going to help identify which of these elements matter most, how to generalize them across a large variety of tasks and architectures, and in particular contexts where most examples are unlabeled, i.e., including with an unsupervised component in the training criterion.
Joint Training of Deep Boltzmann Machines
We now consider the problem of joint training of all layers of a specific unsupervised model, the Deep Boltzmann Machine(DBM). Whereas much progress (albeit with many unanswered questions) has been made on jointly training all the layers of deep architectures using back-propagated gradients(i.e., mostly in the supervised setting), much less work has been done on their purely unsupervised counterpart, e.g. with
DBMs23. Note however that one could hope that the successful techniques described in the previous section could be applied to unsupervised learning algorithms.
Like the RBM, the DBM is another particular subset of the Boltzmann machine family of models where the units
22. https://sites.google.com/site/nips2011workshop/optimization-challenges
23. Joint training of all the layers of a Deep Belief Net is much more challenging because of the much harder inference problem involved. are again arranged in layers. However unlike the RBM, the DBM possesses multiple layers of hidden units, with units in odd-numbered layers being conditionally independent given even-numbered layers, and vice-versa. With respect to the Boltzmann energy function of Eq. 7, the DBM corresponds to setting U = 0 and a sparse connectivity structure in both V and W. We can make the structure of the DBM more explicit by specifying its energy function. For the model with two hidden layers it is given as:
EDBM θ(v, h(1), h(2); θ) = − vT Wh(1) − h(1) T V h(2)− d(1) T h(1) − d(2) T h(2) − bT v, (29) with θ = {W, V, d(1), d(2), b}. The DBM can also be characterized as a bipartite graph between two sets of vertices, formed by the units in odd and even-numbered layers (with v := h(0)).
Mean-field approximate inference
A key point of departure from the RBM is that the posterior distribution over the hidden units (given the visibles) is no longer tractable, due to the interactions between the hidden units. Salakhutdinov and Hinton (2009) resort to a mean-field approximation to the posterior. Specifically, in the case of a model with two hidden layers, we wish to approximate P
� h(1), h(2) | v
� with the factored distribution
Qv(h(1), h(2)) = �N1 j=1 Qv
� h(1) j
� �N2 i=1 Qv
� h(2) i
�, such that the KL divergence KL
�
P
� h(1), h(2) | v
�
∥Qv(h1, h2)
� is minimized or equivalently, that a lower bound to the log likelihood is maximized: log P(v) > L(Qv) ≡
� h(1)
� h(2)
Qv(h(1), h(2)) log
�P(v, h(1), h(2))
Qv(h(1), h(2))
�
Maximizing this lower-bound with respect to the mean-field distribution Qv(h1, h2) (by setting derivatives to zero) yields the following mean field update equations:
ˆh(1) i
← sigmoid
�� j
Wjivj +
� k
Vikˆh(2) k
+ d(1) i
�
ˆh(2) k
← sigmoid
�� i
Vikˆh(1) i
+ d(2) k
�
Note how the above equations ostensibly look like a fixed point recurrent neural network, i.e., with constant input. In the same way that an RBM can be associated with a simple auto-encoder, the above mean-field update equations for the DBM can be associated with a recurrent auto-encoder. In that case the training criterion involves the reconstruction error at the last or at consecutive time steps. This type of model has been explored by Savard (2011) and Seung (1998) and shown to do a better job at denoising than ordinary auto-encoders.
Iterating Eq. (31-32) until convergence yields the Q parameters used to estimate the "variational positive phase" of Eq. 33:
L(Qv) =EQv
� log P(v, h(1), h(2)) − log Qv(h(1), h(2))
�
=EQv
�
−EDBM θ(v, h(1), h(2)) − log Qv(h(1), h(2))
�
− log Zθ
∂L(Qv)
∂θ
= −EQv
�∂EDBM θ(v, h(1), h(2))
∂θ
�
+ EP
�∂EDBM θ(v, h(1), h(2))
∂θ
�
Note that this variational learning procedure leaves the "negative phase" untouched. It can thus be estimated through SML or Contrastive Divergence (Hinton, 2000) as in the RBM case.
Training Deep Boltzmann Machines
Despite the intractability of inference in the DBM, its training should not, in theory, be much more complicated than that of the RBM. The major difference being that instead of maximizing the likelihood directly, we instead choose parameters to maximize the lower-bound on the likelihood given in Eq. 30.
The SML-based algorithm for maximizing this lower-bound is as follows:
1) Clamp the visible units to a training example.
2) Iterate over Eq. (31-32) until convergence.
3) Generate negative phase samples v−, h(1)− and h(2)− through SML.
4) Compute ∂L(Qv) /∂θ using the values obtained in steps
2-3.
5) Finally, update the model parameters with a step of approximate stochastic gradient ascent.
While the above procedure appears to be a simple extension of the highly effective SML scheme for training RBMs, as we demonstrate in Desjardins et al. (2012), this procedure seems vulnerable to falling in poor local minima which leave many hidden units effectively dead (not significantly different from its random initialization with small norm).
The failure of the SML joint training strategy was noted by Salakhutdinov and Hinton (2009). As an alternative, they proposed a greedy layer-wise training strategy. This procedure consists in pre-training the layers of the DBM, in much the same way as the Deep Belief Network: i.e. by stacking RBMs and training each layer to independently model the output of the previous layer. A final joint "fine-tuning" is done following the above SML-based procedure.
BUILDING-IN INVARIANCE
It is well understood that incorporating prior domain knowledge is an almost sure way to boost performance of any machine-learning-based prediction system. Exploring good strategies for doing so is a very important research avenue.
However, if we are to advance our understanding of core machine learning principles, it is important that we keep comparisons between predictors fair and maintain a clear awareness of the prior domain knowledge used by different learning algorithms, especially when comparing their performance on benchmark problems. We have so far tried to present feature learning and deep learning approaches without enlisting specific domain knowledge, only generic inductive biases for high dimensional problems. The approaches as we presented them should thus be potentially applicable to any high dimensional problem. In this section, we briefly review how basic domain knowledge can be leveraged to learn yet better features.
The most prevalent approach to incorporating prior knowledge is to hand-design better features to feed a generic classifier, and has been used extensively in computer vision(e.g. (Lowe, 1999)). Here, we rather focus on how basic domain knowledge of the input, in particular its topological structure (e.g. bitmap images having a 2D structure), may be used to learn better features.
Augmenting the dataset with known input deformations
Since machine learning approaches learn from data, it is usually possible to improve results by feeding them a larger quantity of representative data. Thus, a straightforward and generic way to leverage prior domain knowledge of input deformations that are known not to change its class (e.g. small affine transformations of images such as translations, rotations, scaling, shearing), is to use them to generate more training data. While being an old approach (Baird, 1990; Poggio and Vetter, 1992), it has been recently applied with great success in the work of Ciresan et al. (2010) who used an efficient GPU implementation (40× speedup) to train a standard but large deep multilayer Perceptron on deformed MNIST digits. Using both affine and elastic deformations (Simard et al., 2003), with plain old stochastic gradient descent, they reach a record
0.32% classification error rate.
Convolution and pooling
Another powerful approach is based on even more basic knowledge of merely the topological structure of the input dimensions. By this we mean e.g., the 2D layout of pixels in images or audio spectrograms, the 3D structure of videos, the 1D sequential structure of text or of temporal sequences in general. Based on such structure, one can define local receptive fields (Hubel and Wiesel, 1959), so that each lowlevel feature will be computed from only a subset of the input: a neighborhood in the topology (e.g. a sub-image at a given position). This topological locality constraint corresponds to a layer having a very sparse weight matrix with non-zeros only allowed for topologically local connections. Computing the associated matrix products can of course be made much more efficient than having to handle a dense matrix, in addition to the statistical gain from a much smaller number of free parameters. In domains with such topological structure, similar input patterns are likely to appear at different positions, and nearby values (e.g. consecutive frames or nearby pixels) are likely to have stronger dependencies that are also important to model the data. In fact these dependencies can be exploited to discover the topology (Le Roux et al., 2008a), i.e. recover a regular grid of pixels out of a set of vectors without any order information, e.g. after the elements have been arbitrarily shuffled in the same way for all examples. Thus a same local feature computation is likely to be relevant at all translated positions of the receptive field. Hence the idea of sweeping such
25 a local feature extractor over the topology: this corresponds to a convolution, and transforms an input into a similarly shaped feature map. Equivalently to sweeping, this may be seen as static but differently positioned replicated feature extractors that all share the same parameters. This is at the heart of convolutional networks (LeCun et al., 1989, 1998b) which have been applied both to object recognition and to image segmentation (Turaga et al., 2010). Another hallmark of the convolutional architecture is that values computed by the same feature detector applied at several neighboring input locations are then summarized through a pooling operation, typically taking their max or their sum. This confers the resulting pooled feature layer some degree of invariance to input translations, and this style of architecture (alternating selective feature extraction and invariance-creating pooling) has been the basis of convolutional networks, the Neocognitron (Fukushima and Miyake, 1982) and HMAX (Riesenhuber and Poggio, 1999) models, and argued to be the architecture used by mammalian brains for object recognition (Riesenhuber and Poggio, 1999;
Serre et al., 2007; DiCarlo et al., 2012). The output of a pooling unit will be the same irrespective of where a specific feature is located inside its pooling region. Empirically the use of pooling seems to contribute significantly to improved classification accuracy in object classification tasks (LeCun et al., 1998b; Boureau et al., 2010, 2011). A successful variant of pooling connected to sparse coding is L2 pooling (Hyv¨arinen et al., 2009; Kavukcuoglu et al., 2009; Le et al., 2010), for which the pool output is the square root of the possibly weighted sum of squares of filter outputs. Ideally, we would like to generalize feature-pooling so as to learn what features should be pooled together, e.g. as successfully done in several papers (Hyv¨arinen and Hoyer, 2000; Kavukcuoglu et al., 2009; Le et al., 2010; Ranzato and Hinton, 2010;
Courville et al., 2011b; Coates and Ng, 2011b; Gregor et al., 2011). In this way, the features learned are becoming invariant to the variations captured by the span of the features pooled.
Patch-based training
The simplest approach for learning a convolutional layer in an unsupervised fashion is patch-based training: simply feeding a generic unsupervised feature learning algorithm with local patches extracted at random positions of the inputs. The resulting feature extractor can then be swiped over the input to produce the convolutional feature maps. That map may be used as a new input for the next layer, and the operation repeated to thus learn and stack several layers. Such an approach was recently used with Independent Subspace Analysis (Le et al., 2011c) on 3D video blocks, reaching the state-of-the-art on Hollywood2, UCF, KTH and YouTube action recognition datasets. Similarly (Coates and Ng, 2011a) compared several feature learners with patch-based training and reached stateof-the-art results on several classification benchmarks. Interestingly, in this work performance was almost as good with very simple k-means clustering as with more sophisticated feature learners. We however conjecture that this is the case only because patches are rather low dimensional (compared to the dimension of a whole image). A large dataset might provide sufficient coverage of the space of e.g. edges prevalent in 6 × 6 patches, so that a distributed representation is not absolutely necessary. Another plausible explanation for this success is that the clusters identified in each image patch are then pooled into a histogram of cluster counts associated with a larger sub-image. Whereas the output of a regular clustering is a one-hot non-distributed code, this histogram is itself a distributed representation, and the "soft" k-means (Coates and Ng, 2011a) representation allows not only the nearest filter but also its neighbors to be active.
Convolutional and tiled-convolutional training
It is also possible to directly train large convolutional layers using an unsupervised criterion. An early approach is that of Jain and Seung (2008) who trained a standard but deep convolutional MLP on the task of denoising images, i.e. as a deep, convolutional, denoising auto-encoder. Convolutional versions of the RBM or its extensions have also been developed (Desjardins and Bengio, 2008; Lee et al., 2009a;
Taylor et al., 2010) as well as a probabilistic max-pooling operation built into Convolutional Deep Networks (Lee et al., 2009a,b; Krizhevsky, 2010). Other unsupervised feature learning approaches that were adapted to the convolutional setting include PSD (Kavukcuoglu et al., 2009, 2010; Jarrett et al., 2009; Farabet et al., 2011; Henaff et al., 2011), a convolutional version of sparse coding called deconvolutional networks (Zeiler et al., 2010), Topographic ICA (Le et al., 2010), and mPoT that Kivinen and Williams (2012) applied to modeling natural textures. Gregor and LeCun (2010a);
Le et al. (2010) also demonstrated the technique of tiledconvolution, where parameters are shared only between feature extractors whose receptive fields are k steps away (so the ones looking at immediate neighbor locations are not shared).
This allows pooling units to be invariant to more than just translations, and is a kind of hybrid between convolutional networks and earlier neural networks with local connections but no weight sharing (LeCun, 1986, 1989).
Alternatives to pooling
Alternatively, one can also use explicit knowledge of the expected invariants expressed mathematically to define transformations that are robust to a known family of input deformations, using so-called scattering operators (Mallat, 2011;
Bruna and Mallat, 2011), which can be computed in a way interestingly analogous to deep convolutional networks and wavelets. Like convolutional networks, the scattering operators alternate two types of operations: convolving with a filter followed by pooling (as a norm). Unlike convolutional networks, the proposed approach keeps at each level all of the information about the input (in a way that can be inverted), and automatically yields a very sparse (but also very highdimensional) representation. Another difference is that the filters are not learned but instead set so as to guarantee that a priori specified invariances are robustly achieved. Just a few levels were sufficient to achieve impressive results on several benchmark datasets.
Temporal coherence and slow features
The principle of identifying slowly moving/changing factors in temporal/spatial data has been investigated by many (Becker
26 and Hinton, 1993; Wiskott and Sejnowski, 2002; Hurri and Hyv¨arinen, 2003; K¨ording et al., 2004; Cadieu and Olshausen, 2009) as a principle for finding useful representations. In particular this idea has been applied to image sequences and as an explanation for why V1 simple and complex cells behave the way they do. A good overview can be found in Hurri and Hyv¨arinen (2003); Berkes and Wiskott (2005).
More recently, temporal coherence has been successfully exploited in deep architectures to model video (Mobahi et al., 2009). It was also found that temporal coherence discovered visual features similar to those obtained by ordinary unsupervised feature learning (Bergstra and Bengio, 2009), and a temporal coherence penalty has been combined with a training criterion for unsupervised feature learning (Zou et al., 2011), sparse auto-encoders with L1 regularization, in this case, yielding improved classification performance.
The temporal coherence prior can be expressed in several ways. The simplest and most commonly used is just the squared difference between feature values at times t and t + 1. Other plausible temporal coherence priors include the following. First, instead of penalizing the squared change, penalizing the absolute value (or a similar sparsity penalty) would state that most of the time the change should be exactly
0, which would intuitively make sense for the real-life factors that surround us. Second, one would expect that instead of just being slowly changing, different factors could be associated with their own different time scale. The specificity of their time scale could thus become a hint to disentangle explanatory factors. Third, one would expect that some factors should really be represented by a group of numbers (such as x, y, and z position of some object in space and the pose parameters of Hinton et al. (2011)) rather than by a single scalar, and that these groups tend to move together. Structured sparsity penalties (Kavukcuoglu et al., 2009; Jenatton et al., 2009;
Bach et al., 2011; Gregor et al., 2011) could be used for this purpose.
Algorithms to Disentangle Factors of Variation
The goal of building invariant features is to remove sensitivity of the representation to directions of variance in the data that are uninformative to the task at hand. However it is often the case that the goal of feature extraction is the disentangling or separation of many distinct but informative factors in the data, e.g., in a video of people: subject identity, action performed, subject pose relative to the camera, etc. In this situation, the methods of generating invariant features, such as featurepooling, may be inadequate.
Roughly speaking, the process of building invariant features can be seen as consisting of two steps (often performed together). First, a set of low-level features are recovered that account for the data. Second, subsets of these low level features are pooled together to form higher-level invariant features, exemplified by the pooling and subsampling layers of convolutional neural networks (Fukushima, 1980; LeCun et al., 1989, 1998c). With this arrangement, the invariant representation formed by the pooling features offers a somewhat incomplete window on the data as the detailed representation of the lower-level features is abstracted away in the pooling procedure. While we would like higher-level features to be more abstract and exhibit greater invariance, we have little control over what information is lost through feature pooling.
For example, consider a higher-level feature made invariant to the color of its target stimulus by forming a subspace of low-level features that represent the target stimulus in various colors (forming a basis for the subspace). If this is the only higher-level feature that is associated with these low-level colored features then the color information of the stimulus is lost to the higher-level pooling feature and every layer above. This loss of information becomes a problem when the information that is lost is necessary to successfully complete the task at hand such as object classification. In the above example, color is often a very discriminative feature in object classification tasks. Losing color information through feature-pooling would result in significantly poorer classification performance.
Obviously, what we really would like is for a particular feature set to be invariant to the irrelevant features and disentangle the relevant features. Unfortunately, it is often difficult to determine a priori which set of features will ultimately be relevant to the task at hand.
An interesting approach to taking advantage of some of the factors of variation known to exist in the data is the transforming auto-encoder (Hinton et al., 2011): instead of a scalar pattern detector (e.g,. corresponding to the probability of presence of a particular form in the input) one can think of the features as organized in groups that include both a pattern detector and pose parameters that specify attributes of the detected pattern. In (Hinton et al., 2011), what is assumed a priori is that pairs of inputs (or consecutive inputs) are observed with an associated value for the corresponding change in the pose parameters. For example, an animal that controls its eyes knows what changes to its ocular motor system were applied when going from one image on its retina to the next image associated with the following saccade and controlled head motion. In that work, it is also assumed that the pose changes are the same for all the pattern detectors, and this makes sense for global changes such as image translation and camera geometry changes. Instead, we would like to discover the pose parameters and attributes that should be associated with each feature detector, without having to specify ahead of time what they should be, force them to be the same for all features, and having to necessarily observe the changes in all of the pose parameters or attributes.
The approach taken recently in the Manifold Tangent Classifier, discussed in section 8.3, is interesting in this respect.
Without using any supervision or prior knowledge, it finds prominent local factors of variation (the tangent vectors to the manifold, extracted from a CAE, interpreted as locally valid input "deformations"). Higher-level features are subsequently encouraged to be invariant to these factors of variation, so that they must depend on other characteristics. In a sense this approach is disentangling valid local deformations along the data manifold from other, more drastic changes, associated to other factors of variation such as those that affect class
27 identity.24
One solution to the problem of information loss that would fit within the feature-pooling paradigm, is to consider many overlapping pools of features based on the same low-level feature set. Such a structure would have the potential to learn a redundant set of invariant features that may not cause significant loss of information. However it is not obvious what learning principle could be applied that can ensure that the features are invariant while maintaining as much information as possible. While a Deep Belief Network or a Deep Boltzmann Machine (as discussed in sections 4 and 10.2 respectively) with two hidden layers would, in principle, be able to preserve information into the "pooling" second hidden layer, there is no guarantee that the second layer features are more invariant than the "low-level" first layer features.
However, there is some empirical evidence that the second layer of the DBN tends to display more invariance than the first layer (Erhan et al., 2010a). A second issue with this approach is that it could nullify one of the major motivations for pooling features: to reduce the size of the representation. A pooling arrangement with a large number of overlapping pools could lead to as many pooling features as low-level features
– a situation that is both computationally and statistically undesirable.
A more principled approach, from the perspective of ensuring a more robust compact feature representation, can be conceived by reconsidering the disentangling of features through the lens of its generative equivalent – feature composition. Since many unsupervised learning algorithms have a generative interpretation (or a way to reconstruct inputs from their high-level representation), the generative perspective can provide insight into how to think about disentangling factors. The majority of the models currently used to construct invariant features have the interpretation that their low-level features linearly combine to construct the data.25 This is a fairly rudimentary form of feature composition with significant limitations. For example, it is not possible to linearly combine a feature with a generic transformation (such as translation) to generate a transformed version of the feature. Nor can we even consider a generic color feature being linearly combined with a gray-scale stimulus pattern to generate a colored pattern. It would seem that if we are to take the notion of disentangling seriously we require a richer interaction of features than that offered by simple linear combinations.
CONCLUSION
This review of representation learning and deep learning has covered three major and apparently disconnected approaches: the probabilistic models (both the directed kind such as sparse coding and the undirected kind such as Boltzmann
24. The changes that affect class identity might, in input space, actually be of similar magnitude to local deformations, but not follow along the manifold, i.e. cross zones of low density.
25. As an aside, if we are given only the values of the higher-level pooling features, we cannot accurately recover the data because we do not know how to apportion credit for the pooling feature values to the lower-level features. This is simply the generative version of the consequences of the loss of information caused by pooling. machines), the reconstruction-based algorithms related to autoencoders, and the geometrically motivated manifold-learning approaches. Drawing connections between these approaches is currently a very active area of research and is likely to continue to produce models and methods that take advantage of the relative strengths of each paradigm.
Practical Concerns and Guidelines. One of the criticisms addressed to artificial neural networks and deep learning algorithms is that they have many hyper-parameters and variants and that exploring their configurations and architectures is an art. This has motivated an earlier book on the "Tricks of the Trade" (Orr and Muller, 1998) of which LeCun et al. (1998a) is still relevant for training deep architectures, in particular what concerns initialization, ill-conditioning and stochastic gradient descent. A good and more modern compendium of good training practice, particularly adapted to training RBMs, is provided in Hinton (2010), while a similar guide oriented more towards deep neural networks can be found in Bengio(2013), both of which are part of a novel version of the above book, entitled "Neural Networks: Tricks of the Trade, Reloaded".
Incorporating Generic AI-level Priors. We have covered many high-level generic priors that we believe could be very useful to bring machine learning closer to AI, and that can be incorporated into representation-learning algorithms. Many of these relate to the assumed existence of multiple underlying factors of variation, whose variations are in some sense orthogonal to each other in input space. They are expected to be organized at multiple levels of abstraction, hence the need for deep architectures, which also have statistical advantages because they allow to re-use parameters in a combinatorially efficient way. Only a few of these factors are relevant for any particular example, justifying the sparsity of representation prior. Subsets of these factors explain different random variables of interest(inputs, tasks) and they vary in structured ways in time and space (temporal and spatial coherence). We expect future successful applications of representation learning to refine and increase that list of priors, and to incorporate most of them instead of focusing on only one. Research in training criteria that better take these priors into account are likely to move us closer to the long-term objective of discovering learning algorithms that can disentangle the underlying explanatory factors for AI tasks.
Inference. We anticipate that methods based on directly parametrizing a representation function will incorporate more and more of the iterative type of computation one finds in the inference procedures of probabilistic latent-variable models.
There is already movement in the other direction, with probabilistic latent-variable models exploiting approximate inference mechanisms that are themselves learned (i.e., producing a parametric description of the representation function). A major appeal of probabilistic models is that the semantics of the latent variables are clear and this allows a clean separation of the problems of modeling (choose the energy function), inference (estimating P(h|x)), and learning (optimizing the parameters), using generic tools in each case. On the other hand, doing approximate inference and not taking that approximation into account explicitly in the approximate optimization
28 for learning could have detrimental effects, hence the appeal of learning approximate inference. More fundamentally, there is the question of the multimodality of the posterior P(h|x). If there are exponentially many probable configurations of values of the factors hi that can explain x, then we seem to be stuck with very poor inference, either focusing on a single mode(MAP inference), assuming some kind of strong factorization(as in variational inference) or using an MCMC that cannot visit enough modes of P(h|x) to possibly do a good job of inference. What we propose as food for thought is the idea of dropping the requirement of an explicit representation of the posterior and settle for an implicit representation that exploits potential structure in P(h|x) in order to represent it compactly: even though P(h|x) may have an exponential number of modes, it may be possible to represent it with a small set of numbers. For example, consider computing a deterministic feature representation f(x) that implicitly captures the information about a highly multi-modal P(h|x), in the sense that all the questions (e.g. making some prediction about some target concept) that can be asked from P(h|x) can also be answered from f(x).
Optimization. Much remains to be done to better understand the successes and failures of training deep architectures, both in the supervised case (with many recent successes) and the unsupervised case (where much more work needs to be done). Although regularization effects can be important on small datasets, the effects that persist on very large datasets suggest some optimization issues are involved. Are they more due to local minima (we now know there are huge numbers of them) and the dynamics of the training procedure? Or are they due mostly to ill-conditioning and may be handled by approximate second-order methods? These basic questions remain unanswered and deserve much more study.
Acknowledgments
The author would like to thank David Warde-Farley, Razvan
Pascanu and Ian Goodfellow for useful feedback, as well as
NSERC, CIFAR and the Canada Research Chairs for funding.
REFERENCES
Amari, S. (1998).
Natural gradient works efficiently in learning. Neural Computation, 10(2), 251–276.
Bach, F., Jenatton, R., Mairal, J., and Obozinski, G. (2011).
Structured sparsity through convex optimization.
CoRR, abs/1109.2397.
Bagnell, J. A. and Bradley, D. M. (2009). Differentiable sparse coding. In NIPS'2009, pages 113–120.
Baird, H. (1990). Document image defect models. In IAPR
Workshop, Syntactic & Structural Patt. Rec., pages 38–46.
Becker, S. and Hinton, G. (1992). A self-organizing neural network that discovers surfaces in random-dot stereograms.
Nature, 355, 161–163.
Becker, S. and Hinton, G. E. (1993). Learning mixture models of spatial coherence. Neural Computation, 5, 267–277.
Belkin, M. and Niyogi, P. (2003). Laplacian eigenmaps for dimensionality reduction and data representation.
Neural
Computation, 15(6), 1373–1396.
Bell, A. and Sejnowski, T. J. (1997). The independent components of natural scenes are edge filters. Vision Research, 37, 3327–3338.
Bengio, Y. (1993).
A connectionist approach to speech recognition. International Journal on Pattern Recognition and Artificial Intelligence, 7(4), 647–668.
Bengio, Y. (2008). Neural net language models. Scholarpedia, Bengio, Y. (2009).
Learning deep architectures for AI.
Foundations and Trends in Machine Learning, 2(1), 1–127.
Also published as a book. Now Publishers, 2009.
Bengio, Y. (2011).
Deep learning of representations for unsupervised and transfer learning. In JMLR W&CP: Proc.
Unsupervised and Transfer Learning.
Bengio, Y. (2013). Practical recommendations for gradientbased training of deep architectures.
In K.-R. M¨uller, G. Montavon, and G. B. Orr, editors, Neural Networks:
Tricks of the Trade, Reloaded. Springer.
Bengio, Y. and Delalleau, O. (2009). Justifying and generalizing contrastive divergence. Neural Computation, 21(6), 1601–1621.
Bengio, Y. and Delalleau, O. (2011). On the expressive power of deep architectures. In ALT'2011.
Bengio, Y. and LeCun, Y. (2007). Scaling learning algorithms towards AI. In L. Bottou, O. Chapelle, D. DeCoste, and J. Weston, editors, Large Scale Kernel Machines. MIT Press.
Bengio, Y. and Monperrus, M. (2005). Non-local manifold tangent learning. In NIPS'2004, pages 129–136. MIT Press.
Bengio, Y., Simard, P., and Frasconi, P. (1994). Learning longterm dependencies with gradient descent is difficult. IEEE
Transactions on Neural Networks, 5(2), 157–166.
Bengio, Y., Ducharme, R., Vincent, P., and Jauvin, C. (2003).
A neural probabilistic language model.
JMLR, 3, 1137–
Bengio, Y., Paiement, J.-F., Vincent, P., Delalleau, O., Le
Roux, N., and Ouimet, M. (2004). Out-of-sample extensions for LLE, Isomap, MDS, Eigenmaps, and Spectral Clustering. In NIPS'2003.
Bengio, Y., Delalleau, O., and Le Roux, N. (2006a). The curse of highly variable functions for local kernel machines. In
NIPS'2005, pages 107–114.
Bengio, Y., Larochelle, H., and Vincent, P. (2006b). Non-local manifold Parzen windows. In NIPS'2005, pages 115–122.
MIT Press.
Bengio, Y., Lamblin, P., Popovici, D., and Larochelle, H.(2007). Greedy layer-wise training of deep networks. In
NIPS'2006.
Bengio, Y., Louradour, J., Collobert, R., and Weston, J. (2009).
Curriculum learning. In ICML'09.
Bengio, Y., Delalleau, O., and Simard, C. (2010). Decision trees do not generalize to new variations. Computational
Intelligence, 26(4), 449–467.
Bengio, Y., Mesnil, G., Dauphin, Y., and Rifai, S. (2012a).
Better mixing via deep representations. Technical Report arXiv:1207.4404, Universite de Montreal.
Bengio, Y., Alain, G., and Rifai, S. (2012b). Implicit density estimation by local moment matching to sample from autoencoders. Technical report, arXiv:1207.0057.
Bergstra, J. and Bengio, Y. (2009). Slow, decorrelated features for pretraining complex cell-like networks. In NIPS'2009, pages 99–107.
Berkes, P. and Wiskott, L. (2005). Slow feature analysis yields a rich repertoire of complex cell properties.
Journal of Vision, 5(6), 579–602.
Besag, J. (1975). Statistical analysis of non-lattice data. The Statistician, 24(3), 179–195.
Bordes, A., Glorot, X., Weston, J., and Bengio, Y. (2012). Joint learning of words and meaning representations for open-text semantic parsing. AISTATS'2012.
Boulanger-Lewandowski, N., Bengio, Y., and Vincent, P.
Modeling temporal dependencies in highdimensional sequences: Application to polyphonic music generation and transcription. In ICML'2012.
Boureau, Y., Ponce, J., and LeCun, Y. (2010).
A theoretical analysis of feature pooling in vision algorithms.
In
ICML'10.
Boureau, Y., Le Roux, N., Bach, F., Ponce, J., and LeCun, Y.(2011). Ask the locals: multi-way local pooling for image recognition. In ICCV'11.
Bourlard, H. and Kamp, Y. (1988).
Auto-association by multilayer perceptrons and singular value decomposition.
Biological Cybernetics, 59, 291–294.
Brand, M. (2003). Charting a manifold. In NIPS'2002, pages
961–968. MIT Press.
Breuleux, O., Bengio, Y., and Vincent, P. (2011). Quickly generating representative samples from an rbm-derived process.
Neural Computation, 23(8), 2053–2073.
Bruna, J. and Mallat, S. (2011). Classification with scattering operators. In ICPR'2011.
Cadieu, C. and Olshausen, B. (2009). Learning transformational invariants from natural movies. In NIPS'2009, pages
209–216. MIT Press.
Carreira-Perpi˜nan, M. A. and Hinton, G. E. (2005).
On contrastive divergence learning.
In AISTATS'2005, pages
33–40.
Cayton, L. (2005). Algorithms for manifold learning. Technical Report CS2008-0923, UCSD.
Chen, M., Xu, Z., Winberger, K. Q., and Sha, F. (2012).
Marginalized denoising autoencoders for domain adaptation. In ICML'2012.
Cho, K., Raiko, T., and Ilin, A. (2010). Parallel tempering is efficient for learning restricted Boltzmann machines. In
IJCNN'2010.
Cho, K., Raiko, T., and Ilin, A. (2011). Enhanced gradient and adaptive learning rate for training restricted Boltzmann machines. In ICML'2011, pages 105–112.
Ciresan, D., Meier, U., and Schmidhuber, J. (2012). Multicolumn deep neural networks for image classification. Technical report, arXiv:1202.2745.
Ciresan, D. C., Meier, U., Gambardella, L. M., and Schmidhuber, J. (2010). Deep big simple neural nets for handwritten digit recognition. Neural Computation, 22, 1–14.
Coates, A. and Ng, A. Y. (2011a). The importance of encoding versus training with sparse coding and vector quantization.
In ICML'2011.
Coates, A. and Ng, A. Y. (2011b). Selecting receptive fields in deep networks. In NIPS'2011.
Collobert, R. and Weston, J. (2008). A unified architecture for natural language processing: Deep neural networks with multitask learning. In ICML'2008.
Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., and Kuksa, P. (2011). Natural language processing (almost) from scratch.
Journal of Machine
Learning Research, 12, 2493–2537.
Comon, P. (1994). Independent component analysis - a new concept? Signal Processing, 36, 287–314.
Courville, A., Bergstra, J., and Bengio, Y. (2011a). A spike and slab restricted Boltzmann machine. In AISTATS'2011.
Courville, A., Bergstra, J., and Bengio, Y. (2011b).
Unsupervised models of images by spike-and-slab RBMs.
In
ICML'2011.
Dahl, G. E., Ranzato, M., Mohamed, A., and Hinton, G. E.
Phone recognition with the mean-covariance restricted Boltzmann machine. In NIPS'2010.
Dahl, G. E., Yu, D., Deng, L., and Acero, A. (2012). Contextdependent pre-trained deep neural networks for large vocabulary speech recognition. IEEE Transactions on Audio, Speech, and Language Processing, 20(1), 33–42.
Dempster, A. P., Laird, N. M., and Rubin, D. B. (1977).
Maximum-likelihood from incomplete data via the EM algorithm. J. Royal Statistical Society B, 39, 1–38.
Desjardins, G. and Bengio, Y. (2008). Empirical evaluation of convolutional RBMs for vision. Technical Report 1327, Dept. IRO, U. Montr´eal.
Desjardins, G., Courville, A., Bengio, Y., Vincent, P., and Delalleau, O. (2010). Tempered Markov chain monte carlo for training of restricted Boltzmann machine.
In JMLR
W&CP: Proc. AISTATS'2010, volume 9, pages 145–152.
Desjardins, G., Courville, A., and Bengio, Y. (2011).
On tracking the partition function. In NIPS'2011.
Desjardins, G., Courville, A., and Bengio, Y. (2012).
On training deep Boltzmann machines.
Technical Report arXiv:1203.4416v1, Universit´e de Montr´eal.
DiCarlo, J., Zoccolan, D., and Rust, N. (2012). How does the brain solve visual object recognition? Neuron.
Donoho, D. L. and Grimes, C. (2003).
Hessian eigenmaps: new locally linear embedding techniques for highdimensional data. Technical Report 2003-08, Dept. Statistics, Stanford University.
Eisner, J. (2012).
Learning approximate inference policies for fast prediction.
Keynote talk at ICML Workshop on
Inferning: Interactions Between Search and Learning.
Erhan, D., Courville, A., and Bengio, Y. (2010a). Understanding representations learned in deep architectures. Technical
Report 1355, Universit´e de Montr´eal/DIRO.
Erhan, D., Bengio, Y., Courville, A., Manzagol, P.-A., Vincent, P., and Bengio, S. (2010b). Why does unsupervised pretraining help deep learning? Journal of Machine Learning
Research, 11, 625–660.
Farabet, C., LeCun, Y., Kavukcuoglu, K., Culurciello, E., Martini, B., Akselrod, P., and Talay, S. (2011).
Largescale fpga-based convolutional networks. In R. Bekkerman, M. Bilenko, and J. Langford, editors, Scaling up Machine
Learning: Parallel and Distributed Approaches. Cambridge
University Press.
Freund, Y. and Haussler, D. (1994). Unsupervised learning of distributions on binary vectors using two layer networks.
Technical Report UCSC-CRL-94-25, University of California, Santa Cruz.
Friedman, J. H. and Stuetzle, W. (1981). Projection pursuit regression.
J. American Statistical Association, 76(376), 817–823.
Fukushima, K. (1980). Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. Biological Cybernetics, 36, 193–202.
Fukushima, K. and Miyake, S. (1982). Neocognitron: A new algorithm for pattern recognition tolerant of deformations and shifts in position. Pattern Recognition, 15, 455–469.
Glorot, X. and Bengio, Y. (2010).
Understanding the difficulty of training deep feedforward neural networks.
In
AISTATS'2010, pages 249–256.
Glorot, X., Bordes, A., and Bengio, Y. (2011a). Deep sparse rectifier neural networks. In AISTATS'2011.
Glorot, X., Bordes, A., and Bengio, Y. (2011b).
Domain adaptation for large-scale sentiment classification: A deep learning approach. In ICML'2011.
Goodfellow, I., Le, Q., Saxe, A., and Ng, A. (2009). Measuring invariances in deep networks. In NIPS'2009, pages 646–
Goodfellow, I., Courville, A., and Bengio, Y. (2011). Spikeand-slab sparse coding for unsupervised feature discovery.
In NIPS Workshop on Challenges in Learning Hierarchical
Models.
Goodfellow, I. J., Courville, A., and Bengio, Y. (2012). Spikeand-slab sparse coding for unsupervised feature discovery. arXiv:1201.3382.
Gregor, K. and LeCun, Y. (2010a). Emergence of complexlike cells in a temporal product network with local receptive fields. Technical report, arXiv:1006.0448.
Gregor, K. and LeCun, Y. (2010b). Learning fast approximations of sparse coding. In ICML'2010.
Gregor, K., Szlam, A., and LeCun, Y. (2011). Structured sparse coding via lateral inhibition. In NIPS'2011.
Gribonval, R. (2011). Should penalized least squares regression be interpreted as Maximum A Posteriori estimation?
IEEE Transactions on Signal Processing, 59(5), 2405–2410.
Grosse, R., Raina, R., Kwong, H., and Ng, A. Y. (2007).
Shift-invariant sparse coding for audio classification.
In
UAI'2007.
Grubb, A. and Bagnell, J. A. D. (2010). Boosted backpropagation learning for training deep modular networks.
In
ICML'2010.
Gutmann, M. and Hyvarinen, A. (2010).
Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In AISTATS'2010.
H˚astad, J. (1986).
Almost optimal lower bounds for small depth circuits. In STOC'86, pages 6–20.
H˚astad, J. and Goldmann, M. (1991). On the power of smalldepth threshold circuits. Computational Complexity, 1, 113–
Henaff, M., Jarrett, K., Kavukcuoglu, K., and LeCun, Y.(2011). Unsupervised learning of sparse features for scalable audio classification. In ISMIR'11.
Hinton, G., Krizhevsky, A., and Wang, S. (2011). Transforming auto-encoders. In ICANN'2011.
Hinton, G. E. (1986). Learning distributed representations of concepts. In Proceedings of the Eighth Annual Conference of the Cognitive Science Society, pages 1–12, Amherst 1986.
Lawrence Erlbaum, Hillsdale.
Hinton, G. E. (1999). Products of experts. In ICANN'1999.
Hinton, G. E. (2000).
Training products of experts by minimizing contrastive divergence. Technical Report GCNU
TR 2000-004, Gatsby Unit, University College London.
Hinton, G. E. (2010).
A practical guide to training restricted Boltzmann machines. Technical Report UTML TR
2010-003, Department of Computer Science, University of Toronto.
Hinton, G. E. and Roweis, S. (2003).
Stochastic neighbor embedding. In NIPS'2002.
Hinton, G. E. and Salakhutdinov, R. (2006).
Reducing the dimensionality of data with neural networks.
Science, 313(5786), 504–507.
Hinton, G. E. and Zemel, R. S. (1994).
Autoencoders, minimum description length, and helmholtz free energy. In
NIPS'1993.
Hinton, G. E., Osindero, S., and Teh, Y. (2006a). A fast learning algorithm for deep belief nets. Neural Computation, 18, 1527–1554.
Hinton, G. E., Osindero, S., Welling, M., and Teh, Y. (2006b).
Unsupervised discovery of non-linear structure using contrastive backpropagation.
Cognitive Science, 30(4), 725–
Hotelling, H. (1933).
Analysis of a complex of statistical variables into principal components. Journal of Educational
Psychology, 24, 417–441, 498–520.
Hubel, D. H. and Wiesel, T. N. (1959).
Receptive fields of single neurons in the cat's striate cortex.
Journal of Physiology, 148, 574–591.
Hurri, J. and Hyv¨arinen, A. (2003). Temporal coherence, natural image sequences, and the visual cortex. In NIPS'2002.
Hyv¨arinen, A. (2005a). Estimation of non-normalized statistical models using score matching. J. Machine Learning
Res., 6.
Hyv¨arinen, A. (2005b).
Estimation of non-normalized statistical models using score matching. Journal of Machine
Learning Research, 6, 695–709.
Hyv¨arinen, A. (2007). Some extensions of score matching.
Computational Statistics and Data Analysis, 51, 2499–2512.
Hyv¨arinen, A. (2008). Optimal approximation of signal priors.
Neural Computation, 20(12), 3087–3110.
Hyv¨arinen, A. and Hoyer, P. (2000). Emergence of phase and shift invariant features by decomposition of natural images into independent feature subspaces. Neural Computation, 12(7), 1705–1720.
Hyv¨arinen, A., Karhunen, J., and Oja, E. (2001a). Independent
Component Analysis. Wiley-Interscience.
Hyv¨arinen, A., Hoyer, P. O., and Inki, M. (2001b).
Topographic independent component analysis. Neural Computation, 13(7), 1527–1558.
Hyv¨arinen, A., Hurri, J., and Hoyer, P. O. (2009). Natural
Image Statistics: A probabilistic approach to early computational vision. Springer-Verlag.
Jaeger, H. (2007). Echo state network. Scholarpedia, 2(9), Jain, V. and Seung, S. H. (2008). Natural image denoising with convolutional networks. In NIPS'2008.
Jarrett, K., Kavukcuoglu, K., Ranzato, M., and LeCun, Y.(2009). What is the best multi-stage architecture for object recognition? In ICCV'09.
Jenatton, R., Audibert, J.-Y., and Bach, F. (2009). Structured variable selection with sparsity-inducing norms. Technical report, arXiv:0904.3523.
Jutten, C. and Herault, J. (1991). Blind separation of sources, part I: an adaptive algorithm based on neuromimetic architecture. Signal Processing, 24, 1–10.
Kavukcuoglu, K., Ranzato, M., and LeCun, Y. (2008). Fast inference in sparse coding algorithms with applications to object recognition. CBLL-TR-2008-12-01, NYU.
Kavukcuoglu, K., Ranzato, M.-A., Fergus, R., and LeCun, Y. (2009). Learning invariant features through topographic filter maps. In CVPR'2009.
Kavukcuoglu, K., Sermanet, P., Boureau, Y.-L., Gregor, K., Mathieu, M., and LeCun, Y. (2010). Learning convolutional feature hierarchies for visual recognition. In NIPS'2010.
Kingma, D. and LeCun, Y. (2010). Regularized estimation of image statistics by score matching. In NIPS'2010.
Kivinen, J. J. and Williams, C. K. I. (2012). Multiple texture
Boltzmann machines. In AISTATS'2012.
K¨ording, K. P., Kayser, C., Einh¨auser, W., and K¨onig, P.
How are complex cell properties adapted to the statistics of natural stimuli? J. Neurophysiology, 91.
Krizhevsky, A. (2010). Convolutional deep belief networks on cifar-10. Technical report, U. Toronto.
Krizhevsky, A. and Hinton, G. (2009).
Learning multiple layers of features from tiny images. Technical report, U.
Toronto.
Krizhevsky, A., Sutskever, I., and Hinton, G. (2012). ImageNet classification with deep convolutional neural networks. In
Advances in Neural Information Processing Systems 25(NIPS'2012).
Larochelle, H. and Bengio, Y. (2008). Classification using discriminative restricted Boltzmann machines. In ICML'2008.
Larochelle, H., Bengio, Y., Louradour, J., and Lamblin, P.(2009). Exploring strategies for training deep neural networks. Journal of Machine Learning Research, 10, 1–40.
Lazebnik, S., Schmid, C., and Ponce, J. (2006).
Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories. In CVPR'2006.
Le, Q., Ngiam, J., Chen, Z., hao Chia, D. J., Koh, P. W., and Ng, A. (2010).
Tiled convolutional neural networks.
In
NIPS'2010.
Le, Q., Ngiam, J., Coates, A., Lahiri, A., Prochnow, B., and Ng, A. (2011a). On optimization methods for deep learning.
In ICML'2011.
Le, Q. V., Karpenko, A., Ngiam, J., and Ng, A. Y. (2011b).
ICA with reconstruction cost for efficient overcomplete feature learning. In NIPS'2011.
Le, Q. V., Zou, W. Y., Yeung, S. Y., and Ng, A. Y.(2011c). Learning hierarchical spatio-temporal features for action recognition with independent subspace analysis. In
CVPR'2011.
Le Roux, N., Bengio, Y., Lamblin, P., Joliveau, M., and Kegl, B. (2008a).
Learning the 2-D topology of images.
In
NIPS'07.
Le Roux, N., Manzagol, P.-A., and Bengio, Y. (2008b). Topmoumoute online natural gradient algorithm. In NIPS'07.
LeCun, Y. (1986).
Learning processes in an asymmetric threshold network. In F. Fogelman-Souli´e, E. Bienenstock, and G. Weisbuch, editors, Disordered Systems and Biological Organization, pages 233–240. Springer-Verlag, Les
Houches, France.
LeCun, Y. (1987). Mod`eles connexionistes de l'apprentissage.
Ph.D. thesis, Universit´e de Paris VI.
LeCun, Y. (1989). Generalization and network design strategies. In Connectionism in Perspective. Elsevier Publishers.
LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., and Jackel, L. D. (1989). Backpropagation applied to handwritten zip code recognition. Neural
Computation, 1(4), 541–551.
LeCun, Y., Bottou, L., Orr, G. B., and M¨uller, K.-R. (1998a).
Efficient backprop. In Neural Networks, Tricks of the Trade, Lecture Notes in Computer Science LNCS 1524. Springer
Verlag.
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998b).
Gradient-based learning applied to document recognition.
Proceedings of the IEEE, 86(11), 2278–2324.
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998c).
Gradient based learning applied to document recognition.
IEEE, 86(11), 2278–2324.
Lee, H., Ekanadham, C., and Ng, A. (2008).
Sparse deep belief net model for visual area V2. In NIPS'07.
Lee, H., Grosse, R., Ranganath, R., and Ng, A. Y. (2009a).
Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations.
In
ICML'2009.
Lee, H., Pham, P., Largman, Y., and Ng, A. (2009b).
Unsupervised feature learning for audio classification using convolutional deep belief networks. In NIPS'2009.
Lin, Y., Tong, Z., Zhu, S., and Yu, K. (2010). Deep coding network. In NIPS'2010.
Lowe, D. (1999). Object recognition from local scale invariant features. In ICCV'99.
Mallat, S. (2011). Group invariant scattering. Communications in Pure and Applied Mathematics. to appear.
Marlin, B. and de Freitas, N. (2011). Asymptotic efficiency of deterministic estimators for discrete energy-based models:
Ratio matching and pseudolikelihood. In UAI'2011.
Marlin, B., Swersky, K., Chen, B., and de Freitas, N. (2010).
Inductive principles for restricted Boltzmann machine learning. In AISTATS'2010, pages 509–516.
Martens, J. (2010). Deep learning via Hessian-free optimization. In ICML'2010, pages 735–742.
Martens, J. and Sutskever, I. (2011). Learning recurrent neural networks with Hessian-free optimization. In ICML'2011.
Memisevic, R. and Hinton, G. E. (2010). Learning to represent
32 spatial transformations with factored higher-order Boltzmann machines. Neural Comp., 22(6).
Mesnil, G., Dauphin, Y., Glorot, X., Rifai, S., Bengio, Y., Goodfellow, I., Lavoie, E., Muller, X., Desjardins, G., Warde-Farley, D., Vincent, P., Courville, A., and Bergstra, J. (2011). Unsupervised and transfer learning challenge: a deep learning approach. In JMLR W&CP: Proc. Unsupervised and Transfer Learning, volume 7.
Mikolov, T., Deoras, A., Kombrink, S., Burget, L., and Cernocky, J. (2011).
Empirical evaluation and combination of advanced language modeling techniques.
In INTERSPEECH'2011.
Mobahi, H., Collobert, R., and Weston, J. (2009).
Deep learning from temporal coherence in video. In ICML'2009.
Mohamed, A., Dahl, G., and Hinton, G. (2012).
Acoustic modeling using deep belief networks. IEEE Trans. on Audio, Speech and Language Processing, 20(1), 14–22.
Montufar, G. F. and Morton, J. (2012). When does a mixture of products contain a product of mixtures? Technical report, arXiv:1206.0387.
Murray, I. and Salakhutdinov, R. (2009). Evaluating probabilities under high-dimensional latent variable models. In
NIPS'2008, pages 1137–1144.
Nair, V. and Hinton, G. E. (2010).
Rectified linear units improve restricted Boltzmann machines. In ICML'10.
Narayanan, H. and Mitter, S. (2010). Sample complexity of testing the manifold hypothesis. In NIPS'2010.
Neal, R. M. (1992). Connectionist learning of belief networks.
Artificial Intelligence, 56, 71–113.
Neal, R. M. (1993).
Probabilistic inference using Markov chain Monte-Carlo methods.
Technical Report CRG-TR93-1, Dept. of Computer Science, University of Toronto.
Ngiam, J., Chen, Z., Koh, P., and Ng, A. (2011). Learning deep energy models. In Proc. ICML'2011. ACM.
Olshausen, B. A. and Field, D. J. (1996).
Emergence of simple-cell receptive field properties by learning a sparse code for natural images. Nature, 381, 607–609.
Olshausen, B. A. and Field, D. J. (1997).
Sparse coding with an overcomplete basis set: a strategy employed by V1?
Vision Research, 37, 3311–3325.
Orr, G. and Muller, K.-R., editors (1998). Neural networks: tricks of the trade. Lect. Notes Comp. Sc.,. Springer-Verlag.
Pearson, K. (1901).
On lines and planes of closest fit to systems of points in space. Philosophical Magazine, 2(6).
Poggio, T. and Vetter, T. (1992). Recognition and structure from one 2d model view: Observations on prototypes, object classes and symmetries. AI Lab Memo No. 1347, MIT.
Raiko, T., Valpola, H., and LeCun, Y. (2012). Deep learning made easier by linear transformations in perceptrons.
In
AISTATS'2012.
Raina, R., Battle, A., Lee, H., Packer, B., and Ng, A. Y. (2007).
Self-taught learning: transfer learning from unlabeled data.
In ICML'2007.
Ranzato, M. and Hinton, G. H. (2010). Modeling pixel means and covariances using factorized third-order Boltzmann machines. In CVPR'2010, pages 2551–2558.
Ranzato, M., Poultney, C., Chopra, S., and LeCun, Y. (2007).
Efficient learning of sparse representations with an energybased model. In NIPS'06.
Ranzato, M., Boureau, Y., and LeCun, Y. (2008).
Sparse feature learning for deep belief networks. In NIPS'2007.
Ranzato, M., Krizhevsky, A., and Hinton, G. (2010a). Factored
3-way restricted Boltzmann machines for modeling natural images. In AISTATS'2010, pages 621–628.
Ranzato, M., Mnih, V., and Hinton, G. (2010b). Generating more realistic images using gated MRF's. In NIPS'2010.
Ranzato, M., Susskind, J., Mnih, V., and Hinton, G. (2011).
On deep generative models with applications to recognition.
In CVPR'2011.
Riesenhuber, M. and Poggio, T. (1999). Hierarchical models of object recognition in cortex. Nature Neuroscience.
Rifai, S., Vincent, P., Muller, X., Glorot, X., and Bengio, Y. (2011a). Contractive auto-encoders: Explicit invariance during feature extraction. In ICML'2011.
Rifai, S., Mesnil, G., Vincent, P., Muller, X., Bengio, Y., Dauphin, Y., and Glorot, X. (2011b).
Higher order contractive auto-encoder. In ECML PKDD.
Rifai, S., Dauphin, Y., Vincent, P., Bengio, Y., and Muller, X.(2011c). The manifold tangent classifier. In NIPS'2011.
Rifai, S., Bengio, Y., Dauphin, Y., and Vincent, P. (2012). A generative process for sampling contractive auto-encoders.
In ICML'2012.
Roweis, S. (1997). EM algorithms for PCA and sensible PCA.
CNS Technical Report CNS-TR-97-02, Caltech.
Roweis, S. and Saul, L. K. (2000). Nonlinear dimensionality reduction by locally linear embedding. Science, 290(5500), 2323–2326.
Salakhutdinov, R. (2010a).
Learning deep Boltzmann machines using adaptive MCMC. In ICML'2010.
Salakhutdinov, R. (2010b). Learning in Markov random fields using tempered transitions. In NIPS'2010.
Salakhutdinov, R. and Hinton, G. E. (2007). Semantic hashing.
In SIGIR'2007.
Salakhutdinov, R. and Hinton, G. E. (2009). Deep Boltzmann machines. In AISTATS'2009, pages 448–455.
Salakhutdinov, R. and Larochelle, H. (2010). Efficient learning of deep Boltzmann machines. In AISTATS'2010.
Salakhutdinov, R., Mnih, A., and Hinton, G. (2007).
Restricted Boltzmann machines for collaborative filtering. In
ICML'2007, pages 791–798.
Savard, F. (2011). R´eseaux de neurones `a relaxation entraˆın´es par crit`ere d'autoencodeur d´ebruitant.
Master's thesis, Universit´e de Montr´eal.
Schmah, T., Hinton, G. E., Zemel, R., Small, S. L., and Strother, S. (2009). Generative versus discriminative training of RBMs for classification of fMRI images.
In
NIPS'2008, pages 1409–1416.
Sch¨olkopf, B., Smola, A., and M¨uller, K.-R. (1998). Nonlinear component analysis as a kernel eigenvalue problem. Neural
Computation, 10, 1299–1319.
Schwenk, H., Rousseau, A., and Attik, M. (2012).
Large, pruned or continuous space language models on a gpu for statistical machine translation. In Workshop on the future of language modeling for HLT.
Seide, F., Li, G., and Yu, D. (2011). Conversational speech transcription using context-dependent deep neural networks.
In Interspeech 2011, pages 437–440.
Serre, T., Wolf, L., Bileschi, S., and Riesenhuber, M. (2007).
Robust object recognition with cortex-like mechanisms.
IEEE Trans. Pattern Anal. Mach. Intell., 29(3), 411–426.
Seung, S. H. (1998). Learning continuous attractors in recurrent networks. In NIPS'1997.
Simard, D., Steinkraus, P. Y., and Platt, J. C. (2003). Best practices for convolutional neural networks. In ICDAR'2003.
Simard, P., Victorri, B., LeCun, Y., and Denker, J. (1992). Tangent prop - A formalism for specifying selected invariances in an adaptive network. In NIPS'1991.
Simard, P. Y., LeCun, Y., and Denker, J. (1993).
Efficient pattern recognition using a new transformation distance. In
NIPS'92, pages 50–58.
Smolensky, P. (1986). Information processing in dynamical systems: Foundations of harmony theory. In D. E. Rumelhart and J. L. McClelland, editors, Parallel Distributed
Processing, volume 1, chapter 6, pages 194–281. MIT Press, Cambridge.
Socher, R., Huang, E. H., Pennington, J., Ng, A. Y., and Manning, C. D. (2011a).
Dynamic pooling and unfolding recursive autoencoders for paraphrase detection.
In
NIPS'2011.
Socher, R., Pennington, J., Huang, E. H., Ng, A. Y., and Manning, C. D. (2011b).
Semi-supervised recursive autoencoders for predicting sentiment distributions.
In
EMNLP'2011.
Stoyanov, V., Ropson, A., and Eisner, J. (2011). Empirical risk minimization of graphical model parameters given approximate inference, decoding, and model structure. In
AISTATS'2011.
Sutskever, I. (2012).
Training Recurrent Neural Networks.
Ph.D. thesis, Departement of computer science, University of Toronto.
Sutskever, I. and Tieleman, T. (2010). On the Convergence
Properties of Contrastive Divergence. In AISTATS'2010.
Sutskever, I., Hinton, G. E., and Taylor, G. (2009). The recurrent temporal restricted Boltzmann machine. In NIPS'2008.
Swersky, K. (2010).
Inductive Principles for Learning Restricted Boltzmann Machines.
Master's thesis, University of British Columbia.
Swersky, K., Ranzato, M., Buchman, D., Marlin, B., and de Freitas, N. (2011). On score matching for energy based models: Generalizing autoencoders and simplifying deep learning. In Proc. ICML'2011. ACM.
Taylor, G. and Hinton, G. (2009).
Factored conditional restricted Boltzmann machines for modeling motion style.
In ICML'2009, pages 1025–1032.
Taylor, G., Fergus, R., LeCun, Y., and Bregler, C. (2010).
Convolutional learning of spatio-temporal features.
In
ECCV'10, pages 140–153.
Tenenbaum, J., de Silva, V., and Langford, J. C. (2000). A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500), 2319–2323.
Tieleman, T. (2008).
Training restricted Boltzmann machines using approximations to the likelihood gradient. In
ICML'2008, pages 1064–1071.
Tieleman, T. and Hinton, G. (2009). Using fast weights to improve persistent contrastive divergence. In ICML'2009.
Tipping, M. E. and Bishop, C. M. (1999). Probabilistic principal components analysis. Journal of the Royal Statistical
Society B, 61(3), 611–622.
Turaga, S. C., Murray, J. F., Jain, V., Roth, F., Helmstaedter, M., Briggman, K., Denk, W., and Seung, H. S. (2010).
Convolutional networks can learn to generate affinity graphs for image segmentation. Neural Computation, 22, 511–538. van der Maaten, L. (2009). Learning a parametric embedding by preserving local structure. In AISTATS'2009. van der Maaten, L. and Hinton, G. E. (2008). Visualizing data using t-sne. J. Machine Learning Res., 9.
Vincent, P. (2011). A connection between score matching and denoising autoencoders. Neural Computation, 23(7).
Vincent, P. and Bengio, Y. (2003). Manifold Parzen windows.
In NIPS'2002. MIT Press.
Vincent, P., Larochelle, H., Bengio, Y., and Manzagol, P.A. (2008). Extracting and composing robust features with denoising autoencoders. In ICML 2008.
Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., and Manzagol, P.-A. (2010). Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. J. Machine Learning Res., 11.
Weinberger, K. Q. and Saul, L. K. (2004).
Unsupervised learning of image manifolds by semidefinite programming.
In CVPR'2004, pages 988–995.
Welling, M. (2009). Herding dynamic weights for partially observed random field models. In UAI'2009.
Welling, M., Hinton, G. E., and Osindero, S. (2003). Learning sparse topographic representations with products of Studentt distributions. In NIPS'2002.
Weston, J., Ratle, F., and Collobert, R. (2008). Deep learning via semi-supervised embedding. In ICML 2008.
Weston, J., Bengio, S., and Usunier, N. (2010). Large scale image annotation: learning to rank with joint word-image embeddings. Machine Learning, 81(1), 21–35.
Wiskott, L. and Sejnowski, T. (2002). Slow feature analysis:
Unsupervised learning of invariances. Neural Computation, 14(4), 715–770.
Younes, L. (1999). On the convergence of markovian stochastic algorithms with rapidly decreasing ergodicity rates.
Stochastics and Stochastic Reports, 65(3), 177–228.
Yu, D., Wang, S., and Deng, L. (2010). Sequential labeling using deep-structured conditional random fields.
IEEE
Journal of Selected Topics in Signal Processing.
Yu, K. and Zhang, T. (2010). Improved local coordinate coding using local tangents. In ICML'2010.
Yu, K., Zhang, T., and Gong, Y. (2009). Nonlinear learning using local coordinate coding. In NIPS'2009.
Yu, K., Lin, Y., and Lafferty, J. (2011).
Learning image representations from the pixel level via hierarchical sparse coding. In CVPR.
Yuille, A. L. (2005). The convergence of contrastive divergences. In NIPS'2004, pages 1593–1600.
Zeiler, M., Krishnan, D., Taylor, G., and Fergus, R. (2010).
Deconvolutional networks. In CVPR'2010.
Zou, W. Y., Ng, A. Y., and Yu, K. (2011).
Unsupervised learning of visual invariance with temporal coherence. In
NIPS 2011 Workshop on Deep Learning and Unsupervised
Feature Learning.Journal of Machine Learning Research 11 (2010) 625-660
Submitted 8/09; Published 2/10
Why Does Unsupervised Pre-training Help Deep Learning?
Dumitru Erhan∗
DUMITRU.ERHAN@UMONTREAL.CA
Yoshua Bengio
YOSHUA.BENGIO@UMONTREAL.CA
Aaron Courville
AARON.COURVILLE@UMONTREAL.CA
Pierre-Antoine Manzagol
PIERRE-ANTOINE.MANZAGOL@UMONTREAL.CA
Pascal Vincent
PASCAL.VINCENT@UMONTREAL.CA
D´epartement d'informatique et de recherche op´erationnelle
Universit´e de Montr´eal
2920, chemin de la Tour
Montr´eal, Qu´ebec, H3T 1J8, Canada
Samy Bengio
BENGIO@GOOGLE.COM
Google Research
1600 Amphitheatre Parkway
Mountain View, CA, 94043, USA
Editor: L´eon Bottou
Abstract
Much recent research has been devoted to learning algorithms for deep architectures such as Deep
Belief Networks and stacks of auto-encoder variants, with impressive results obtained in several areas, mostly on vision and language data sets. The best results obtained on supervised learning tasks involve an unsupervised learning component, usually in an unsupervised pre-training phase.
Even though these new algorithms have enabled training deep models, many questions remain as to the nature of this difficult learning problem. The main question investigated here is the following: how does unsupervised pre-training work? Answering this questions is important if learning in deep architectures is to be further improved. We propose several explanatory hypotheses and test them through extensive simulations. We empirically show the influence of pre-training with respect to architecture depth, model capacity, and number of training examples. The experiments confirm and clarify the advantage of unsupervised pre-training. The results suggest that unsupervised pretraining guides the learning towards basins of attraction of minima that support better generalization from the training data set; the evidence from these results supports a regularization explanation for the effect of pre-training.
Keywords: deep architectures, unsupervised pre-training, deep belief networks, stacked denoising auto-encoders, non-convex optimization
1. Introduction
Deep learning methods aim at learning feature hierarchies with features from higher levels of the hierarchy formed by the composition of lower level features. They include learning methods for a wide array of deep architectures (Bengio, 2009 provides a survey), including neural networks with many hidden layers (Bengio et al., 2007; Ranzato et al., 2007; Vincent et al., 2008; Collobert and Weston, 2008) and graphical models with many levels of hidden variables (Hinton et al., 2006), ∗. Part of this work was done while Dumitru Erhan was at Google Research. c⃝2010 Dumitru Erhan, Yoshua Bengio, Aaron Courville, Pierre-Antoine Manzagol, Pascal Vincent and Samy Bengio.
ERHAN, BENGIO, COURVILLE, MANZAGOL, VINCENT AND BENGIO among others (Zhu et al., 2009; Weston et al., 2008). Theoretical results (Yao, 1985; H˚astad, 1986;
H˚astad and Goldmann, 1991; Bengio et al., 2006), reviewed and discussed by Bengio and LeCun(2007), suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g., in vision, language, and other AI-level tasks), one may need deep architectures.
The recent surge in experimental work in the field seems to support this notion, accumulating evidence that in challenging AI-related tasks—such as computer vision (Bengio et al., 2007; Ranzato et al., 2007; Larochelle et al., 2007; Ranzato et al., 2008; Lee et al., 2009; Mobahi et al., 2009; Osindero and Hinton, 2008), natural language processing (NLP) (Collobert and Weston, 2008; Weston et al., 2008), robotics (Hadsell et al., 2008), or information retrieval (Salakhutdinov and Hinton, 2007; Salakhutdinov et al., 2007)—deep learning methods significantly out-perform comparable but shallow competitors, and often match or beat the state-of-the-art.
These recent demonstrations of the potential of deep learning algorithms were achieved despite the serious challenge of training models with many layers of adaptive parameters. In virtually all instances of deep learning, the objective function is a highly non-convex function of the parameters, with the potential for many distinct local minima in the model parameter space. The principal difficulty is that not all of these minima provide equivalent generalization errors and, we suggest, that for deep architectures, the standard training schemes (based on random initialization) tend to place the parameters in regions of the parameters space that generalize poorly—as was frequently observed empirically but rarely reported (Bengio and LeCun, 2007).
The breakthrough to effective training strategies for deep architectures came in 2006 with the algorithms for training deep belief networks (DBN) (Hinton et al., 2006) and stacked autoencoders (Ranzato et al., 2007; Bengio et al., 2007), which are all based on a similar approach: greedy layer-wise unsupervised pre-training followed by supervised fine-tuning. Each layer is pretrained with an unsupervised learning algorithm, learning a nonlinear transformation of its input(the output of the previous layer) that captures the main variations in its input. This unsupervised pre-training sets the stage for a final training phase where the deep architecture is fine-tuned with respect to a supervised training criterion with gradient-based optimization. While the improvement in performance of trained deep models offered by the pre-training strategy is impressive, little is understood about the mechanisms underlying this success.
The objective of this paper is to explore, through extensive experimentation, how unsupervised pre-training works to render learning deep architectures more effective and why they appear to work so much better than traditional neural network training methods. There are a few reasonable hypotheses why unsupervised pre-training might work. One possibility is that unsupervised pretraining acts as a kind of network pre-conditioner, putting the parameter values in the appropriate range for further supervised training. Another possibility, suggested by Bengio et al. (2007), is that unsupervised pre-training initializes the model to a point in parameter space that somehow renders the optimization process more effective, in the sense of achieving a lower minimum of the empirical cost function.
Here, we argue that our experiments support a view of unsupervised pre-training as an unusual form of regularization: minimizing variance and introducing bias towards configurations of the parameter space that are useful for unsupervised learning. This perspective places unsupervised pretraining well within the family of recently developed semi-supervised methods. The unsupervised pre-training approach is, however, unique among semi-supervised training strategies in that it acts by defining a particular initialization point for standard supervised training rather than either modifying the supervised objective function (Barron, 1991) or explicitly imposing constraints on the parameWHY DOES UNSUPERVISED PRE-TRAINING HELP DEEP LEARNING? ters throughout training (Lasserre et al., 2006). This type of initialization-as-regularization strategy has precedence in the neural networks literature, in the shape of the early stopping idea (Sj¨oberg and Ljung, 1995; Amari et al., 1997), and in the Hidden Markov Models (HMM) community (Bahl et al., 1986; Povey and Woodland, 2002) where it was found that first training an HMM as a generative model was essential (as an initialization step) before fine-tuning it discriminatively. We suggest that, in the highly non-convex situation of training a deep architecture, defining a particular initialization point implicitly imposes constraints on the parameters in that it specifies which minima (out of a very large number of possible minima) of the cost function are allowed. In this way, it may be possible to think of unsupervised pre-training as being related to the approach of Lasserre et al.
Another important and distinct property of the unsupervised pre-training strategy is that in the standard situation of training using stochastic gradient descent, the beneficial generalization effects due to pre-training do not appear to diminish as the number of labeled examples grows very large.
We argue that this is a consequence of the combination of the non-convexity (multi-modality) of the objective function and the dependency of the stochastic gradient descent method on example ordering. We find that early changes in the parameters have a greater impact on the final region (basin of attraction of the descent procedure) in which the learner ends up. In particular, unsupervised pre-training sets the parameter in a region from which better basins of attraction can be reached, in terms of generalization. Hence, although unsupervised pre-training is a regularizer, it can have a positive effect on the training objective when the number of training examples is large.
As previously stated, this paper is concerned with an experimental assessment of the various competing hypotheses regarding the role of unsupervised pre-training in the recent success of deep learning methods. To this end, we present a series of experiments design to pit these hypotheses against one another in an attempt to resolve some of the mystery surrounding the effectiveness of unsupervised pre-training.
In the first set of experiments (in Section 6), we establish the effect of unsupervised pre-training on improving the generalization error of trained deep architectures. In this section we also exploit dimensionality reduction techniques to illustrate how unsupervised pre-training affects the location of minima in parameter space.
In the second set of experiments (in Section 7), we directly compare the two alternative hypotheses (pre-training as a pre-conditioner; and pre-training as an optimization scheme) against the hypothesis that unsupervised pre-training is a regularization strategy. In the final set of experiments, (in Section 8), we explore the role of unsupervised pre-training in the online learning setting, where the number of available training examples grows very large. In these experiments, we test key aspects of our hypothesis relating to the topology of the cost function and the role of unsupervised pre-training in manipulating the region of parameter space from which supervised training is initiated.
Before delving into the experiments, we begin with a more in-depth view of the challenges in training deep architectures and how we believe unsupervised pre-training works towards overcoming these challenges.
ERHAN, BENGIO, COURVILLE, MANZAGOL, VINCENT AND BENGIO
2. The Challenges of Deep Learning
In this section, we present a perspective on why standard training of deep models through gradient backpropagation appears to be so difficult. First, it is important to establish what we mean in stating that training is difficult.
We believe the central challenge in training deep architectures is dealing with the strong dependencies that exist during training between the parameters across layers. One way to conceive the difficulty of the problem is that we must simultaneously:
1. adapt the lower layers in order to provide adequate input to the final (end of training) setting of the upper layers
2. adapt the upper layers to make good use of the final (end of training) setting of the lower layers.
The second problem is easy on its own (i.e., when the final setting of the other layers is known). It is not clear how difficult is the first one, and we conjecture that a particular difficulty arises when both sets of layers must be learned jointly, as the gradient of the objective function is limited to a local measure given the current setting of other parameters. Furthermore, because with enough capacity the top two layers can easily overfit the training set, training error does not necessarily reveal the difficulty in optimizing the lower layers. As shown in our experiments here, the standard training schemes tend to place the parameters in regions of the parameters space that generalize poorly.
A separate but related issue appears if we focus our consideration of traditional training methods for deep architectures on stochastic gradient descent. A sequence of examples along with an online gradient descent procedure defines a trajectory in parameter space, which converges in some sense(the error does not improve anymore, maybe because we are near a local minimum). The hypothesis is that small perturbations of that trajectory (either by initialization or by changes in which examples are seen when) have more effect early on. Early in the process of following the stochastic gradient, changes in the weights tend to increase their magnitude and, consequently, the amount of nonlinearity of the network increases. As this happens, the set of regions accessible by stochastic gradient descent on samples of the training distribution becomes smaller. Early on in training small perturbations allow the model parameters to switch from one basin to a nearby one, whereas later on (typically with larger parameter values), it is unlikely to "escape" from such a basin of attraction.
Hence the early examples can have a larger influence and, in practice, trap the model parameters in particular regions of parameter space that correspond to the specific and arbitrary ordering of the training examples.1 An important consequence of this phenomenon is that even in the presence of a very large (effectively infinite) amounts of supervised data, stochastic gradient descent is subject to a degree of overfitting to the training data presented early in the training process. In that sense, unsupervised pre-training interacts intimately with the optimization process, and when the number of training examples becomes large, its positive effect is seen not only on generalization error but also on training error.
1. This process seems similar to the "critical period" phenomena observed in neuroscience and psychology (Bornstein, WHY DOES UNSUPERVISED PRE-TRAINING HELP DEEP LEARNING?
3. Unsupervised Pre-training Acts as a Regularizer
As stated in the introduction, we believe that greedy layer-wise unsupervised pre-training overcomes the challenges of deep learning by introducing a useful prior to the supervised fine-tuning training procedure. We claim that the regularization effect is a consequence of the pre-training procedure establishing an initialization point of the fine-tuning procedure inside a region of parameter space in which the parameters are henceforth restricted. The parameters are restricted to a relatively small volume of parameter space that is delineated by the boundary of the local basin of attraction of the supervised fine-tuning cost function.
The pre-training procedure increases the magnitude of the weights and in standard deep models, with a sigmoidal nonlinearity, this has the effect of rendering both the function more nonlinear and the cost function locally more complicated with more topological features such as peaks, troughs and plateaus. The existence of these topological features renders the parameter space locally more difficult to travel significant distances via a gradient descent procedure. This is the core of the restrictive property imposed by the pre-training procedure and hence the basis of its regularizing properties.
But unsupervised pre-training restricts the parameters to particular regions: those that correspond to capturing structure in the input distribution P(X). To simply state that unsupervised pretraining is a regularization strategy somewhat undermines the significance of its effectiveness. Not all regularizers are created equal and, in comparison to standard regularization schemes such as
L1 and L2 parameter penalization, unsupervised pre-training is dramatically effective. We believe the credit for its success can be attributed to the unsupervised training criteria optimized during unsupervised pre-training.
During each phase of the greedy unsupervised training strategy, layers are trained to represent the dominant factors of variation extant in the data. This has the effect of leveraging knowledge of X to form, at each layer, a representation of X consisting of statistically reliable features of X that can then be used to predict the output (usually a class label) Y. This perspective places unsupervised pre-training well within the family of learning strategies collectively know as semisupervised methods. As with other recent work demonstrating the effectiveness of semi-supervised methods in regularizing model parameters, we claim that the effectiveness of the unsupervised pretraining strategy is limited to the extent that learning P(X) is helpful in learning P(Y|X). Here, we find transformations of X—learned features—that are predictive of the main factors of variation in P(X), and when the pre-training strategy is effective,2 some of these learned features of X are also predictive of Y. In the context of deep learning, the greedy unsupervised strategy may also have a special function. To some degree it resolves the problem of simultaneously learning the parameters at all layers (mentioned in Section 2) by introducing a proxy criterion. This proxy criterion encourages significant factors of variation, present in the input data, to be represented in intermediate layers.
To clarify this line of reasoning, we can formalize the effect of unsupervised pre-training in inducing a prior distribution over the parameters. Let us assume that parameters are forced to be chosen in a bounded region S ⊂ Rd. Let S be split in regions {Rk} that are the basins of attraction of descent procedures in the training error (note that {Rk} depends on the training set, but the dependency decreases as the number of examples increases). We have ∪kRk = S and Ri ∩ R j = /0 for i ̸= j. Let vk =
R 1θ∈Rkdθ be the volume associated with region Rk (where θ are our model's
2. Acting as a form of (data-dependent) "prior" on the parameters, as we are about to formalize.
ERHAN, BENGIO, COURVILLE, MANZAGOL, VINCENT AND BENGIO parameters). Let rk be the probability that a purely random initialization (according to our initialization procedure, which factorizes across parameters) lands in Rk, and let πk be the probability that pre-training (following a random initialization) lands in Rk, that is, ∑k rk = ∑k πk = 1. We can now take into account the initialization procedure as a regularization term: regularizer = −logP(θ).
For pre-trained models, the prior is Ppre−training(θ) = ∑ k
1θ∈Rkπk/vk.
For the models without unsupervised pre-training, the prior is Pno−pre−training(θ) = ∑ k
1θ∈Rkrk/vk.
One can verify that Ppre−training(θ ∈ Rk) = πk and Pno−pre−training(θ ∈ Rk) = rk. When πk is tiny, the penalty is high when θ ∈ Rk, with unsupervised pre-training. The derivative of this regularizer is zero almost everywhere because we have chosen a uniform prior inside each region Rk. Hence, to take the regularizer into account, and having a generative model Ppre−training(θ) for θ (i.e., this is the unsupervised pre-training procedure), it is reasonable to sample an initial θ from it (knowing that from this point on the penalty will not increase during the iterative minimization of the training criterion), and this is exactly how the pre-trained models are obtained in our experiments.
Note that this formalization is just an illustration: it is there to simply show how one could conceptually think of an initialization point as a regularizer and should not be taken as a literal interpretation of how regularization is explicitly achieved, since we do not have an analytic formula for computing the πk's and vk's. Instead these are implicitly defined by the whole unsupervised pre-training procedure.
4. Previous Relevant Work
We start with an overview of the literature on semi-supervised learning (SSL), since the SSL framework is essentially the one in which we operate as well.
4.1 Related Semi-Supervised Methods
It has been recognized for some time that generative models are less prone to overfitting than discriminant ones (Ng and Jordan, 2002). Consider input variable X and target variable Y. Whereas a discriminant model focuses on P(Y|X), a generative model focuses on P(X,Y) (often parametrized as P(X|Y)P(Y)), that is, it also cares about getting P(X) right, which can reduce the freedom of fitting the data when the ultimate goal is only to predict Y given X.
Exploiting information about P(X) to improve generalization of a classifier has been the driving idea behind semi-supervised learning (Chapelle et al., 2006). For example, one can use unsupervised learning to map X into a representation (also called embedding) such that two examples x1 and x2 that belong to the same cluster (or are reachable through a short path going through neighboring examples in the training set) end up having nearby embeddings. One can then use supervised learning(e.g., a linear classifier) in that new space and achieve better generalization in many cases (Belkin
WHY DOES UNSUPERVISED PRE-TRAINING HELP DEEP LEARNING? and Niyogi, 2002; Chapelle et al., 2003). A long-standing variant of this approach is the application of Principal Components Analysis as a pre-processing step before applying a classifier (on the projected data). In these models the data is first transformed in a new representation using unsupervised learning, and a supervised classifier is stacked on top, learning to map the data in this new representation into class predictions.
Instead of having separate unsupervised and supervised components in the model, one can consider models in which P(X) (or P(X,Y)) and P(Y|X) share parameters (or whose parameters are connected in some way), and one can trade-off the supervised criterion −logP(Y|X) with the unsupervised or generative one (−logP(X) or −logP(X,Y)). It can then be seen that the generative criterion corresponds to a particular form of prior (Lasserre et al., 2006), namely that the structure of P(X) is connected to the structure of P(Y|X) in a way that is captured by the shared parametrization.
By controlling how much of the generative criterion is included in the total criterion, one can find a better trade-off than with a purely generative or a purely discriminative training criterion (Lasserre et al., 2006; Larochelle and Bengio, 2008).
In the context of deep architectures, a very interesting application of these ideas involves adding an unsupervised embedding criterion at each layer (or only one intermediate layer) to a traditional supervised criterion (Weston et al., 2008). This has been shown to be a powerful semi-supervised learning strategy, and is an alternative to the kind of algorithms described and evaluated in this paper, which also combine unsupervised learning with supervised learning.
In the context of scarcity of labelled data (and abundance of unlabelled data), deep architectures have shown promise as well. Salakhutdinov and Hinton (2008) describe a method for learning the covariance matrix of a Gaussian Process, in which the usage of unlabelled examples for modeling
P(X) improves P(Y|X) quite significantly. Note that such a result is to be expected: with few labelled samples, modeling P(X) usually helps. Our results show that even in the context of abundant labelled data, unsupervised pre-training still has a pronounced positive effect on generalization: a somewhat surprising conclusion.
4.2 Early Stopping as a Form of Regularization
We stated that pre-training as initialization can be seen as restricting the optimization procedure to a relatively small volume of parameter space that corresponds to a local basin of attraction of the supervised cost function. Early stopping can be seen as having a similar effect, by constraining the optimization procedure to a region of the parameter space that is close to the initial configuration of parameters. With τ the number of training iterations and η the learning rate used in the update procedure, τη can be seen as the reciprocal of a regularization parameter. Indeed, restricting either quantity restricts the area of parameter space reachable from the starting point. In the case of the optimization of a simple linear model (initialized at the origin) using a quadratic error function and simple gradient descent, early stopping will have a similar effect to traditional regularization.
Thus, in both pre-training and early stopping, the parameters of the supervised cost function are constrained to be close to their initial values.3 A more formal treatment of early stopping as regularization is given by Sj¨oberg and Ljung (1995) and Amari et al. (1997). There is no equivalent treatment of pre-training, but this paper sheds some light on the effects of such initialization in the case of deep architectures.
3. In the case of pre-training the "initial values" of the parameters for the supervised phase are those that were obtained at the end of pre-training.
ERHAN, BENGIO, COURVILLE, MANZAGOL, VINCENT AND BENGIO
5. Experimental Setup and Methodology
In this section, we describe the setting in which we test the hypothesis introduced in Section 3 and previously proposed hypotheses. The section includes a description of the deep architectures used, the data sets and the details necessary to reproduce our results.
5.1 Models
All of the successful methods (Hinton et al., 2006; Hinton and Salakhutdinov, 2006; Bengio et al., 2007; Ranzato et al., 2007; Vincent et al., 2008; Weston et al., 2008; Ranzato et al., 2008; Lee et al., 2008) in the literature for training deep architectures have something in common: they rely on an unsupervised learning algorithm that provides a training signal at the level of a single layer.
Most work in two main phases. In a first phase, unsupervised pre-training, all layers are initialized using this layer-wise unsupervised learning signal. In a second phase, fine-tuning, a global training criterion (a prediction error, using labels in the case of a supervised task) is minimized. In the algorithms initially proposed (Hinton et al., 2006; Bengio et al., 2007; Ranzato et al., 2007), the unsupervised pre-training is done in a greedy layer-wise fashion: at stage k, the k-th layer is trained(with respect to an unsupervised criterion) using as input the output of the previous layer, and while the previous layers are kept fixed.
We shall consider two deep architectures as representatives of two families of models encountered in the deep learning literature.
5.1.1 DEEP BELIEF NETWORKS
The first model is the Deep Belief Net (DBN) by Hinton et al. (2006), obtained by training and stacking several layers of Restricted Boltzmann Machines (RBM) in a greedy manner. Once this stack of RBMs is trained, it can be used to initialize a multi-layer neural network for classification.
An RBM with n hidden units is a Markov Random Field (MRF) for the joint distribution between hidden variables hi and observed variables xj such that P(h|x) and P(x|h) factorize, that is, P(h|x) = ∏i P(hi|x) and P(x|h) = ∏ j P(xj|h). The sufficient statistics of the MRF are typically hi, xj and hixj, which gives rise to the following joint distribution:
P(x,h) ∝ eh′Wx+b′x+c′h with corresponding parameters θ = (W,b,c) (with ′ denoting transpose, ci associated with hi, b j with xj, and Wij with hixj). If we restrict hi and xj to be binary units, it is straightforward to show that
P(x|h)
= ∏ j
P(xj|h) with
P(xj = 1|h)
= sigmoid(b j +∑ i
Wijhi). where sigmoid(a) = 1/(1+exp(−a)) (applied element-wise on a vector a), and P(h|x) also has a similar form:
P(h|x)
= ∏ i
P(hi|x) with
P(hi = 1|x)
= sigmoid(ci +∑ j
Wijxj).
WHY DOES UNSUPERVISED PRE-TRAINING HELP DEEP LEARNING?
The RBM form can be generalized to other conditional distributions besides the binomial, including continuous variables. Welling et al. (2005) describe a generalization of RBM models to conditional distributions from the exponential family.
RBM models can be trained by approximate stochastic gradient descent. Although P(x) is not tractable in an RBM, the Contrastive Divergence estimator (Hinton, 2002) is a good stochastic approximation of ∂logP(x)
∂θ, in that it very often has the same sign (Bengio and Delalleau, 2009).
A DBN is a multi-layer generative model with layer variables h0 (the input or visible layer), h1, h2, etc. The top two layers have a joint distribution which is an RBM, and P(hk|hk+1) are parametrized in the same way as for an RBM. Hence a 2-layer DBN is an RBM, and a stack of RBMs share parametrization with a corresponding DBN. The contrastive divergence update direction can be used to initialize each layer of a DBN as an RBM, as follows. Consider the first layer of the DBN trained as an RBM P1 with hidden layer h1 and visible layer v1. We can train a second RBM P2 that models (in its visible layer) the samples h1 from P1(h1|v1) when v1 is sampled from the training data set. It can be shown that this maximizes a lower bound on the log-likelihood of the DBN. The number of layers can be increased greedily, with the newly added top layer trained as an RBM to model the samples produced by chaining the posteriors P(hk|hk−1) of the lower layers (starting from h0 from the training data set).
The parameters of a DBN or of a stack of RBMs also correspond to the parameters of a deterministic feed-forward multi-layer neural network. The i-th unit of the k-th layer of the neural network outputs ˆhki = sigmoid(cki +∑jWkijˆhk−1,j), using the parameters ck and Wk of the k-th layer of the DBN. Hence, once the stack of RBMs or the DBN is trained, one can use those parameters to initialize the first layers of a corresponding multi-layer neural network. One or more additional layers can be added to map the top-level features ˆhk to the predictions associated with a target variable(here the probabilities associated with each class in a classification task). Bengio (2009) provides more details on RBMs and DBNs, and a survey of related models and deep architectures.
5.1.2 STACKED DENOISING AUTO-ENCODERS
The second model, by Vincent et al. (2008), is the so-called Stacked Denoising Auto-Encoder(SDAE). It borrows the greedy principle from DBNs, but uses denoising auto-encoders as a building block for unsupervised modeling. An auto-encoder learns an encoder h(·) and a decoder g(·) whose composition approaches the identity for examples in the training set, that is, g(h(x)) ≈ x for x in the training set.
Assuming that some constraint prevents g(h(·)) from being the identity for arbitrary arguments, the auto-encoder has to capture statistical structure in the training set in order to minimize reconstruction error. However, with a high capacity code (h(x) has too many dimensions), a regular auto-encoder could potentially learn a trivial encoding. Note that there is an intimate connection between minimizing reconstruction error for auto-encoders and contrastive divergence training for
RBMs, as both can be shown to approximate a log-likelihood gradient (Bengio and Delalleau, 2009).
The denoising auto-encoder (Vincent et al., 2008; Seung, 1998; LeCun, 1987; Gallinari et al., 1987) is a stochastic variant of the ordinary auto-encoder with the distinctive property that even with a high capacity model, it cannot learn the identity mapping. A denoising autoencoder is explicitly trained to denoise a corrupted version of its input. Its training criterion can also be viewed as a variational lower bound on the likelihood of a specific generative model. It has been shown on an array of data sets to perform significantly better than ordinary auto-encoders and similarly or better
ERHAN, BENGIO, COURVILLE, MANZAGOL, VINCENT AND BENGIO than RBMs when stacked into a deep supervised architecture (Vincent et al., 2008). Another way to prevent regular auto-encoders with more code units than inputs to learn the identity is to restrict the capacity of the representation by imposing sparsity on the code (Ranzato et al., 2007, 2008).
We now summarize the training algorithm of the Stacked Denoising Auto-Encoders. More details are given by Vincent et al. (2008). Each denoising auto-encoder operates on its inputs x, either the raw inputs or the outputs of the previous layer. The denoising auto-encoder is trained to reconstruct x from a stochastically corrupted (noisy) transformation of it. The output of each denoising auto-encoder is the "code vector" h(x), not to confuse with the reconstruction obtained by applying the decoder to that code vector. In our experiments h(x) = sigmoid(b +Wx) is an ordinary neural network layer, with hidden unit biases b, and weight matrix W. Let C(x) represent a stochastic corruption of x. As done by Vincent et al. (2008), we setCi(x) = xi or 0, with a random subset (of a fixed size) selected for zeroing. We have also considered a salt and pepper noise, where we select a random subset of a fixed size and set Ci(x) = Bernoulli(0.5). The denoised "reconstruction" is obtained from the noisy input with ˆx = sigmoid(c+W Th(C(x))), using biases c and the transpose of the feedforward weights W. In the experiments on images, both the raw input xi and its reconstruction ˆxi for a particular pixel i can be interpreted as a Bernoulli probability for that pixel: the probability of painting the pixel as black at that location. We denote CE(x||ˆx) = ∑i CE(xi|| ˆxi) the sum of the component-wise cross-entropy between the Bernoulli probability distributions associated with each element of x and its reconstruction probabilities ˆx: CE(x||ˆx) = −∑i (xilog ˆxi +(1−xi)log(1− ˆxi)).
The Bernoulli model only makes sense when the input components and their reconstruction are in ; another option is to use a Gaussian model, which corresponds to a Mean Squared Error (MSE) criterion.
With either DBN or SDAE, an output logistic regression layer is added after unsupervised training. This layer uses softmax (multinomial logistic regression) units to estimate P(class|x) = softmaxclass(a), where ai is a linear combination of outputs from the top hidden layer. The whole network is then trained as usual for multi-layer perceptrons, to minimize the output (negative loglikelihood) prediction error.
5.2 Data Sets
We experimented on three data sets, with the motivation that our experiments would help understand previously presented results with deep architectures, which were mostly with the MNIST data set and variations (Hinton et al., 2006; Bengio et al., 2007; Ranzato et al., 2007; Larochelle et al., 2007;
Vincent et al., 2008):
MNIST the digit classification data set by LeCun et al. (1998), containing 60,000 training and 10,000 testing examples of 28x28 handwritten digits in gray-scale.
InfiniteMNIST a data set by Loosli et al. (2007), which is an extension of MNIST from which one can obtain a quasi-infinite number of examples. The samples are obtained by performing random elastic deformations of the original MNIST digits. In this data set, there is only one set of examples, and the models will be compared by their (online) performance on it.
Shapeset is a synthetic data set with a controlled range of geometric invariances. The underlying task is binary classification of 10 × 10 images of triangles and squares. The examples show
WHY DOES UNSUPERVISED PRE-TRAINING HELP DEEP LEARNING? images of shapes with many variations, such as size, orientation and gray-level. The data set is composed of 50000 training, 10000 validation and 10000 test images.4
5.3 Setup
The models used are
1. Deep Belief Networks containing Bernoulli RBM layers, 2. Stacked Denoising Auto-Encoders with Bernoulli input units, and 3. standard feed-forward multi-layer neural networks, each with 1–5 hidden layers. Each hidden layer contains the same number of hidden units, which is a hyperparameter. The other hyperparameters are the unsupervised and supervised learning rates, the L2 penalty / weight decay,5 and the fraction of stochastically corrupted inputs (for the SDAE).
For MNIST, the number of supervised and unsupervised passes through the data (epochs) is 50 and 50 per layer, respectively. With InfiniteMNIST, we perform 2.5 million unsupervised updates followed by 7.5 million supervised updates.6 The standard feed-forward networks are trained using
10 million supervised updates. For MNIST, model selection is done by choosing the hyperparameters that optimize the supervised (classification) error on the validation set. For InfiniteMNIST, we use the average online error over the last million examples for hyperparameter selection. In all cases, purely stochastic gradient updates are applied.
The experiments involve the training of deep architectures with a variable number of layers with and without unsupervised pre-training. For a given layer, weights are initialized using random samples from uniform[−1/
√ k,1/
√ k], where k is the number of connections that a unit receives from the previous layer (the fan-in). Either supervised gradient descent or unsupervised pre-training follows.
In most cases (for MNIST), we first launched a number of experiments using a cross-product of hyperparameter values7 applied to 10 different random initialization seeds. We then selected the hyperparameter sets giving the best validation error for each combination of model (with or without pre-training), number of layers, and number of training iterations. Using these hyper-parameters, we launched experiments using an additional 400 initialization seeds. For InfiniteMNIST, only one seed is considered (an arbitrarily chosen value).
In the discussions below we sometimes use the word apparent local minimum to mean the solution obtained after training, when no further noticeable progress seems achievable by stochastic gradient descent. It is possible that these are not really near a true local minimum (there could be a tiny ravine towards significant improvement, not accessible by gradient descent), but it is clear that these end-points represent regions where gradient descent is stuck. Note also that when we write of number of layers it is to be understood as the number of hidden layers in the network.
4. The data set can be downloaded from http://www.iro.umontreal.ca/˜lisa/twiki/bin/view.cgi/Public/
ShapesetDataForJMLR.
5. A penalizing term λ||θ||2
2 is added to the supervised objective, where θ are the weights of the network, and λ is a hyper-parameter modulating the strength of the penalty.
6. The number of examples was chosen to be as large as possible, while still allowing for the exploration a variety of hyper-parameters.
7. Number of hidden units ∈ {400,800,1200}; learning rate ∈ {0.1,0.05,0.02,0.01,0.005}; ℓ2 penalty coefficient λ ∈ {10−4,10−5,10−6,0}; pre-training learning rate ∈ {0.01,0.005,0.002,0.001,0.0005}; corruption probability
∈ {0.0,0.1,0.25,0.4}; tied weights ∈ {yes,no}.
ERHAN, BENGIO, COURVILLE, MANZAGOL, VINCENT AND BENGIO
6. The Effect of Unsupervised Pre-training
We start by a presentation of large-scale simulations that were intended to confirm some of the previously published results about deep architectures. In the process of analyzing them, we start making connections to our hypotheses and motivate the experiments that follow.
6.1 Better Generalization
When choosing the number of units per layer, the learning rate and the number of training iterations to optimize classification error on the validation set, unsupervised pre-training gives substantially lower test classification error than no pre-training, for the same depth or for smaller depth on various vision data sets (Ranzato et al., 2007; Bengio et al., 2007; Larochelle et al., 2009, 2007; Vincent et al., 2008) no larger than the MNIST digit data set (experiments reported from 10,000 to 50,000 training examples).
Such work was performed with only one or a handful of different random initialization seeds, so one of the goals of this study was to ascertain the effect of the random seed used when initializing ordinary neural networks (deep or shallow) and the pre-training procedure. For this purpose, between 50 and 400 different seeds were used to obtain the graphics on MNIST.
Figure 1: Effect of depth on performance for a model trained (left) without unsupervised pretraining and (right) with unsupervised pre-training, for 1 to 5 hidden layers (networks with 5 layers failed to converge to a solution, without the use of unsupervised pretraining). Experiments on MNIST. Box plots show the distribution of errors associated with 400 different initialization seeds (top and bottom quartiles in box, plus outliers beyond top and bottom quartiles). Other hyperparameters are optimized away (on the validation set). Increasing depth seems to increase the probability of finding poor apparent local minima.
Figure 1 shows the resulting distribution of test classification error, obtained with and without pre-training, as we increase the depth of the network. Figure 2 shows these distributions as histograms in the case of 1 and 4 layers. As can be seen in Figure 1, unsupervised pre-training allows
WHY DOES UNSUPERVISED PRE-TRAINING HELP DEEP LEARNING? classification error to go down steadily as we move from 1 to 4 hidden layers, whereas without pre-training the error goes up after 2 hidden layers. It should also be noted that we were unable to effectively train 5-layer models without use of unsupervised pre-training. Not only is the error obtained on average with unsupervised pre-training systematically lower than without the pre-training, it appears also more robust to the random initialization. With unsupervised pre-training the variance stays at about the same level up to 4 hidden layers, with the number of bad outliers growing slowly.
Contrast this with the case without pre-training: the variance and number of bad outliers grows sharply as we increase the number of layers beyond 2. The gain obtained with unsupervised pretraining is more pronounced as we increase the number of layers, as is the gain in robustness to random initialization. This can be seen in Figure 2. The increase in error variance and mean for deeper architectures without pre-training suggests that increasing depth increases the probability of finding poor apparent local minima when starting from random initialization. It is also interesting to note the low variance and small spread of errors obtained with 400 seeds with unsupervised pre-training: it suggests that unsupervised pre-training is robust with respect to the random initialization seed (the one used to initialize parameters before pre-training).
Figure 2: Histograms presenting the test errors obtained on MNIST using models trained with or without pre-training (400 different initializations each). Left: 1 hidden layer. Right: 4 hidden layers.
These experiments show that the variance of final test error with respect to initialization random seed is larger without pre-training, and this effect is magnified for deeper architectures. It should however be noted that there is a limit to the success of this technique: performance degrades for 5 layers on this problem.
6.2 Visualization of Features
Figure 3 shows the weights (called filters) of the first layer of the DBN before and after supervised fine-tuning. For visualizing what units do on the 2nd and 3rd layer, we used the activation maximization technique described by Erhan et al. (2009): to visualize what a unit responds most to, the method looks for the bounded input pattern that maximizes the activation of a given unit. This is an
ERHAN, BENGIO, COURVILLE, MANZAGOL, VINCENT AND BENGIO optimization problem which is solved by performing gradient ascent in the space of the inputs, to find a local maximum of the activation function. Interestingly, nearly the same maximal activation input pattern is recovered from most random initializations of the input pattern.
Figure 3: Visualization of filters learned by a DBN trained on InfiniteMNIST. The top figures contain a visualization of filters after pre-training, while the bottoms ones picture the same units after supervised fine-tuning; from left to right: units from the 1st, 2nd and 3rd layers, respectively.
For comparison, we have also visualized the filters of a network for 1–3 layers in which no pretraining was performed (Figure 4). While the first layer filters do seem to correspond to localized features, 2nd and 3rd layers are not as interpretable anymore. Qualitatively speaking, filters from the bottom row of Figure 3 and those from Figure 4 have little in common, which is an interesting conclusion in itself. In addition, there seems to be more interesting visual structures in the features learned in networks with unsupervised pre-training.
Several interesting conclusions can be drawn from Figure 3. First, supervised fine-tuning (after unsupervised pre-training), even with 7.5 million updates, does not change the weights in a significant way (at least visually): they seem stuck in a certain region of weight space, and the sign of weights does not change after fine-tuning (hence the same pattern is seen visually). Second, different layers change differently: the first layer changes least, while supervised training has more effect when performed on the 3rd layer. Such observations are consistent with the predictions made by our hypothesis: namely that the early dynamics of stochastic gradient descent, the dynamics induced by unsupervised pre-training, can "lock" the training in a region of the parameter space that is essentially inaccessible for models that are trained in a purely supervised way.
Finally, the features increase in complexity as we add more layers. First layer weights seem to encode basic stroke-like detectors, second layer weights seem to detect digit parts, while top layer weights detect entire digits. The features are more complicated as we add more layers, and displaying only one image for each "feature" does not do justice to the non-linear nature of that
WHY DOES UNSUPERVISED PRE-TRAINING HELP DEEP LEARNING? feature. For example, it does not show the set of patterns on which the feature is highly active (or highly inactive).
While Figures 3–4 show only the filters obtained on InfiniteMNIST, the visualizations are similar when applied on MNIST. Likewise, the features obtained with SDAE result in qualitatively similar conclusions; Erhan et al. (2009) gives more details.
Figure 4: Visualization of filters learned by a network without pre-training, trained on
InfiniteMNIST. The filters are shown after supervised training; from left to right: units from the 1st, 2nd and 3rd layers, respectively.
6.3 Visualization of Model Trajectories During Learning
Visualizing the learned features allows for a qualitative comparison of the training strategies for deep architectures. However it is not useful for investigating how these strategies are influenced by random initialization, as the features learned from multiple initializations look similar. If it was possible for us to visualize a variety of models at the same time, it would allow us to explore our hypothesis, and ascertain to what degree and how the set of pre-trained models (for different random seeds) is far from the set of models without pre-training. Do these two sets cover very different regions in parameter space? Are parameter trajectories getting stuck in many different apparent local minima?
Unfortunately, it is not possible to directly compare parameter values of two architectures, because many permutations of the same parameters give rise to the same model. However, one can take a functional approximation approach in which we compare the function (from input to output) represented by each network, rather than comparing the parameters. The function is the infinite ordered set of output values associated with all possible inputs, and it can be approximated with a finite number of inputs (preferably plausible ones). To visualize the trajectories followed during training, we use the following procedure. For a given model, we compute and concatenate all its outputs on the test set examples as one long vector summarizing where it stands in "function space".
We get one such vector for each partially trained model (at each training iteration). This allows us to plot many learning trajectories, one for each initialization seed, with or without pre-training. Using a dimensionality reduction algorithm we then map these vectors to a two-dimensional space for visualization.8 Figures 5 and 6 present the results using dimensionality reduction techniques that
8. Note that we can and do project the models with and without pre-training at the same time, so as to visualize them in the same space.
ERHAN, BENGIO, COURVILLE, MANZAGOL, VINCENT AND BENGIO focus respectively on local9 and global structure.10 Each point is colored according to the training iteration, to help follow the trajectory movement.
−100
−80
−60
−40
−20
−100
−80
−60
−40
−20
2 layers without pre−training
2 layers with pre−training
Figure 5: 2D visualizations with tSNE of the functions represented by 50 networks with and 50 networks without pre-training, as supervised training proceeds over MNIST. See Section 6.3 for an explanation. Color from dark blue to cyan and red indicates a progression in training iterations (training is longer without pre-training). The plot shows models with 2 hidden layers but results are similar with other depths.
What seems to come out of these visualizations is the following:
1. The pre-trained and not pre-trained models start and stay in different regions of function space.
2. From the visualization focusing on local structure (Figure 5) we see that all trajectories of a given type (with pre-training or without) initially move together. However, at some point(after about 7 epochs) the different trajectories (corresponding to different random seeds) diverge (slowing down into elongated jets) and never get back close to each other (this is more true for trajectories of networks without pre-training). This suggests that each trajectory moves into a different apparent local minimum.11
9. t-Distributed Stochastic Neighbor Embedding, or tSNE, by van der Maaten and Hinton (2008), with the default parameters available in the public implementation: http://ict.ewi.tudelft.nl/˜lvandermaaten/t-SNE.html.
10. Isomap by Tenenbaum et al. (2000), with one connected component.
11. One may wonder if the divergence points correspond to a turning point in terms of overfitting. As shall be seen in Figure 8, the test error does not improve much after the 7th epoch, which reinforces this hypothesis.
WHY DOES UNSUPERVISED PRE-TRAINING HELP DEEP LEARNING?
−4000
−3000
−2000
−1000
−1500
−1000
−500
Without pre−training
With pre−training
Figure 6: 2D visualization with ISOMAP of the functions represented by 50 networks with and 50 networks without pre-training, as supervised training proceeds over MNIST. See Section 6.3 for an explanation. Color from dark blue to cyan indicates a progression in training iterations (training is longer without pre-training). The plot shows models with
2 hidden layers but results are similar with other depths.
3. From the visualization focusing on global structure (Figure 6), we see the pre-trained models live in a disjoint and much smaller region of space than the not pre-trained models. In fact, from the standpoint of the functions found without pre-training, the pre-trained solutions look all the same, and their self-similarity increases (variance across seeds decreases) during training, while the opposite is observed without pre-training. This is consistent with the formalization of pre-training from Section 3, in which we described a theoretical justification for viewing unsupervised pre-training as a regularizer; there, the probabilities of pre-traininig parameters landing in a basin of attraction is small.
The visualizations of the training trajectories do seem to confirm our suspicions. It is difficult to guarantee that each trajectory actually does end up in a different local minimum (corresponding to a different function and not only to different parameters). However, all tests performed (visual inspection of trajectories in function space, but also estimation of second derivatives in the directions of all the estimated eigenvectors of the Jacobian not reported in details here) were consistent with that interpretation.
We have also analyzed models obtained at the end of training, to visualize the training criterion in the neighborhood of the parameter vector θ∗ obtained. This is achieved by randomly sampling a direction v (from the stochastic gradient directions) and by plotting the training criterion around
ERHAN, BENGIO, COURVILLE, MANZAGOL, VINCENT AND BENGIO θ∗ in that direction, that is, at θ = θ∗ + αv, for α ∈ {−2.5,−2.4,...,−0.1,0,0.1,...2.4,2.5}, and v normalized (||v|| = 1). This analysis is visualized in Figure 7. The error curves look close to quadratic and we seem to be near a local minimum in all directions investigated, as opposed to a saddle point or a plateau. A more definite answer could be given by computing the full Hessian eigenspectrum, which would be expensive. Figure 7 also suggests that the error landscape is a bit flatter in the case of unsupervised pre-training, and flatter for deeper architectures.
Figure 7: Training errors obtained on Shapeset when stepping in parameter space around a converged model in 7 random gradient directions (stepsize of 0.1). Top: no pre-training.
Bottom: with unsupervised pre-training. Left: 1 hidden layer. Middle: 2 hidden layers. Right: 3 hidden layers. Compare also with Figure 8, where 1-layer networks with unsupervised pre-training obtain higher training errors.
6.4 Implications
The series of results presented so far show a picture that is consistent with our hypothesis. Better generalization that seems to be robust to random initializations is indeed achieved by pre-trained models, which indicates that unsupervised learning of P(X) is helpful in learning P(Y|X). The function space landscapes that we visualized point to the fact that there are many apparent local minima. The pre-trained models seem to end up in distinct regions of these error landscapes (and, implicitly, in different parts of the parameter space). This is both seen from the function space trajectories and from the fact that the visualizations of the learned features are qualitatively very different from those obtained by models without pre-training.
7. The Role of Unsupervised Pre-training
The observations so far in this paper confirm that starting the supervised optimization from pretrained weights rather than from randomly initialized weights consistently yields better performing
WHY DOES UNSUPERVISED PRE-TRAINING HELP DEEP LEARNING? classifiers on MNIST. To better understand where this advantage came from, it is important to realize that the supervised objective being optimized is exactly the same in both cases. The gradient-based optimization procedure is also the same. The only thing that differs is the starting point in parameter space: either picked at random or obtained after unsupervised pre-training (which also starts from a random initialization).
Deep architectures, since they are built from the composition of several layers of non-linearities, yield an error surface that is non-convex and hard to optimize, with the suspected presence of many local minima (as also shown by the above visualizations). A gradient-based optimization should thus end in the apparent local minimum of whatever basin of attraction we started from. From this perspective, the advantage of unsupervised pre-training could be that it puts us in a region of parameter space where basins of attraction run deeper than when picking starting parameters at random. The advantage would be due to a better optimization.
Now it might also be the case that unsupervised pre-training puts us in a region of parameter space in which training error is not necessarily better than when starting at random (or possibly worse), but which systematically yields better generalization (test error). Such behavior would be indicative of a regularization effect. Note that the two forms of explanation are not necessarily mutually exclusive.
Finally, a very simple explanation could be the most obvious one: namely the disparity in the magnitude of the weights (or more generally, the marginal distribution of the weights) at the start of the supervised training phase. We shall analyze (and rule out) this hypothesis first.
7.1 Experiment 1: Does Pre-training Provide a Better Conditioning Process for Supervised
Learning?
Typically gradient descent training of the deep model is initialized with randomly assigned weights, small enough to be in the linear region of the parameter space (close to zero for most neural network and DBN models). It is reasonable to ask if the advantage imparted by having an initial unsupervised pre-training phase is simply due to the weights being larger and therefore somehow providing a better "conditioning" of the initial values for the optimization process; we wanted to rule out this possibility.
By conditioning, we mean the range and marginal distribution from which we draw initial weights. In other words, could we get the same performance advantage as unsupervised pre-training if we were still drawing the initial weights independently, but from a more suitable distribution than the uniform[−1/
√ k,1/
√ k]? To verify this, we performed unsupervised pre-training, and computed marginal histograms for each layer's pre-trained weights and biases (one histogram per each layer's weights and biases). We then resampled new "initial" random weights and biases according to these histograms (independently for each parameter), and performed fine-tuning from there. The resulting parameters have the same marginal statistics as those obtained after unsupervised pre-training, but not the same joint distribution.
Two scenarios can be imagined. In the first, the initialization from marginals would lead to significantly better performance than the standard initialization (when no pre-training is used).
This would mean that unsupervised pre-training does provide a better marginal conditioning of ERHAN, BENGIO, COURVILLE, MANZAGOL, VINCENT AND BENGIO the weights. In the second scenario, the marginals would lead to performance similar to or worse than that without pre-training.12 initialization.
Uniform
Histogram
Unsup.pre-tr.
1 layer
1.81±0.07
1.94±0.09
1.41±0.07
2 layers
1.77±0.10
1.69±0.11
1.37±0.09
Table 1: Effect of various initialization strategies on 1 and 2-layer architectures: independent uniform densities (one per parameter), independent densities from the marginals after unsupervised pre-training, or unsupervised pre-training (which samples the parameters in a highly dependent way so that they collaborate to make up good denoising auto-encoders.)
Experiments on MNIST, numbers are mean and standard deviation of test errors (across different initialization seeds).
What we observe in Table 1 seems to fall within the first scenario. However, while initializing the weights to match the marginal distributions at the end of pre-training appears to slightly improve the generalization error on MNIST for 2 hidden layers, the difference is not significant and it is far from fully accounting for the discrepancy between the pre-trained and non-pre-trained results.
This experiment constitutes evidence against the preconditioning hypothesis, but does not exclude either the optimization hypothesis or the regularization hypothesis.
7.2 Experiment 2: The Effect of Pre-training on Training Error
The optimization and regularization hypotheses diverge on their prediction on how unsupervised pre-training should affect the training error: the former predicts that unsupervised pre-training should result in a lower training error, while the latter predicts the opposite. To ascertain the influence of these two possible explanatory factors, we looked at the test cost (Negative Log Likelihood on test data) obtained as a function of the training cost, along the trajectory followed in parameter space by the optimization procedure. Figure 8 shows 400 of these curves started from a point in parameter space obtained from random initialization, that is, without pre-training (blue), and 400 started from pre-trained parameters (red).
The experiments were performed for networks with 1, 2 and 3 hidden layers. As can be seen in Figure 8, while for 1 hidden layer, unsupervised pre-training reaches lower training cost than no pre-training, hinting towards a better optimization, this is not necessarily the case for the deeper networks. The remarkable observation is rather that, at a same training cost level, the pre-trained models systematically yield a lower test cost than the randomly initialized ones. The advantage appears to be one of better generalization rather than merely a better optimization procedure.
This brings us to the following result: unsupervised pre-training appears to have a similar effect to that of a good regularizer or a good "prior" on the parameters, even though no explicit regularization term is apparent in the cost being optimized. As we stated in the hypothesis, it might be reasoned that restricting the possible starting points in parameter space to those that minimize the unsupervised pre-training criterion (as with the SDAE), does in effect restrict the set of possible
12. We observed that the distribution of weights after unsupervised pre-training is fat-tailed. It is conceivable that sampling from such a distribution in order to initialize a deep architecture might actually hurt the performance of a deep architecture (compared to random initialization from a uniform distribution).
WHY DOES UNSUPERVISED PRE-TRAINING HELP DEEP LEARNING?
Figure 8: Evolution without pre-training (blue) and with pre-training (red) on MNIST of the log of the test NLL plotted against the log of the train NLL as training proceeds. Each of the 2 × 400 curves represents a different initialization. The errors are measured after each pass over the data. The rightmost points were measured after the first pass of gradient updates. Since training error tends to decrease during training, the trajectories run from right (high training error) to left (low training error). Trajectories moving up (as we go leftward) indicate a form of overfitting. All trajectories are plotted on top of each other. final configurations for parameter values. Like regularizers in general, unsupervised pre-training (in this case, with denoising auto-encoders) might thus be seen as decreasing the variance and introducing a bias (towards parameter configurations suitable for performing denoising). Unlike ordinary regularizers, unsupervised pre-training does so in a data-dependent manner.
7.3 Experiment 3: The Influence of the Layer Size
Another signature characteristic of regularization is that the effectiveness of regularization increases as capacity (e.g., the number of hidden units) increases, effectively trading off one constraint on the model complexity for another. In this experiment we explore the relationship between the number of units per layer and the effectiveness of unsupervised pre-training. The hypothesis that unsupervised pre-training acts as a regularizer would suggest that we should see a trend of increasing effectiveness of unsupervised pre-training as the number of units per layer are increased.
We trained models on MNIST with and without pre-training using increasing layer sizes: 25, 50, 100, 200, 400, 800 units per layer. Results are shown in Figure 9. Qualitatively similar results were obtained on Shapeset. In the case of SDAE, we were expecting the denoising pre-training procedure to help classification performance most for large layers; this is because the denoising pre-training allows useful representations to be learned in the over-complete case, in which a layer is larger than its input (Vincent et al., 2008). What we observe is a more systematic effect: while unsupervised pre-training helps for larger layers and deeper networks, it also appears to hurt for too small networks.
Figure 9 also shows that DBNs behave qualitatively like SDAEs, in the sense that unsupervised pre-training architectures with smaller layers hurts performance. Experiments on InfiniteMNIST reveal results that are qualitatively the same. Such an experiment seemingly points to a re-verification of the regularization hypothesis. In this case, it would seem that unsupervised pre-training acts as an additional regularizer for both DBN and SDAE models—on top of the regularization provided by
ERHAN, BENGIO, COURVILLE, MANZAGOL, VINCENT AND BENGIO
Figure 9: Effect of layer size on the changes brought by unsupervised pre-training, for networks with 1, 2 or 3 hidden layers. Experiments on MNIST. Error bars have a height of two standard deviations (over initialization seed). Pre-training hurts for smaller layer sizes and shallower networks, but it helps for all depths for larger networks. the small size of the hidden layers. As the model size decreases from 800 hidden units, the generalization error increases, and it increases more with unsupervised pre-training presumably because of the extra regularization effect: small networks have a limited capacity already so further restricting it (or introducing an additional bias) can harm generalization. Such a result seems incompatible with a pure optimization effect. We also obtain the result that DBNs and SDAEs seem to have qualitatively similar effects as pre-training strategies.
The effect can be explained in terms of the role of unsupervised pre-training as promoting input transformations (in the hidden layers) that are useful at capturing the main variations in the input distribution P(X). It may be that only a small subset of these variations are relevant for predicting the class label Y. When the hidden layers are small it is less likely that the transformations for predicting Y are included in the lot learned by unsupervised pre-training.
7.4 Experiment 4: Challenging the Optimization Hypothesis
Experiments 1–3 results are consistent with the regularization hypothesis and Experiments 2–3 would appear to directly support the regularization hypothesis over the alternative—that unsupervised pre-training aids in optimizing the deep model objective function.
In the literature there is some support for the optimization hypothesis. Bengio et al. (2007) constrained the top layer of a deep network to have 20 units and measured the training error of networks with and without pre-training. The idea was to prevent the networks from overfitting the training error simply with the top hidden layer, thus to make it clearer whether some optimization
WHY DOES UNSUPERVISED PRE-TRAINING HELP DEEP LEARNING? effect (of the lower layers) was going on. The reported training and test errors were lower for pretrained networks. One problem with the experimental paradigm used by Bengio et al. (2007) is their use of early stopping. This is problematic because, as previously mentioned, early stopping is itself a regularizer, and it can influence greatly the training error that is obtained. It is conceivable that if Bengio et al. (2007) had run the models to convergence, the results could have been different. We needed to verify this.
Figure 10 shows what happens without early stopping. The training error is still higher for pre-trained networks even though the generalization error is lower. This result now favors the regularization hypothesis against the optimization story. What may have happened is that early stopping prevented the networks without pre-training from moving too much towards their apparent local minimum.
Figure 10: For MNIST, a plot of the log(train NLL) vs. log(test NLL) at each epoch of training. The top layer is constrained to 20 units.
7.5 Experiment 5: Comparing pre-training to L1 and L2 regularization
An alternative hypothesis would be that classical ways of regularizing could perhaps achieve the same effect as unsupervised pre-training. We investigated the effect of L1 and L2 regularization(i.e., adding a ||θ||1 or ||θ||2
2 term to the supervised objective function) in a network without pretraining. We found that while in the case of MNIST a small penalty can in principle help, the gain is nowhere near as large as it is with pre-training. For InfiniteMNIST, the optimal amount of L1 and L2 regularization is zero.13
13. Which is consistent with the classical view of regularization, in which its effect should diminish as we add more and more data.
ERHAN, BENGIO, COURVILLE, MANZAGOL, VINCENT AND BENGIO
This is not an entirely surprising finding: not all regularizers are created equal and these results are consistent with the literature on semi-supervised training that shows that unsupervised learning can be exploited as a particularly effective form of regularization.
7.6 Summary of Findings: Experiments 1-5
So far, the results obtained from the previous experiments point towards a pretty clear explanation of the effect of unsupervised pre-training: namely, that its effect is a regularization effect. We have seen that it is not simply sufficient to sample random weights with the same magnitude: the (datadependent) unsupervised initialization is crucial. We have also observed that canonical regularizers(L1/L2 penalties on the weights) do not achieve the same level of performance.
The most compelling pieces of evidence in support of the regularization hypothesis are Figures
8 and 9. The alternative explanation—that unsupervised pre-training has an optimization effect— suggested by Bengio et al. (2007) doesn't seem to be supported by our experimental setup.
8. The Online Learning Setting
Our hypothesis included not only the statistical/phenomenological hypothesis that unsupervised pre-training acted as a regularizer, but also contains a mechanism for how such behavior arises both as a consequence of the dynamic nature of training—following a stochastic gradient through two phases of training and as a consequence of the non-convexity of the supervised objective function.
In our hypothesis, we posited that early examples induce changes in the magnitude of the weights that increase the amount of non-linearity of the network, which in turn decreases the number of regions accessible to the stochastic gradient descent procedure. This means that the early examples (be they pre-training examples or otherwise) determine the basin of attraction for the remainder of training; this also means that the early examples have a disproportionate influence on the configuration of parameters of the trained models.
One consequence to the hypothesized mechanism is that we would predict that in the online learning setting with unbounded or very large data sets, the behavior of unsupervised pre-training would diverge from the behavior of a canonical regularizer (L1/L2). This is because the effectiveness of a canonical regularizer decreases as the data set grows, whereas the effectiveness of unsupervised pre-training as a regularizer is maintained as the data set grows.
Note that stochastic gradient descent in online learning is a stochastic gradient descent optimization of the generalization error, so good online error in principle implies that we are optimizing well the generalization error. Indeed, each gradient ∂L(x,y)
∂θ for example (x,y) (with L(x,y) the supervised loss with input x and label y) sampled from the true generating distribution P(x,y) is an unbiased
Monte-Carlo estimator of the true gradient of generalization error, that is, ∑y
R x
∂L(x,y)
∂θ
P(x,y)dx.
In this section we empirically challenge this aspect of the hypothesis and show that the evidence does indeed support our hypothesis over what is more typically expected from a regularizer.
8.1 Experiment 6: Effect of Pre-training with Very Large Data Sets
The results presented here are perhaps the most surprising findings of this paper. Figure 11 shows the online classification error (on the next block of examples, as a moving average) for 6 architectures that are trained on InfiniteMNIST: 1 and 3-layer DBNs, 1 and 3-layer SDAE, as well as 1 and 3-layer networks without pre-training.
WHY DOES UNSUPERVISED PRE-TRAINING HELP DEEP LEARNING?
Figure 11: Comparison between 1 and 3-layer networks trained on InfiniteMNIST. Online classification error, computed as an average over a block of last 100,000 errors.
We can draw several observations from these experiments. First, 3-layer networks without pre-training are worse at generalization, compared to the 1-layer equivalent. This confirms the hypothesis that even in an online setting, optimization of deep networks is harder than shallow ones. Second, 3-layer SDAE models seem to generalize better than 3-layer DBNs. Finally and most importantly, the pre-training advantage does not vanish as the number of training examples increases, on the contrary.
Note that the number of hidden units of each model is a hyperparameter.14 So theoretical results suggest that 1-layer networks without pre-training should in principle be able to represent the input distribution as capacity and data grow. Instead, without pre-training, the networks are not able to take advantage of the additional capacity, which again points towards the optimization explanation.
It is clear, however, that the starting point of the non-convex optimization matters, even for networks that are seemingly "easier" to optimize (1-layer ones), which supports our hypothesis.
Another experiment that shows the effects of large-scale online stochastic non-convex optimization is shown in Figure 12. In the setting of InfiniteMNIST, we compute the error on the training set, in the same order that we presented the examples to the models. We observe several interesting results: first, note that both models are better at classifying more recently seen examples. This is a natural effect of stochastic gradient descent with a constant learning rate (which gives exponentially more weight to recent examples). Note also that examples at the beginning of training are essentially like test examples for both models, in terms of error. Finally, we observe that the pre-trained
14. This number was chosen individually for each model s.t. the error on the last 1 million examples is minimized. In practice, this meant 2000 units for 1-layer networks and 1000 units/layer for 3-layer networks.
ERHAN, BENGIO, COURVILLE, MANZAGOL, VINCENT AND BENGIO
Figure 12: Error of 1-layer network with RBM pre-training and without, on the 10 million examples used for training it. The errors are calculated in the same order (from left to right, above) as the examples were presented during training. Each error bar corresponds to a block of consecutive training examples. model is better across the board on the training set. This fits well with the optimization hypothesis, since it shows that unsupervised pre-training has an optimization effect.
What happens in this setting is that the training and generalization errors converge as the empirical distribution (defined by the training set) converges to the true data distribution. These results show that the effectiveness of unsupervised pre-training does not diminish with increasing data set sizes. This would be unexpected from a superficial understanding of unsupervised pre-training as a regularization method. However it is entirely consistent with our interpretation, stated in our hypothesis, of the role of unsupervised pre-training in the online setting with stochastic gradient descent training on a non-convex objective function.
8.2 Experiment 7: The Effect of Example Ordering
The hypothesized mechanism implies, due to the dynamics of learning—the increase in weight magnitude and non-linearity as training proceeds, as well as the dependence of the basin of attraction on early data—that, when training with stochastic gradient descent, we should see increased sensitivity to early examples. In the case of InfiniteMNIST we operate in an online stochastic optimization regime, where we try to find a local minimum of a highly non-convex objective function. It is then interesting to study to what extent the outcome of this optimization is influenced by the examples seen at different points during training, and whether the early examples have a stronger influence(which would not be the case with a convex objective).
To quantify the variance of the outcome with respect to training samples at different points during training, and to compare these variances for models with and without pre-training, we proceeded with the following experiment. Given a data set with 10 million examples, we vary (by resampling)
WHY DOES UNSUPERVISED PRE-TRAINING HELP DEEP LEARNING? the first million examples (across 10 different random draws, sampling a different set of 1 million examples each time) and keep the other ones fixed. After training the (10) models, we measure the variance (across the 10 draws) of the output of the networks on a fixed test set (i.e., we measure the variance in function space). We then vary the next million examples in the same fashion, and so on, to see how much each of the ten parts of the training set influenced the final function.
Figure 13: Variance of the output of a trained network with 1 layer. The variance is computed as a function of the point at which we vary the training samples. Note that the 0.25 mark corresponds to the start of pre-training.
Figure 13 shows the outcome of such an analysis. The samples at the beginning15 do seem to influence the output of the networks more than the ones at the end. However, this variance is lower for the networks that have been pre-trained. In addition to that, one should note that the variance of pre-trained network at 0.25 (i.e., the variance of the output as a function of the first samples used for supervised training) is lower than the variance of the supervised network at 0.0. Such results imply that unsupervised pre-training can be seen as a sort of variance reduction technique, consistent with a regularization hypothesis. Finally, both networks are more influenced by the last examples used for optimization, which is simply due to the fact that we use stochastic gradient with a constant learning rate, where the most recent examples' gradient has a greater influence.
These results are consistent with what our hypothesis predicts: both the fact that early examples have greater influence (i.e., the variance is higher) and that pre-trained models seem to reduce this variance are in agreement with what we would have expected.
15. Which are unsupervised examples, for the red curve, until the 0.25 mark in Figure 13.
ERHAN, BENGIO, COURVILLE, MANZAGOL, VINCENT AND BENGIO
8.3 Experiment 8: Pre-training only k layers
From Figure 11 we can see that unsupervised pre-training makes quite a difference for 3 layers, on
InfiniteMNIST. In Figure 14 we explore the link between depth and unsupervised pre-training in more detail. The setup is as follows: for both MNIST and InfiniteMNIST we pre-train only the bottom k layers and randomly initialize the top n − k layers in the usual way. In this experiment, n = 3 and we vary k from 0 (which corresponds to a network with no pre-training) to k = n (which corresponds to the normal pre-trained case).
For MNIST, we plot the log(train NLL) vs. log(test NLL) trajectories, where each point corresponds to a measurement after a certain number of epochs. The trajectories go roughly from the right to left and from top to bottom, corresponding to the lowering of the training and test errors.
We can also see that models overfit from a certain point onwards.
Figure 14: On the left: for MNIST, a plot of the log(train NLL) vs. log(test NLL) at each epoch of training. We pre-train the first layer, the first two layers and all three layers using RBMs and randomly initialize the other layers; we also compare with the network whose layers are all randomly initialized. On the right: InfiniteMNIST, the online classification error. We pre-train the first layer, the first two layers or all three layers using denoising auto-encoders and leave the rest of the network randomly initialized.
For InfiniteMNIST, we simply show the online error. The results are ambiguous w.r.t the difficulty of optimizing the lower layers versus the higher ones. We would have expected that the largest incremental benefit came from pre-training the first layer or first two layers. It is true for the first two layers, but not the first. As we pre-train more layers, the models become better at generalization. In the case of the finite MNIST, note how the final training error (after the same number of epochs) becomes worse with pre-training of more layers. This clearly brings additional support to the regularization explanation.
WHY DOES UNSUPERVISED PRE-TRAINING HELP DEEP LEARNING?
9. Discussion and Conclusions
We have shown that unsupervised pre-training adds robustness to a deep architecture. The same set of results also suggests that increasing the depth of an architecture that is not pre-trained increases the probability of finding poor apparent local minima. Pre-trained networks give consistently better generalization. Our visualizations point to the observations that pre-trained networks learn qualitatively different features (if networks are visualized in the weight space) compared to networks without pre-training. Moreover, the trajectories of networks with different initialization seeds seem to fall into many distinct apparent local minima, which are again different (and seemingly far apart) depending on whether we use pre-training or not.
We have shown that unsupervised pre-training is not simply a way of getting a good initial marginal distribution, and that it captures more intricate dependencies between parameters. One of our findings is that deep networks with unsupervised pre-training seem to exhibit some properties of a regularizer: with small enough layers, pre-trained deep architectures are systematically worse than randomly initialized deep architectures. Moreover, when the layers are big enough, the pre-trained models obtain worse training errors, but better generalization performance. Additionally, we have re-done an experiment which purportedly showed that unsupervised pre-training can be explained with an optimization hypothesis and observed a regularization effect instead. We also showed that classical regularization techniques (such as L1/L2 penalties on the network weights) cannot achieve the same performance as unsupervised pre-training, and that the effect of unsupervised pre-training does not go away with more training data, so if unsupervised pre-training is a regularizer, it is certainly of a rather different kind.
The two unsupervised pre-training strategies considered—denoising auto-encoders and Restricted
Boltzmann Machines—seem to produce qualitatively similar observations. We have observed that, surprisingly, the pre-training advantage is present even in the case of really large training sets, pointing towards the conclusion that the starting point in the non-convex optimization problem is indeed quite important; a fact confirmed by our visualizations of filters at various levels in the network.
Finally, the other important set of results show that unsupervised pre-training acts like a variance reduction technique, yet a network with pre-training has a lower training error on a very large data set, which supports an optimization interpretation of the effect of pre-training.
How do we make sense of all these results? The contradiction between what looks like regularization effects and what looks like optimization effects appears, on the surface, unresolved. Instead of sticking to these labels, we attempted to draw a hypothesis, described in Section 3 about the dynamics of learning in an architecture that is trained using two phases (unsupervised pre-training and supervised fine-tuning), which we believe to be consistent with all the above results.
This hypothesis suggests that there are consequences of the non-convexity of the supervised objective function, which we observed in various ways throughout our experiments. One of these consequences is that early examples have a big influence on the outcome of training and this is one of the reasons why in a large-scale setting the influence of unsupervised pre-training is still present.
Throughout this paper, we have delved on the idea that the basin of attraction induced by the early examples (in conjunction with unsupervised pre-training) is, for all practical purposes, a basin from which supervised training does not escape.
This effect can be observed from the various visualizations and performance evaluations that we made. Unsupervised pre-training, as a regularizer that only influences the starting point of supervised training, has an effect that, contrary to classical regularizers, does not disappear with
ERHAN, BENGIO, COURVILLE, MANZAGOL, VINCENT AND BENGIO more data (at least as far as we can see from our results). Basically, unsupervised pre-training favors hidden units that compute features of the input X that correspond to major factors of variation in the true P(X). Assuming that some of these are near features useful at predicting variations in Y, unsupervised pre-training sets up the parameters near a solution of low predictive generalization error.
One of the main messages that our results imply is that the optimization of a non-convex objective function with stochastic gradient descent presents challenges for analysis, especially in a regime with large amounts of data. Our analysis so far shows that it is possible for networks that are trained in such a regime to be influenced more by early examples. This can pose problems in scenarios where we would like our networks to be able to capture more of the information in later examples, that is, when training from very large data sets and trying to capture a lot of information from them.
One interesting realization is that with a small training set, we do not usually put a lot of importance on minimizing the training error, because overfitting is a major issue; the training error is not a good way to distinguish between the generalization performance of two models. In that setting, unsupervised pre-training helps to find apparent local minima that have better generalization error.
With a large training set, as we saw in Figure 12, the empirical and true distributions converge. In such a scenario, finding a better apparent local minimum will matter and stronger (better) optimization strategies should have a significant impact on generalization when the training set is very large. Note also that it would be interesting to extend our experimental techniques to the problem of training deep auto-encoders (with a bottleneck), where previous results (Hinton and Salakhutdinov, 2006) show that not only test error but also training error is greatly reduced by unsupervised pre-training, which is a strong indicator of an optimization effect. We hypothesize that the presence of the bottleneck is a crucial element that distinguishes the deep auto-encoders from the deep classifiers studied here.
In spite of months of CPU time on a cluster devoted to the experiments described here (which is orders of magnitude more than most previous work in this area), more could certainly be done to better understand these effects. Our original goal was to have well-controlled experiments with well understood data sets. It was not to advance a particular algorithm but rather to try to better understand a phenomenon that has been well documented elsewhere. Nonetheless, our results are limited by the data sets used and it is plausible that different conclusions could be drawn, should the same experiments be carried out on other data.
Our results suggest that optimization in deep networks is a complicated problem that is influenced in great part by the early examples during training. Future work should clarify this hypothesis.
If it is true and we want our learners to capture really complicated distributions from very large training sets, it may mean that we should consider learning algorithms that reduce the effect of the early examples, allowing parameters to escape from the attractors in which current learning dynamics get stuck.
The observations reported here suggest more detailed explanations than those already discussed, which could be tested in future work. We hypothesize that the factors of variation present in the input distribution are disentangled more and more as we go from the input layer to higher-levels of the feature hierarchy. This is coherent with observations of increasing invariance to geometric transformations in DBNs trained on images (Goodfellow et al., 2009), as well as by visualizing the variations in input images generated by sampling from the model (Hinton, 2007; Susskind et al., 2008), or when considering the preferred input associated with different units at different depths (Lee et al., WHY DOES UNSUPERVISED PRE-TRAINING HELP DEEP LEARNING?
2009; Erhan et al., 2009). As a result, during early stages of learning, the upper layers (those that typically learn quickly) would have access to a more robust representation of the input and are less likely to be hindered by the entangling of factors variations present in the input. If this disentangling hypothesis is correct, it would help to explain how unsupervised pre-training can address the chicken-and-egg issue explained in Section 2: the lower layers of a supervised deep architecture need the upper layers to define what they should extract, and vice-versa. Instead, the lower layers can extract robust and disentangled representations of the factors of variation and the upper layers select and combine the appropriate factors (sometimes not all at the top hidden layer). Note that as factors of variation are disentangled, it could also happen that some of them are not propagated upward (before fine-tuning), because RBMs do not try to represent in their hidden layer input bits that are independent.
To further explain why smaller hidden layers yield worse performance with pre-training than without (Figure 9), one may hypothesize further that, for some data sets, the leading factors of variation present in P(X) (presumably the only ones captured in a smaller layer) are less predictive of Y than random projections16 can be, precisely because of the hypothesized disentangling effect.
With enough hidden units, unsupervised pre-training may extract among the larger set of learned features some that are highly predictive of Y (more so than random projections). This additional hypothesis could be tested by measuring the mutual information between each hidden unit and the object categories (as done by Lee et al., 2009), as the number of hidden units is varied (like in Figure 9). It is expected that the unit with the most mutual information will be less informative with pre-training when the number of hidden units is too small, and more informative with pre-training when the number of hidden units is large enough.
Under the hypothesis that we have proposed in Section 3, the following result is unaccounted for: in Figure 8(a), training error is lower with pre-training when there is only one hidden layer, but worse with more layers. This may be explained by the following additional hypothesis. Although each layer extracts information about Y in some of its features, it is not guaranteed that all of that information is preserved when moving to higher layers. One may suspect this in particular for RBMs, which would not encode in their hidden layer any input bits that would be marginally independent of the others, because these bits would be explained by the visible biases: perfect disentangling of Y from the other factors of variation in X may yield marginally independent bits about
Y. Although supervised fine-tuning should help to bubble up that information towards the output layer, it might be more difficult to do so for deeper networks, explaining the above-stated feature of Figure 8. Instead, in the case of a single hidden layer, less information about Y would have been dropped (if at all), making the job of the supervised output layer easier. This is consistent with earlier results (Larochelle et al., 2009) showing that for several data sets supervised fine-tuning significantly improves classification error, when the output layer only takes input from the top hidden layer. This hypothesis is also consistent with the observation made here (Figure 1) that unsupervised pre-training actually does not help (and can hurt) for too deep networks.
In addition to exploring the above hypotheses, future work should include an investigation of the connection between the results presented in this paper and by Hinton and Salakhutdinov (2006), where it seems to be hard to obtain a good training reconstruction error with deep auto-encoders (in an unsupervised setting) without performing pre-training. Other avenues for future work include the analysis and understanding of deep semi-supervised techniques where one does not separate
16. Meaning the random initialization of hidden layers.
ERHAN, BENGIO, COURVILLE, MANZAGOL, VINCENT AND BENGIO between the pre-training phase and the supervised phase, such as work by Weston et al. (2008) and Larochelle and Bengio (2008). Such algorithms fall more squarely into the realm of semi-supervised methods. We expect that analyses similar to the ones we performed would be potentially harder, but perhaps revealing as well.
Many open questions remain towards understanding and improving deep architectures. Our conviction is that devising improved strategies for learning in deep architectures requires a more profound understanding of the difficulties that we face with them. This work helps with such understanding via extensive simulations and puts forward a hypothesis explaining the mechanisms behind unsupervised pre-training, which is well supported by our results.
Acknowledgments
This research was supported by funding from NSERC, MITACS, FQRNT, and the Canada Research
Chairs. The authors also would like to thank the editor and reviewers, as well as Fernando Pereira for their helpful comments and suggestions.
References
Shun-ichi Amari, Noboru Murata, Klaus-Robert M¨uller, Michael Finke, and Howard Hua Yang.
Asymptotic statistical theory of overtraining and cross-validation. IEEE Transactions on Neural
Networks, 8(5):985–996, 1997.
Lalit Bahl, Peter Brown, Peter deSouza, and Robert Mercer. Maximum mutual information estimation of hidden markov parameters for speech recognition. In International Conference on
Acoustics, Speech and Signal Processing (ICASSP), pages 49–52, Tokyo, Japan, 1986.
Andrew E. Barron. Complexity regularization with application to artificial neural networks. In
G. Roussas, editor, Nonparametric Functional Estimation and Related Topics, pages 561–576.
Kluwer Academic Publishers, 1991.
Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps and spectral techniques for embedding and clustering. In T.G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural
Information Processing Systems 14 (NIPS'01), Cambridge, MA, 2002. MIT Press.
Yoshua Bengio. Learning deep architectures for AI. Foundations and Trends in Machine Learning, 2(1):1–127, 2009. Also published as a book. Now Publishers, 2009.
Yoshua Bengio and Olivier Delalleau. Justifying and generalizing contrastive divergence. Neural
Computation, 21(6):1601–1621, June 2009.
Yoshua Bengio and Yann LeCun. Scaling learning algorithms towards AI. In L. Bottou, O. Chapelle, D. DeCoste, and J. Weston, editors, Large Scale Kernel Machines, pages 321–360. MIT Press, Yoshua Bengio, Olivier Delalleau, and Nicolas Le Roux. The curse of highly variable functions for local kernel machines. In Y. Weiss, B. Sch¨olkopf, and J. Platt, editors, Advances in Neural Information Processing Systems 18 (NIPS'05), pages 107–114. MIT Press, Cambridge, MA, WHY DOES UNSUPERVISED PRE-TRAINING HELP DEEP LEARNING?
Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle. Greedy layer-wise training of deep networks. In Bernhard Sch¨olkopf, John Platt, and Thomas Hoffman, editors, Advances in Neural Information Processing Systems 19 (NIPS'06), pages 153–160. MIT Press, 2007.
Marc H. Bornstein. Sensitive periods in development : interdisciplinary perspectives / edited by
Marc H. Bornstein. Lawrence Erlbaum Associates, Hillsdale, N.J. :, 1987.
Olivier Chapelle, Jason Weston, and Bernhard Sch¨olkopf. Cluster kernels for semi-supervised learning. In S. Becker, S. Thrun, and K. Obermayer, editors, Advances in Neural Information Processing Systems 15 (NIPS'02), pages 585–592, Cambridge, MA, 2003. MIT Press.
Olivier Chapelle, Bernhard Sch¨olkopf, and Alexander Zien. Semi-Supervised Learning. MIT Press, Ronan Collobert and Jason Weston. A unified architecture for natural language processing: Deep neural networks with multitask learning. In William W. Cohen, Andrew McCallum, and Sam T.
Roweis, editors, Proceedings of the Twenty-fifth International Conference on Machine Learning(ICML'08), pages 160–167. ACM, 2008.
Dumitru Erhan, Yoshua Bengio, Aaron Courville, and Pascal Vincent. Visualizing higher-layer features of a deep network. Technical Report 1341, Universit´e de Montr´eal, 2009.
Patrick Gallinari, Yann LeCun, Sylvie Thiria, and Francoise Fogelman-Soulie. Memoires associatives distribuees. In Proceedings of COGNITIVA 87, Paris, La Villette, 1987.
Ian Goodfellow, Quoc Le, Andrew Saxe, and Andrew Ng. Measuring invariances in deep networks.
In Y. Bengio, D. Schuurmans, J. Lafferty, C. K. I. Williams, and A. Culotta, editors, Advances in Neural Information Processing Systems 22, pages 646–654. 2009.
Raia Hadsell, Ayse Erkan, Pierre Sermanet, Marco Scoffier, Urs Muller, and Yann LeCun. Deep belief net learning in a long-range vision system for autonomous off-road driving.
In Proc.
Intelligent Robots and Systems (IROS'08), pages 628–633, 2008.
Johan H˚astad. Almost optimal lower bounds for small depth circuits. In Proceedings of the 18th annual ACM Symposium on Theory of Computing, pages 6–20, Berkeley, California, 1986. ACM
Press.
Johan H˚astad and Mikael Goldmann. On the power of small-depth threshold circuits. Computational Complexity, 1:113–129, 1991.
Geoffrey E. Hinton. Training products of experts by minimizing contrastive divergence. Neural
Computation, 14:1771–1800, 2002.
Geoffrey E. Hinton. To recognize shapes, first learn to generate images. In Paul Cisek, Trevor
Drew, and John Kalaska, editors, Computational Neuroscience: Theoretical Insights into Brain
Function. Elsevier, 2007.
Geoffrey E. Hinton and Ruslan Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504–507, July 2006.
ERHAN, BENGIO, COURVILLE, MANZAGOL, VINCENT AND BENGIO
Goeffrey E. Hinton, Simon Osindero, and Yee Whye Teh. A fast learning algorithm for deep belief nets. Neural Computation, 18:1527–1554, 2006.
Hugo Larochelle and Yoshua Bengio. Classification using discriminative restricted Boltzmann machines. In William W. Cohen, Andrew McCallum, and Sam T. Roweis, editors, Proceedings of the Twenty-fifth International Conference on Machine Learning (ICML'08), pages 536–543.
ACM, 2008.
Hugo Larochelle, Dumitru Erhan, Aaron Courville, James Bergstra, and Yoshua Bengio. An empirical evaluation of deep architectures on problems with many factors of variation. In Int. Conf.
Mach. Learn., pages 473–480, 2007.
Hugo Larochelle, Yoshua Bengio, Jerome Louradour, and Pascal Lamblin. Exploring strategies for training deep neural networks. The Journal of Machine Learning Research, 10:1–40, January
Julia A. Lasserre, Christopher M. Bishop, and Thomas P. Minka. Principled hybrids of generative and discriminative models.
In Proceedings of the Computer Vision and Pattern Recognition
Conference (CVPR'06), pages 87–94, Washington, DC, USA, 2006. IEEE Computer Society.
Yann LeCun. Mod`eles connexionistes de l'apprentissage. PhD thesis, Universit´e de Paris VI, 1987.
Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.
Honglak Lee, Chaitanya Ekanadham, and Andrew Ng. Sparse deep belief net model for visual area
V2. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information
Processing Systems 20 (NIPS'07), pages 873–880. MIT Press, Cambridge, MA, 2008.
Honglak Lee, Roger Grosse, Rajesh Ranganath, and Andrew Y. Ng. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. In L´eon Bottou and Michael Littman, editors, Proceedings of the Twenty-sixth International Conference on Machine
Learning (ICML'09). ACM, Montreal (Qc), Canada, 2009.
Ga¨elle Loosli, St´ephane Canu, and L´eon Bottou. Training invariant support vector machines using selective sampling. In L´eon Bottou, Olivier Chapelle, Dennis DeCoste, and Jason Weston, editors, Large Scale Kernel Machines, pages 301–320. MIT Press, Cambridge, MA., 2007.
Hossein Mobahi, Ronan Collobert, and Jason Weston. Deep learning from temporal coherence in video. In L´eon Bottou and Michael Littman, editors, Proceedings of the 26th International
Conference on Machine Learning, pages 737–744, Montreal, June 2009. Omnipress.
Andrew Y. Ng and Michael I. Jordan. On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes. In T.G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing Systems 14 (NIPS'01), pages 841–848, 2002.
Simon Osindero and Geoffrey E. Hinton. Modeling image patches with a directed hierarchy of markov random field. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20 (NIPS'07), pages 1121–1128, Cambridge, MA, 2008.
MIT Press.
WHY DOES UNSUPERVISED PRE-TRAINING HELP DEEP LEARNING?
Dan Povey and Philip C. Woodland. Minimum phone error and i-smoothing for improved discriminative training. In Acoustics, Speech, and Signal Processing, 2002. Proceedings. (ICASSP '02).
IEEE International Conference on, volume 1, pages I–105–I–108 vol.1, 2002.
Marc'Aurelio Ranzato, Christopher Poultney, Sumit Chopra, and Yann LeCun. Efficient learning of sparse representations with an energy-based model. In B. Sch¨olkopf, J. Platt, and T. Hoffman, editors, Advances in Neural Information Processing Systems 19 (NIPS'06), pages 1137–1144.
MIT Press, 2007.
Marc'Aurelio Ranzato, Y-Lan Boureau, and Yann LeCun. Sparse feature learning for deep belief networks. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20 (NIPS'07), pages 1185–1192, Cambridge, MA, 2008. MIT Press.
Ruslan Salakhutdinov and Geoffrey E. Hinton. Using deep belief nets to learn covariance kernels for Gaussian processes. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20 (NIPS'07), pages 1249–1256, Cambridge, MA, 2008.
MIT Press.
Ruslan Salakhutdinov and Geoffrey E. Hinton. Semantic hashing. In Proceedings of the 2007 Workshop on Information Retrieval and applications of Graphical Models (SIGIR 2007), Amsterdam, 2007. Elsevier.
Ruslan Salakhutdinov, Andriy Mnih, and Geoffrey E. Hinton. Restricted Boltzmann machines for collaborative filtering. In Zoubin Ghahramani, editor, Proceedings of the Twenty-fourth International Conference on Machine Learning (ICML'07), pages 791–798, New York, NY, USA, 2007.
ACM.
Sebastian H. Seung.
Learning continuous attractors in recurrent networks.
In M.I. Jordan, M.J. Kearns, and S.A. Solla, editors, Advances in Neural Information Processing Systems 10(NIPS'97), pages 654–660. MIT Press, 1998.
Jonas Sj¨oberg and Lennart Ljung. Overtraining, regularization and searching for a minimum, with application to neural networks. International Journal of Control, 62(6):1391–1407, 1995.
Joshua M. Susskind, Geoffrey E., Javier R. Movellan, and Adam K. Anderson. Generating facial expressions with deep belief nets. In V. Kordic, editor, Affective Computing, Emotion Modelling, Synthesis and Recognition, pages 421–440. ARS Publishers, 2008.
Joshua Tenenbaum, Vin de Silva, and John C. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500):2319–2323, December 2000.
Laurens van der Maaten and Geoffrey E. Hinton. Visualizing data using t-sne. Journal of Machine
Learning Research, 9:2579–2605, November 2008.
Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and composing robust features with denoising autoencoders. In Andrew McCallum and Sam Roweis, editors, Proceedings of the 25th Annual International Conference on Machine Learning (ICML
2008), pages 1096–1103. Omnipress, 2008.
ERHAN, BENGIO, COURVILLE, MANZAGOL, VINCENT AND BENGIO
Max Welling, Michal Rosen-Zvi, and Geoffrey E. Hinton. Exponential family harmoniums with an application to information retrieval. In L.K. Saul, Y. Weiss, and L. Bottou, editors, Advances in Neural Information Processing Systems 17 (NIPS'04), pages 1481–1488, Cambridge, MA, 2005.
MIT Press.
Jason Weston, Fr´ed´eric Ratle, and Ronan Collobert. Deep learning via semi-supervised embedding. In William W. Cohen, Andrew McCallum, and Sam T. Roweis, editors, Proceedings of the Twenty-fifth International Conference on Machine Learning (ICML'08), pages 1168–1175, New
York, NY, USA, 2008. ACM.
Andrew Yao. Separating the polynomial-time hierarchy by oracles. In Proceedings of the 26th
Annual IEEE Symposium on Foundations of Computer Science, pages 1–10, 1985.
Long Zhu, Yuanhao Chen, and Alan Yuille. Unsupervised learning of probabilistic grammar-markov models for object categories. IEEE Transactions on Pattern Analysis and Machine Intelligence, 31(1):114–128, 2009.Practical Bayesian Optimization of Machine
Learning Algorithms
Jasper Snoek
Department of Computer Science
University of Toronto jasper@cs.toronto.edu
Hugo Larochelle
Department of Computer Science
University of Sherbrooke hugo.larochelle@usherbrooke.edu
Ryan P. Adams
School of Engineering and Applied Sciences
Harvard University rpa@seas.harvard.edu
Abstract
The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a "black art" requiring expert experience, rules of thumb, or sometimes bruteforce search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand.
In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieve expertlevel performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.
Introduction
Machine learning algorithms are rarely parameter-free: parameters controlling the rate of learning or the capacity of the underlying model must often be specified. These parameters are often considered nuisances, making it appealing to develop machine learning algorithms with fewer of them.
Another, more flexible take on this issue is to view the optimization of such parameters as a procedure to be automated. Specifically, we could view such tuning as the optimization of an unknown black-box function and invoke algorithms developed for such problems. A good choice is Bayesian optimization, which has been shown to outperform other state of the art global optimization algorithms on a number of challenging optimization benchmark functions. For continuous functions, Bayesian optimization typically works by assuming the unknown function was sampled from a Gaussian process and maintains a posterior distribution for this function as observations are made or, in our case, as the results of running learning algorithm experiments with different hyperparameters are observed. To pick the hyperparameters of the next experiment, one can optimize the expected improvement (EI) over the current best result or the Gaussian process upper confidence bound (UCB). EI and UCB have been shown to be efficient in the number of function evaluations required to find the global optimum of many multimodal black-box functions.
Machine learning algorithms, however, have certain characteristics that distinguish them from other black-box optimization problems. First, each function evaluation can require a variable amount of time: training a small neural network with 10 hidden units will take less time than a bigger network with 1000 hidden units. Even without considering duration, the advent of cloud computing makes it possible to quantify economically the cost of requiring large-memory machines for learning, changing the actual cost in dollars of an experiment with a different number of hidden units.
Second, machine learning experiments are often run in parallel, on multiple cores or machines. In both situations, the standard sequential approach of GP optimization can be suboptimal.
In this work, we identify good practices for Bayesian optimization of machine learning algorithms.
We argue that a fully Bayesian treatment of the underlying GP kernel is preferred to the approach based on optimization of the GP hyperparameters, as previously proposed. Our second contribution is the description of new algorithms for taking into account the variable and unknown cost of experiments or the availability of multiple cores to run experiments in parallel.
Gaussian processes have proven to be useful surrogate models for computer experiments and good practices have been established in this context for sensitivity analysis, calibration and prediction.
While these strategies are not considered in the context of optimization, they can be useful to researchers in machine learning who wish to understand better the sensitivity of their models to various hyperparameters. Hutter et al. have developed sequential model-based optimization strategies for the configuration of satisfiability and mixed integer programming solvers using random forests. The machine learning algorithms we consider, however, warrant a fully Bayesian treatment as their expensive nature necessitates minimizing the number of evaluations. Bayesian optimization strategies have also been used to tune the parameters of Markov chain Monte Carlo algorithms. Recently, Bergstra et al. have explored various strategies for optimizing the hyperparameters of machine learning algorithms. They demonstrated that grid search strategies are inferior to random search, and suggested the use of Gaussian process Bayesian optimization, optimizing the hyperparameters of a squared-exponential covariance, and proposed the Tree Parzen Algorithm.
Bayesian Optimization with Gaussian Process Priors
As in other kinds of optimization, in Bayesian optimization we are interested in finding the minimum of a function f(x) on some bounded set X, which we will take to be a subset of RD. What makes Bayesian optimization different from other procedures is that it constructs a probabilistic model for f(x) and then exploits this model to make decisions about where in X to next evaluate the function, while integrating out uncertainty. The essential philosophy is to use all of the information available from previous evaluations of f(x) and not simply rely on local gradient and Hessian approximations. This results in a procedure that can find the minimum of difficult non-convex functions with relatively few evaluations, at the cost of performing more computation to determine the next point to try. When evaluations of f(x) are expensive to perform — as is the case when it requires training a machine learning algorithm — then it is easy to justify some extra computation to make better decisions. For an overview of the Bayesian optimization formalism and a review of previous work, see, e.g., Brochu et al.. In this section we briefly review the general Bayesian optimization approach, before discussing our novel contributions in Section 3.
There are two major choices that must be made when performing Bayesian optimization. First, one must select a prior over functions that will express assumptions about the function being optimized.
For this we choose the Gaussian process prior, due to its flexibility and tractability. Second, we must choose an acquisition function, which is used to construct a utility function from the model posterior, allowing us to determine the next point to evaluate.
Gaussian Processes
The Gaussian process (GP) is a convenient and powerful prior distribution on functions, which we will take here to be of the form f : X ! R. The GP is defined by the property that any finite set of N points {xn 2 X}N n=1 induces a multivariate Gaussian distribution on RN. The nth of these points is taken to be the function value f(xn), and the elegant marginalization properties of the Gaussian distribution allow us to compute marginals and conditionals in closed form. The support and properties of the resulting distribution on functions are determined by a mean function m : X ! R and a positive definite covariance function K : X ⇥ X ! R. We will discuss the impact of covariance functions in Section 3.1. For an overview of Gaussian processes, see Rasmussen and Williams.
Acquisition Functions for Bayesian Optimization
We assume that the function f(x) is drawn from a Gaussian process prior and that our observations are of the form {xn, yn}N n=1, where yn ⇠ N(f(xn), ⌫) and ⌫ is the variance of noise introduced into the function observations. This prior and these data induce a posterior over functions; the acquisition function, which we denote by a : X ! R+, determines what point in X should be evaluated next via a proxy optimization xnext = argmaxx a(x), where several different functions have been proposed. In general, these acquisition functions depend on the previous observations, as well as the GP hyperparameters; we denote this dependence as a(x ; {xn, yn}, ✓). There are several popular choices of acquisition function. Under the Gaussian process prior, these functions depend on the model solely through its predictive mean function µ(x ; {xn, yn}, ✓) and predictive variance function �2(x ; {xn, yn}, ✓). In the proceeding, we will denote the best current value as xbest = argminxn f(xn), �(·) will denote the cumulative distribution function of the standard normal, and �(·) will denote the standard normal density function.
Probability of Improvement
One intuitive strategy is to maximize the probability of improving over the best current value. Under the GP this can be computed analytically as aPI(x ; {xn, yn}, ✓) = �(�(x)), �(x) = f(xbest) � µ(x ; {xn, yn}, ✓)
�(x ; {xn, yn}, ✓)
Expected Improvement
Alternatively, one could choose to maximize the expected improvement(EI) over the current best. This also has closed form under the Gaussian process: aEI(x ; {xn, yn}, ✓) = �(x ; {xn, yn}, ✓) (�(x) �(�(x)) + N(�(x) ; 0, 1))
GP Upper Confidence Bound
A more recent development is the idea of exploiting lower confidence bounds (upper, when considering maximization) to construct acquisition functions that minimize regret over the course of their optimization. These acquisition functions have the form aLCB(x ; {xn, yn}, ✓) = µ(x ; {xn, yn}, ✓) �  �(x ; {xn, yn}, ✓), (3) with a tunable  to balance exploitation against exploration.
In this work we will focus on the EI criterion, as it has been shown to be better-behaved than probability of improvement, but unlike the method of GP upper confidence bounds (GP-UCB), it does not require its own tuning parameter. Although the EI algorithm performs well in minimization problems, we wish to note that the regret formalization may be more appropriate in some settings.
We perform a direct comparison between our EI-based approach and GP-UCB in Section 4.1.
Practical Considerations for Bayesian Optimization of Hyperparameters
Although an elegant framework for optimizing expensive functions, there are several limitations that have prevented it from becoming a widely-used technique for optimizing hyperparameters in machine learning problems. First, it is unclear for practical problems what an appropriate choice is for the covariance function and its associated hyperparameters. Second, as the function evaluation itself may involve a time-consuming optimization procedure, problems may vary significantly in duration and this should be taken into account. Third, optimization algorithms should take advantage of multi-core parallelism in order to map well onto modern computational environments. In this section, we propose solutions to each of these issues.
Covariance Functions and Treatment of Covariance Hyperparameters
The power of the Gaussian process to express a rich distribution on functions rests solely on the shoulders of the covariance function. While non-degenerate covariance functions correspond to infinite bases, they nevertheless can correspond to strong assumptions regarding likely functions. In particular, the automatic relevance determination (ARD) squared exponential kernel
KSE(x, x0) = ✓0 exp
⇢
�1
2r2(x, x0)
� r2(x, x0) =
D
X d=1(xd � x0 d)2/✓2 d.(4) is often a default choice for Gaussian process regression. However, sample functions with this covariance function are unrealistically smooth for practical optimization problems. We instead propose
3 the use of the ARD Mat´ern 5/2 kernel:
KM52(x, x0) = ✓0
✓
1 + p
5r2(x, x0) + 5
3r2(x, x0)
◆ exp n
� p
5r2(x, x0) o
This covariance function results in sample functions which are twice-differentiable, an assumption that corresponds to those made by, e.g., quasi-Newton methods, but without requiring the smoothness of the squared exponential.
After choosing the form of the covariance, we must also manage the hyperparameters that govern its behavior (Note that these "hyperparameters" are distinct from those being subjected to the overall
Bayesian optimization.), as well as that of the mean function. For our problems of interest, typically we would have D + 3 Gaussian process hyperparameters: D length scales ✓1:D, the covariance amplitude ✓0, the observation noise ⌫, and a constant mean m. The most commonly advocated approach is to use a point estimate of these parameters by optimizing the marginal likelihood under the Gaussian process, p(y | {xn}N n=1, ✓, ⌫, m) = N(y | m1, ⌃✓ + ⌫I), where y = [y1, y2, · · ·, yN]T, and ⌃✓ is the covariance matrix resulting from the N input points under the hyperparameters ✓.
However, for a fully-Bayesian treatment of hyperparameters (summarized here by ✓ alone), it is desirable to marginalize over hyperparameters and compute the integrated acquisition function:
ˆa(x ; {xn, yn}) =
Z a(x ; {xn, yn}, ✓) p(✓ | {xn, yn}N n=1) d✓, (6) where a(x) depends on ✓ and all of the observations. For probability of improvement and EI, this expectation is the correct generalization to account for uncertainty in hyperparameters. We can therefore blend acquisition functions arising from samples from the posterior over GP hyperparameters and have a Monte Carlo estimate of the integrated expected improvement. These samples can be acquired efficiently using slice sampling, as described in Murray and Adams. As both optimization and Markov chain Monte Carlo are computationally dominated by the cubic cost of solving an N-dimensional linear system (and our function evaluations are assumed to be much more expensive anyway), the fully-Bayesian treatment is sensible and our empirical evaluations bear this out.
Figure 1 shows how the integrated expected improvement changes the acquistion function.
Modeling Costs
Ultimately, the objective of Bayesian optimization is to find a good setting of our hyperparameters as quickly as possible. Greedy acquisition procedures such as expected improvement try to make the best progress possible in the next function evaluation. From a practial point of view, however, we are not so concerned with function evaluations as with wallclock time. Different regions of the parameter space may result in vastly different execution times, due to varying regularization, learning rates, etc. To improve our performance in terms of wallclock time, we propose optimizing with the expected improvement per second, which prefers to acquire points that are not only likely to be good, but that are also likely to be evaluated quickly. This notion of cost can be naturally generalized to other budgeted resources, such as reagents or money.
Just as we do not know the true objective function f(x), we also do not know the duration function c(x) : X ! R+. We can nevertheless employ our Gaussian process machinery to model ln c(x) alongside f(x). In this work, we assume that these functions are independent of each other, although their coupling may be usefully captured using GP variants of multi-task learning (e.g., ).
Under the independence assumption, we can easily compute the predicted expected inverse duration and use it to compute the expected improvement per second as a function of x.
Monte Carlo Acquisition for Parallelizing Bayesian Optimization
With the advent of multi-core computing, it is natural to ask how we can parallelize our Bayesian optimization procedures. More generally than simply batch parallelism, however, we would like to be able to decide what x should be evaluated next, even while a set of points are being evaluated.
Clearly, we cannot use the same acquisition function again, or we will repeat one of the pending experiments. Ideally, we could perform a roll-out of our acquisition policy, to choose a point that appropriately balanced information gain and exploitation. However, such roll-outs are generally intractable. Instead we propose a sequential strategy that takes advantage of the tractable inference properties of the Gaussian process to compute Monte Carlo estimates of the acquisiton function under different possible results from pending function evaluations.(a) Posterior samples under varying hyperparameters(b) Expected improvement under varying hyperparameters(c) Integrated expected improvement
Figure 1: Illustration of integrated expected improvement. (a) Three posterior samples are shown, each with different length scales, after the same five observations. (b) Three expected improvement acquisition functions, with the same data and hyperparameters.
The maximum of each is shown. (c) The integrated expected improvement, with its maximum shown.(a) Posterior samples after three data(b) Expected improvement under three fantasies(c) Expected improvement across fantasies
Figure 2: Illustration of the acquisition with pending evaluations. (a) Three data have been observed and three posterior functions are shown, with "fantasies" for three pending evaluations. (b) Expected improvement, conditioned on the each joint fantasy of the pending outcome. (c) Expected improvement after integrating over the fantasy outcomes.
Consider the situation in which N evaluations have completed, yielding data {xn, yn}N n=1, and in which J evaluations are pending at locations {xj}J j=1. Ideally, we would choose a new point based on the expected acquisition function under all possible outcomes of these pending evaluations:
ˆa(x ; {xn, yn}, ✓, {xj}) =
Z
RJ a(x ; {xn, yn}, ✓, {xj, yj}) p({yj}J j=1 | {xj}J j=1, {xn, yn}N n=1) dy1 · · · dyJ.
This is simply the expectation of a(x) under a J-dimensional Gaussian distribution, whose mean and covariance can easily be computed. As in the covariance hyperparameter case, it is straightforward to use samples from this distribution to compute the expected acquisition and use this to select the next point. Figure 2 shows how this procedure would operate with queued evaluations. We note that a similar approach is touched upon briefly by Ginsbourger and Riche, but they view it as too intractable to warrant attention. We have found our Monte Carlo estimation procedure to be highly effective in practice, however, as will be discussed in Section 4.
Empirical Analyses
In this section, we empirically analyse1 the algorithms introduced in this paper and compare to existing strategies and human performance on a number of challenging machine learning problems.
We refer to our method of expected improvement while marginalizing GP hyperparameters as "GP
EI MCMC", optimizing hyperparameters as "GP EI Opt", EI per second as "GP EI per Second", and N times parallelized GP EI MCMC as "Nx GP EI MCMC". Each results figure plots the progression of minxn f(xn) over the number of function evaluations or time, averaged over multiple runs of each algorithm. If not specified otherwise, xnext = argmaxx a(x) is computed using gradientbased search with multiple restarts (see supplementary material for details). The code used is made publicly available at http://www.cs.toronto.edu/˜jasper/software.html.
1All experiments were conducted on identical machines using the Amazon EC2 service.
Min Function Value
Function evaluations
GP EI Opt
GP EI MCMC
GP−UCB
TPA(a)
Min Function Value
Function Evaluations
GP EI MCMC
GP EI Opt
GP EI per Sec
Tree Parzen Algorithm(b)
Min Function Value
Minutes
GP EI MCMC
GP EI per Second(c)
Figure 3: Comparisons on the Branin-Hoo function (3a) and training logistic regression on MNIST (3b). (3c) shows GP EI MCMC and GP EI per Second from (3b), but in terms of time elapsed.
Min Function Value
Function evaluations
GP EI MCMC
GP EI per second
GP EI Opt
Random Grid Search
3x GP EI MCMC
5x GP EI MCMC
10x GP EI MCMC(a)
Min function value
Time (Days)
GP EI MCMC
GP EI per second
GP EI Opt
3x GP EI MCMC
5x GP EI MCMC
10x GP EI MCMC(b)
Min Function Value
Function evaluations
3x GP EI MCMC (On grid)
5x GP EI MCMC (On grid)
3x GP EI MCMC (Off grid)
5x GP EI MCMC (Off grid)(c)
Figure 4: Different strategies of optimization on the Online LDA problem compared in terms of function evaluations (4a), walltime (4b) and constrained to a grid or not (4c).
Branin-Hoo and Logistic Regression
We first compare to standard approaches and the recent Tree Parzen Algorithm2 (TPA) of Bergstra et al. on two standard problems. The Branin-Hoo function is a common benchmark for Bayesian optimization techniques that is defined over x 2 R2 where 0  x1  15 and �5  x2  15. We also compare to TPA on a logistic regression classification task on the popular MNIST data. The algorithm requires choosing four hyperparameters, the learning rate for stochastic gradient descent, on a log scale from 0 to 1, the `2 regularization parameter, between 0 and 1, the mini batch size, from 20 to 2000 and the number of learning epochs, from 5 to 2000. Each algorithm was run on the Branin-Hoo and logistic regression problems 100 and 10 times respectively and mean and standard error are reported. The results of these analyses are presented in Figures 3a and 3b in terms of the number of times the function is evaluated. On Branin-Hoo, integrating over hyperparameters is superior to using a point estimate and the GP EI significantly outperforms TPA, finding the minimum in less than half as many evaluations, in both cases. For logistic regression, 3b and 3c show that although EI per second is less efficient in function evaluations it outperforms standard EI in time.
Online LDA
Latent Dirichlet Allocation (LDA) is a directed graphical model for documents in which words are generated from a mixture of multinomial "topic" distributions. Variational Bayes is a popular paradigm for learning and, recently, Hoffman et al. proposed an online learning approach in that context. Online LDA requires 2 learning parameters, ⌧0 and , that control the learning rate
⇢t = (⌧0 + t)� used to update the variational parameters of LDA based on the tth minibatch of document word count vectors. The size of the minibatch is also a third parameter that must be chosen. Hoffman et al. relied on an exhaustive grid search of size 6 ⇥ 6 ⇥ 8, for a total of 288 hyperparameter configurations.
We used the code made publically available by Hoffman et al. to run experiments with online
LDA on a collection of Wikipedia articles. We downloaded a random set of 249 560 articles, split into training, validation and test sets of size 200 000, 24 560 and 25 000 respectively. The documents are represented as vectors of word counts from a vocabulary of 7702 words. As reported in Hoffman et al., we used a lower bound on the per word perplexity of the validation set documents as the performance measure. One must also specify the number of topics and the hyperparameters ⌘ for the symmetric Dirichlet prior over the topic distributions and ↵ for the symmetric Dirichlet prior over the per document topic mixing weights. We followed Hoffman et al. and used 100 topics and ⌘ = ↵ = 0.01 in our experiments in order to emulate their analysis and repeated exactly the grid
2Using the publicly available code from https://github.com/jaberg/hyperopt/wiki
Time (hours)
Min function value
GP EI MCMC
GP EI per Second
3x GP EI MCMC
3x GP EI per Second
Random Grid Search(a)
Min Function Value
Function evaluations
GP EI MCMC
GP EI per Second
3x GP EI MCMC
3x GP EI per Second(b)
Min Function Value
Function evaluations
Matern 52 ARD
SqExp
SqExp ARD
Matern 32 ARD(c)
Figure 5: A comparison of various strategies for optimizing the hyperparameters of M3E models on the protein motif finding task in terms of walltime (5a), function evaluations (5b) and different covariance functions(5c). search reported in the paper3. Each online LDA evaluation generally took between five to ten hours to converge, thus the grid search requires approximately 60 to 120 processor days to complete.
In Figures 4a and 4b we compare our various strategies of optimization over the same grid on this expensive problem. That is, the algorithms were restricted to only the exact parameter settings as evaluated by the grid search. Each optimization was then repeated 100 times (each time picking two different random experiments to initialize the optimization with) and the mean and standard error are reported4. Figure 4c also presents a 5 run average of optimization with 3 and 5 times parallelized
GP EI MCMC, but without restricting the new parameter setting to be on the pre-specified grid (see supplementary material for details). A comparison with their "on grid" versions is illustrated.
Clearly integrating over hyperparameters is superior to using a point estimate in this case. While
GP EI MCMC is the most efficient in terms of function evaluations, we see that parallelized GP EI
MCMC finds the best parameters in significantly less time. Finally, in Figure 4c we see that the parallelized GP EI MCMC algorithms find a significantly better minimum value than was found in the grid search used by Hoffman et al. while running a fraction of the number of experiments.
Motif Finding with Structured Support Vector Machines
In this example, we consider optimizing the learning parameters of Max-Margin Min-Entropy(M3E) Models, which include Latent Structured Support Vector Machines as a special case. Latent structured SVMs outperform SVMs on problems where they can explicitly model problem-dependent hidden variables. A popular example task is the binary classification of protein DNA sequences. The hidden variable to be modeled is the unknown location of particular subsequences, or motifs, that are indicators of positive sequences.
Setting the hyperparameters, such as the regularisation term, C, of structured SVMs remains a challenge and these are typically set through a time consuming grid search procedure as is done in. Indeed, Kumar et al. avoided hyperparameter selection for this task as it was too computationally expensive. However, Miller et al. demonstrate that results depend highly on the setting of the parameters, which differ for each protein. M3E models introduce an entropy term, parameterized by ↵, which enables the model to outperform latent structured SVMs. This additional performance, however, comes at the expense of an additional problem-dependent hyperparameter.
We emulate the experiments of Miller et al. for one protein with approximately 40 000 sequences. We explore 25 settings of the parameter C, on a log scale from 10�1 to 106, 14 settings of ↵, on a log scale from 0.1 to 5 and the model convergence tolerance, ✏ 2 {10�4,10�3,10�2,10�1}.
We ran a grid search over the 1400 possible combinations of these parameters, evaluating each over 5 random 50-50 training and test splits.
In Figures 5a and 5b, we compare the randomized grid search to GP EI MCMC, GP EI per Second and their 3x parallelized versions, all constrained to the same points on the grid. Each algorithm was repeated 100 times and the mean and standard error are shown. We observe that the Bayesian optimization strategies are considerably more efficient than grid search which is the status quo. In this case, GP EI MCMC is superior to GP EI per Second in terms of function evaluations but GP
EI per Second finds better parameters faster than GP EI MCMC as it learns to use a less strict
3i.e. the only difference was the randomly sampled collection of articles in the data set and the choice of the vocabulary. We ran each evaluation for 10 hours or until convergence.
4The restriction of the search to the same grid was chosen for efficiency reasons: it allowed us to repeat the experiments several times efficiently, by first computing all function evaluations over the whole grid and reusing these values within each repeated experiment.
Min Function Value
Function evaluations
GP EI MCMC
GP EI Opt
GP EI per Second
GP EI MCMC 3x Parallel
Human Expert
Min function value
Time (Hours)
GP EI MCMC
GP EI Opt
GP EI per Second
GP EI MCMC 3x Parallel
Figure 6: Validation error on the CIFAR-10 data for different optimization strategies. convergence tolerance early on while exploring the other parameters. Indeed, 3x GP EI per second, is the least efficient in terms of function evaluations but finds better parameters faster than all the other algorithms. Figure 5c compares the use of various covariance functions in GP EI MCMC optimization on this problem, again repeating the optimization 100 times. It is clear that the selection of an appropriate covariance significantly affects performance and the estimation of length scale parameters is critical. The assumption of the infinite differentiability as imposed by the commonly used squared exponential is too restrictive for this problem.
Convolutional Networks on CIFAR-10
Neural networks and deep learning methods notoriously require careful tuning of numerous hyperparameters. Multi-layer convolutional neural networks are an example of such a model for which a thorough exploration of architechtures and hyperparameters is beneficial, as demonstrated in Saxe et al., but often computationally prohibitive. While Saxe et al. demonstrate a methodology for efficiently exploring model architechtures, numerous hyperparameters, such as regularisation parameters, remain. In this empirical analysis, we tune nine hyperparameters of a three-layer convolutional network on the CIFAR-10 benchmark dataset using the code provided 5. This model has been carefully tuned by a human expert to achieve a highly competitive result of 18% test error on the unaugmented data, which matches the published state of the art result on CIFAR10. The parameters we explore include the number of epochs to run the model, the learning rate, four weight costs (one for each layer and the softmax output weights), and the width, scale and power of the response normalization on the pooling layers of the network.
We optimize over the nine parameters for each strategy on a withheld validation set and report the mean validation error and standard error over five separate randomly initialized runs. Results are presented in Figure 6 and contrasted with the average results achieved using the best parameters found by the expert. The best hyperparameters found by the GP EI MCMC approach achieve an error on the test set of 14.98%, which is over 3% better than the expert and the state of the art on
CIFAR-10. The same procedure was repeated on the CIFAR-10 data augmented with horizontal reflections and translations, similarly improving on the expert from 11% to 9.5% test error and achieving to our knowledge the lowest error reported on the competitive CIFAR-10 benchmark.
Conclusion
We presented methods for performing Bayesian optimization for hyperparameter selection of general machine learning algorithms. We introduced a fully Bayesian treatment for EI, and algorithms for dealing with variable time regimes and running experiments in parallel. The effectiveness of our approaches were demonstrated on three challenging recently published problems spanning different areas of machine learning. The resulting Bayesian optimization finds better hyperparameters significantly faster than the approaches used by the authors and surpasses a human expert at selecting hyperparameters on the competitive CIFAR-10 dataset, beating the state of the art by over 3%.
Acknowledgements
The authors thank Alex Krizhevsky, Hoffman et al. and Miller et al. for making their code and data available, and George Dahl for valuable feedback. This work was funded by DARPA Young
Faculty Award N66001-12-1-4219, NSERC and an Amazon AWS in Research grant.
5Available at: http://code.google.com/p/cuda-convnet/
References
 Jonas Mockus, Vytautas Tiesis, and Antanas Zilinskas. The application of Bayesian methods for seeking the extremum. Towards Global Optimization, 2:117–129, 1978.
 D.R. Jones. A taxonomy of global optimization methods based on response surfaces. Journal of Global Optimization, 21(4):345–383, 2001.
 Niranjan Srinivas, Andreas Krause, Sham Kakade, and Matthias Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. In Proceedings of the 27th International Conference on Machine Learning, 2010.
 Adam D. Bull. Convergence rates of efficient global optimization algorithms. Journal of Machine Learning Research, (3-4):2879–2904, 2011.
 James S. Bergstra, R´emi Bardenet, Yoshua Bengio, and B´al´azs K´egl. Algorithms for hyperparameter optimization. In Advances in Neural Information Processing Systems 25. 2011.
 Marc C. Kennedy and Anthony O'Hagan. Bayesian calibration of computer models. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 63(3), 2001.
 Frank Hutter, Holger H. Hoos, and Kevin Leyton-Brown. Sequential model-based optimization for general algorithm configuration. In Learning and Intelligent Optimization 5, 2011.
 Nimalan Mahendran, Ziyu Wang, Firas Hamze, and Nando de Freitas. Adaptive mcmc with bayesian optimization. In AISTATS, 2012.
 James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. Journal of Machine Learning Research, 13:281–305, 2012.
 Eric Brochu, Vlad M. Cora, and Nando de Freitas. A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning. pre-print, 2010. arXiv:1012.2599.
 Carl E. Rasmussen and Christopher Williams. Gaussian Processes for Machine Learning. MIT
Press, 2006.
 H. J. Kushner. A new method for locating the maximum point of an arbitrary multipeak curve in the presence of noise. Journal of Basic Engineering, 86, 1964.
 Iain Murray and Ryan P. Adams. Slice sampling covariance hyperparameters of latent Gaussian models. In Advances in Neural Information Processing Systems 24, pages 1723–1731. 2010.
 Yee Whye Teh, Matthias Seeger, and Michael I. Jordan. Semiparametric latent factor models.
In AISTATS, 2005.
 Edwin V. Bonilla, Kian Ming A. Chai, and Christopher K. I. Williams. Multi-task Gaussian process prediction. In Advances in Neural Information Processing Systems 22, 2008.
 David Ginsbourger and Rodolphe Le Riche. Dealing with asynchronicity in parallel Gaussian process based global optimization. http://hal.archives-ouvertes.fr/ hal-00507632, 2010.
 Matthew Hoffman, David M. Blei, and Francis Bach. Online learning for latent Dirichlet allocation. In Advances in Neural Information Processing Systems 24, 2010.
 Kevin Miller, M. Pawan Kumar, Benjamin Packer, Danny Goodman, and Daphne Koller. Maxmargin min-entropy models. In AISTATS, 2012.
 Chun-Nam John Yu and Thorsten Joachims. Learning structural SVMs with latent variables.
In Proceedings of the 26th International Conference on Machine Learning, 2009.
 M. Pawan Kumar, Benjamin Packer, and Daphne Koller. Self-paced learning for latent variable models. In Advances in Neural Information Processing Systems 25. 2010.
 Andrew Saxe, Pang Wei Koh, Zhenghao Chen, Maneesh Bhand, Bipin Suresh, and Andrew Ng.
On random weights and unsupervised feature learning. In Proceedings of the 28th International
Conference on Machine Learning, 2011.
 Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, Department of Computer Science, University of Toronto, 2009.
 Adam Coates and Andrew Y. Ng. Selecting receptive fields in deep networks. In Advances in Neural Information Processing Systems 25. 2011.Algorithms for Hyper-Parameter Optimization
James Bergstra
The Rowland Institute
Harvard University bergstra@rowland.harvard.edu
R´emi Bardenet
Laboratoire de Recherche en Informatique
Universit´e Paris-Sud bardenet@lri.fr
Yoshua Bengio
D´ept. d'Informatique et Recherche Op´erationelle
Universit´e de Montr´eal yoshua.bengio@umontreal.ca
Bal´azs K´egl
Linear Accelerator Laboratory
Universit´e Paris-Sud, CNRS balazs.kegl@gmail.com
Abstract
Several recent advances to the state of the art in image classification benchmarks have come from better configurations of existing techniques rather than novel approaches to feature learning. Traditionally, hyper-parameter optimization has been the job of humans because they can be very efficient in regimes where only a few trials are possible. Presently, computer clusters and GPU processors make it possible to run more trials and we show that algorithmic approaches can find better results. We present hyper-parameter optimization results on tasks of training neural networks and deep belief networks (DBNs). We optimize hyper-parameters using random search and two new greedy sequential methods based on the expected improvement criterion. Random search has been shown to be sufficiently efficient for learning neural networks for several datasets, but we show it is unreliable for training DBNs. The sequential algorithms are applied to the most difficult
DBN learning problems from and find significantly better results than the best previously reported. This work contributes novel techniques for making response surface models P(y|x) in which many elements of hyper-parameter assignment(x) are known to be irrelevant given particular values of other elements.
Introduction
Models such as Deep Belief Networks (DBNs), stacked denoising autoencoders, convolutional networks, as well as classifiers based on sophisticated feature extraction techniques have from ten to perhaps fifty hyper-parameters, depending on how the experimenter chooses to parametrize the model, and how many hyper-parameters the experimenter chooses to fix at a reasonable default. The difficulty of tuning these models makes published results difficult to reproduce and extend, and makes even the original investigation of such methods more of an art than a science.
Recent results such as,, and demonstrate that the challenge of hyper-parameter optimization in large and multilayer models is a direct impediment to scientific progress. These works have advanced state of the art performance on image classification problems by more concerted hyper-parameter optimization in simple algorithms, rather than by innovative modeling or machine learning strategies. It would be wrong to conclude from a result such as that feature learning is useless. Instead, hyper-parameter optimization should be regarded as a formal outer loop in the learning process. A learning algorithm, as a functional from data to classifier (taking classification problems as an example), includes a budgeting choice of how many CPU cycles are to be spent on hyper-parameter exploration, and how many CPU cycles are to be spent evaluating each hyperparameter choice (i.e. by tuning the regular parameters). The results of and suggest that with current generation hardware such as large computer clusters and GPUs, the optimal alloca1 tion of CPU cycles includes more hyper-parameter exploration than has been typical in the machine learning literature.
Hyper-parameter optimization is the problem of optimizing a loss function over a graph-structured configuration space. In this work we restrict ourselves to tree-structured configuration spaces. Configuration spaces are tree-structured in the sense that some leaf variables (e.g. the number of hidden units in the 2nd layer of a DBN) are only well-defined when node variables (e.g. a discrete choice of how many layers to use) take particular values. Not only must a hyper-parameter optimization algorithm optimize over variables which are discrete, ordinal, and continuous, but it must simultaneously choose which variables to optimize.
In this work we define a configuration space by a generative process for drawing valid samples.
Random search is the algorithm of drawing hyper-parameter assignments from that process and evaluating them. Optimization algorithms work by identifying hyper-parameter assignments that could have been drawn, and that appear promising on the basis of the loss function's value at other points. This paper makes two contributions: 1) Random search is competitive with the manual optimization of DBNs in, and 2) Automatic sequential optimization outperforms both manual and random search.
Section 2 covers sequential model-based optimization, and the expected improvement criterion. Section 3 introduces a Gaussian Process based hyper-parameter optimization algorithm. Section 4 introduces a second approach based on adaptive Parzen windows. Section 5 describes the problem of DBN hyper-parameter optimization, and shows the efficiency of random search. Section 6 shows the efficiency of sequential optimization on the two hardest datasets according to random search.
The paper concludes with discussion of results and concluding remarks in Section 7 and Section 8.
Sequential Model-based Global Optimization
Sequential Model-Based Global Optimization (SMBO) algorithms have been used in many applications where evaluation of the fitness function is expensive. In an application where the true fitness function f : X → R is costly to evaluate, model-based algorithms approximate f with a surrogate that is cheaper to evaluate. Typically the inner loop in an SMBO algorithm is the numerical optimization of this surrogate, or some transformation of the surrogate. The point x∗ that maximizes the surrogate (or its transformation) becomes the proposal for where the true function f should be evaluated. This active-learning-like algorithm template is summarized in Figure 1. SMBO algorithms differ in what criterion they optimize to obtain x∗ given a model (or surrogate) of f, and in they model f via observation history H.
SMBO
� f, M0, T, S
�
H ← ∅, For t ← 1 to T, 3 x∗ ← argminx S(x, Mt−1), Evaluate f(x∗), ▷ Expensive step
H ← H ∪ (x∗, f(x∗)), Fit a new model Mt to H.
7 return H
Figure 1: The pseudo-code of generic Sequential Model-Based Optimization.
The algorithms in this work optimize the criterion of Expected Improvement (EI). Other criteria have been suggested, such as Probability of Improvement and Expected Improvement, minimizing the Conditional Entropy of the Minimizer, and the bandit-based criterion described in. We chose to use the EI criterion in our work because it is intuitive, and has been shown to work well in a variety of settings. We leave the systematic exploration of improvement criteria for future work. Expected improvement is the expectation under some model M of f : X → RN that f(x) will exceed (negatively) some threshold y∗:
EIy∗(x) :=
� ∞
−∞ max(y∗ − y, 0)pM(y|x)dy.
The contribution of this work is two novel strategies for approximating f by modeling H: a hierarchical Gaussian Process and a tree-structured Parzen estimator. These are described in Section 3 and Section 4 respectively.
The Gaussian Process Approach (GP)
Gaussian Processes have long been recognized as a good method for modeling loss functions in model-based optimization literature. Gaussian Processes (GPs, ) are priors over functions that are closed under sampling, which means that if the prior distribution of f is believed to be a GP with mean 0 and kernel k, the conditional distribution of f knowing a sample H = (xi, f(xi))n i=1 of its values is also a GP, whose mean and covariance function are analytically derivable. GPs with generic mean functions can in principle be used, but it is simpler and sufficient for our purposes to only consider zero mean processes. We do this by centering the function values in the considered data sets. Modelling e.g. linear trends in the GP mean leads to undesirable extrapolation in unexplored regions during SMBO.
The above mentioned closedness property, along with the fact that GPs provide an assessment of prediction uncertainty incorporating the effect of data scarcity, make the GP an elegant candidate for both finding candidate x∗ (Figure 1, step 3) and fitting a model Mt (Figure 1, step 6). The runtime of each iteration of the GP approach scales cubically in |H| and linearly in the number of variables being optimized, however the expense of the function evaluations f(x∗) typically dominate even this cubic cost.
Optimizing EI in the GP
We model f with a GP and set y∗ to the best value found after observing H: y∗ = min{f(xi), 1 ≤ i ≤ n}. The model pM in (1) is then the posterior GP knowing H. The EI function in (1) encapsulates a compromise between regions where the mean function is close to or better than y∗ and under-explored regions where the uncertainty is high.
EI functions are usually optimized with an exhaustive grid search over the input space, or a Latin
Hypercube search in higher dimensions. However, some information on the landscape of the EI criterion can be derived from simple computations : 1) it is always non-negative and zero at training points from D, 2) it inherits the smoothness of the kernel k, which is in practice often at least once differentiable, and noticeably, 3) the EI criterion is likely to be highly multi-modal, especially as the number of training points increases. The authors of used the preceding remarks on the landscape of EI to design an evolutionary algorithm with mixture search, specifically aimed at optimizing EI, that is shown to outperform exhaustive search for a given budget in EI evaluations. We borrow here their approach and go one step further. We keep the Estimation of Distribution (EDA, ) approach on the discrete part of our input space (categorical and discrete hyper-parameters), where we sample candidate points according to binomial distributions, while we use the Covariance
Matrix Adaptation - Evolution Strategy (CMA-ES, ) for the remaining part of our input space(continuous hyper-parameters). CMA-ES is a state-of-the-art gradient-free evolutionary algorithm for optimization on continuous domains, which has been shown to outperform the Gaussian search
EDA. Notice that such a gradient-free approach allows non-differentiable kernels for the GP regression. We do not take on the use of mixtures in, but rather restart the local searches several times, starting from promising places. The use of tesselations suggested by is prohibitive here, as our task often means working in more than 10 dimensions, thus we start each local search at the center of mass of a simplex with vertices randomly picked among the training points.
Finally, we remark that all hyper-parameters are not relevant for each point. For example, a DBN with only one hidden layer does not have parameters associated to a second or third layer. Thus it is not enough to place one GP over the entire space of hyper-parameters. We chose to group the hyper-parameters by common use in a tree-like fashion and place different independent GPs over each group. As an example, for DBNs, this means placing one GP over common hyper-parameters, including categorical parameters that indicate what are the conditional groups to consider, three
GPs on the parameters corresponding to each of the three layers, and a few 1-dimensional GPs over individual conditional hyper-parameters, like ZCA energy (see Table 1 for DBN parameters).
Tree-structured Parzen Estimator Approach (TPE)
Anticipating that our hyper-parameter optimization tasks will mean high dimensions and small fitness evaluation budgets, we now turn to another modeling strategy and EI optimization scheme for the SMBO algorithm. Whereas the Gaussian-process based approach modeled p(y|x) directly, this strategy models p(x|y) and p(y).
Recall from the introduction that the configuration space X is described by a graph-structured generative process (e.g. first choose a number of DBN layers, then choose the parameters for each).
The tree-structured Parzen estimator (TPE) models p(x|y) by transforming that generative process, replacing the distributions of the configuration prior with non-parametric densities. In the experimental section, we will see that the configuation space is described using uniform, log-uniform, quantized log-uniform, and categorical variables. In these cases, the TPE algorithm makes the following replacements: uniform → truncated Gaussian mixture, log-uniform → exponentiated truncated Gaussian mixture, categorical → re-weighted categorical. Using different observations
{x(1),..., x(k)} in the non-parametric densities, these substitutions represent a learning algorithm that can produce a variety of densities over the configuration space X. The TPE defines p(x|y) using two such densities: p(x|y) =
�ℓ(x) if y < y∗ g(x) if y ≥ y∗, (2) where ℓ(x) is the density formed by using the observations {x(i)} such that corresponding loss f(x(i)) was less than y∗ and g(x) is the density formed by using the remaining observations.
Whereas the GP-based approach favoured quite an aggressive y∗ (typically less than the best observed loss), the TPE algorithm depends on a y∗ that is larger than the best observed f(x) so that some points can be used to form ℓ(x). The TPE algorithm chooses y∗ to be some quantile γ of the observed y values, so that p(y < y∗) = γ, but no specific model for p(y) is necessary. By maintaining sorted lists of observed variables in H, the runtime of each iteration of the TPE algorithm can scale linearly in |H| and linearly in the number of variables (dimensions) being optimized.
Optimizing EI in the TPE algorithm
The parametrization of p(x, y) as p(y)p(x|y) in the TPE algorithm was chosen to facilitate the optimization of EI.
EIy∗(x) =
� y∗
−∞(y∗ − y)p(y|x)dy =
� y∗
−∞(y∗ − y)p(x|y)p(y) p(x) dy
By construction, γ = p(y < y∗) and p(x) =
�
R p(x|y)p(y)dy = γℓ(x) + (1 − γ)g(x). Therefore
� y∗
−∞(y∗ − y)p(x|y)p(y)dy
= ℓ(x)
� y∗
−∞(y∗ − y)p(y)dy = γy∗ℓ(x) − ℓ(x)
� y∗
−∞ p(y)dy, so that finally EIy∗(x) = γy∗ℓ(x)−ℓ(x)
� y∗
−∞ p(y)dy γℓ(x)+(1−γ)g(x)
∝
� γ + g(x) ℓ(x) (1 − γ)
�−1. This last expression shows that to maximize improvement we would like points x with high probability under ℓ(x) and low probability under g(x). The tree-structured form of ℓ and g makes it easy to draw many candidates according to ℓ and evaluate them according to g(x)/ℓ(x). On each iteration, the algorithm returns the candidate x∗ with the greatest EI.
Details of the Parzen Estimator
The models ℓ(x) and g(x) are hierarchical processes involving discrete-valued and continuousvalued variables. The Adaptive Parzen Estimator yields a model over X by placing density in the vicinity of K observations B = {x(1),..., x(K)} ⊂ H. Each continuous hyper-parameter was specified by a uniform prior over some interval (a, b), or a Gaussian, or a log-uniform distribution.
The TPE substitutes an equally-weighted mixture of that prior with Gaussians centered at each of the x(i) ∈ B. The standard deviation of each Gaussian was set to the greater of the distances to the left and right neighbor, but clipped to remain in a reasonable range. In the case of the uniform, the points a and b were considered to be potential neighbors. For discrete variables, supposing the prior was a vector of N probabilities pi, the posterior vector elements were proportional to Npi + Ci where Ci counts the occurrences of choice i in B. The log-uniform hyper-parameters were treated as uniforms in the log domain.
Table 1: Distribution over DBN hyper-parameters for random sampling. Options separated by "or" such as pre-processing (and including the random seed) are weighted equally. Symbol U means uniform, N means Gaussian-distributed, and log U means uniformly distributed in the log-domain.
CD (also known as CD-1) stands for contrastive divergence, the algorithm used to initialize the layer parameters of the DBN.
Whole model
Per-layer
Parameter
Prior
Parameter
Prior pre-processing raw or ZCA n. hidden units log U(128, 4096)
ZCA energy
U(.5, 1)
W init
U(−a, a) or N(0, a2) random seed
5 choices a algo A or B (see text) classifier learn rate log U(0.001, 10) algo A coef
U(.2, 2) classifier anneal start log U(100, 104)
CD epochs log U(1, 104) classifier ℓ2-penalty
0 or log U(10−7, 10−4)
CD learn rate log U(10−4, 1) n. layers
1 to 3
CD anneal start log U(10, 104) batch size
20 or 100
CD sample data yes or no
Random Search for Hyper-Parameter Optimization in DBNs
One simple, but recent step toward formalizing hyper-parameter optimization is the use of random search. showed that random search was much more efficient than grid search for optimizing the parameters of one-layer neural network classifiers. In this section, we evaluate random search for DBN optimization, compared with the sequential grid-assisted manual search carried out in.
We chose the prior listed in Table 1 to define the search space over DBN configurations. The details of the datasets, the DBN model, and the greedy layer-wise training procedure based on CD are provided in. This prior corresponds to the search space of except for the following differences:(a) we allowed for ZCA pre-processing, (b) we allowed for each layer to have a different size, (c) we allowed for each layer to have its own training parameters for CD, (d) we allowed for the possibility of treating the continuous-valued data as either as Bernoulli means (more theoretically correct) or Bernoulli samples (more typical) in the CD algorithm, and (e) we did not discretize the possible values of real-valued hyper-parameters. These changes expand the hyper-parameter search problem, while maintaining the original hyper-parameter search space as a subset of the expanded search space.
The results of this preliminary random search are in Figure 2. Perhaps surprisingly, the result of manual search can be reliably matched with 32 random trials for several datasets. The efficiency of random search in this setting is explored further in. Where random search results match human performance, it is not clear from Figure 2 whether the reason is that it searched the original space as efficiently, or that it searched a larger space where good performance is easier to find. But the objection that random search is somehow cheating by searching a larger space is backward – the search space outlined in Table 1 is a natural description of the hyper-parameter optimization problem, and the restrictions to that space by were presumably made to simplify the search problem and make it tractable for grid-search assisted manual search. Critically, both methods train
DBNs on the same datasets.
The results in Figure 2 indicate that hyper-parameter optimization is harder for some datasets. For example, in the case of the "MNIST rotated background images" dataset (MRBI), random sampling appears to converge to a maximum relatively quickly (best models among experiments of 32 trials show little variance in performance), but this plateau is lower than what was found by manual search.
In another dataset (convex), the random sampling procedure exceeds the performance of manual search, but is slow to converge to any sort of plateau. There is considerable variance in generalization when the best of 32 models is selected. This slow convergence indicates that better performance is probably available, but we need to search the configuration space more efficiently to find it. The remainder of this paper explores sequential optimization strategies for hyper-parameter optimization for these two datasets: convex and MRBI.
Sequential Search for Hyper-Parameter Optimization in DBNs
We validated our GP approach of Section 3.1 by comparing with random sampling on the Boston
Housing dataset, a regression task with 506 points made of 13 scaled input variables and a scalar
128 experiment size (# trials)
1.0 accuracy mnist basic
128 experiment size (# trials)
0.9 accuracy mnist background images
128 experiment size (# trials)
0.6 accuracy mnist rotated background images
128 experiment size (# trials)
0.85 accuracy convex
128 experiment size (# trials)
1.0 accuracy rectangles
128 experiment size (# trials)
0.80 accuracy rectangles images
Figure 2: Deep Belief Network (DBN) performance according to random search. Random search is used to explore up to 32 hyper-parameters (see Table 1). Results found using a grid-search-assisted manual search over a similar domain with an average 41 trials are given in green (1-layer DBN) and red (3-layer DBN). Each box-plot (for N = 1, 2, 4,...) shows the distribution of test set performance when the best model among N random trials is selected. The datasets "convex" and "mnist rotated background images" are used for more thorough hyper-parameter optimization. regressed output. We trained a Multi-Layer Perceptron (MLP) with 10 hyper-parameters, including learning rate, ℓ1 and ℓ2 penalties, size of hidden layer, number of iterations, whether a PCA preprocessing was to be applied, whose energy was the only conditional hyper-parameter. Our results are depicted in Figure 3. The first 30 iterations were made using random sampling, while from the 30th on, we differentiated the random samples from the GP approach trained on the updated history. The experiment was repeated 20 times. Although the number of points is particularly small compared to the dimensionality, the surrogate modelling approach finds noticeably better points than random, which supports the application of SMBO approaches to more ambitious tasks and datasets.
Applying the GP to the problem of optimizing DBN performance, we allowed 3 random restarts to the CMA+ES algorithm per proposal x∗, and up to 500 iterations of conjugate gradient method in fitting the length scales of the GP. The squared exponential kernel was used for every node.
The CMA-ES part of GPs dealt with boundaries using a penalty method, the binomial sampling part dealt with it by nature. The GP algorithm was initialized with 30 randomly sampled points in H.
After 200 trials, the prediction of a point x∗ using this GP took around 150 seconds.
For the TPE-based algorithm, we chose γ = 0.15 and picked the best among 100 candidates drawn from ℓ(x) on each iteration as the proposal x∗. After 200 trials, the prediction of a point x∗ using this TPE algorithm took around 10 seconds. TPE was allowed to grow past the initial bounds used with for random sampling in the course of optimization, whereas the GP and random search were restricted to stay within the initial bounds throughout the course of optimization. The TPE algorithm was also initialized with the same 30 randomly sampled points as were used to seed the GP.
Parallelizing Sequential Search
Both the GP and TPE approaches were actually run asynchronously in order to make use of multiple compute nodes and to avoid wasting time waiting for trial evaluations to complete. For the GP approach, the so-called constant liar approach was used: each time a candidate point x∗ was proposed, a fake fitness evaluation equal to the mean of the y's within the training set D was assigned temporarily, until the evaluation completed and reported the actual loss f(x∗). For the TPE approach, we simply ignored recently proposed points and relied on the stochasticity of draws from ℓ(x) to provide different candidates from one iteration to the next. The consequence of parallelization is that each proposal x∗ is based on less feedback. This makes search less efficient, though faster in terms of wall time.
Time
Best value so far
Figure 3: After time 30, GP optimizing the MLP hyper-parameters on the Boston
Housing regression task.
Best minimum found so far every 5 iterations, against time. Red = GP, Blue = Random. Shaded areas = one-sigma error bars. convex
MRBI
TPE
14.13 ±0.30 %
44.55 ±0.44%
GP
16.70 ± 0.32%
47.08 ± 0.44%
Manual
18.63 ± 0.34%
47.39 ± 0.44%
Random
18.97 ± 0.34 %
50.52 ± 0.44%
Table 2: The test set classification error of the best model found by each search algorithm on each problem. Each search algorithm was allowed up to 200 trials. The manual searches used 82 trials for convex and 27 trials MRBI.
Runtime per trial was limited to 1 hour of GPU computation regardless of whether execution was on a GTX 285, 470, 480, or 580. The difference in speed between the slowest and fastest machine was roughly two-fold in theory, but the actual efficiency of computation depended also on the load of the machine and the configuration of the problem (the relative speed of the different cards is different in different hyper-parameter configurations). With the parallel evaluation of up to five proposals from the GP and TPE algorithms, each experiment took about 24 hours of wall time using five GPUs.
Discussion
The trajectories (H) constructed by each algorithm up to 200 steps are illustrated in Figure 4, and compared with random search and the manual search carried out in. The generalization scores of the best models found using these algorithms and others are listed in Table 2. On the convex dataset (2-way classification), both algorithms converged to a validation score of 13% error. In generalization, TPE's best model had 14.1% error and GP's best had 16.7%. TPE's best was significantly better than both manual search (19%) and random search with 200 trials (17%). On the MRBI dataset (10-way classification), random search was the worst performer (50% error), the GP approach and manual search approximately tied (47% error), while the TPE algorithm found a new best result (44% error). The models found by the TPE algorithm in particular are better than previously found ones on both datasets. The GP and TPE algorithms were slightly less efficient than manual search: GP and EI identified performance on par with manual search within 80 trials, the manual search of used 82 trials for convex and 27 trials for MRBI.
There are several possible reasons for why the TPE approach outperformed the GP approach in these two datasets. Perhaps the inverse factorization of p(x|y) is more accurate than the p(y|x) in the Gaussian process. Perhaps, conversely, the exploration induced by the TPE's lack of accuracy turned out to be a good heuristic for search. Perhaps the hyper-parameters of the GP approach itself were not set to correctly trade off exploitation and exploration in the DBN configuration space. More empirical work is required to test these hypotheses. Critically though, all four SMBO runs matched or exceeded both random search and a careful human-guided search, which are currently the state of the art methods for hyper-parameter optimization.
The GP and TPE algorithms work well in both of these settings, but there are certainly settings in which these algorithms, and in fact SMBO algorithm in general, would not be expected to do well. Sequential optimization algorithms work by leveraging structure in observed (x, y) pairs. It is possible for SMBO to be arbitrarily bad with a bad choice of p(y|x). It is also possible to be slower than random sampling at finding a global optimum with a apparently good p(y|x), if it extracts structure in H that leads only to a local optimum.
Conclusion
This paper has introduced two sequential hyper-parameter optimization algorithms, and shown them to meet or exceed human performance and the performance of a brute-force random search in two difficult hyper-parameter optimization tasks involving DBNs. We have relaxed standard constraints(e.g. equal layer sizes at all layers) on the search space, and fall back on a more natural hyperparameter space of 32 variables (including both discrete and continuous variables) in which many
200 time(trials)
0.50 error (fraction incorrect)
Dataset: convex manual
99.5'th q.
GP
TPE
200 time(trials)
0.9 error (fraction incorrect)
Dataset: mnist rotated background images manual
99.5'th q.
GP
TPE
Figure 4: Efficiency of Gaussian Process-based (GP) and graphical model-based (TPE) sequential optimization algorithms on the task of optimizing the validation set performance of a DBN of up to three layers on the convex task (left) and the MRBI task (right). The dots are the elements of the trajectory H produced by each SMBO algorithm. The solid coloured lines are the validation set accuracy of the best trial found before each point in time. Both the TPE and GP algorithms make significant advances from their random initial conditions, and substantially outperform the manual and random search methods. A
95% confidence interval about the best validation means on the convex task extends 0.018 above and below each point, and on the MRBI task extends 0.021 above and below each point. The solid black line is the test set accuracy obtained by domain experts using a combination of grid search and manual search. The dashed line is the 99.5% quantile of validation performance found among trials sampled from our prior distribution (see
Table 1), estimated from 457 and 361 random trials on the two datasets respectively. variables are sometimes irrelevant, depending on the value of other parameters (e.g. the number of layers). In this 32-dimensional search problem, the TPE algorithm presented here has uncovered new best results on both of these datasets that are significantly better than what DBNs were previously believed to achieve. Moreover, the GP and TPE algorithms are practical: the optimization for each dataset was done in just 24 hours using five GPU processors. Although our results are only for
DBNs, our methods are quite general, and extend naturally to any hyper-parameter optimization problem in which the hyper-parameters are drawn from a measurable set.
We hope that our work may spur researchers in the machine learning community to treat the hyperparameter optimization strategy as an interesting and important component of all learning algorithms. The question of "How well does a DBN do on the convex task?" is not a fully specified, empirically answerable question – different approaches to hyper-parameter optimization will give different answers. Algorithmic approaches to hyper-parameter optimization make machine learning results easier to disseminate, reproduce, and transfer to other domains. The specific algorithms we have presented here are also capable, at least in some cases, of finding better results than were previously known. Finally, powerful hyper-parameter optimization algorithms broaden the horizon of models that can realistically be studied; researchers need not restrict themselves to systems of a few variables that can readily be tuned by hand.
The TPE algorithm presented in this work, as well as parallel evaluation infrastructure, is available as BSD-licensed free open-source software, which has been designed not only to reproduce the results in this work, but also to facilitate the application of these and similar algorithms to other hyper-parameter optimization problems.1
Acknowledgements
This work was supported by the National Science and Engineering Research Council of Canada, Compute Canada, and by the ANR-2010-COSI-002 grant of the French National Research Agency.
GPU implementations of the DBN model were provided by Theano.
1"Hyperopt" software package: https://github.com/jaberg/hyperopt
References
 H. Larochelle, D. Erhan, A. Courville, J. Bergstra, and Y. Bengio. An empirical evaluation of deep architectures on problems with many factors of variation. In ICML 2007, pages 473–480, 2007.
 G. E. Hinton, S. Osindero, and Y. Teh. A fast learning algorithm for deep belief nets. Neural Computation, 18:1527–1554, 2006.
 P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P. A. Manzagol. Stacked denoising autoencoders:
Learning useful representations in a deep network with a local denoising criterion. Machine Learning
Research, 11:3371–3408, 2010.
 Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition.
Proceedings of the IEEE, 86(11):2278–2324, November 1998.
 Nicolas Pinto, David Doukhan, James J. DiCarlo, and David D. Cox. A high-throughput screening approach to discovering good forms of biologically inspired visual representation.
PLoS Comput Biol, 5(11):e1000579, 11 2009.
 A. Coates, H. Lee, and A. Ng. An analysis of single-layer networks in unsupervised feature learning.
NIPS Deep Learning and Unsupervised Feature Learning Workshop, 2010.
 A. Coates and A. Y. Ng. The importance of encoding versus training with sparse coding and vector quantization. In Proceedings of the Twenty-eighth International Conference on Machine Learning (ICML F. Hutter. Automated Configuration of Algorithms for Solving Hard Computational Problems. PhD thesis, University of British Columbia, 2009.
 F. Hutter, H. Hoos, and K. Leyton-Brown. Sequential model-based optimization for general algorithm configuration. In LION-5, 2011. Extended version as UBC Tech report TR-2010-10.
 D.R. Jones. A taxonomy of global optimization methods based on response surfaces. Journal of Global
Optimization, 21:345–383, 2001.
 J. Villemonteix, E. Vazquez, and E. Walter. An informational approach to the global optimization of expensive-to-evaluate functions. Journal of Global Optimization, 2006.
 N. Srinivas, A. Krause, S. Kakade, and M. Seeger. Gaussian process optimization in the bandit setting:
No regret and experimental design. In ICML, 2010.
 J. Mockus, V. Tiesis, and A. Zilinskas. The application of Bayesian methods for seeking the extremum.
In L.C.W. Dixon and G.P. Szego, editors, Towards Global Optimization, volume 2, pages 117–129. North
Holland, New York, 1978.
 C.E. Rasmussen and C. Williams. Gaussian Processes for Machine Learning.
 D. Ginsbourger, D. Dupuy, A. Badea, L. Carraro, and O. Roustant. A note on the choice and the estimation of kriging models for the analysis of deterministic computer experiments. 25:115–131, 2009.
 R. Bardenet and B. K´egl. Surrogating the surrogate: accelerating Gaussian Process optimization with mixtures. In ICML, 2010.
 P. Larra˜naga and J. Lozano, editors. Estimation of Distribution Algorithms: A New Tool for Evolutionary
Computation. Springer, 2001.
 N. Hansen. The CMA evolution strategy: a comparing review. In J.A. Lozano, P. Larranaga, I. Inza, and E. Bengoetxea, editors, Towards a new evolutionary computation. Advances on estimation of distribution algorithms, pages 75–102. Springer, 2006.
 J. Bergstra and Y. Bengio. Random search for hyper-parameter optimization. The Learning Workshop(Snowbird), 2011.
 A. Hyv¨arinen and E. Oja. Independent component analysis: Algorithms and applications. Neural Networks, 13(4–5):411–430, 2000.
 J. Bergstra and Y. Bengio. Random search for hyper-parameter optimization. JMLR, 2012. Accepted.
 C. Bishop. Neural networks for pattern recognition. 1995.
 J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu, G. Desjardins, J. Turian, and Y. Bengio.
Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientific Computing Conference (SciPy), June 2010.On Optimization Methods for Deep Learning
Quoc V. Le quocle@cs.stanford.edu
Jiquan Ngiam jngiam@cs.stanford.edu
Adam Coates acoates@cs.stanford.edu
Abhik Lahiri alahiri@cs.stanford.edu
Bobby Prochnow prochnow@cs.stanford.edu
Andrew Y. Ng ang@cs.stanford.edu
Computer Science Department, Stanford University, Stanford, CA 94305, USA
Abstract
The predominant methodology in training deep learning advocates the use of stochastic gradient descent methods (SGDs).
Despite its ease of implementation, SGDs are difficult to tune and parallelize. These problems make it challenging to develop, debug and scale up deep learning algorithms with SGDs.
In this paper, we show that more sophisticated off-the-shelf optimization methods such as Limited memory BFGS (L-BFGS) and Conjugate gradient (CG) with line search can significantly simplify and speed up the process of pretraining deep algorithms.
In our experiments, the difference between LBFGS/CG and SGDs are more pronounced if we consider algorithmic extensions (e.g., sparsity regularization) and hardware extensions (e.g., GPUs or computer clusters).
Our experiments with distributed optimization support the use of L-BFGS with locally connected networks and convolutional neural networks. Using L-BFGS, our convolutional network model achieves 0.69% on the standard MNIST dataset. This is a state-of-theart result on MNIST among algorithms that do not use distortions or pretraining.
1. Introduction
Stochastic
Gradient
Descent methods(SGDs) have been extensively employed in machine learning(Bottou, LeCun et al., Shalev-Shwartz et al., Bottou & Bousquet, Appearing in Proceedings of the 28 th International Conference on Machine Learning, Bellevue, WA, USA, 2011.
Copyright 2011 by the author(s)/owner(s).
2008; Zinkevich et al., 2010). A strength of SGDs is that they are simple to implement and also fast for problems that have many training examples.
However, SGD methods have many disadvantages.
One key disadvantage of SGDs is that they require much manual tuning of optimization parameters such as learning rates and convergence criteria. If one does not know the task at hand well, it is very difficult to find a good learning rate or a good convergence criterion. A standard strategy in this case is to run the learning algorithm with many optimization parameters and pick the model that gives the best performance on a validation set. Since one needs to search over the large space of possible optimization parameters, this makes SGDs difficult to train in settings where running the optimization procedure many times is computationally expensive. The second weakness of SGDs is that they are inherently sequential: it is very difficult to parallelize them using GPUs or distribute them using computer clusters.
Batch methods, such as Limited memory BFGS (LBFGS) or Conjugate Gradient (CG), with the presence of a line search procedure, are usually much more stable to train and easier to check for convergence. These methods also enjoy parallelism by computing the gradient on GPUs (Raina et al., 2009) and/or distributing that computation across machines (Chu et al., 2007). These methods, conventionally considered to be slow, can be fast thanks to the availability of large amounts of RAMs, multicore CPUs, GPUs and computer clusters with fast network hardware.
On a single machine, the speed benefits of L-BFGS come from using the approximated second-order information (modelling the interactions between variables). On the other hand, for CG, the benefits come from using conjugacy information during optimization.
Thanks to these bookkeeping steps, L-BFGS and CG
On Optimization Methods for Deep Learning can be faster and more stable than SGDs.
A weakness of batch L-BFGS and CG, which require the computation of the gradient on the entire dataset to make an update, is that they do not scale gracefully with the number of examples.
We found that minibatch training, which requires the computation of the gradient on a small subset of the dataset, addresses this weakness well. We found that minibatch
LBFGS/CG are fast when the dataset is large.
Our experimental results reflect the different strengths and weaknesses of the different optimization methods. Among the problems we considered, L-BFGS is highly competitive or sometimes superior to SGDs/CG for low dimensional problems, especially convolutional models. For high dimensional problems, CG is more competitive and usually outperforms L-BFGS and SGDs. Additionally, using a large minibatch and line search with SGDs can improve performance.
More significant speed improvements of L-BFGS and CG over SGDs are observed in our experiments with sparse autoencoders. This is because having a larger minibatch makes the optimization problem easier for sparse autoencoders: in this case, the cost of estimating the second-order and conjugate information is small compared to the cost of computing the gradient.
Furthermore, when training autoencoders, L-BFGS and CG can be both sped up significantly (2x) by simply performing the computations on GPUs. Conversely, only small speed improvements were observed when SGDs are used with GPUs on the same problem.
We also present results showing that Map-Reduce style optimization works well for L-BFGS when the model utilizes locally connected networks (Le et al., 2010) or convolutional neural networks (LeCun et al., 1998).
Our experimental results show that the speed improvements are close to linear in the number of machines when locally connected networks and convolutional networks are used (up to 8 machines considered in the experiments).
We applied our findings to train a convolutional network model (similar to Ranzato et al. (2007)) with LBFGS on a GPU cluster and obtained 0.69% test set error.
This is the state-of-the-art result on MNIST among algorithms that do not use pretraining or distortions.
Batch optimization is also behind the success of feature learning algorithms that achieve state-of-the-art performance on a variety of object recognition problems (Le et al., 2010; Coates et al., 2011) and action recognition problems (Le et al., 2011).
2. Related work
Optimization research has a long history.
Examples of successful unconstrained optimization methods include Newton-Raphson's method, BFGS methods, Conjugate Gradient methods and Stochastic Gradient
Descent methods. These methods are usually associated with a line search method to ensure that the algorithms consistently improve the objective function.
When it comes to large scale machine learning, the favorite optimization method is usually SGDs.
Recent work on SGDs focuses on adaptive strategies for the learning rate (Shalev-Shwartz et al., 2007;
Bartlett et al., 2008; Do et al., 2009) or improving
SGD convergence by approximating second-order information (Vishwanathan et al., 2007; Bordes et al., 2010). In practice, plain SGDs with constant learning rates or learning rates of the form α β+t are still popular thanks to their ease of implementation.
These simple methods are even more common in deep learning (Hinton, 2010) because the optimization problems are nonconvex and the convergence properties of complex methods (Shalev-Shwartz et al., 2007;
Bartlett et al., 2008; Do et al., 2009) no longer hold.
Recent proposals for training deep networks argue for the use of layerwise pretraining (Hinton et al., 2006;
Bengio et al., 2007; Bengio, 2009). Optimization techniques for training these models include Contrastive
Divergence (Hinton et al., 2006), Conjugate Gradient (Hinton & Salakhutdinov, 2006), stochastic diagonal Levenberg-Marquardt (LeCun et al., 1998) and Hessian-free optimization (Martens, 2010). Convolutional neural networks (LeCun et al., 1998) have traditionally employed SGDs with the stochastic diagonal
Levenberg-Marquardt, which uses a diagonal approximation to the Hessian (LeCun et al., 1998).
In this paper, it is our goal to empirically study the pros and cons of off-the-shelf optimization algorithms in the context of unsupervised feature learning and deep learning. In that direction, we focus on comparing L-BFGS, CG and SGDs.
Parallel optimization methods have recently attracted attention as a way to scale up machine learning algorithms.
Map-Reduce (Dean & Ghemawat, 2008) style optimization methods (Chu et al., 2007;
Teo et al., 2007) have been successful early approaches.
We also note recent studies (Mann et al., 2009; Zinkevich et al., 2010) that have parallelized
SGDs without using the Map-Reduce framework.
In our experiments, we found that if we use tiled (locally connected) networks (Le et al., 2010) (which includes convolutional architectures (LeCun et al., 1998;
On Optimization Methods for Deep Learning
Lee et al., 2009a)), Map-Reduce style parallelism is still an effective mechanism for scaling up.
In such cases, the cost of communicating the parameters across the network is small relative to the cost of computing the objective function value and gradient.
3. Deep learning algorithms
3.1. Restricted Boltzmann Machines
In RBMs (Smolensky, 1986; Hinton et al., 2006), the gradient used in training is an approximation formed by a taking small number of Gibbs sampling steps(Contrastive Divergence). Given the biased nature of the gradient and intractability of the objective function, it is difficult to use any optimization methods other than plain SGDs. For this reason we will not consider RBMs in our experiments.
3.2. Autoencoders and denoising autoencoders
Given an unlabelled dataset {x(i)}m i=1, an autoencoder is a two-layer network that learns nonlinear codes to represent (or "reconstruct") the data.
Specifically, we want to learn representations h(x(i); W, b) = σ(Wx(i) + b) such that σ(W T h(x(i); W, b) + c) is approximately x(i), minimize
W,b,c m
X i=1
'''σ
`
W T σ(Wx(i) + b) + c
´
− x(i)'''
Here, we use the L2 norm to penalize the difference between the reconstruction and the input.
In other studies, when x is binary, the cross entropy cost can also be used (Bengio et al., 2007). Typically, we set the activation function σ to be the sigmoid or hyperbolic tangent function.
Unlike RBMs, the gradient of the autoencoder objective can be computed exactly and this gives rise to an opportunity to use more advanced optimization methods, such as L-BFGS and CG, to train the networks.
Denoising autoencoders (Vincent et al., 2008) are also algorithms that can be trained by L-BFGS/CG.
3.3. Sparse RBMs and Autoencoders
Sparsity regularization typically leads to more interpretable features that perform well for classification.
Sparse coding was first proposed by (Olshausen & Field, 1996) as a model of simple cells in the visual cortex.
Lee et al. (2007); Raina et al.(2007) applied sparse coding to learn features for machine learning applications.
Lee et al. (2008) combined sparsity and RBMs to learn representations that mimic certain properties of the area V2 in the visual cortex.
The key idea in their approach is to penalize the deviation between the expected value of the hidden representations E
� hj(x; W, b)
� and a preferred target activation ρ. By setting ρ to be close to zero, the hidden unit will be sparsely activated.
Sparse representations have been employed successfully in many applications such as object recognition (Ranzato et al., 2007; Lee et al., 2009a;
Nair & Hinton, Yang et al., 2009), speech recognition (Lee et al., 2009b) and activity recognition (Taylor et al., 2010; Le et al., 2011).
Training sparse RBMs is usually difficult.
This is due to the stochastic nature of RBMs.
Specifically, in stochastic mode, the estimate of the expectation
E
� hj(x; W, b)
� is very noisy.
A common practice to train sparse RBMs is to use a running estimate of E
� hj(x; W, b)
� and penalizing only the bias (Lee et al., 2008; Larochelle & Bengio, 2008). This further complicates the optimization procedure and makes it hard to debug the learning algorithm. Moreover, it is important to tune the learning rates correctly for the different parameters W, b and c. Consequently, it can be difficult to train sparse RBMs.
In our experience, it is often faster and simpler to obtain sparse representations via autoencoders with the proposed sparsity penalties, especially when batch or large minibatch optimization methods are used.
In detail, we consider sparse autoencoders with a target activation of ρ and penalize it using the KL divergence (Hinton, 2010): n
X j=1
DKL
" ρ
''' 1 m m
X i=1 hj(x(i); W, b)
", (2) where m is the number of examples and n is the number of hidden units.
To train sparse autoencoders, we need to estimate the expected activation value for each hidden unit. However, we will not be able to compute this statistic unless we run the optimization method in batch mode. In practice, if we have a small dataset, it is better to use a batch method to train a sparse autoencoder because we do not have to tweak optimization parameters, such as minibatch size, λ as described below.
Using a minibatch of size m′
<< m, it is typical to keep a running estimate τ of the expectation
E
� h(x; W, b)
�. In this case, the KL penalty is n
X j=1
DKL
" ρ
'''λ 1 m′ m′
X i=1 hj(x(i); W, b) + (1 − λ)τj
"
On Optimization Methods for Deep Learning where λ is another tunable parameter.
3.4. Tiled and locally connected networks
RBMs and autoencoders have densely-connected network architectures which do not scale well to large images. For large images, the most common approach is to use convolutional neural networks (LeCun et al., 1998; Lee et al., 2009a).
Convolutional neural networks have local receptive field architectures: each hidden unit can only connect to a small region of the image. Translational invariance is usually hardwired by weight tying.
Recent approaches try to relax this constraint (Le et al., 2010) in their tiled convolutional architectures to also learn other invariances (Goodfellow et al., 2010).
Our experimental results show that local architectures, such as tiled convolutional or convolutional architectures, can be efficiently trained with a computer cluster using the Map-Reduce framework. With local architectures, the cost of communicating the gradient over the network is often smaller than the cost of computing it (e.g., cases considered in the experiments).
4. Experiments
4.1. Datasets and computers
Our experiments were carried out on the standard
MNIST dataset. We used up to 8 machines for our experiments; each machine has 4 Intel CPU cores (at
2.67 GHz) and a GeForce GTX 285 GPU. Most experiments below are done on a single machine unless indicated with "parallel."
We performed our experiments using Matlab and its
GPU-plugin Jacket.1
For parallel experiments, we used our custom toolbox that makes remote procedure calls in Matlab and Java.
In the experiments below, we report the standard metric in machine learning: the objective function evaluated on test data (i.e., test error) against time. We note that the objective function evaluated on the training shows similar trends.
4.2. Optimization methods
We are interested in off-the-shelf SGDs, L-BFGS and CG. For SGDs, we used a learning rate schedule of α β+t where t is the iteration number. In our experiments, we found that it is better to use this learning
1http://www.accelereyes.com/ rate schedule than a constant learning rate. We also use momentum, and vary the number of examples used to compute the gradient. In summary, the optimization parameters associated with SGDs are: α, β, momentum parameters (Hinton, 2010) and the number of examples in a minibatch.
We run L-BFGS and CG with a fixed minibatch for several iterations and then resample a new minibatch from the larger training set.
For each new minibatch, we discard the cached optimization history in L-BFGS/CG.
In our settings, for CG and L-BFGS, there are two optimization parameters: minibatch size and number of iterations per minibatch. We use the default values2 for other optimization parameters, such as line search parameters. For CG and LBFGS, we replaced the minibatch after 3 iterations and 20 iterations respectively. We found that these parameters generally work very well for many problems. Therefore, the only remaining tunable parameter is the minibatch size.
4.3. Autoencoder training
We compare L-BFGS, CG against SGDs for training autoencoders. Our autoencoders have 10000 hidden units and the sigmoid activation function (σ). As a result, our model has approximately 8 × 105 parameters, which is considered challenging for high order optimization methods like L-BFGS.3
Figure 1. Autoencoder training with 10000 units on one machine.
2We used LBFGS in minFunc by Mark Schmidt and a CG implementation from Carl Rasmussen. We note that both of these methods are fairly optimized implementations of these algorithms; less sophisticated implementations of these algorithms may perform worse.
3For lower dimensional problems, L-BFGS works much better than other candidates (we omit the results due to space constraints).
On Optimization Methods for Deep Learning
For
L-BFGS, we vary the minibatch size in {1000, 10000}; whereas for CG, we vary the minibatch size in {100, 10000}.
For SGDs, we tried 20 combinations of optimization parameters, including varying the minibatch size in {1, 10, 100, 1000} (when the minibatch is large, this method is also called minibatch
Gradient Descent).
We compared the reconstruction errors on the test set of different optimization methods and summarize the results in Figure 1.
For SGDs, we only report the results for two best parameter settings.
The results show that minibatch L-BFGS and CG with line search converge faster than carefully tuned plain
SGDs. In particular, CG performs better compared to
L-BFGS because computing the conjugate information can be less expensive than estimating the Hessian. CG also performs better than SGDs thanks to both the line search and conjugate information.
To understand how much estimating conjugacy helps
CG, we also performed a control experiment where we tuned (increased) the minibatch size and added a line search procedure to SGDs.
Figure 2. Control experiment with line search for SGDs.
The results are shown in Figure 2 which confirm that having a line search procedure makes SGDs simpler to tune and faster. Using information in the previous steps to form the Hessian approximation (L-BFGS) or conjugate directions (CG) further improves the results.
4.4. Sparse autoencoder training
In this experiment, we trained the autoencoders with the KL sparsity penalty.
The target activation ρ is set to be 10% (a typical value for sparse autoencoders or RBMs). The weighting between the estimate for the current sample and the old estimate (λ) is set to the ratio between the minibatch size m′ and 1000(= min
� m′
�
). This means that our estimates of the hidden unit activations are computed by averaging over at least about 1000 examples.
Figure 3. Sparse autoencoder training with 10000 units, ρ = 0.1, one machine.
We report the performance of different methods in Figure 3. The results show that L-BFGS/CG are much faster than SGDs. The difference, however, is more significant than in the case of standard autoencoders.
This is because L-BFGS and CG prefer larger minibatch size and consequently it is easier to estimate the expected value of the hidden activation. In contrast, SGDs have to deal with a noisy estimate of the hidden activation and we have to set the learning rate parameters to be small to make the algorithm more stable. Interestingly, the line search does not significantly improve SGDs, unlike the previous experiment.
A close inspection of the line search shows that initial step sizes are chosen to be slightly smaller (more conservative) than the tuned step size.
4.5. Training autoencoders with GPUs
Figure 4. Autoencoder training with 10000 units, one machine with GPUs. L-BFGS and CG enjoy a speed up of approximately 2x, while no significant improvement is observed for plain SGDs.
The idea of using GPUs for training deep learning alOn Optimization Methods for Deep Learning gorithms was first proposed in (Raina et al., 2009). In this section, we will consider GPUs and carry out experiments with standard autoencoders to understand how different optimization algorithms perform.
Using the same experimental protocols described in 4.3, we compared optimization methods and their gains in switching from CPUs to GPUs and present the results in Figure 4.
From the figure, the speed up gains are much higher for L-BFGS and CG than
SGDs. This is because L-BFGS and CG prefer larger minibatch sizes which can be parallelized more efficiently on the GPUs.
4.6. Parallel training of dense networks
In this experiment, we explore optimization methods for training autoencoders in a distributed fashion using the Map-Reduce framework (Chu et al., 2007).4
We also used the same settings for all algorithms as mentioned above in Section 4.3.
Our results with training dense autoencoders (omitted due to lack of space) show that parallelizing densely connected networks in this manner can result in slower convergence than running the method on a standalone machine. This can be attributed to the communication costs involve in passing the models and gradients across the network: the parameter vectors have a size of 64Mb, which can be a considerable amount of network traffic since it is frequently communicated.
4.7. Parallel training of local networks
If we use tiled (locally connected) networks (Le et al., 2010), Map-Reduce style gradient computation can be used as an effective way for training. In tiled networks, the number of parameters is small and thus the cost of transferring the gradient across the network can often be smaller than the cost of computing it. Specifically, in this experiment, we constrain each hidden unit to connect to a small section of the image. Furthermore, we do not share any weights across hidden units (no weight tying constraints). We learn 20 feature maps, where each map consists of 441 filters, each of size 8x8.
The results presented in Figure 5 show that SGDs are slower when a computer cluster is used. On the other hand, thanks to its preference of a larger minibatch size, L-BFGS enjoys more significant speed improve4In detail, for parallelized methods, one central machine ("master") runs the optimization procedure while the slaves compute the objective values and gradients. At every step during optimization, the master sends the parameter across all slaves, the slaves then compute the objective function and gradient and send back to the master. ments.5
Figure 5. Parallel training of locally connected networks.
With locally connected networks, the communication cost is reduced significantly.
The inset figure shows the (LBFGS) speed improvement as a function of number of slaves.
The speed up factor is measured by taking the amount of time that requires each method to reach a test objective equal or better than 2.
Also, the figure shows that L-BFGS enjoys an almost linear speed up (up to 8 slave machines considered in the experiments) when locally connected networks are used. On models where the number of parameters is small, L-BFGS's bookkeeping and communication cost are both small compared to gradient computations (which is distributed across the machines).
4.8. Parallel training of supervised CNNs
In this experiment, we compare different optimization methods for supervised training of two-layer convolutional neural networks (CNNs). Specifically, our model has has 16 maps of 5x5 filters in the first layer, followed by (non-overlapping) pooling units that pool over a 3x3 region. The second layer has 16 maps of 4x4 filters, without any pooling units. Additionally, we have a softmax classification layer which is connected to all the output units from the second layer. In this experiment, we distribute the gradient computations across many machines with GPUs.
The experimental results (Figure 4.8) show that LBFGS is better than CG and SGDs on this problem because of low dimensionality (less than 10000 parameters). Map-Reduce style parallelism also significantly improves the performance of both L-BFGS and CG.
5In this experiment, we did not tune the minibatch size, i.e., when we have 4 slaves, the minibatch size per computer is divided by 4. We expect that tuning this minibatch size will improve the results when the number of computers goes up.
On Optimization Methods for Deep Learning
Figure 6. Parallel training of CNNs.
4.9. Classification on standard MNIST
Finally, we carried out experiments to determine if LBFGS affects classification accuracy. We used a convolution network with a first layer having 32 maps of 5x5 filters and 3x3 pooling with subsampling. The second layer had 64 maps of 5x5 filters and 2x2 pooling with subsampling. This architecture is similar to that described in Ranzato et al. (2007), with the following two differences: (i) we did not use an additional hidden layer of 200 hidden units; (ii) the receptive field of our first layer pooling units is slightly larger (for computational reasons).
Table 1. Classification error on MNIST test set for some representative methods without pretraining.
SGDs with diagonal Levenberg-Marquardt are used in (LeCun et al., 1998; Ranzato et al., 2007).
LeNet-5, SGDs, no distortions (LeCun et al., 1998)
LeNet-5, SGDs, huge distortions (LeCun et al., 1998)
LeNet-5, SGDs, distortions (LeCun et al., 1998)
ConvNet, SGDs, no distortions (Ranzato et al., 2007)
ConvNet, L-BFGS, no distortions (this paper)
We trained our network using 4 machines (with
GPUs).
For every epoch, we saved the parameters to disk and used a hold-out validation set of 10000 examples6 to select the best model. The best model is used to make predictions on the test set. The results of our method (ConvNet) using minibatch LBFGS are reported in Table 1.
The results show that the CNN, trained with L-BFGS, achieves an encouraging classification result: 0.69%. We note that this is the best result for MNIST among algorithms that do not use unsupervised pretraining or distortions.
In particular, engineering distortions, typically viewed as a way to introduce domain knowledge, can improve classification results for MNIST. In fact, state-of-the-art results involve more careful dis6We used a reduced training set of 50000 examples throughout the classification experiments. tortion engineering and/or unsupervised pretraining, e.g., 0.4% (Simard et al., 2003), 0.53% (Jarrett et al., 2009), 0.39% (Ciresan et al., 2010).
5. Discussion
In our experiments, different optimization algorithms appear to be superior on different problems. On contrary to what appears to be a widely-held belief, that
SGDs are almost always preferred, we found that LBFGS and CG can be superior to SGDs in many cases.
Among the problems we considered, L-BFGS is a good candidate for optimization for low dimensional problems, where the number of parameters are relatively small (e.g., convolutional neural networks). For high dimensional problems, CG often does well.
Sparsity provides another compelling case for using LBFGS/CG. In our experiments, L-BFGS and CG outperform SGDs on training sparse autoencoders.
We note that there are cases where L-BFGS may not be expected to perform well (e.g., if the Hessian is not well approximated with a low-rank estimate). For instance, on local networks (Le et al., 2010) where the overlaps between receptive fields are small, the Hessian has a block-diagonal structure and L-BFGS, which uses low-rank updates, may not perform well.7 In such cases, algorithms that exploit the problem structures may perform much better.
CG and L-BFGS are also methods that can take better advantage of the GPUs thanks to their preference of larger minibatch sizes. Furthermore, if one uses tiled(locally connected) networks or other networks with a relatively small number of parameters, it is possible to compute the gradients in a Map-Reduce framework and speed up training with L-BFGS.
Acknowledgments:
We thank Andrew Maas, Andrew Saxe, Quinn Slack, Alex Smola and Will Zou for comments and discussions. This work is supported by the DARPA Deep Learning program under contract number FA8650-10-C-7020.
References
Bartlett, P., Hazan, E., and Rakhlin, A. Adaptive online gradient descent. In NIPS, 2008.
Bengio, Y. Learning deep architectures for AI. Foundations and Trends in Machine Learning, pp. 1–127, 2009.
Bengio, Y., Lamblin, P., Popovici, D., and Larochelle, H.
Greedy layerwise training of deep networks. In NIPS, 7Personal communications with Will Zou.
On Optimization Methods for Deep Learning
Bordes, A., Bottou, L., and Gallinari, P. SGD-QN: Careful quasi-newton stochastic gradient descent.
JMLR, pp.
1737–1754, 2010.
Bottou, L. Stochastic gradient learning in neural networks.
In Proceedings of Neuro-Nˆımes 91, 1991.
Bottou, L. and Bousquet, O. The tradeoffs of large scale learning. In NIPS. 2008.
Chu, C.T., Kim, S. K., Lin, Y. A., Yu, Y. Y., Bradski, G., Ng, A. Y., and Olukotun, K. Map-Reduce for machine learning on multicore. In NIPS 19, 2007.
Ciresan, D. C., Meier, U., Gambardella, L. M., and Schmidhuber, J. Deep big simple neural nets excel on handwritten digit recognition. CoRR, 2010.
Coates, A., Lee, H., and Ng, A. Y. An analysis of singlelayer networks in unsupervised feature learning. In AISTATS 14, 2011.
Dean, J. and Ghemawat, S. Map-Reduce: simplified data processing on large clusters. Comm. ACM, pp. 107–113, Do, C.B., Le, Q.V., and Foo, C.S. Proximal regularization for online and batch learning. In ICML, 2009.
Goodfellow, I., Le, Q.V., Saxe, A., Lee, H., and Ng, A.Y.
Measuring invariances in deep networks. In NIPS, 2010.
Hinton, G. A practical guide to training restricted boltzmann machines. Technical report, U. of Toronto, 2010.
Hinton, G. E. and Salakhutdinov, R.R. Reducing the dimensionality of data with neural networks.
Science, Hinton, G. E., Osindero, S., and Teh, Y.W. A fast learning algorithm for deep belief nets. Neu. Comp., 2006.
Jarrett, K., Kavukcuoglu, K., Ranzato, M.A., and LeCun, Y. What is the best multi-stage architecture for object recognition? In ICCV, 2009.
Larochelle, H. and Bengio, Y. Classification using discriminative restricted boltzmann machines. In ICML, 2008.
Le, Q. V., Ngiam, J., Chen, Z., Chia, D., Koh, P. W., and Ng, A. Y. Tiled convolutional neural networks. In NIPS, Le, Q. V., Zou, W., Yeung, S. Y., and Ng, A. Y. Learning hierarchical spatio-temporal features for action recognition with independent subspace analysis. In CVPR, LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient based learning applied to document recognition.
Proceeding of the IEEE, pp. 2278–2323, 1998.
LeCun, Y., Bottou, L., Orr, G., and Muller, K. Efficient backprop. In Neural Networks: Tricks of the trade, pp.
5–50. Springer, 1998.
Lee, H., Battle, A., Raina, R., and Ng, Andrew Y. Efficient sparse coding algorithms. In NIPS, 2007.
Lee, H., Ekanadham, C., and Ng, A. Y. Sparse deep belief net model for visual area V2. In NIPS, 2008.
Lee, H., Grosse, R., Ranganath, R., and Ng, A.Y. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. In ICML, 2009a.
Lee, H., Largman, Y., Pham, P., and Ng, A. Y. Unsupervised feature learning for audioclassification using convolutional deep belief networks. In NIPS, 2009b.
Mann, G., McDonald, R., Mohri, M., Silberman, N., and Walker, D. Efficient large-scale distributed training of conditional maximum entropy models. In NIPS, 2009.
Martens, J. Deep learning via hessian-free optimization.
In ICML, 2010.
Nair, V. and Hinton, G. E.
3D object recognition with deep belief nets. In NIPS, 2009.
Olshausen, B. and Field, D. Emergence of simple-cell receptive field properties by learning a sparse code for natural images. Nature, pp. 607–609, 1996.
Raina, R., Battle, A., Lee, H., Packer, B., and Ng, A.Y.
Self-taught learning: Transfer learning from unlabelled data. In ICML, 2007.
Raina, R., Madhavan, A., and Ng, A. Y.
Large-scale deep unsupervised learning using graphics processors. In
ICML, 2009.
Ranzato, M., Huang, F. J, Boureau, Y., and LeCun, Y. Unsupervised learning of invariant feature hierarchies with applications to object recognition. In CVPR, 2007.
Shalev-Shwartz, S., Singer, Y., and Srebro, N. Pegasos:
Primal estimated sub-gradient solver for svm. In ICML, Simard, P., Steinkraus, D., and Platt, J. Best practices for convolutional neural networks applied to visual document analysis. In ICDAR, 2003.
Smolensky, P.
Information processing in dynamical systems: foundations of harmony theory. In Parallel distributed processing, 1986.
Taylor, G.W., Fergus, R., Lecun, Y., and Bregler, C.
Convolutional learning of spatio-temporal features. In
ECCV, 2010.
Teo, C. H., Le, Q. V., Smola, A. J., and Vishwanathan, S. V. N. A scalable modular convex solver for regularized risk minimization. In KDD, 2007.
Vincent, P., Larochelle, H., Bengio, Y., and Manzagol, P. A. Extracting and composing robust features with denoising autoencoders. In ICML, 2008.
Vishwanathan, S. V. N., Schraudolph, N. N., Schmidt, M. W., and Murphy, K. P. Accelerated training of conditional random fields with stochastic gradient methods.
In ICML, 2007.
Yang, J., Yu, K., Gong, Y., and Huang, T. Linear spatial pyramid matching using sparse coding for image classification. In CVPR, 2009.
Zinkevich, M., Weimer, M., Smola, A., and Li, L. Parallelized stochastic gradient descent. In NIPS, 2010.Deep Learning in Neural Networks: An Overview
Technical Report IDSIA-03-14 / arXiv:1404.7828 v3 [cs.NE]
J¨urgen Schmidhuber
The Swiss AI Lab IDSIA
Istituto Dalle Molle di Studi sull'Intelligenza Artificiale
University of Lugano & SUPSI
Galleria 2, 6928 Manno-Lugano
Switzerland
2 July 2014
Abstract
In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning & evolutionary computation, and indirect search for short programs encoding deep and large networks.
LATEX source file: http://www.idsia.ch/˜juergen/DeepLearning2July2014.tex
Complete BIBTEX file: http://www.idsia.ch/˜juergen/bib.bib
Preface
This is the draft of an invited Deep Learning (DL) overview. One of its goals is to assign credit to those who contributed to the present state of the art. I acknowledge the limitations of attempting to achieve this goal. The DL research community itself may be viewed as a continually evolving, deep network of scientists who have influenced each other in complex ways. Starting from recent DL results, I tried to trace back the origins of relevant ideas through the past half century and beyond, sometimes using
"local search" to follow citations of citations backwards in time. Since not all DL publications properly acknowledge earlier relevant work, additional global search strategies were employed, aided by consulting numerous neural network experts. As a result, the present draft mostly consists of references (866 entries so far). Nevertheless, through an expert selection bias I may have missed important work. A related bias was surely introduced by my special familiarity with the work of my own DL research group in the past quarter-century. For these reasons, the present draft should be viewed as merely a snapshot of an ongoing credit assignment process. To help improve it, please do not hesitate to send corrections and suggestions to juergen@idsia.ch.
Contents
Introduction to Deep Learning (DL) in Neural Networks (NNs)
Event-Oriented Notation for Activation Spreading in FNNs / RNNs
Depth of Credit Assignment Paths (CAPs) and of Problems
Recurring Themes of Deep Learning
Dynamic Programming for Supervised / Reinforcement Learning (SL / RL)........
Unsupervised Learning (UL) Facilitating SL and RL
Learning Hierarchical Representations Through Deep SL, UL, RL.............
Occam's Razor: Compression and Minimum Description Length (MDL)..........
Fast Graphics Processing Units (GPUs) for DL in NNs...................
Supervised NNs, Some Helped by Unsupervised NNs
Early NNs Since the 1940s (and the 1800s)
Around 1960: Visual Cortex Provides Inspiration for DL (Sec. 5.4, 5.11)..........
1965: Deep Networks Based on the Group Method of Data Handling (GMDH)
1979: Convolution + Weight Replication + Subsampling (Neocognitron).........
1960-1981 and Beyond: Development of Backpropagation (BP) for NNs..........
BP for Weight-Sharing Feedforward NNs (FNNs) and Recurrent NNs (RNNs)
Late 1980s-2000 and Beyond: Numerous Improvements of NNs..............
Ideas for Dealing with Long Time Lags and Deep CAPs..............
Better BP Through Advanced Gradient Descent (Compare Sec. 5.24)
Searching For Simple, Low-Complexity, Problem-Solving NNs (Sec. 5.24)....
Potential Benefits of UL for SL (Compare Sec. 5.7, 5.10, 5.15)...........
1987: UL Through Autoencoder (AE) Hierarchies (Compare Sec. 5.15)..........
1989: BP for Convolutional NNs (CNNs, Sec. 5.4)
1991: Fundamental Deep Learning Problem of Gradient Descent..............
5.10 1991: UL-Based History Compression Through a Deep Hierarchy of RNNs........
5.11 1992: Max-Pooling (MP): Towards MPCNNs (Compare Sec. 5.16, 5.19)..........
5.12 1994: Early Contest-Winning NNs..............................
5.13 1995: Supervised Recurrent Very Deep Learner (LSTM RNN)...............
5.14 2003: More Contest-Winning/Record-Setting NNs; Successful Deep NNs.........
5.15 2006/7: UL For Deep Belief Networks (DBNs) / AE Stacks Fine-Tuned by BP
5.16 2006/7: Improved CNNs / GPU-CNNs / BP-Trained MPCNNs / LSTM Stacks
5.17 2009: First Official Competitions Won by RNNs, and with MPCNNs............
5.18 2010: Plain Backprop (+ Distortions) on GPU Breaks MNIST Record...........
5.19 2011: MPCNNs on GPU Achieve Superhuman Vision Performance
5.20 2011: Hessian-Free Optimization for RNNs.........................
5.21 2012: First Contests Won on ImageNet & Object Detection & Segmentation........
5.22 2013-: More Contests and Benchmark Records
5.23 Currently Successful Supervised Techniques: LSTM RNNs / GPU-MPCNNs.......
5.24 Recent Tricks for Improving SL Deep NNs (Compare Sec. 5.6.2, 5.6.3)
5.25 Consequences for Neuroscience
5.26 DL with Spiking Neurons?..................................
DL in FNNs and RNNs for Reinforcement Learning (RL)
RL Through NN World Models Yields RNNs With Deep CAPs...............
Deep FNNs for Traditional RL and Markov Decision Processes (MDPs)..........
Deep RL RNNs for Partially Observable MDPs (POMDPs).................
RL Facilitated by Deep UL in FNNs and RNNs.......................
Deep Hierarchical RL (HRL) and Subgoal Learning with FNNs and RNNs.........
Deep RL by Direct NN Search / Policy Gradients / Evolution................
Deep RL by Indirect Policy Search / Compressed NN Search................
Universal RL
Conclusion
Acknowledgments
Abbreviations in Alphabetical Order
AE: Autoencoder
ANN: Artificial Neural Network
BFGS: Broyden-Fletcher-Goldfarb-Shanno
BNN: Biological Neural Network
BM: Boltzmann Machine
BP: Backpropagation
BRNN: Bi-directional Recurrent Neural Network
CAP: Credit Assignment Path
CEC: Constant Error Carousel
CFL: Context Free Language
CMA-ES: Covariance Matrix Estimation Evolution
Strategies
CNN: Convolutional Neural Network
CoSyNE: Co-Synaptic Neuro-Evolution
CSL: Context Senistive Language
CTC : Connectionist Temporal Classification
DBN: Deep Belief Network
DCT: Discrete Cosine Transform
DL: Deep Learning
DP: Dynamic Programming
DS: Direct Policy Search
EA: Evolutionary Algorithm
EM: Expectation Maximization
FMS: Flat Minimum Search
FNN: Feedforward Neural Network
FSA: Finite State Automaton
GMDH: Group Method of Data Handling
GOFAI: Good Old-Fashioned Artificial Intelligence
GP: Genetic Programming
GPU: Graphics Processing Unit
GPU-MPCNN: GPU-Based Max-Pooling Convolutional Neural Network
HMM: Hidden Markov Model
HRL: Hierarchical Reinforcement Learning
HTM: Hierarchical Temporal Memory
HMAX: Hierarchical Model "and X"
LSTM: Long Short-Term Memory (Recurrent Neural Network)
MC: Multi-Column
MDL: Minimum Description Length
MDP: Markov Decision Process
MNIST: Mixed National Institute of Standards and Technology Database
MP: Max-Pooling
MPCNN: Max-Pooling Convolutional Neural Network
NEAT: NeuroEvolution of Augmenting Topologies
NES: Natural Evolution Strategies
NFQ: Neural Fitted Q-Learning
NN: Neural Network
PCC: Potential Causal Connection
PDCC: Potential Direct Causal Connection
PM: Predictability Minimization
POMDP: Partially Observable Markov Decision
Process
RAAM: Recursive Auto-Associative Memory
RBM: Restricted Boltzmann Machine
ReLU: Rectified Linear Unit
RL: Reinforcement Learning
RNN: Recurrent Neural Network
R-prop: Resilient Backpropagation
SL: Supervised Learning
SLIM NN: Self-Delimiting Neural Network
SOTA: Self-Organising Tree Algorithm
SVM: Support Vector Machine
TDNN: Time-Delay Neural Network
TIMIT: TI/SRI/MIT Acoustic-Phonetic Continuous
Speech Corpus
UL: Unsupervised Learning
WTA: Winner-Take-All
Introduction to Deep Learning (DL) in Neural Networks (NNs)
Which modifiable components of a learning system are responsible for its success or failure? What changes to them improve performance? This has been called the fundamental credit assignment problem (Minsky, 1963). There are general credit assignment methods for universal problem solvers that are time-optimal in various theoretical senses (Sec. 6.8). The present survey, however, will focus on the narrower, but now commercially important, subfield of Deep Learning (DL) in Artificial Neural Networks (NNs). We are interested in accurate credit assignment across possibly many, often nonlinear, computational stages of NNs.
Shallow NN-like models have been around for many decades if not centuries (Sec. 5.1). Models with several successive nonlinear layers of neurons date back at least to the 1960s (Sec. 5.3) and 1970s (Sec. 5.5).
An efficient gradient descent method for teacher-based Supervised Learning (SL) in discrete, differentiable networks of arbitrary depth called backpropagation (BP) was developed in the 1960s and 1970s, and applied to NNs in 1981 (Sec. 5.5). BP-based training of deep NNs with many layers, however, had been found to be difficult in practice by the late 1980s (Sec. 5.6), and had become an explicit research subject by the early 1990s (Sec. 5.9). DL became practically feasible to some extent through the help of Unsupervised
Learning (UL) (e.g., Sec. 5.10, 5.15). The 1990s and 2000s also saw many improvements of purely supervised DL (Sec. 5). In the new millennium, deep NNs have finally attracted wide-spread attention, mainly by outperforming alternative machine learning methods such as kernel machines (Vapnik, 1995; Sch¨olkopf et al., 1998) in numerous important applications. In fact, supervised deep NNs have won numerous official international pattern recognition competitions (e.g., Sec. 5.17, 5.19, 5.21, 5.22), achieving the first superhuman visual pattern recognition results in limited domains (Sec. 5.19). Deep NNs also have become relevant for the more general field of Reinforcement Learning (RL) where there is no supervising teacher(Sec. 6).
Both feedforward (acyclic) NNs (FNNs) and recurrent (cyclic) NNs (RNNs) have won contests (Sec.
5.12, 5.14, 5.17, 5.19, 5.21, 5.22). In a sense, RNNs are the deepest of all NNs (Sec. 3)—they are general computers more powerful than FNNs, and can in principle create and process memories of arbitrary sequences of input patterns (e.g., Siegelmann and Sontag, 1991; Schmidhuber, 1990a). Unlike traditional methods for automatic sequential program synthesis (e.g., Waldinger and Lee, 1969; Balzer, 1985; Soloway, 1986; Deville and Lau, 1994), RNNs can learn programs that mix sequential and parallel information processing in a natural and efficient way, exploiting the massive parallelism viewed as crucial for sustaining the rapid decline of computation cost observed over the past 75 years.
The rest of this paper is structured as follows. Sec. 2 introduces a compact, event-oriented notation that is simple yet general enough to accommodate both FNNs and RNNs. Sec. 3 introduces the concept of Credit Assignment Paths (CAPs) to measure whether learning in a given NN application is of the deep or shallow type. Sec. 4 lists recurring themes of DL in SL, UL, and RL. Sec. 5 focuses on SL and UL, and on how UL can facilitate SL, although pure SL has become dominant in recent competitions (Sec. 5.17–5.23).
Sec. 5 is arranged in a historical timeline format with subsections on important inspirations and technical contributions. Sec. 6 on deep RL discusses traditional Dynamic Programming (DP)-based RL combined with gradient-based search techniques for SL or UL in deep NNs, as well as general methods for direct and indirect search in the weight space of deep FNNs and RNNs, including successful policy gradient and evolutionary methods.
Event-Oriented Notation for Activation Spreading in FNNs / RNNs
Throughout this paper, let i, j, k, t, p, q, r denote positive integer variables assuming ranges implicit in the given contexts. Let n, m, T denote positive integer constants.
An NN's topology may change over time (e.g., Sec. 5.3, 5.6.3). At any given moment, it can be described as a finite subset of units (or nodes or neurons) N = {u1, u2,..., } and a finite set H ⊆ N × N of directed edges or connections between nodes. FNNs are acyclic graphs, RNNs cyclic. The first (input) layer is the set of input units, a subset of N. In FNNs, the k-th layer (k > 1) is the set of all nodes u ∈ N such that there is an edge path of length k − 1 (but no longer path) between some input unit and u. There may be shortcut connections between distant layers.
The NN's behavior or program is determined by a set of real-valued, possibly modifiable, parameters or weights wi (i = 1,..., n). We now focus on a single finite episode or epoch of information processing and activation spreading, without learning through weight changes. The following slightly unconventional notation is designed to compactly describe what is happening during the runtime of the system.
During an episode, there is a partially causal sequence xt(t = 1,..., T) of real values that I call events. Each xt is either an input set by the environment, or the activation of a unit that may directly depend on other xk(k < t) through a current NN topology-dependent set int of indices k representing incoming causal connections or links. Let the function v encode topology information and map such event index pairs (k, t) to weight indices. For example, in the non-input case we may have xt = ft(nett) with real-valued nett = � k∈int xkwv(k,t) (additive case) or nett = � k∈int xkwv(k,t) (multiplicative case), where ft is a typically nonlinear real-valued activation function such as tanh. In many recent competitionwinning NNs (Sec. 5.19, 5.21, 5.22) there also are events of the type xt = maxk∈int(xk); some network types may also use complex polynomial activation functions (Sec. 5.3). xt may directly affect certain xk(k > t) through outgoing connections or links represented through a current set outt of indices k with t ∈ ink. Some of the non-input events are called output events.
Note that many of the xt may refer to different, time-varying activations of the same unit in sequenceprocessing RNNs (e.g., Williams, 1989, "unfolding in time"), or also in FNNs sequentially exposed to time-varying input patterns of a large training set encoded as input events. During an episode, the same weight may get reused over and over again in topology-dependent ways, e.g., in RNNs, or in convolutional
NNs (Sec. 5.4, 5.8). I call this weight sharing across space and/or time. Weight sharing may greatly reduce the NN's descriptive complexity, which is the number of bits of information required to describe the NN(Sec. 4.4).
In Supervised Learning (SL), certain NN output events xt may be associated with teacher-given, realvalued labels or targets dt yielding errors et, e.g., et = 1/2(xt − dt)2. A typical goal of supervised NN training is to find weights that yield episodes with small total error E, the sum of all such et. The hope is that the NN will generalize well in later episodes, causing only small errors on previously unseen sequences of input events. Many alternative error functions for SL and UL are possible.
SL assumes that input events are independent of earlier output events (which may affect the environment through actions causing subsequent perceptions). This assumption does not hold in the broader fields of Sequential Decision Making and Reinforcement Learning (RL) (Kaelbling et al., 1996; Sutton and Barto, 1998; Hutter, 2005; Wiering and van Otterlo, 2012) (Sec. 6). In RL, some of the input events may encode real-valued reward signals given by the environment, and a typical goal is to find weights that yield episodes with a high sum of reward signals, through sequences of appropriate output actions.
Sec. 5.5 will use the notation above to compactly describe a central algorithm of DL, namely, backpropagation (BP) for supervised weight-sharing FNNs and RNNs. (FNNs may be viewed as RNNs with certain fixed zero weights.) Sec. 6 will address the more general RL case.
Depth of Credit Assignment Paths (CAPs) and of Problems
To measure whether credit assignment in a given NN application is of the deep or shallow type, I introduce the concept of Credit Assignment Paths or CAPs, which are chains of possibly causal links between events.
Let us first focus on SL. Consider two events xp and xq (1 ≤ p < q ≤ T). Depending on the application, they may have a Potential Direct Causal Connection (PDCC) expressed by the Boolean predicate pdcc(p, q), which is true if and only if p ∈ inq. Then the 2-element list (p, q) is defined to be a CAP (a minimal one) from p to q. A learning algorithm may be allowed to change wv(p,q) to improve performance in future episodes.
More general, possibly indirect, Potential Causal Connections (PCC) are expressed by the recursively defined Boolean predicate pcc(p, q), which in the SL case is true only if pdcc(p, q), or if pcc(p, k) for some k and pdcc(k, q). In the latter case, appending q to any CAP from p to k yields a CAP from p to q (this is a recursive definition, too). The set of such CAPs may be large but is finite. Note that the same weight may affect many different PDCCs between successive events listed by a given CAP, e.g., in the case of RNNs, or weight-sharing FNNs.
Suppose a CAP has the form (..., k, t,..., q), where k and t (possibly t = q) are the first successive elements with modifiable wv(k,t). Then the length of the suffix list (t,..., q) is called the CAP's depth(which is 0 if there are no modifiable links at all). This depth limits how far backwards credit assignment can move down the causal chain to find a modifiable weight.1
Suppose an episode and its event sequence x1,..., xT satisfy a computable criterion used to decide whether a given problem has been solved (e.g., total error E below some threshold). Then the set of used weights is called a solution to the problem, and the depth of the deepest CAP within the sequence is called the solution depth. There may be other solutions (yielding different event sequences) with different depths.
Given some fixed NN topology, the smallest depth of any solution is called the problem depth.
Sometimes we also speak of the depth of an architecture: SL FNNs with fixed topology imply a problem-independent maximal problem depth bounded by the number of non-input layers. Certain SL
RNNs with fixed weights for all connections except those to output units (Jaeger, 2001; Maass et al., 2002;
Jaeger, 2004; Schrauwen et al., 2007) have a maximal problem depth of 1, because only the final links in the corresponding CAPs are modifiable. In general, however, RNNs may learn to solve problems of potentially unlimited depth.
Note that the definitions above are solely based on the depths of causal chains, and agnostic to the temporal distance between events. For example, shallow FNNs perceiving large "time windows" of input events may correctly classify long input sequences through appropriate output events, and thus solve shallow problems involving long time lags between relevant events.
At which problem depth does Shallow Learning end, and Deep Learning begin? Discussions with DL experts have not yet yielded a conclusive response to this question. Instead of committing myself to a precise answer, let me just define for the purposes of this overview: problems of depth > 10 require Very
Deep Learning.
The difficulty of a problem may have little to do with its depth. Some NNs can quickly learn to solve certain deep problems, e.g., through random weight guessing (Sec. 5.9) or other types of direct search(Sec. 6.6) or indirect search (Sec. 6.7) in weight space, or through training an NN first on shallow problems whose solutions may then generalize to deep problems, or through collapsing sequences of (non)linear operations into a single (non)linear operation (but see an analysis of non-trivial aspects of deep linear networks, Baldi and Hornik, 1994, Section B). In general, however, finding an NN that precisely models a given training set is an NP-complete problem (Judd, 1990; Blum and Rivest, 1992), also in the case of deep NNs (S´ıma, 1994; de Souto et al., 1999; Windisch, 2005); compare a survey of negative results (S´ıma, 2002, Section 1).
Above we have focused on SL. In the more general case of RL in unknown environments, pcc(p, q) is also true if xp is an output event and xq any later input event—any action may affect the environment and thus any later perception. (In the real world, the environment may even influence non-input events computed on a physical hardware entangled with the entire universe, but this is ignored here.) It is possible to model and replace such unmodifiable environmental PCCs through a part of the NN that has already learned to predict (through some of its units) input events (including reward signals) from former input events and actions (Sec. 6.1). Its weights are frozen, but can help to assign credit to other, still modifiable weights used to compute actions (Sec. 6.1). This approach may lead to very deep CAPs though.
Some DL research is about automatically rephrasing problems such that their depth is reduced (Sec. 4).
In particular, sometimes UL is used to make SL problems less deep, e.g., Sec. 5.10. Often Dynamic
Programming (Sec. 4.1) is used to facilitate certain traditional RL problems, e.g., Sec. 6.2. Sec. 5 focuses on CAPs for SL, Sec. 6 on the more complex case of RL.
Recurring Themes of Deep Learning
Dynamic Programming for Supervised / Reinforcement Learning (SL / RL)
One recurring theme of DL is Dynamic Programming (DP) (Bellman, 1957), which can help to facilitate credit assignment under certain assumptions. For example, in SL NNs, backpropagation itself can
1An alternative would be to count only modifiable links when measuring depth. In many typical NN applications this would not make a difference, but in some it would, e.g., Sec. 6.1.
6 be viewed as a DP-derived method (Sec. 5.5). In traditional RL based on strong Markovian assumptions, DP-derived methods can help to greatly reduce problem depth (Sec. 6.2). DP algorithms are also essential for systems that combine concepts of NNs and graphical models, such as Hidden Markov Models(HMMs) (Stratonovich, 1960; Baum and Petrie, 1966) and Expectation Maximization (EM) (Dempster et al., 1977; Friedman et al., 2001), e.g., (Bottou, 1991; Bengio, 1991; Bourlard and Morgan, 1994; Baldi and Chauvin, 1996; Jordan and Sejnowski, 2001; Bishop, 2006; Hastie et al., 2009; Poon and Domingos, 2011; Dahl et al., 2012; Hinton et al., 2012a; Wu and Shao, 2014).
Unsupervised Learning (UL) Facilitating SL and RL
Another recurring theme is how UL can facilitate both SL (Sec. 5) and RL (Sec. 6). UL (Sec. 5.6.4) is normally used to encode raw incoming data such as video or speech streams in a form that is more convenient for subsequent goal-directed learning. In particular, codes that describe the original data in a less redundant or more compact way can be fed into SL (Sec. 5.10, 5.15) or RL machines (Sec. 6.4), whose search spaces may thus become smaller (and whose CAPs shallower) than those necessary for dealing with the raw data. UL is closely connected to the topics of regularization and compression (Sec. 4.4, 5.6.3).
Learning Hierarchical Representations Through Deep SL, UL, RL
Many methods of Good Old-Fashioned Artificial Intelligence (GOFAI) (Nilsson, 1980) as well as more recent approaches to AI (Russell et al., 1995) and Machine Learning (Mitchell, 1997) learn hierarchies of more and more abstract data representations. For example, certain methods of syntactic pattern recognition (Fu, 1977) such as grammar induction discover hierarchies of formal rules to model observations.
The partially (un)supervised Automated Mathematician / EURISKO (Lenat, 1983; Lenat and Brown, 1984) continually learns concepts by combining previously learnt concepts. Such hierarchical representation learning (Ring, 1994; Bengio et al., 2013; Deng and Yu, 2014) is also a recurring theme of DL NNs for SL(Sec. 5), UL-aided SL (Sec. 5.7, 5.10, 5.15), and hierarchical RL (Sec. 6.5). Often, abstract hierarchical representations are natural by-products of data compression (Sec. 4.4), e.g., Sec. 5.10.
Occam's Razor: Compression and Minimum Description Length (MDL)
Occam's razor favors simple solutions over complex ones. Given some programming language, the principle of Minimum Description Length (MDL) can be used to measure the complexity of a solution candidate by the length of the shortest program that computes it (e.g., Solomonoff, 1964; Kolmogorov, 1965b;
Chaitin, 1966; Wallace and Boulton, 1968; Levin, 1973a; Solomonoff, 1978; Rissanen, 1986; Blumer et al., 1987; Li and Vit´anyi, 1997; Gr¨unwald et al., 2005). Some methods explicitly take into account program runtime (Allender, 1992; Watanabe, 1992; Schmidhuber, 1997, 2002); many consider only programs with constant runtime, written in non-universal programming languages (e.g., Rissanen, 1986; Hinton and van
Camp, 1993). In the NN case, the MDL principle suggests that low NN weight complexity corresponds to high NN probability in the Bayesian view (e.g., MacKay, 1992; Buntine and Weigend, 1991; Neal, 1995; De Freitas, 2003), and to high generalization performance (e.g., Baum and Haussler, 1989), without overfitting the training data. Many methods have been proposed for regularizing NNs, that is, searching for solution-computing but simple, low-complexity SL NNs (Sec. 5.6.3) and RL NNs (Sec. 6.7). This is closely related to certain UL methods (Sec. 4.2, 5.6.4).
Fast Graphics Processing Units (GPUs) for DL in NNs
While the previous millennium saw several attempts at creating fast NN-specific hardware (e.g., Jackel et al., 1990; Faggin, 1992; Ramacher et al., 1993; Widrow et al., 1994; Heemskerk, 1995; Korkin et al., 1997; Urlbe, 1999), and at exploiting standard hardware (e.g., Anguita et al., 1994; Muller et al., 1995;
Anguita and Gomes, 1996), the new millennium brought a DL breakthrough in form of cheap, multiprocessor graphics cards or GPUs. GPUs are widely used for video games, a huge and competitive market that has driven down hardware prices. GPUs excel at the fast matrix and vector multiplications required not only for convincing virtual realities but also for NN training, where they can speed up learning by a factor
7 of 50 and more. Some of the GPU-based FNN implementations (Sec. 5.16–5.19) have greatly contributed to recent successes in contests for pattern recognition (Sec. 5.19–5.22), image segmentation (Sec. 5.21), and object detection (Sec. 5.21–5.22).
Supervised NNs, Some Helped by Unsupervised NNs
The main focus of current practical applications is on Supervised Learning (SL), which has dominated recent pattern recognition contests (Sec. 5.17–5.23). Several methods, however, use additional Unsupervised
Learning (UL) to facilitate SL (Sec. 5.7, 5.10, 5.15). It does make sense to treat SL and UL in the same section: often gradient-based methods, such as BP (Sec. 5.5.1), are used to optimize objective functions of both UL and SL, and the boundary between SL and UL may blur, for example, when it comes to time series prediction and sequence classification, e.g., Sec. 5.10, 5.12.
A historical timeline format will help to arrange subsections on important inspirations and technical contributions (although such a subsection may span a time interval of many years). Sec. 5.1 briefly mentions early, shallow NN models since the 1940s (and 1800s), Sec. 5.2 additional early neurobiological inspiration relevant for modern Deep Learning (DL). Sec. 5.3 is about GMDH networks (since 1965), to my knowledge the first (feedforward) DL systems. Sec. 5.4 is about the relatively deep Neocognitron
NN (1979) which is very similar to certain modern deep FNN architectures, as it combines convolutional
NNs (CNNs), weight pattern replication, and subsampling mechanisms. Sec. 5.5 uses the notation of Sec. 2 to compactly describe a central algorithm of DL, namely, backpropagation (BP) for supervised weight-sharing FNNs and RNNs. It also summarizes the history of BP 1960-1981 and beyond. Sec. 5.6 describes problems encountered in the late 1980s with BP for deep NNs, and mentions several ideas from the previous millennium to overcome them. Sec. 5.7 discusses a first hierarchical stack (1987) of coupled
UL-based Autoencoders (AEs)—this concept resurfaced in the new millennium (Sec. 5.15). Sec. 5.8 is about applying BP to CNNs (1989), which is important for today's DL applications. Sec. 5.9 explains
BP's Fundamental DL Problem (of vanishing/exploding gradients) discovered in 1991. Sec. 5.10 explains how a deep RNN stack of 1991 (the History Compressor) pre-trained by UL helped to solve previously unlearnable DL benchmarks requiring Credit Assignment Paths (CAPs, Sec. 3) of depth 1000 and more.
Sec. 5.11 discusses a particular winner-take-all (WTA) method called Max-Pooling (MP, 1992) widely used in today's deep FNNs. Sec. 5.12 mentions a first important contest won by SL NNs in 1994. Sec. 5.13 describes a purely supervised DL RNN (Long Short-Term Memory, LSTM, 1995) for problems of depth
1000 and more. Sec. 5.14 mentions an early contest of 2003 won by an ensemble of shallow FNNs, as well as good pattern recognition results with CNNs and deep FNNs and LSTM RNNs (2003). Sec. 5.15 is mostly about Deep Belief Networks (DBNs, 2006) and related stacks of Autoencoders (AEs, Sec. 5.7), both pre-trained by UL to facilitate subsequent BP-based SL (compare Sec. 5.6.1). Sec. 5.16 mentions the first SL-based GPU-CNNs (2006), BP-trained MPCNNs (2007), and LSTM stacks (2007). Sec. 5.17–
5.22 focus on official competitions with secret test sets won by (mostly purely supervised) deep NNs since
2009, in sequence recognition, image classification, image segmentation, and object detection. Many RNN results depended on LSTM (Sec. 5.13); many FNN results depended on GPU-based FNN code developed since 2004 (Sec. 5.16, 5.17, 5.18, 5.19), in particular, GPU-MPCNNs (2011, Sec. 5.19). Sec. 5.24 mentions recent tricks for improving DL in NNs, many of them closely related to earlier tricks from the previous millennium (e.g., Sec. 5.6.2, 5.6.3). Sec. 5.25 discusses how artificial NNs can help to understand biological
NNs; Sec. 5.26 addresses the possibility of DL in NNs with spiking neurons.
Early NNs Since the 1940s (and the 1800s)
Early NN architectures (McCulloch and Pitts, 1943) did not learn. The first ideas about UL were published a few years later (Hebb, 1949). The following decades brought simple NNs trained by SL (e.g., Rosenblatt, 1958, 1962; Widrow and Hoff, 1962; Narendra and Thathatchar, 1974) and UL (e.g., Grossberg, 1969;
Kohonen, 1972; von der Malsburg, 1973; Willshaw and von der Malsburg, 1976), as well as closely related associative memories (e.g., Palm, 1980; Hopfield, 1982).
In a sense NNs have been around even longer, since early supervised NNs were essentially variants of linear regression methods going back at least to the early 1800s (e.g., Legendre, 1805; Gauss, 1809, 1821);
Gauss also refers to his work of 1795. Early NNs had a maximal CAP depth of 1 (Sec. 3).
Around 1960: Visual Cortex Provides Inspiration for DL (Sec. 5.4, 5.11)
Simple cells and complex cells were found in the cat's visual cortex (e.g., Hubel and Wiesel, 1962; Wiesel and Hubel, 1959). These cells fire in response to certain properties of visual sensory inputs, such as the orientation of edges. Complex cells exhibit more spatial invariance than simple cells. This inspired later deep NN architectures (Sec. 5.4, 5.11) used in certain modern award-winning Deep Learners (Sec. 5.19–
1965: Deep Networks Based on the Group Method of Data Handling (GMDH)
Networks trained by the Group Method of Data Handling (GMDH) (Ivakhnenko and Lapa, 1965;
Ivakhnenko et al., 1967; Ivakhnenko, 1968, 1971) were perhaps the first DL systems of the Feedforward
Multilayer Perceptron type. The units of GMDH nets may have polynomial activation functions implementing Kolmogorov-Gabor polynomials (more general than other widely used NN activation functions, Sec. 2). Given a training set, layers are incrementally grown and trained by regression analysis (e.g., Legendre, 1805; Gauss, 1809, 1821) (Sec. 5.1), then pruned with the help of a separate validation set (using today's terminology), where Decision Regularisation is used to weed out superfluous units (compare
Sec. 5.6.3). The numbers of layers and units per layer can be learned in problem-dependent fashion. To my knowledge, this was the first example of hierarchical representation learning in NNs (Sec. 4.3). A paper of 1971 already described a deep GMDH network with 8 layers (Ivakhnenko, 1971). There have been numerous applications of GMDH-style nets, e.g. (Ikeda et al., 1976; Farlow, 1984; Madala and Ivakhnenko, 1994; Ivakhnenko, 1995; Kondo, 1998; Kord´ık et al., 2003; Witczak et al., 2006; Kondo and Ueno, 2008).
1979: Convolution + Weight Replication + Subsampling (Neocognitron)
Apart from deep GMDH networks (Sec. 5.3), the Neocognitron (Fukushima, 1979, 1980, 2013a) was perhaps the first artificial NN that deserved the attribute deep, and the first to incorporate the neurophysiological insights of Sec. 5.2. It introduced convolutional NNs (today often called CNNs or convnets), where the (typically rectangular) receptive field of a convolutional unit with given weight vector is shifted step by step across a 2-dimensional array of input values, such as the pixels of an image. The resulting 2D array of subsequent activation events of this unit can then provide inputs to higher-level units, and so on. Due to massive weight replication (Sec. 2), relatively few parameters (Sec. 4.4) may be necessary to describe the behavior of such a convolutional layer.
Subsampling or downsampling layers consist of units whose fixed-weight connections originate from physical neighbours in the convolutional layers below. Subsampling units become active if at least one of their inputs is active; their responses are insensitive to certain small image shifts (compare Sec. 5.2).
The Neocognitron is very similar to the architecture of modern, contest-winning, purely supervised, feedforward, gradient-based Deep Learners with alternating convolutional and downsampling layers (e.g., Sec. 5.19–5.22). Fukushima, however, did not set the weights by supervised backpropagation(Sec. 5.5, 5.8), but by local, WTA-based unsupervised learning rules (e.g., Fukushima, 2013b), or by prewiring. In that sense he did not care for the DL problem (Sec. 5.9), although his architecture was comparatively deep indeed. For downsampling purposes he used Spatial Averaging (Fukushima, 1980, 2011) instead of Max-Pooling (MP, Sec. 5.11), currently a particularly convenient and popular WTA mechanism.
Today's CNN-based DL machines also profit a lot from later CNN work (e.g., Sec. 5.8, 5.16, 5.16, 5.19).
1960-1981 and Beyond: Development of Backpropagation (BP) for NNs
The minimisation of errors through gradient descent (Hadamard, 1908) in the parameter space of complex, nonlinear, differentiable (Leibniz, 1684), multi-stage, NN-related systems has been discussed at least since the early 1960s (e.g., Kelley, 1960; Bryson, 1961; Bryson and Denham, 1961; Pontryagin et al., 1961;
Dreyfus, 1962; Wilkinson, 1965; Amari, 1967; Bryson and Ho, 1969; Director and Rohrer, 1969), initially within the framework of Euler-LaGrange equations in the Calculus of Variations (e.g., Euler, 1744).
Steepest descent in the weight space of such systems can be performed (Bryson, 1961; Kelley, 1960;
Bryson and Ho, 1969) by iterating the ancient chain rule (Leibniz, 1676; L'Hˆopital, 1696) in Dynamic
Programming (DP) style (Bellman, 1957). A simplified derivation of this backpropagation method uses the chain rule only (Dreyfus, 1962).
The systems of the 1960s were already efficient in the DP sense. However, they backpropagated derivative information through standard Jacobian matrix calculations from one "layer" to the previous one, explicitly addressing neither direct links across several layers nor potential additional efficiency gains due to network sparsity (but perhaps such enhancements seemed obvious to the authors). Given all the prior work on learning in multilayer NN-like systems (see also Sec. 5.3 on deep nonlinear nets since 1965), it seems surprising in hindsight that a book (Minsky and Papert, 1969) on the limitations of simple linear perceptrons with a single layer (Sec. 5.1) discouraged some researchers from further studying NNs.
Explicit, efficient error backpropagation (BP) in arbitrary, discrete, possibly sparsely connected, NNlike networks apparently was first described in a 1970 master's thesis (Linnainmaa, 1970, 1976), albeit without reference to NNs. BP is also known as the reverse mode of automatic differentiation (Griewank, 2012), where the costs of forward activation spreading essentially equal the costs of backward derivative calculation. See early FORTRAN code (Linnainmaa, 1970) and closely related work (Ostrovskii et al., 1971). Efficient BP was soon explicitly used to minimize cost functions by adapting control parameters(weights) (Dreyfus, 1973). Compare some preliminary, NN-specific discussion (Werbos, 1974, section
5.5.1), a method for multilayer threshold NNs (Bobrowski, 1978), and a computer program for automatically deriving and implementing BP for given differentiable systems (Speelpenning, 1980).
To my knowledge, the first NN-specific application of efficient BP as above was described in 1981 (Werbos, 1981, 2006). Related work was published several years later (Parker, 1985; LeCun, 1985, 1988). A paper of 1986 significantly contributed to the popularisation of BP (Rumelhart et al., 1986). See generalisations for sequence-processing recurrent NNs (e.g., Williams, 1989; Robinson and Fallside, 1987;
Werbos, 1988; Williams and Zipser, 1988, 1989b,a; Rohwer, 1989; Pearlmutter, 1989; Gherrity, 1989;
Williams and Peng, 1990; Schmidhuber, 1992a; Pearlmutter, 1995; Baldi, 1995; Kremer and Kolen, 2001;
Atiya and Parlos, 2000), also for equilibrium RNNs (Almeida, 1987; Pineda, 1987) with stationary inputs.
BP for Weight-Sharing Feedforward NNs (FNNs) and Recurrent NNs (RNNs)
Using the notation of Sec. 2 for weight-sharing FNNs or RNNs, after an episode of activation spreading through differentiable ft, a single iteration of gradient descent through BP computes changes of all wi in proportion to ∂E
∂wi = � t
∂E
∂nett
∂nett
∂wi as in Algorithm 5.5.1 (for the additive case), where each weight wi is associated with a real-valued variable △i initialized by 0.
Alg. 5.5.1: One iteration of BP for weight-sharing FNNs or RNNs for t = T,..., 1 do to compute
∂E
∂nett, inititalize real-valued error signal variable δt by 0; if xt is an input event then continue with next iteration; if there is an error et then δt := xt − dt; add to δt the value � k∈outt wv(t,k)δk; (this is the elegant and efficient recursive chain rule application collecting impacts of nett on future events) multiply δt by f ′ t(nett); for all k ∈ int add to △wv(k,t) the value xkδt end for change each wi in proportion to △i and a small real-valued learning rate
The computational costs of the backward (BP) pass are essentially those of the forward pass (Sec. 2).
Forward and backward passes are re-iterated until sufficient performance is reached.
As of 2014, this simple BP method is still the central learning algorithm for FNNs and RNNs. Notably, most contest-winning NNs up to 2014 (Sec. 5.12, 5.14, 5.17, 5.19, 5.21, 5.22) did not augment supervised
BP by some sort of unsupervised learning as discussed in Sec. 5.7, 5.10, 5.15.
Late 1980s-2000 and Beyond: Numerous Improvements of NNs
By the late 1980s it seemed clear that BP by itself (Sec. 5.5) was no panacea. Most FNN applications focused on FNNs with few hidden layers. Additional hidden layers often did not seem to offer empirical benefits. Many practitioners found solace in a theorem (Kolmogorov, 1965a; Hecht-Nielsen, 1989; Hornik et al., 1989) stating that an NN with a single layer of enough hidden units can approximate any multivariate continous function with arbitrary accuracy.
Likewise, most RNN applications did not require backpropagating errors far. Many researchers helped their RNNs by first training them on shallow problems (Sec. 3) whose solutions then generalized to deeper problems.
In fact, some popular RNN algorithms restricted credit assignment to a single step backwards (Elman, 1990; Jordan, 1986, 1997), also in more recent studies (Jaeger, 2002; Maass et al., 2002;
Jaeger, 2004).
Generally speaking, although BP allows for deep problems in principle, it seemed to work only for shallow problems. The late 1980s and early 1990s saw a few ideas with a potential to overcome this problem, which was fully understood only in 1991 (Sec. 5.9).
Ideas for Dealing with Long Time Lags and Deep CAPs
To deal with long time lags between relevant events, several sequence processing methods were proposed, including Focused BP based on decay factors for activations of units in RNNs (Mozer, 1989, 1992), TimeDelay Neural Networks (TDNNs) (Lang et al., 1990) and their adaptive extension (Bodenhausen and Waibel, 1991), Nonlinear AutoRegressive with eXogenous inputs (NARX) RNNs (Lin et al., 1996), certain hierarchical RNNs (Hihi and Bengio, 1996) (compare Sec. 5.10), RL economies in RNNs with WTA units and local learning rules (Schmidhuber, 1989b), and other methods (e.g., Ring, 1993, 1994; Plate, 1993; de Vries and Principe, 1991; Sun et al., 1993a; Bengio et al., 1994). However, these algorithms either worked for shallow CAPs only, could not generalize to unseen CAP depths, had problems with greatly varying time lags between relevant events, needed external fine tuning of delay constants, or suffered from other problems. In fact, it turned out that certain simple but deep benchmark problems used to evaluate such methods are more quickly solved by randomly guessing RNN weights until a solution is found (Hochreiter and Schmidhuber, 1996).
While the RNN methods above were designed for DL of temporal sequences, the Neural Heat Exchanger (Schmidhuber, 1990c) consists of two parallel deep FNNs with opposite flow directions. Input patterns enter the first FNN and are propagated "up". Desired outputs (targets) enter the "opposite" FNN and are propagated "down". Using a local learning rule, each layer in each net tries to be similar (in information content) to the preceding layer and to the adjacent layer of the other net. The input entering the first net slowly "heats up" to become the target. The target entering the opposite net slowly "cools down" to become the input. The Helmholtz Machine (Dayan et al., 1995; Dayan and Hinton, 1996) may be viewed as an unsupervised (Sec. 5.6.4) variant thereof (Peter Dayan, personal communication, 1994).
A hybrid approach (Shavlik and Towell, 1989; Towell and Shavlik, 1994) initializes a potentially deep
FNN through a domain theory in propositional logic, which may be acquired through explanation-based learning (Mitchell et al., 1986; DeJong and Mooney, 1986; Minton et al., 1989). The NN is then finetuned through BP (Sec. 5.5). The NN's depth reflects the longest chain of reasoning in the original set of logical rules. An extension of this approach (Maclin and Shavlik, 1993; Shavlik, 1994) initializes an RNN by domain knowledge expressed as a Finite State Automaton (FSA). BP-based fine-tuning has become important for later DL systems pre-trained by UL, e.g., Sec. 5.10, 5.15.
Better BP Through Advanced Gradient Descent (Compare Sec. 5.24)
Numerous improvements of steepest descent through BP (Sec. 5.5) have been proposed. Least-squares methods (Gauss-Newton, Levenberg-Marquardt) (Gauss, 1809; Newton, 1687; Levenberg, 1944; Marquardt, 1963; Schaback and Werner, 1992) and quasi-Newton methods (Broyden-Fletcher-GoldfarbShanno, BFGS) (Broyden et al., 1965; Fletcher and Powell, 1963; Goldfarb, 1970; Shanno, 1970) are computationally too expensive for large NNs.
Partial BFGS (Battiti, 1992; Saito and Nakano, 1997) and conjugate gradient (Hestenes and Stiefel, 1952; Møller, 1993) as well as other methods (Solla, 1988;
Schmidhuber, 1989a; Cauwenberghs, 1993) provide sometimes useful fast alternatives. BP can be treated as a linear least-squares problem (Biegler-K¨onig and B¨armann, 1993), where second-order gradient information is passed back to preceding layers.
To speed up BP, momentum was introduced (Rumelhart et al., 1986), ad-hoc constants were added to the slope of the linearized activation function (Fahlman, 1988), or the nonlinearity of the slope was exaggerated (West and Saad, 1995).
Only the signs of the error derivatives are taken into account by the successful and widely used BP variant R-prop (Riedmiller and Braun, 1993) and the robust variation iRprop+ (Igel and H¨usken, 2003), which was also successfully applied to RNNs.
The local gradient can be normalized based on the NN architecture (Schraudolph and Sejnowski, 1996), through a diagonalized Hessian approach (Becker and Le Cun, 1989), or related efficient methods (Schraudolph, 2002).
Some algorithms for controlling BP step size adapt a global learning rate (Lapedes and Farber, 1986;
Vogl et al., 1988; Battiti, 1989; LeCun et al., 1993; Yu et al., 1995), while others compute individual learning rates for each weight (Jacobs, 1988; Silva and Almeida, 1990). In online learning, where BP is applied after each pattern presentation, the vario-η algorithm (Neuneier and Zimmermann, 1996) sets each weight's learning rate inversely proportional to the empirical standard deviation of its local gradient, thus normalizing the stochastic weight fluctuations. Compare a local online step size adaptation method for nonlinear NNs (Almeida et al., 1997).
Many additional tricks for improving NNs have been described (e.g., Orr and M¨uller, 1998; Montavon et al., 2012). Compare Sec. 5.6.3 and recent developments mentioned in Sec. 5.24.
Searching For Simple, Low-Complexity, Problem-Solving NNs (Sec. 5.24)
Many researchers used BP-like methods to search for "simple," low-complexity NNs (Sec. 4.4) with high generalization capability. Most approaches address the bias/variance dilemma (Geman et al., 1992) through strong prior assumptions. For example, weight decay (Hanson and Pratt, 1989; Weigend et al., 1991;
Krogh and Hertz, 1992) encourages near-zero weights, by penalizing large weights. In a Bayesian framework (Bayes, 1763), weight decay can be derived (Hinton and van Camp, 1993) from Gaussian or Laplacian weight priors (Gauss, 1809; Laplace, 1774); see also (Murray and Edwards, 1993). An extension of this approach postulates that a distribution of networks with many similar weights generated by Gaussian mixtures is "better" a priori (Nowlan and Hinton, 1992).
Often weight priors are implicit in additional penalty terms (MacKay, 1992) or in methods based on validation sets (Mosteller and Tukey, 1968; Stone, 1974; Eubank, 1988; Hastie and Tibshirani, 1990;
Craven and Wahba, 1979; Golub et al., 1979), Akaike's information criterion and final prediction error (Akaike, 1970, 1973, 1974), or generalized prediction error (Moody and Utans, 1994; Moody, 1992).
See also (Holden, 1994; Wang et al., 1994; Amari and Murata, 1993; Wang et al., 1994; Guyon et al., 1992;
Vapnik, 1992; Wolpert, 1994). Similar priors (or biases towards simplicity) are implicit in constructive and pruning algorithms, e.g., layer-by-layer sequential network construction (e.g., Ivakhnenko, 1968, 1971;
Ash, 1989; Moody, 1989; Gallant, 1988; Honavar and Uhr, 1988; Ring, 1991; Fahlman, 1991; Weng et al., 1992; Honavar and Uhr, 1993; Burgess, 1994; Fritzke, 1994; Parekh et al., 2000; Utgoff and Stracuzzi, 2002) (see also Sec. 5.3, 5.11), input pruning (Moody, 1992; Refenes et al., 1994), unit pruning (e.g., Ivakhnenko, 1968, 1971; White, 1989; Mozer and Smolensky, 1989; Levin et al., 1994), weight pruning, e.g., optimal brain damage (LeCun et al., 1990b), and optimal brain surgeon (Hassibi and Stork, 1993).
A very general but not always practical approach for discovering low-complexity SL NNs or RL NNs searches among weight matrix-computing programs written in a universal programming language, with a bias towards fast and short programs (Schmidhuber, 1997) (Sec. 6.7).
Flat Minimum Search (FMS) (Hochreiter and Schmidhuber, 1997a, 1999) searches for a "flat" minimum of the error function: a large connected region in weight space where error is low and remains approximately constant, that is, few bits of information are required to describe low-precision weights with high variance. Compare perturbation tolerance conditions (Minai and Williams, 1994; Murray and Edwards, 1993; Neti et al., 1992; Matsuoka, 1992; Bishop, 1993; Kerlirzin and Vallet, 1993; Carter et al., 1990). An MDL-based, Bayesian argument suggests that flat minima correspond to "simple" NNs and low expected overfitting. Compare Sec. 5.6.4 and more recent developments mentioned in Sec. 5.24.
Potential Benefits of UL for SL (Compare Sec. 5.7, 5.10, 5.15)
The notation of Sec. 2 introduced teacher-given labels dt. Many papers of the previous millennium, however, were about unsupervised learning (UL) without a teacher (e.g., Hebb, 1949; von der Malsburg, 1973;
Kohonen, 1972, 1982, 1988; Willshaw and von der Malsburg, 1976; Grossberg, 1976a,b; Watanabe, 1985;
Pearlmutter and Hinton, 1986; Barrow, 1987; Field, 1987; Oja, 1989; Barlow et al., 1989; Baldi and Hornik, 1989; Rubner and Tavan, 1989; Sanger, 1989; Ritter and Kohonen, 1989; Rubner and Schulten, 1990;
F¨oldi´ak, 1990; Martinetz et al., 1990; Kosko, 1990; Mozer, 1991; Palm, 1992; Atick et al., 1992; Miller, 1994; Saund, 1994; F¨oldi´ak and Young, 1995; Deco and Parra, 1997); see also post-2000 work (e.g., Carreira-Perpinan, 2001; Wiskott and Sejnowski, 2002; Franzius et al., 2007; Waydo and Koch, 2008).
Many UL methods are designed to maximize entropy-related, information-theoretic (Boltzmann, 1909;
Shannon, 1948; Kullback and Leibler, 1951) objectives (e.g., Linsker, 1988; Barlow et al., 1989; MacKay and Miller, 1990; Plumbley, 1991; Schmidhuber, 1992b,c; Schraudolph and Sejnowski, 1993; Redlich, 1993; Zemel, 1993; Zemel and Hinton, 1994; Field, 1994; Hinton et al., 1995; Dayan and Zemel, 1995;
Amari et al., 1996; Deco and Parra, 1997). Many do this to uncover and disentangle hidden underlying sources of signals (e.g., Jutten and Herault, 1991; Schuster, 1992; Andrade et al., 1993; Molgedey and Schuster, 1994; Comon, 1994; Cardoso, 1994; Bell and Sejnowski, 1995; Karhunen and Joutsensalo, 1995;
Belouchrani et al., 1997; Hyv¨arinen et al., 2001; Szab´o et al., 2006; Shan et al., 2007; Shan and Cottrell, Many UL methods automatically and robustly generate distributed, sparse representations of input patterns (F¨oldi´ak, 1990; Hinton and Ghahramani, 1997; Lewicki and Olshausen, 1998; Hyv¨arinen et al., 1999;
Hochreiter and Schmidhuber, 1999; Falconbridge et al., 2006) through well-known feature detectors (e.g., Olshausen and Field, 1996; Schmidhuber et al., 1996), such as off-center-on-surround-like structures, as well as orientation sensitive edge detectors and Gabor filters (Gabor, 1946). They extract simple features related to those observed in early visual pre-processing stages of biological systems (e.g., De Valois et al., 1982; Jones and Palmer, 1987).
UL can also serve to extract invariant features from different data items (e.g., Becker, 1991) through coupled NNs observing two different inputs (Schmidhuber and Prelinger, 1992), also called Siamese
NNs (e.g., Bromley et al., 1993; Hadsell et al., 2006; Taylor et al., 2011; Chen and Salman, 2011).
UL can help to encode input data in a form advantageous for further processing. In the context of DL, one important goal of UL is redundancy reduction. Ideally, given an ensemble of input patterns, redundancy reduction through a deep NN will create a factorial code (a code with statistically independent components) of the ensemble (Barlow et al., 1989; Barlow, 1989), to disentangle the unknown factors of variation (compare Bengio et al., 2013). Such codes may be sparse and can be advantageous for (1) data compression, (2) speeding up subsequent BP (Becker, 1991), (3) trivialising the task of subsequent naive yet optimal Bayes classifiers (Schmidhuber et al., 1996).
Most early UL FNNs had a single layer. Methods for deeper UL FNNs include hierarchical (Sec. 4.3) self-organizing Kohonen maps (e.g., Koikkalainen and Oja, 1990; Lampinen and Oja, 1992; Versino and Gambardella, 1996; Dittenbach et al., 2000; Rauber et al., 2002), hierarchical Gaussian potential function networks (Lee and Kil, 1991), the Self-Organising Tree Algorithm (SOTA) (Herrero et al., 2001), and nonlinear Autoencoders (AEs) with more than 3 (e.g., 5) layers (Kramer, 1991; Oja, 1991; DeMers and Cottrell, 1993). Such AE NNs (Rumelhart et al., 1986) can be trained to map input patterns to themselves, for example, by compactly encoding them through activations of units of a narrow bottleneck hidden layer.
Certain nonlinear AEs suffer from certain limitations (Baldi, 2012).
LOCOCODE (Hochreiter and Schmidhuber, 1999) uses FMS (Sec. 5.6.3) to find low-complexity AEs with low-precision weights describable by few bits of information, often producing sparse or factorial codes. Predictability Minimization (PM) (Schmidhuber, 1992c) searches for factorial codes through nonlinear feature detectors that fight nonlinear predictors, trying to become both as informative and as unpredictable as possible. PM-based UL was applied not only to FNNs but also to RNNs (e.g., Schmidhuber, 1993b; Lindst¨adt, 1993a,b). Compare Sec. 5.10 on UL-based RNN stacks (1991), as well as later UL
RNNs (e.g., Klapper-Rybicka et al., 2001; Steil, 2007).
1987: UL Through Autoencoder (AE) Hierarchies (Compare Sec. 5.15)
Perhaps the first work to study potential benefits of UL-based pre-training was published in 1987. It proposed unsupervised AE hierarchies (Ballard, 1987), closely related to certain post-2000 feedforward
Deep Learners based on UL (Sec. 5.15). The lowest-level AE NN with a single hidden layer is trained to map input patterns to themselves. Its hidden layer codes are then fed into a higher-level AE of the same type, and so on. The hope is that the codes in the hidden AE layers have properties that facilitate subsequent learning. In one experiment, a particular AE-specific learning algorithm (different from traditional BP of Sec. 5.5.1) was used to learn a mapping in an AE stack pre-trained by this type of UL (Ballard, 1987). This was faster than learning an equivalent mapping by BP through a single deeper AE without pre-training.
On the other hand, the task did not really require a deep AE, that is, the benefits of UL were not that obvious from this experiment. Compare an early survey (Hinton, 1989) and the somewhat related Recursive
Auto-Associative Memory (RAAM) (Pollack, 1988, 1990; Melnik et al., 2000), originally used to encode sequential linguistic structures of arbitrary size through a fixed number of hidden units. More recently, RAAMs were also used as unsupervised pre-processors to facilitate deep credit assignment for RL (Gisslen et al., 2011) (Sec. 6.4).
In principle, many UL methods (Sec. 5.6.4) could be stacked like the AEs above, the historycompressing RNNs of Sec. 5.10, the Restricted Boltzmann Machines (RBMs) of Sec. 5.15, or hierarchical
Kohonen nets (Sec. 5.6.4), to facilitate subsequent SL. Compare Stacked Generalization (Wolpert, 1992;
Ting and Witten, 1997), and FNNs that profit from pre-training by competitive UL (e.g., Rumelhart and Zipser, 1986) prior to BP-based fine-tuning (Maclin and Shavlik, 1995). See also more recent methods using UL to improve SL (e.g., Escalante-B. and Wiskott, 2013).
1989: BP for Convolutional NNs (CNNs, Sec. 5.4)
In 1989, backpropagation (Sec. 5.5) was applied (LeCun et al., 1989, 1990a, 1998) to Neocognitron-like, weight-sharing, convolutional neural layers (Sec. 5.4) with adaptive connections. This combination, augmented by Max-Pooling (MP, Sec. 5.11, 5.16), and sped up on graphics cards (Sec. 5.19), has become an essential ingredient of many modern, competition-winning, feedforward, visual Deep Learners (Sec. 5.19–
5.23). This work also introduced the MNIST data set of handwritten digits (LeCun et al., 1989), which over time has become perhaps the most famous benchmark of Machine Learning. CNNs helped to achieve good performance on MNIST (LeCun et al., 1990a) (CAP depth 5) and on fingerprint recognition (Baldi and Chauvin, 1993); similar CNNs were used commercially in the 1990s.
1991: Fundamental Deep Learning Problem of Gradient Descent
A diploma thesis (Hochreiter, 1991) represented a milestone of explicit DL research. As mentioned in Sec. 5.6, by the late 1980s, experiments had indicated that traditional deep feedforward or recurrent networks are hard to train by backpropagation (BP) (Sec. 5.5). Hochreiter's work formally identified a major reason: Typical deep NNs suffer from the now famous problem of vanishing or exploding gradients. With standard activation functions (Sec. 1), cumulative backpropagated error signals (Sec. 5.5.1) either shrink rapidly, or grow out of bounds. In fact, they decay exponentially in the number of layers or CAP depth(Sec. 3), or they explode. This is also known as the long time lag problem. Much subsequent DL research of the 1990s and 2000s was motivated by this insight. Later work (Bengio et al., 1994) also studied basins of attraction and their stability under noise from a dynamical systems point of view: either the dynamics are not robust to noise, or the gradients vanish. See also (Hochreiter et al., 2001a; Tiˇno and Hammer, 2004). Over the years, several ways of partially overcoming the Fundamental Deep Learning Problem were explored:
I A Very Deep Learner of 1991 (the History Compressor, Sec. 5.10) alleviates the problem through unsupervised pre-training for a hierarchy of RNNs. This greatly facilitates subsequent supervised credit assignment through BP (Sec. 5.5). Compare conceptually related AE stacks (Sec. 5.7) and Deep Belief Networks (DBNs) (Sec. 5.15) for the FNN case.
II LSTM-like networks (Sec. 5.13, 5.16, 5.17, 5.21–5.23) alleviate the problem through a special architecture unaffected by it.
III Today's GPU-based computers have a million times the computational power of desktop machines of the early 1990s. This allows for propagating errors a few layers further down within reasonable time, even in traditional NNs (Sec. 5.18). That is basically what is winning many of the image recognition competitions now (Sec. 5.19, 5.21, 5.22). (Although this does not really overcome the problem in a fundamental way.)
IV Hessian-free optimization (Sec. 5.6.2) can alleviate the problem for FNNs (Møller, 1993; Pearlmutter, 1994; Schraudolph, 2002; Martens, 2010) (Sec. 5.6.2) and RNNs (Martens and Sutskever, 2011)(Sec. 5.20).
V The space of NN weight matrices can also be searched without relying on error gradients, thus avoiding the Fundamental Deep Learning Problem altogether. Random weight guessing sometimes works better than more sophisticated methods (Hochreiter and Schmidhuber, 1996). Certain more complex problems are better solved by using Universal Search (Levin, 1973b) for weight matrixcomputing programs written in a universal programming language (Schmidhuber, 1997). Some are better solved by using linear methods to obtain optimal weights for connections to output events(Sec. 2), and evolving weights of connections to other events—this is called Evolino (Schmidhuber et al., 2007). Compare related RNNs pre-trained by certain UL rules (Steil, 2007), also in the case of spiking neurons (Klampfl and Maass, 2013) (Sec. 5.26). Direct search methods are relevant not only for SL but also for more general RL, and are discussed in more detail in Sec. 6.6.
1991: UL-Based History Compression Through a Deep Hierarchy of RNNs
A working Very Deep Learner (Sec. 3) of 1991 (Schmidhuber, 1992b, 2013a) could perform credit assignment across hundreds of nonlinear operators or neural layers, by using unsupervised pre-training for a stack of RNNs.
The basic idea is still relevant today. Each RNN is trained for a while in unsupervised fashion to predict its next input (e.g., Connor et al., 1994; Dorffner, 1996). From then on, only unexpected inputs (errors) convey new information and get fed to the next higher RNN which thus ticks on a slower, self-organising time scale. It can easily be shown that no information gets lost. It just gets compressed (much of machine learning is essentially about compression, e.g., Sec. 4.4, 5.6.3, 6.7). For each individual input sequence, we get a series of less and less redundant encodings in deeper and deeper levels of this History Compressor or Neural Sequence Chunker, which can compress data in both space (like feedforward NNs) and time.
This is another good example of hierarchical representation learning (Sec. 4.3). There also is a continuous variant of the history compressor (Schmidhuber et al., 1993).
The RNN stack is essentially a deep generative model of the data, which can be reconstructed from its compressed form. Adding another RNN to the stack improves a bound on the data's description length— equivalent to the negative logarithm of its probability (Huffman, 1952; Shannon, 1948)—as long as there is remaining local learnable predictability in the data representation on the corresponding level of the hierarchy. Compare a similar result for feedforward Deep Belief Networks (DBNs, 2006, Sec. 5.15).
The system was able to learn many previously unlearnable DL tasks. One ancient illustrative DL experiment (Schmidhuber, 1993b) required CAPs (Sec. 3) of depth 1200. The top level code of the initially unsupervised RNN stack, however, got so compact that (previously infeasible) sequence classification through additional BP-based SL became possible. Essentially the system used UL to greatly reduce problem depth. Compare earlier BP-based fine-tuning of NNs initialized by rules of propositional logic (Shavlik and Towell, 1989) (Sec. 5.6.1).
There is a way of compressing higher levels down into lower levels, thus fully or partially collapsing the RNN stack. The trick is to retrain a lower-level RNN to continually imitate (predict) the hidden units of an already trained, slower, higher-level RNN (the "conscious" chunker), through additional predictive output neurons (Schmidhuber, 1992b). This helps the lower RNN (the "automatizer") to develop appropriate, rarely changing memories that may bridge very long time lags. Again, this procedure can greatly reduce the required depth of the BP process.
The 1991 system was a working Deep Learner in the modern post-2000 sense, and also a first Neural Hierarchical Temporal Memory (HTM). It is conceptually similar to earlier AE hierarchies (1987, Sec. 5.7) and later Deep Belief Networks (2006, Sec. 5.15), but more general in the sense that it uses sequence-processing RNNs instead of FNNs with unchanging inputs. More recently, well-known entrepreneurs (Hawkins and George, 2006; Kurzweil, 2012) also got interested in HTMs; compare also hierarchical HMMs (e.g., Fine et al., 1998), as well as later UL-based recurrent systems (Klapper-Rybicka et al., 2001; Steil, 2007; Klampfl and Maass, 2013; Young et al., 2014). Clockwork RNNs (Koutn´ık et al., 2014) also consist of interacting RNN modules with different clock rates, but do not require UL to set those rates. Stacks of RNNs were used in later work on SL with great success, e.g., Sec. 5.13, 5.16, 5.17, 5.22.
1992: Max-Pooling (MP): Towards MPCNNs (Compare Sec. 5.16, 5.19)
The Neocognitron (Sec. 5.4) inspired the Cresceptron (Weng et al., 1992), which adapts its topology during training (Sec. 5.6.3); compare the incrementally growing and shrinking GMDH networks (1965, Sec. 5.3).
Instead of using alternative local subsampling or WTA methods (e.g., Fukushima, 1980; Schmidhuber, 1989b; Maass, 2000; Fukushima, 2013a), the Cresceptron uses Max-Pooling (MP) layers. Here a 2-dimensional layer or array of unit activations is partitioned into smaller rectangular arrays. Each is replaced in a downsampling layer by the activation of its maximally active unit. A later, more complex version of the Cresceptron (Weng et al., 1997) also included "blurring" layers to improve object location tolerance.
The neurophysiologically plausible topology of the feedforward HMAX model (Riesenhuber and Poggio, 1999) is very similar to the one of the 1992 Cresceptron (and thus to the 1979 Neocognitron). HMAX does not learn though. Its units have hand-crafted weights; biologically plausible learning rules were later proposed for similar models (e.g., Serre et al., 2002; Teichmann et al., 2012).
When CNNs or convnets (Sec. 5.4, 5.8) are combined with MP, they become Cresceptron-like or HMAX-like MPCNNs with alternating convolutional and max-pooling layers. Unlike Cresceptron and HMAX, however, MPCNNs are trained by BP (Sec. 5.5, 5.16) (Ranzato et al., 2007). Advantages of doing this were pointed out subsequently (Scherer et al., 2010). BP-trained MPCNNs have become central to many modern, competition-winning, feedforward, visual Deep Learners (Sec. 5.17, 5.19–5.23).
1994: Early Contest-Winning NNs
Back in the 1990s, certain NNs already won certain controlled pattern recognition contests with secret test sets. Notably, an NN with internal delay lines won the Santa Fe time-series competition on chaotic intensity pulsations of an NH3 laser (Wan, 1994; Weigend and Gershenfeld, 1993). No very deep CAPs (Sec. 3) were needed though.
1995: Supervised Recurrent Very Deep Learner (LSTM RNN)
Supervised Long Short-Term Memory (LSTM) RNN (Hochreiter and Schmidhuber, 1997b; Gers et al., 2000; P´erez-Ortiz et al., 2003) could eventually perform similar feats as the deep RNN hierarchy of 1991(Sec. 5.10), overcoming the Fundamental Deep Learning Problem (Sec. 5.9) without any unsupervised pretraining. LSTM could also learn DL tasks without local sequence predictability (and thus unlearnable by the partially unsupervised 1991 History Compressor, Sec. 5.10), dealing with very deep problems (Sec. 3) (e.g., Gers et al., 2002).
The basic LSTM idea is very simple. Some of the units are called Constant Error Carousels (CECs).
Each CEC uses as an activation function f, the identity function, and has a connection to itself with fixed weight of 1.0. Due to f's constant derivative of 1.0, errors backpropagated through a CEC cannot vanish or explode (Sec. 5.9) but stay as they are (unless they "flow out" of the CEC to other, typically adaptive parts of the NN). CECs are connected to several nonlinear adaptive units (some with multiplicative activation functions) needed for learning nonlinear behavior. Weight changes of these units often profit from error signals propagated far back in time through CECs. CECs are the main reason why LSTM nets can learn to discover the importance of (and memorize) events that happened thousands of discrete time steps ago, while previous RNNs already failed in case of minimal time lags of 10 steps.
Many different LSTM variants and topologies are allowed. It is possible to evolve good problemspecific topologies (Bayer et al., 2009). Some LSTM variants also use modifiable self-connections of CECs (Gers and Schmidhuber, 2001).
To a certain extent, LSTM is biologically plausible (O'Reilly, 2003). LSTM learned to solve many previously unlearnable DL tasks involving: Recognition of the temporal order of widely separated events in noisy input streams; Robust storage of high-precision real numbers across extended time intervals; Arithmetic operations on continuous input streams; Extraction of information conveyed by the temporal distance between events; Recognition of temporally extended patterns in noisy input sequences (Hochreiter and Schmidhuber, 1997b; Gers et al., 2000); Stable generation of precisely timed rhythms, as well as smooth and non-smooth periodic trajectories (Gers and Schmidhuber, 2000). LSTM clearly outperformed previous
RNNs on tasks that require learning the rules of regular languages describable by deterministic Finite State
Automata (FSAs) (Watrous and Kuhn, 1992; Casey, 1996; Siegelmann, 1992; Blair and Pollack, 1997;
Kalinke and Lehmann, 1998; Zeng et al., 1994; Manolios and Fanelli, 1994; Omlin and Giles, 1996; Vahed and Omlin, 2004), both in terms of reliability and speed.
LSTM also worked on tasks involving context free languages (CFLs) that cannot be represented by
HMMs or similar FSAs discussed in the RNN literature (Sun et al., 1993b; Wiles and Elman, 1995; Andrews et al., 1995; Steijvers and Grunwald, 1996; Tonkes and Wiles, 1997; Rodriguez et al., 1999; Rodriguez and Wiles, 1998). CFL recognition (Lee, 1996) requires the functional equivalent of a runtime stack. Some previous RNNs failed to learn small CFL training sets (Rodriguez and Wiles, 1998). Those that did not (Rodriguez et al., 1999; Bod´en and Wiles, 2000) failed to extract the general rules, and did not generalize well on substantially larger test sets. Similar for context-sensitive languages (CSLs) (e.g., Chalup and Blair, 2003). LSTM generalized well though, requiring only the 30 shortest exemplars (n ≤ 10) of the CSL anbncn to correctly predict the possible continuations of sequence prefixes for n up to 1000 and more. A combination of a decoupled extended Kalman filter (Kalman, 1960; Williams, 1992b; Puskorius and Feldkamp, 1994; Feldkamp et al., 1998; Haykin, 2001; Feldkamp et al., 2003) and an LSTM
RNN (P´erez-Ortiz et al., 2003) learned to deal correctly with values of n up to 10 million and more. That is, after training the network was able to read sequences of 30,000,000 symbols and more, one symbol at a time, and finally detect the subtle differences between legal strings such as a10,000,000b10,000,000c10,000,000 and very similar but illegal strings such as a10,000,000b9,999,999c10,000,000. Compare also more recent RNN algorithms able to deal with long time lags (Sch¨afer et al., 2006; Martens and Sutskever, 2011; Zimmermann et al., 2012; Koutn´ık et al., 2014).
Bi-directional RNNs (BRNNs) (Schuster and Paliwal, 1997; Schuster, 1999) are designed for input sequences whose starts and ends are known in advance, such as spoken sentences to be labeled by their phonemes; compare (Fukada et al., 1999). To take both past and future context of each sequence element into account, one RNN processes the sequence from start to end, the other backwards from end to start.
At each time step their combined outputs predict the corresponding label (if there is any). BRNNs were successfully applied to secondary protein structure prediction (Baldi et al., 1999). DAG-RNNs (Baldi and Pollastri, 2003; Wu and Baldi, 2008) generalize BRNNs to multiple dimensions. They learned to predict properties of small organic molecules (Lusci et al., 2013) as well as protein contact maps (Tegge et al., 2009), also in conjunction with a growing deep FNN (Di Lena et al., 2012) (Sec. 5.21). BRNNs and DAGRNNs unfold their full potential when combined with the LSTM concept (Graves and Schmidhuber, 2005, 2009; Graves et al., 2009).
Particularly successful in recent competitions are stacks (Sec. 5.10) of LSTM RNNs (Fernandez et al., 2007; Graves and Schmidhuber, 2009) trained by Connectionist Temporal Classification (CTC) (Graves et al., 2006), a gradient-based method for finding RNN weights that maximize the probability of teachergiven label sequences, given (typically much longer and more high-dimensional) streams of real-valued input vectors. CTC-LSTM performs simultaneous segmentation (alignment) and recognition (Sec. 5.22).
In the early 2000s, speech recognition was still dominated by HMMs combined with FNNs (e.g., Bourlard and Morgan, 1994). Nevertheless, when trained from scratch on utterances from the TIDIGITS speech database, in 2003 LSTM already obtained results comparable to those of HMM-based systems (Graves et al., 2003; Beringer et al., 2005; Graves et al., 2006). A decade later, LSTM achieved best known results on the famous TIMIT phoneme recognition benchmark (Graves et al., 2013) (Sec. 5.22). Besides speech recognition and keyword spotting (Fern´andez et al., 2007), important applications of LSTM include protein analysis (Hochreiter and Obermayer, 2005), robot localization (F¨orster et al., 2007) and 17 robot control (Mayer et al., 2008), handwriting recognition (Graves et al., 2008, 2009; Graves and Schmidhuber, 2009; Bluche et al., 2014), optical character recognition (Breuel et al., 2013), and others. RNNs can also be used for metalearning (Schmidhuber, 1987; Schaul and Schmidhuber, 2010; Prokhorov et al., 2002), because they can in principle learn to run their own weight change algorithm (Schmidhuber, 1993a). A successful metalearner (Hochreiter et al., 2001b) used an LSTM RNN to quickly learn a learning algorithm for quadratic functions (compare Sec. 6.8).
More recently, LSTM RNNs won several international pattern recognition competitions and set benchmark records on large and complex data sets, e.g., Sec. 5.17, 5.21, 5.22. Gradient-based LSTM is no panacea though—other methods sometimes outperformed it at least on certain tasks (Jaeger, 2004; Schmidhuber et al., 2007; Martens and Sutskever, 2011; Pascanu et al., 2013b; Koutn´ık et al., 2014); compare
Sec. 5.20.
2003: More Contest-Winning/Record-Setting NNs; Successful Deep NNs
In the decade around 2000, many practical and commercial pattern recognition applications were dominated by non-neural machine learning methods such as Support Vector Machines (SVMs) (Vapnik, 1995;
Sch¨olkopf et al., 1998). Nevertheless, at least in certain domains, NNs outperformed other techniques.
A Bayes NN (Neal, 2006) based on an ensemble (Breiman, 1996; Schapire, 1990; Wolpert, 1992;
Hashem and Schmeiser, 1992; Ueda, 2000; Dietterich, 2000a) of NNs won the NIPS 2003 Feature Selection Challenge with secret test set (Neal and Zhang, 2006). The NN was not very deep though—it had two hidden layers and thus rather shallow CAPs (Sec. 3) of depth 3.
Important for many present competition-winning pattern recognisers (Sec. 5.19, 5.21, 5.22) were developments in the CNN department. A BP-trained (LeCun et al., 1989) CNN (Sec. 5.4, Sec. 5.8) set a new MNIST record of 0.4% (Simard et al., 2003), using training pattern deformations (Baird, 1990) but no unsupervised pre-training (Sec. 5.7, 5.10, 5.15). A standard BP net achieved 0.7% (Simard et al., 2003).
Again, the corresponding CAP depth was low. Compare further improvements in Sec. 5.16, 5.18, 5.19.
Good image interpretation results (Behnke, 2003) were achieved with rather deep NNs trained by the BP variant R-prop (Riedmiller and Braun, 1993) (Sec. 5.6.2). FNNs with CAP depth up to 6 were used to successfully classify high-dimensional data (Vieira and Barradas, 2003).
Deep LSTM RNNs started to obtain certain first speech recognition results comparable to those of HMM-based systems (Graves et al., 2003); compare Sec. 5.13, 5.16, 5.21, 5.22.
2006/7: UL For Deep Belief Networks (DBNs) / AE Stacks Fine-Tuned by BP
While learning networks with numerous non-linear layers date back at least to 1965 (Sec. 5.3), and explicit
DL research results have been published at least since 1991 (Sec. 5.9, 5.10), the expression Deep Learning was actually coined around 2006, when unsupervised pre-training of deep FNNs helped to accelerate subsequent SL through BP (Hinton and Salakhutdinov, 2006; Hinton et al., 2006). Compare earlier terminology on loading deep networks (S´ıma, 1994; Windisch, 2005) and learning deep memories (Gomez and Schmidhuber, 2005). Compare also BP-based (Sec. 5.5) fine-tuning (Sec. 5.6.1) of (not so deep) FNNs pre-trained by competitive UL (Maclin and Shavlik, 1995).
The Deep Belief Network (DBN) is a stack of Restricted Boltzmann Machines (RBMs) (Smolensky, 1986), which in turn are Boltzmann Machines (BMs) (Hinton and Sejnowski, 1986) with a single layer of feature-detecting units; compare also Higher-Order BMs (Memisevic and Hinton, 2010). Each RBM perceives pattern representations from the level below and learns to encode them in unsupervised fashion. At least in theory under certain assumptions, adding more layers improves a bound on the data's negative log probability (Hinton et al., 2006) (equivalent to the data's description length—compare the corresponding observation for RNN stacks, Sec. 5.10). There are extensions for Temporal RBMs (Sutskever et al., 2008).
Without any training pattern deformations (Sec. 5.14), a DBN fine-tuned by BP achieved 1.2% error rate (Hinton and Salakhutdinov, 2006) on the MNIST handwritten digits (Sec. 5.8, 5.14). This result helped to arouse interest in DBNs. DBNs also achieved good results on phoneme recognition, with an error rate of 26.7% on the TIMIT core test set (Mohamed et al., 2009; Mohamed and Hinton, 2010); compare further improvements through FNNs (Hinton et al., 2012a; Deng and Yu, 2014) and LSTM RNNs (Sec. 5.22).
A DBN-based technique called Semantic Hashing (Salakhutdinov and Hinton, 2009) maps semantically similar documents (of variable size) to nearby addresses in a space of document representations. It outperformed previous searchers for similar documents, such as Locality Sensitive Hashing (Buhler, 2001;
Datar et al., 2004). See the RBM/DBN tutorial (Fischer and Igel, 2014).
Autoencoder (AE) stacks (Ballard, 1987) (Sec. 5.7) became a popular alternative way of pre-training deep FNNs in unsupervised fashion, before fine-tuning (Sec. 5.6.1) them through BP (Sec. 5.5) (Bengio et al., 2007; Vincent et al., 2008; Erhan et al., 2010). Sparse coding (Sec. 5.6.4) was formulated as a combination of convex optimization problems (Lee et al., 2007a). Recent surveys of stacked RBM and AE methods focus on post-2006 developments (Bengio, 2009; Arel et al., 2010). Unsupervised DBNs and AE stacks are conceptually similar to, but in a certain sense less general than, the unsupervised RNN stackbased History Compressor of 1991 (Sec. 5.10), which can process and re-encode not only stationary input patterns, but entire pattern sequences.
2006/7: Improved CNNs / GPU-CNNs / BP-Trained MPCNNs / LSTM Stacks
Also in 2006, a BP-trained (LeCun et al., 1989) CNN (Sec. 5.4, Sec. 5.8) set a new MNIST record of 0.39% (Ranzato et al., 2006), using training pattern deformations (Sec. 5.14) but no unsupervised pretraining. Compare further improvements in Sec. 5.18, 5.19. Similar CNNs were used for off-road obstacle avoidance (LeCun et al., 2006). A combination of CNNs and TDNNs later learned to map fixed-size representations of variable-size sentences to features relevant for language processing, using a combination of SL and UL (Collobert and Weston, 2008).
2006 also saw an early GPU-based CNN implementation (Chellapilla et al., 2006) up to 4 times faster than CPU-CNNs; compare also earlier GPU implementations of standard FNNs with a reported speed-up factor of 20 (Oh and Jung, 2004). GPUs or graphics cards have become more and more important for DL in subsequent years (Sec. 5.18–5.22).
In 2007, BP (Sec. 5.5) was applied for the first time (Ranzato et al., 2007) to Neocognitron-inspired(Sec. 5.4), Cresceptron-like (or HMAX-like) MPCNNs (Sec. 5.11) with alternating convolutional and maxpooling layers. BP-trained MPCNNs have become an essential ingredient of many modern, competitionwinning, feedforward, visual Deep Learners (Sec. 5.17, 5.19–5.23).
Also in 2007, hierarchical stacks of LSTM RNNs were introduced (Fernandez et al., 2007). They can be trained by hierarchical Connectionist Temporal Classification (CTC) (Graves et al., 2006). For tasks of sequence labelling, every LSTM RNN level (Sec. 5.13) predicts a sequence of labels fed to the next level.
Error signals at every level are back-propagated through all the lower levels. On spoken digit recognition, LSTM stacks outperformed HMMs, despite making fewer assumptions about the domain. LSTM stacks do not necessarily require unsupervised pre-training like the earlier UL-based RNN stacks (Schmidhuber, 1992b) of Sec. 5.10.
2009: First Official Competitions Won by RNNs, and with MPCNNs
Stacks of LSTM RNNs trained by CTC (Sec. 5.13, 5.16) became the first RNNs to win official international pattern recognition contests (with secret test sets known only to the organisers). More precisely, three connected handwriting competitions at ICDAR 2009 in three different languages (French, Arab, Farsi) were won by deep LSTM RNNs without any a priori linguistic knowledge, performing simultaneous segmentation and recognition. Compare (Graves and Schmidhuber, 2005; Graves et al., 2009; Schmidhuber et al., 2011; Graves et al., 2013; Graves and Jaitly, 2014) (Sec. 5.22).
To detect human actions in surveillance videos, a 3-dimensional CNN (e.g., Jain and Seung, 2009;
Prokhorov, 2010), combined with SVMs, was part of a larger system (Yang et al., 2009) using a bag of features approach (Nowak et al., 2006) to extract regions of interest. The system won three 2009 TRECVID competitions. These were possibly the first official international contests won with the help of (MP)CNNs(Sec. 5.16). An improved version of the method was published later (Ji et al., 2013).
2009 also saw a GPU-DBN implementation (Raina et al., 2009) orders of magnitudes faster than previous CPU-DBNs (see Sec. 5.15); see also (Coates et al., 2013). The Convolutional DBN (Lee et al., 2009a)(with a probabilistic variant of MP, Sec. 5.11) combines ideas from CNNs and DBNs, and was successfully applied to audio classification (Lee et al., 2009b).
2010: Plain Backprop (+ Distortions) on GPU Breaks MNIST Record
In 2010, a new MNIST (Sec. 5.8) record of 0.35% error rate was set by good old BP (Sec. 5.5) in deep but otherwise standard NNs (Ciresan et al., 2010), using neither unsupervised pre-training (e.g., Sec. 5.7, 5.10, 5.15) nor convolution (e.g., Sec. 5.4, 5.8, 5.14, 5.16). However, training pattern deformations (e.g., Sec. 5.14) were important to generate a big training set and avoid overfitting. This success was made possible mainly through a GPU implementation of BP that was up to 50 times faster than standard CPU versions. A good value of 0.95% was obtained without distortions except for small saccadic eye movement-like translations—compare Sec. 5.15.
Since BP was 3-5 decades old by then (Sec. 5.5), and pattern deformations 2 decades (Baird, 1990)(Sec. 5.14), these results seemed to suggest that advances in exploiting modern computing hardware were more important than advances in algorithms.
2011: MPCNNs on GPU Achieve Superhuman Vision Performance
In 2011, the first GPU-implementation (Ciresan et al., 2011a) of Max-Pooling (MP) CNNs or Convnets was described (the GPU-MPCNN), extending earlier work on MP (Weng et al., 1992) (Sec. 5.11) CNNs(Fukushima, 1979; LeCun et al., 1989) (Sec. 5.4, 5.8, 5.16), and on early GPU-based CNNs without
MP (Chellapilla et al., 2006) (Sec. 5.16); compare early GPU-NNs (Oh and Jung, 2004) and GPUDBNs (Raina et al., 2009) (Sec. 5.17). MPCNNs have alternating convolutional layers (Sec. 5.4) and max-pooling layers (MP, Sec. 5.11) topped by standard fully connected layers. All weights are trained by
BP (Sec. 5.5, 5.8, 5.16) (Ranzato et al., 2007; Scherer et al., 2010). GPU-MPCNNs have become essential for many contest-winning FNNs (Sec. 5.21, Sec. 5.22).
Multi-Column (MC) GPU-MPCNNs (Ciresan et al., 2011b) are committees (Breiman, 1996; Schapire, 1990; Wolpert, 1992; Hashem and Schmeiser, 1992; Ueda, 2000; Dietterich, 2000a) of GPU-MPCNNs with simple democratic output averaging. Several MPCNNs see the same input; their output vectors are used to assign probabilities to the various possible classes. The class with the on average highest probability is chosen as the system's classification of the present input. Compare earlier, more sophisticated ensemble methods (Schapire, 1990), the contest-winning ensemble Bayes-NN (Neal, 2006) of Sec. 5.14, and recent related work (Shao et al., 2014).
An MC-GPU-MPCNN was the first system to achieve superhuman visual pattern recognition (Ciresan et al., 2011b, 2012b) in a controlled competition, namely, the IJCNN 2011 traffic sign recognition contest in San Jose (CA) (Stallkamp et al., 2011, 2012). This is of interest for fully autonomous, self-driving cars in traffic (e.g., Dickmanns et al., 1994). The MC-GPU-MPCNN obtained 0.56% error rate and was twice better than human test subjects, three times better than the closest artificial NN competitor (Sermanet and LeCun, 2011), and six times better than the best non-neural method.
A few months earlier, the qualifying round was won in a 1st stage online competition, albeit by a much smaller margin: 1.02% (Ciresan et al., 2011b) vs 1.03% for second place (Sermanet and LeCun, 2011). After the deadline, the organisers revealed that human performance on the test set was 1.19%. That is, the best methods already seemed human-competitive. However, during the qualifying it was possible to incrementally gain information about the test set by probing it through repeated submissions. This is illustrated by better and better results obtained by various teams over time (Stallkamp et al., 2012) (the organisers eventually imposed a limit of 10 resubmissions). In the final competition this was not possible.
This illustrates a general problem with benchmarks whose test sets are public, or at least can be probed to some extent: competing teams tend to overfit on the test set even when it cannot be directly used for training, only for evaluation.
In 1997 many thought it a big deal that human chess world champion Kasparov was beaten by an IBM computer. But back then computers could not at all compete with little kids in visual pattern recognition, which seems much harder than chess from a computational perspective. Of course, the traffic sign domain is highly restricted, and kids are still much better general pattern recognisers. Nevertheless, by 2011, deep
NNs could already learn to rival them in important limited visual domains.
An MC-GPU-MPCNN was also the first method to achieve human-competitive performance (around
0.2%) on MNIST (Ciresan et al., 2012c). This represented a dramatic improvement, since by then the MNIST record had hovered around 0.4% for almost a decade (Sec. 5.14, 5.16, 5.18).
Given all the prior work on (MP)CNNs (Sec. 5.4, 5.8, 5.11, 5.16) and GPU-CNNs (Sec. 5.16), GPUMPCNNs are not a breakthrough in the scientific sense. But they are a commercially relevant breakthrough in efficient coding that has made a difference in several contests since 2011. Today, most feedforward competition-winning deep NNs are GPU-MPCNNs (Sec. 5.21–5.23).
2011: Hessian-Free Optimization for RNNs
Also in 2011 it was shown (Martens and Sutskever, 2011) that Hessian-free optimization (e.g., Møller, 1993; Pearlmutter, 1994; Schraudolph, 2002) (Sec. 5.6.2) can alleviate the Fundamental Deep Learning
Problem (Sec. 5.9) in RNNs, outperforming standard gradient-based LSTM RNNs (Sec. 5.13) on several tasks. Compare other RNN algorithms (Jaeger, 2004; Schmidhuber et al., 2007; Pascanu et al., 2013b;
Koutn´ık et al., 2014) that also at least sometimes yield better results than steepest descent for LSTM RNNs.
2012: First Contests Won on ImageNet & Object Detection & Segmentation
In 2012, an ensemble of GPU-MPCNNs (Sec. 5.19) achieved best results on the ImageNet classification benchmark (Krizhevsky et al., 2012), which is popular in the computer vision community. Here relatively large image sizes of 256x256 pixels were necessary, as opposed to only 48x48 pixels for the 2011 traffic sign competition (Sec. 5.19). See further improvements in Sec. 5.22.
Also in 2012, the biggest NN so far (109 free parameters) was trained in unsupervised mode (Sec. 5.7, 5.15) on unlabeled data (Le et al., 2012), then applied to ImageNet. The codes across its top layer were used to train a simple supervised classifier, which achieved best results so far on 20,000 classes. Instead of relying on efficient GPU programming, this was done by brute force on 1,000 standard machines with
16,000 cores.
So by 2011/2012, excellent results had been achieved by Deep Learners in image recognition and classification (Sec. 5.19, 5.21). The computer vision community, however, is especially interested in object detection in large images, for applications such as image-based search engines, or for biomedical diagnosis where the goal may be to automatically detect tumors etc in images of human tissue. Object detection presents additional challenges. One natural approach is to train a deep NN classifier on patches of big images, then use it as a feature detector to be shifted across unknown visual scenes, using various rotations and zoom factors. Image parts that yield highly active output units are likely to contain objects similar to those the NN was trained on.
2012 finally saw the first DL system (an MC-GPU-MPCNN, Sec. 5.19) to win a contest on visual object detection (Ciresan et al., 2013) in large images of several million pixels (ICPR 2012 Contest on Mitosis
Detection in Breast Cancer Histological Images, 2012; Roux et al., 2013). Such biomedical applications may turn out to be among the most important applications of DL. The world spends over 10% of GDP on healthcare (> 6 trillion USD per year), much of it on medical diagnosis through expensive experts. Partial automation of this could not only save lots of money, but also make expert diagnostics accessible to many who currently cannot afford it. It is gratifying to observe that today deep NNs may actually help to improve healthcare and perhaps save human lives.
2012 also saw the first pure image segmentation contest won by DL (Ciresan et al., 2012a), again through an MC-GPU-MPCNN (Segmentation of Neuronal Structures in EM Stacks Challenge, 2012).2
EM stacks are relevant for the recently approved huge brain projects in Europe and the US (e.g., Markram, 2012). Given electron microscopy images of stacks of thin slices of animal brains, the goal is to build a detailed 3D model of the brain's neurons and dendrites. But human experts need many hours and days and weeks to annotate the images: Which parts depict neuronal membranes? Which parts are irrelevant background? This needs to be automated (e.g., Turaga et al., 2010). Deep MC-GPU-MPCNNs learned to solve this task through experience with many training images, and won the contest on all three evaluation metrics by a large margin, with superhuman performance in terms of pixel error.
Both object detection (Ciresan et al., 2013) and image segmentation (Ciresan et al., 2012a) profit from fast MPCNN-based image scans that avoid redundant computations. Recent MPCNN scanners speed up
2It should be mentioned, however, that LSTM RNNs already performed simultaneous segmentation and recognition when they became the first recurrent Deep Learners to win official international pattern recognition contests—see Sec. 5.17.
21 naive implementations by up to three orders of magnitude (Masci et al., 2013; Giusti et al., 2013); compare earlier efficient methods for CNNs without MP (Vaillant et al., 1994).
Also in 2012, a system consisting of growing deep FNNs and 2D-BRNNs (Di Lena et al., 2012) won the CASP 2012 contest on protein contact map prediction. On the IAM-OnDoDB benchmark, LSTM
RNNs (Sec. 5.13) outperformed all other methods (HMMs, SVMs) on online mode detection (Otte et al., 2012; Indermuhle et al., 2012) and keyword spotting (Indermuhle et al., 2011). On the long time lag problem of language modelling, LSTM RNNs outperformed all statistical approaches on the IAM-DB benchmark (Frinken et al., 2012); improved results were later obtained through a combination of NNs and HMMs (Zamora-Martnez et al., 2014). Compare other recent RNNs for object recognition (Wyatte et al., 2012; OReilly et al., 2013), extending earlier work on biologically plausible learning rules for RNNs (O'Reilly, 1996).
2013-: More Contests and Benchmark Records
A stack (Fernandez et al., 2007; Graves and Schmidhuber, 2009) (Sec. 5.10) of bi-directional LSTM recurrent NNs (Graves and Schmidhuber, 2005) trained by CTC (Sec. 5.13, 5.17) broke a famous TIMIT speech(phoneme) recognition record, achieving 17.7% test set error rate (Graves et al., 2013), despite thousands of man years previously spent on Hidden Markov Model (HMMs)-based speech recognition research. Compare earlier DBN results (Sec. 5.15). CTC-LSTM also helped to score first at NIST's OpenHaRT2013 evaluation (Bluche et al., 2014). For Optical Character Recognition (OCR), LSTM RNNs outperformed commercial recognizers of historical data (Breuel et al., 2013).
A new record on the ICDAR Chinese handwriting recognition benchmark (over 3700 classes) was set on a desktop machine by an MC-GPU-MPCNN (Sec. 5.19) with almost human performance (Ciresan and Schmidhuber, 2013); compare (Yin et al., 2013).
The MICCAI 2013 Grand Challenge on Mitosis Detection (Veta et al., 2013) also was won by an objectdetecting MC-GPU-MPCNN (Ciresan et al., 2013). Its data set was even larger and more challenging than the one of ICPR 2012 (Sec. 5.21): a real-world dataset including many ambiguous cases and frequently encountered problems such as imperfect slide staining.
Three 2D-CNNs (with mean-pooling instead of MP, Sec. 5.11) observing three orthogonal projections of 3D images outperformed traditional full 3D methods on the task of segmenting tibial cartilage in low field knee MRI scans (Prasoon et al., 2013).
Deep GPU-MPCNNs (Sec. 5.19) also helped to achieve new best results on important benchmarks of the computer vision community: ImageNet classification (Zeiler and Fergus, 2013) and—in conjunction with traditional approaches—PASCAL object detection (Girshick et al., 2013). They also learned to predict bounding box coordinates of objects in the Imagenet 2013 database, and obtained state-of-the-art results on tasks of localization and detection (Sermanet et al., 2013). GPU-MPCNNs also helped to recognise multidigit numbers in Google Street View images (Goodfellow et al., 2014b), where part of the NN was trained to count visible digits; compare earlier work on detecting "numerosity" through DBNs (Stoianov and Zorzi, 2012). This system also excelled at recognising distorted synthetic text in reCAPTCHA puzzles. Other successful CNN applications include scene parsing (Farabet et al., 2013), object detection (Szegedy et al., 2013), shadow detection (Khan et al., 2014), video classification (Karpathy et al., 2014), and Alzheimers disease neuroimaging (Li et al., 2014).
Additional contests are mentioned in the web pages of the Swiss AI Lab IDSIA, the University of Toronto, NY University, and the University of Montreal. (Unlike in most academic contests, winners of contests listed at the commercial web site kaggle.com have to hand their code over to companies.)
Currently Successful Supervised Techniques: LSTM RNNs / GPU-MPCNNs
Most competition-winning or benchmark record-setting Deep Learners actually use one of two supervised techniques: (a) recurrent LSTM (1997) trained by CTC (2006) (Sec. 5.13, 5.17, 5.21, 5.22), or (b) feedforward GPU-MPCNNs (2011, Sec. 5.19, 5.21, 5.22) based on CNNs (1979, Sec. 5.4) with MP (1992, Sec. 5.11) trained through BP (1989–2007, Sec. 5.8, 5.16).
Exceptions include two 2011 contests (Goodfellow et al., 2011; Mesnil et al., 2011; Goodfellow et al., 2012) specialised on Transfer Learning from one dataset to another (e.g., Caruana, 1997; Schmidhuber, 2004; Pan and Yang, 2010). However, deep GPU-MPCNNs do allow for pure SL-based transfer (Ciresan et al., 2012d), where pre-training on one training set greatly improves performance on quite different sets, also in more recent studies (Oquab et al., 2013; Donahue et al., 2013). In fact, deep MPCNNs pre-trained by SL can extract useful features from quite diverse off-training-set images, yielding better results than traditional, widely used features such as SIFT (Lowe, 1999, 2004) on many vision tasks (Razavian et al., 2014). To deal with changing datasets, slowly learning deep NNs were also combined with rapidly adapting
"surface" NNs (Kak et al., 2010).
Remarkably, in the 1990s a trend went from partially unsupervised RNN stacks (Sec. 5.10) to purely supervised LSTM RNNs (Sec. 5.13), just like in the 2000s a trend went from partially unsupervised FNN stacks (Sec. 5.15) to purely supervised MPCNNs (Sec. 5.16–5.22). Nevertheless, in many applications it can still be advantageous to combine the best of both worlds—supervised learning and unsupervised pre-training (Sec. 5.10, 5.15).
Recent Tricks for Improving SL Deep NNs (Compare Sec. 5.6.2, 5.6.3)
DBN training (Sec. 5.15) can be improved through gradient enhancements and automatic learning rate adjustments during stochastic gradient descent (Cho et al., 2013; Cho, 2014), and through Tikhonovtype (Tikhonov et al., 1977) regularization of RBMs (Cho et al., 2012). Contractive AEs (Rifai et al., 2011) discourage hidden unit perturbations in response to input perturbations, similar to how FMS (Sec. 5.6.3) for LOCOCODE AEs (Sec. 5.6.4) discourages output perturbations in response to weight perturbations.
Dropout (Hinton et al., 2012b; Ba and Frey, 2013) removes units from NNs during training to improve generalisation. Some view it as an ensemble method that trains multiple data models simultaneously (Baldi and Sadowski, 2014). Under certain circumstances, it could also be viewed as a form of training set augmentation: effectively, more and more informative complex features are removed from the training data.
Compare dropout for RNNs (Pham et al., 2013; Pachitariu and Sahani, 2013; Pascanu et al., 2013a). A deterministic approximation coined fast dropout (Wang and Manning, 2013) can lead to faster learning and evaluation and was adapted for RNNs (Bayer et al., 2013). Dropout is closely related to older, biologically plausible techniques for adding noise to neurons or synapses during training (e.g., Murray and Edwards, 1993; Schuster, 1992; Nadal and Parga, 1994; Jim et al., 1995; An, 1996), which in turn are closely related to finding perturbation-resistant low-complexity NNs, e.g., through FMS (Sec. 5.6.3). MDL-based stochastic variational methods (Graves, 2011) are also related to FMS. They are useful for RNNs, where classic regularizers such as weight decay (Sec. 5.6.3) represent a bias towards limited memory capacity (e.g., Pascanu et al., 2013b).
The activation function f of Rectified Linear Units (ReLUs) is f(x) = x for x > 0, f(x) = 0 otherwise. ReLU NNs are useful for RBMs (Nair and Hinton, 2010; Maas et al., 2013), outperformed sigmoidal activation functions in deep NNs (Glorot et al., 2011), and helped to obtain best results on several benchmark problems across multiple domains (e.g., Krizhevsky et al., 2012; Dahl et al., 2013).
NNs with competing linear units tend to outperform those with non-competing nonlinear units, and avoid catastrophic forgetting through BP when training sets change over time (Srivastava et al., 2013).
In this context, choosing a learning algorithm may be more important than choosing activation functions (Goodfellow et al., 2014a). Maxout NNs (Goodfellow et al., 2013) combine competitive interactions and dropout (see above) to achieve excellent results on certain benchmarks. Compare early RNNs with competing units for SL and RL (Schmidhuber, 1989b). To address overfitting, instead of depending on pre-wired regularizers and hyper-parameters (Hertz et al., 1991; Bishop, 2006), self-delimiting RNNs(SLIM NNs) with competing units (Schmidhuber, 2012) can in principle learn to select their own runtime and their own numbers of effective free parameters, thus learning their own computable regularisers(Sec. 4.4, 5.6.3), becoming fast and slim when necessary. One may penalize the task-specific total length of connections (e.g., Legenstein and Maass, 2002; Schmidhuber, 2012, 2013b; Clune et al., 2013) and communication costs of SLIM NNs implemented on the 3-dimensional brain-like multi-processor hardware to be expected in the future.
RmsProp (Tieleman and Hinton, 2012; Schaul et al., 2013) can speed up first order gradient descent methods (Sec. 5.5, 5.6.2); compare vario-η (Neuneier and Zimmermann, 1996), Adagrad (Duchi et al., 2011) and Adadelta (Zeiler, 2012). DL in NNs can also be improved by transforming hidden unit activations such that they have zero output and slope on average (Raiko et al., 2012). Many additional, older
23 tricks (Sec. 5.6.2, 5.6.3) should also be applicable to today's deep NNs; compare (Orr and M¨uller, 1998;
Montavon et al., 2012).
Consequences for Neuroscience
It is ironic that artificial NNs (ANNs) can help to better understand biological NNs (BNNs)—see the ISBI
2012 results mentioned in Sec. 5.21 (Segmentation of Neuronal Structures in EM Stacks Challenge, 2012;
Ciresan et al., 2012a).
The feature detectors learned by single-layer visual ANNs are similar to those found in early visual processing stages of BNNs (e.g., Sec. 5.6.4). Likewise, the feature detectors learned in deep layers of visual
ANNs should be highly predictive of what neuroscientists will find in deep layers of BNNs. While the visual cortex of BNNs may use quite different learning algorithms, its objective function to be minimised may be quite similar to the one of visual ANNs. In fact, results obtained with relatively deep artificial
DBNs (Lee et al., 2007b) and CNNs (Yamins et al., 2013) seem compatible with insights about the visual pathway in the primate cerebral cortex, which has been studied for many decades (e.g., Hubel and Wiesel, 1968; Perrett et al., 1982; Desimone et al., 1984; Felleman and Van Essen, 1991; Perrett et al., 1992;
Kobatake and Tanaka, 1994; Logothetis et al., 1995; Bichot et al., 2005; Hung et al., 2005; Lennie and Movshon, 2005; Connor et al., 2007; Kriegeskorte et al., 2008; DiCarlo et al., 2012); compare a computer vision-oriented survey (Kruger et al., 2013).
DL with Spiking Neurons?
Many recent DL results profit from GPU-based traditional deep NNs, e.g., Sec. 5.16–5.19. Current GPUs, however, are little ovens, much hungrier for energy than biological brains, whose neurons efficiently communicate by brief spikes (Hodgkin and Huxley, 1952; FitzHugh, 1961; Nagumo et al., 1962), and often remain quiet. Many computational models of such spiking neurons have been proposed and analyzed (e.g., Gerstner and van Hemmen, 1992; Zipser et al., 1993; Stemmler, 1996; Tsodyks et al., 1996; Maex and Orban, 1996; Maass, 1996, 1997; Kistler et al., 1997; Amit and Brunel, 1997; Tsodyks et al., 1998; Kempter et al., 1999; Song et al., 2000; Stoop et al., 2000; Brunel, 2000; Bohte et al., 2002; Gerstner and Kistler, 2002; Izhikevich et al., 2003; Seung, 2003; Deco and Rolls, 2005; Brette et al., 2007; Brea et al., 2013;
Nessler et al., 2013; Kasabov, 2014; Hoerzer et al., 2014; Rezende and Gerstner, 2014).
Future energy-efficient hardware for DL in NNs may implement aspects of such models; see (e.g., Liu et al., 2001; Roggen et al., 2003; Glackin et al., 2005; Schemmel et al., 2006; Fieres et al., 2008; Khan et al., 2008; Serrano-Gotarredona et al., 2009; Jin et al., 2010; Indiveri et al., 2011; Neil and Liu, 2014). A simulated, event-driven, spiking variant (Neftci et al., 2014) of an RBM (Sec. 5.15) was trained by a variant of the Contrastive Divergence algorithm (Hinton, 2002). A spiking DBN with about 250,000 neurons (as part of a larger NN; Eliasmith et al., 2012; Eliasmith, 2013) achieved 6% error rate on MNIST; compare similar results with a spiking DBN variant of depth 3 using a neuromorphic event-based sensor (O'Connor et al., 2013). In practical applications, however, current artificial networks of spiking neurons cannot yet compete with the best traditional deep NNs (e.g., compare MNIST results of Sec. 5.19).
DL in FNNs and RNNs for Reinforcement Learning (RL)
So far we have focused on Deep Learning (DL) in supervised or unsupervised NNs. Such NNs learn to perceive / encode / predict / classify patterns or pattern sequences, but they do not learn to act in the more general sense of Reinforcement Learning (RL) in unknown environments (see surveys, e.g., Kaelbling et al., 1996; Sutton and Barto, 1998; Wiering and van Otterlo, 2012). Here we add a discussion of DL FNNs and RNNs for RL. It will be shorter than the discussion of FNNs and RNNs for SL and UL (Sec. 5), reflecting the current size of the various fields.
Without a teacher, solely from occasional real-valued pain and pleasure signals, RL agents must discover how to interact with a dynamic, initially unknown environment to maximize their expected cumulative reward signals (Sec. 2). There may be arbitrary, a priori unknown delays between actions and perceivable consequences. The problem is as hard as any problem of computer science, since any task with a computable description can be formulated in the RL framework (e.g., Hutter, 2005). For example, an answer to the famous question of whether P = NP (Levin, 1973b; Cook, 1971) would also set limits for what is achievable by general RL. Compare more specific limitations, e.g., (Blondel and Tsitsiklis, 2000; Madani et al., 2003; Vlassis et al., 2012). The following subsections mostly focus on certain obvious intersections between DL and RL—they cannot serve as a general RL survey.
RL Through NN World Models Yields RNNs With Deep CAPs
In the special case of an RL FNN controller C interacting with a deterministic, predictable environment, a separate FNN called M can learn to become C's world model through system identification, predicting C's inputs from previous actions and inputs (e.g., Werbos, 1981, 1987; Munro, 1987; Jordan, 1988; Werbos, 1989b,a; Robinson and Fallside, 1989; Jordan and Rumelhart, 1990; Schmidhuber, 1990d; Narendra and Parthasarathy, 1990; Werbos, 1992; Gomi and Kawato, 1993; Cochocki and Unbehauen, 1993; Levin and Narendra, 1995; Miller et al., 1995; Ljung, 1998; Prokhorov et al., 2001; Ge et al., 2010). Assume M has learned to produce accurate predictions. We can use M to substitute the environment. Then M and C form an RNN where M's outputs become inputs of C, whose outputs (actions) in turn become inputs of M. Now BP for RNNs (Sec. 5.5.1) can be used to achieve desired input events such as high real-valued reward signals: While M's weights remain fixed, gradient information for C's weights is propagated back through M down into C and back through M etc. To a certain extent, the approach is also applicable in probabilistic or uncertain environments, as long as the inner products of M's C-based gradient estimates and M's "true" gradients tend to be positive.
In general, this approach implies deep CAPs for C, unlike in DP-based traditional RL (Sec. 6.2).
Decades ago, the method was used to learn to back up a model truck (Nguyen and Widrow, 1989). An
RL active vision system used it to learn sequential shifts (saccades) of a fovea, to detect targets in visual scenes (Schmidhuber and Huber, 1991), thus learning to control selective attention. Compare RL-based attention learning without NNs (Whitehead, 1992).
To allow for memories of previous events in partially observable worlds (Sec. 6.3), the most general variant of this technique uses RNNs instead of FNNs to implement both M and C (Schmidhuber, 1990d, 1991c; Feldkamp and Puskorius, 1998). This may cause deep CAPs not only for C but also for M.
M can also be used to optimize expected reward by planning future action sequences (Schmidhuber, 1990d). In fact, the winners of the 2004 RoboCup World Championship in the fast league (Egorova et al., 2004) trained NNs to predict the effects of steering signals on fast robots with 4 motors for 4 different wheels. During play, such NN models were used to achieve desirable subgoals, by optimizing action sequences through quickly planning ahead. The approach also was used to create self-healing robots able to compensate for faulty motors whose effects do not longer match the predictions of the NN models (Gloye et al., 2005; Schmidhuber, 2007).
Typically M is not given in advance. Then an essential question is: which experiments should C conduct to quickly improve M? The Formal Theory of Fun and Creativity (e.g., Schmidhuber, 2006a, 2013b) formalizes driving forces and value functions behind such curious and exploratory behavior: A measure of the learning progress of M becomes the intrinsic reward of C (Schmidhuber, 1991a); compare (Singh et al., 2005; Oudeyer et al., 2013). This motivates C to create action sequences (experiments) such that M makes quick progress.
Deep FNNs for Traditional RL and Markov Decision Processes (MDPs)
The classical approach to RL (Samuel, 1959; Bertsekas and Tsitsiklis, 1996) makes the simplifying assumption of Markov Decision Processes (MDPs): the current input of the RL agent conveys all information necessary to compute an optimal next output event or decision. This allows for greatly reducing CAP depth in RL NNs (Sec. 3, 6.1) by using the Dynamic Programming (DP) trick (Bellman, 1957). The latter is often explained in a probabilistic framework (e.g., Sutton and Barto, 1998), but its basic idea can already be conveyed in a deterministic setting. For simplicity, using the notation of Sec. 2, let input events xt encode the entire current state of the environment, including a real-valued reward rt (no need to introduce additional vector-valued notation, since real values can encode arbitrary vectors of real values). The original RL goal (find weights that maximize the sum of all rewards of an episode) is replaced by an equivalent set of alternative goals set by a real-valued value function V defined on input events. Consider any two subsequent input events xt, xk. Recursively define V (xt) = rt + V (xk), where V (xk) = rk if xk is the last input event. Now search for weights that maximize the V of all input events, by causing appropriate output events or actions.
Due to the Markov assumption, an FNN suffices to implement the policy that maps input to output events. Relevant CAPs are not deeper than this FNN. V itself is often modeled by a separate FNN (also yielding typically short CAPs) learning to approximate V (xt) only from local information rt, V (xk).
Many variants of traditional RL exist (e.g., Barto et al., 1983; Watkins, 1989; Watkins and Dayan, 1992; Moore and Atkeson, 1993; Schwartz, 1993; Baird, 1994; Rummery and Niranjan, 1994; Singh, 1994; Baird, 1995; Kaelbling et al., 1995; Peng and Williams, 1996; Mahadevan, 1996; Tsitsiklis and van Roy, 1996; Bradtke et al., 1996; Santamar´ıa et al., 1997; Prokhorov and Wunsch, 1997; Sutton and Barto, 1998; Wiering and Schmidhuber, 1998b; Baird and Moore, 1999; Meuleau et al., 1999; Morimoto and Doya, 2000; Bertsekas, 2001; Brafman and Tennenholtz, 2002; Abounadi et al., 2002; Lagoudakis and Parr, 2003; Sutton et al., 2008; Maei and Sutton, 2010; van Hasselt, 2012). Most are formulated in a probabilistic framework, and evaluate pairs of input and output (action) events (instead of input events only). To facilitate certain mathematical derivations, some discount delayed rewards, but such distortions of the original RL problem are problematic.
Perhaps the most well-known RL NN is the world-class RL backgammon player (Tesauro, 1994), which achieved the level of human world champions by playing against itself. Its nonlinear, rather shallow FNN maps a large but finite number of discrete board states to values. More recently, a rather deep GPU-CNN was used in a traditional RL framework to play several Atari 2600 computer games directly from 84x84 pixel 60 Hz video input (Mnih et al., 2013), using experience replay (Lin, 1993), extending previous work on Neural Fitted Q-Learning (NFQ) (Riedmiller, 2005). Compare RBM-based RL (Sallans and Hinton, 2004) with high-dimensional inputs (Elfwing et al., 2010), earlier RL Atari players (Gr¨uttner et al., 2010), and an earlier, raw video-based RL NN for computer games (Koutn´ık et al., 2013) trained by Indirect Policy
Search (Sec. 6.7).
Deep RL RNNs for Partially Observable MDPs (POMDPs)
The Markov assumption (Sec. 6.2) is often unrealistic. We cannot directly perceive what is behind our back, let alone the current state of the entire universe. However, memories of previous events can help to deal with partially observable Markov decision problems (POMDPs) (e.g., Schmidhuber, 1990d, 1991c; Ring, 1991, 1993, 1994; Williams, 1992a; Lin, 1993; Teller, 1994; Kaelbling et al., 1995; Littman et al., 1995; Boutilier and Poole, 1996; Littman, 1996; Jaakkola et al., 1995; McCallum, 1996; Kimura et al., 1997; Wiering and Schmidhuber, 1996, 1998a; Otsuka et al., 2010). A naive way of implementing memories without leaving the MDP framework (Sec. 6.2) would be to simply consider a possibly huge state space, namely, the set of all possible observation histories and their prefixes. A more realistic way is to use function approximators such as RNNs that produce compact state features as a function of the entire history seen so far. Generally speaking, POMDP RL often uses DL RNNs to learn which events to memorize and which to ignore. Three basic alternatives are:
1. Use an RNN as a value function mapping arbitrary event histories to values (e.g., Schmidhuber, 1990b, 1991c; Lin, 1993; Bakker, 2002). For example, deep LSTM RNNs were used in this way for
RL robots (Bakker et al., 2003).
2. Use an RNN controller in conjunction with a second RNN as predictive world model, to obtain a combined RNN with deep CAPs—see Sec. 6.1.
3. Use an RNN for RL by Direct Search (Sec. 6.6) or Indirect Search (Sec. 6.7) in weight space.
In general, however, POMDPs may imply greatly increased CAP depth.
RL Facilitated by Deep UL in FNNs and RNNs
RL machines may profit from UL for input preprocessing (e.g., Jodogne and Piater, 2007). In particular, an UL NN can learn to compactly encode environmental inputs such as images or videos, e.g., Sec. 5.7, 5.10, 5.15. The compact codes (instead of the high-dimensional raw data) can be fed into an RL machine, whose job thus may become much easier (Legenstein et al., 2010; Cuccu et al., 2011), just like SL may profit from UL, e.g., Sec. 5.7, 5.10, 5.15. For example, NFQ (Riedmiller, 2005) was applied to realworld control tasks (Lange and Riedmiller, 2010; Riedmiller et al., 2012) where purely visual inputs were compactly encoded by deep autoencoders (Sec. 5.7, 5.15). RL combined with UL based on Slow Feature
Analysis (Wiskott and Sejnowski, 2002; Kompella et al., 2012) enabled a real humanoid robot to learn skills from raw high-dimensional video streams (Luciw et al., 2013). To deal with POMDPs (Sec. 6.3) involving high-dimensional inputs, RBM-based RL was used (Otsuka, 2010), and a RAAM (Pollack, 1988) (Sec. 5.7) was employed as a deep unsupervised sequence encoder for RL (Gisslen et al., 2011). Certain types of RL and UL also were combined in biologically plausible RNNs with spiking neurons (Sec. 5.26) (e.g., Klampfl and Maass, 2013; Rezende and Gerstner, 2014).
Deep Hierarchical RL (HRL) and Subgoal Learning with FNNs and RNNs
Multiple learnable levels of abstraction (Fu, 1977; Lenat and Brown, 1984; Ring, 1994; Bengio et al., 2013; Deng and Yu, 2014) seem as important for RL as for SL. Work on NN-based Hierarchical RL (HRL) has been published since the early 1990s. In particular, gradient-based subgoal discovery with FNNs or RNNs decomposes RL tasks into subtasks for RL submodules (Schmidhuber, 1991b; Schmidhuber and Wahnsiedler, 1992). Numerous alternative HRL techniques have been proposed (e.g., Ring, 1991, 1994;
Jameson, 1991; Tenenberg et al., 1993; Weiss, 1994; Moore and Atkeson, 1995; Precup et al., 1998; Dietterich, 2000b; Menache et al., 2002; Doya et al., 2002; Ghavamzadeh and Mahadevan, 2003; Barto and Mahadevan, 2003; Samejima et al., 2003; Bakker and Schmidhuber, 2004; Whiteson et al., 2005; Simsek and Barto, 2008). While HRL frameworks such as Feudal RL (Dayan and Hinton, 1993) and options (Sutton et al., 1999b; Barto et al., 2004; Singh et al., 2005) do not directly address the problem of automatic subgoal discovery, HQ-Learning (Wiering and Schmidhuber, 1998a) automatically decomposes POMDPs(Sec. 6.3) into sequences of simpler subtasks that can be solved by memoryless policies learnable by reactive sub-agents. Recent HRL organizes potentially deep NN-based RL sub-modules into self-organizing, 2-dimensional motor control maps (Ring et al., 2011) inspired by neurophysiological findings (Graziano, Deep RL by Direct NN Search / Policy Gradients / Evolution
Not quite as universal as the methods of Sec. 6.8, yet both practical and more general than most traditional
RL algorithms (Sec. 6.2), are methods for Direct Policy Search (DS). Without a need for value functions or Markovian assumptions (Sec. 6.2, 6.3), the weights of an FNN or RNN are directly evaluated on the given RL problem. The results of successive trials inform further search for better weights. Unlike with
RL supported by BP (Sec. 5.5, 6.3, 6.1), CAP depth (Sec. 3, 5.9) is not a crucial issue. DS may solve the credit assignment problem without backtracking through deep causal chains of modifiable parameters—it neither cares for their existence, nor tries to exploit them.
An important class of DS methods for NNs are Policy Gradient methods (Williams, 1986, 1988, 1992a;
Baxter and Bartlett, 1999; Sutton et al., 1999a; Baxter and Bartlett, 2001; Aberdeen, 2003; Ghavamzadeh and Mahadevan, 2003; Kohl and Stone, 2004; Wierstra et al., 2007, 2008; R¨uckstieß et al., 2008; Peters and Schaal, 2008b,a; Sehnke et al., 2010; Gr¨uttner et al., 2010; Wierstra et al., 2010; Peters, 2010; Grondman
27 et al., 2012; Heess et al., 2012). Gradients of the total reward with respect to policies (NN weights) are estimated (and then exploited) through repeated NN evaluations.
RL NNs can also be evolved through Evolutionary Algorithms (EAs) (Rechenberg, 1971; Schwefel, 1974; Holland, 1975; Fogel et al., 1966; Goldberg, 1989) in a series of trials. Here several policies are represented by a population of NNs improved through mutations and/or repeated recombinations of the population's fittest individuals (e.g., Montana and Davis, 1989; Fogel et al., 1990; Maniezzo, 1994; Happel and Murre, 1994; Nolfi et al., 1994b). Compare Genetic Programming (GP) (Cramer, 1985) (see also Smith, 1980) which can be used to evolve computer programs of variable size (Dickmanns et al., 1987; Koza, 1992), and Cartesian GP (Miller and Thomson, 2000; Miller and Harding, 2009) for evolving graph-like programs, including NNs (Khan et al., 2010) and their topology (Turner and Miller, 2013). Related methods include probability distribution-based EAs (Baluja, 1994; Saravanan and Fogel, 1995; Sałustowicz and Schmidhuber, 1997; Larraanaga and Lozano, 2001), Covariance Matrix Estimation Evolution Strategies(CMA-ES) (Hansen and Ostermeier, 2001; Hansen et al., 2003; Igel, 2003; Heidrich-Meisner and Igel, 2009), and NeuroEvolution of Augmenting Topologies (NEAT) (Stanley and Miikkulainen, 2002). Hybrid methods combine traditional NN-based RL (Sec. 6.2) and EAs (e.g., Whiteson and Stone, 2006).
Since RNNs are general computers, RNN evolution is like GP in the sense that it can evolve general programs. Unlike sequential programs learned by traditional GP, however, RNNs can mix sequential and parallel information processing in a natural and efficient way, as already mentioned in Sec. 1. Many RNN evolvers have been proposed (e.g., Miller et al., 1989; Wieland, 1991; Cliff et al., 1993; Yao, 1993; Nolfi et al., 1994a; Sims, 1994; Yamauchi and Beer, 1994; Miglino et al., 1995; Moriarty, 1997; Pasemann et al., 1999; Juang, 2004; Whiteson, 2012). One particularly effective family of methods coevolves neurons, combining them into networks, and selecting those neurons for reproduction that participated in the bestperforming networks (Moriarty and Miikkulainen, 1996; Gomez, 2003; Gomez and Miikkulainen, 2003).
This can help to solve deep POMDPs (Gomez and Schmidhuber, 2005). Co-Synaptic Neuro-Evolution(CoSyNE) does something similar on the level of synapses or weights (Gomez et al., 2008); benefits of this were shown on difficult nonlinear POMDP benchmarks.
Natural Evolution Strategies (NES) (Wierstra et al., 2008; Glasmachers et al., 2010; Sun et al., 2009, 2013) link policy gradient methods and evolutionary approaches through the concept of Natural Gradients (Amari, 1998). RNN evolution may also help to improve SL for deep RNNs through Evolino (Schmidhuber et al., 2007) (Sec. 5.9).
Deep RL by Indirect Policy Search / Compressed NN Search
Some DS methods (Sec. 6.6) can evolve NNs with hundreds of weights, but not millions. How to search for large and deep NNs? Most SL and RL methods mentioned so far somehow search the space of weights wi. Some profit from a reduction of the search space through shared wi that get reused over and over again, e.g., in CNNs (Sec. 5.4, 5.8, 5.16, 5.21), or in RNNs for SL (Sec. 5.5, 5.13, 5.17) and RL (Sec. 6.1, 6.3, 6.6).
It may be possible, however, to exploit additional regularities/compressibilities in the space of solutions, through indirect search in weight space. Instead of evolving large NNs directly (Sec. 6.6), one can sometimes greatly reduce the search space by evolving compact encodings of NNs, e.g., through Lindenmeyer Systems (Lindenmayer, 1968; Jacob et al., 1994), graph rewriting (Kitano, 1990), Cellular Encoding (Gruau et al., 1996), HyperNEAT (D'Ambrosio and Stanley, 2007; Stanley et al., 2009; Clune et al., 2011; van den Berg and Whiteson, 2013) (extending NEAT; Sec. 6.6), and extensions thereof (e.g., Risi and Stanley, 2012). This helps to avoid overfitting (compare Sec. 5.6.3, 5.24) and is closely related to the topics of regularisation and MDL (Sec. 4.4).
A general approach (Schmidhuber, 1997) for both SL and RL seeks to compactly encode weights of large NNs (Schmidhuber, 1997) through programs written in a universal programming language (G¨odel, 1931; Church, 1936; Turing, 1936; Post, 1936). Often it is much more efficient to systematically search the space of such programs with a bias towards short and fast programs (Levin, 1973b; Schmidhuber, 1997, 2004), instead of directly searching the huge space of possible NN weight matrices. A previous universal language for encoding NNs was assembler-like (Schmidhuber, 1997). More recent work uses more practical languages based on coefficients of popular transforms (Fourier, wavelet, etc). In particular, RNN weight matrices may be compressed like images, by encoding them through the coefficients of a discrete cosine transform (DCT) (Koutn´ık et al., 2010, 2013). Compact DCT-based descriptions can be
28 evolved through NES or CoSyNE (Sec. 6.6). An RNN with over a million weights learned (without a teacher) to drive a simulated car in the TORCS driving game (Loiacono et al., 2009, 2011), based on a high-dimensional video-like visual input stream (Koutn´ık et al., 2013). The RNN learned both control and visual processing from scratch, without being aided by UL. (Of course, UL might help to generate more compact image codes (Sec. 6.4, 4.2) to be fed into a smaller RNN, to reduce the overall computational effort.)
Universal RL
General purpose learning algorithms may improve themselves in open-ended fashion and environmentspecific ways in a lifelong learning context (Schmidhuber, 1987; Schmidhuber et al., 1997b,a; Schaul and Schmidhuber, 2010). The most general type of RL is constrained only by the fundamental limitations of computability identified by the founders of theoretical computer science (G¨odel, 1931; Church, 1936; Turing, 1936; Post, 1936). Remarkably, there exist blueprints of universal problem solvers or universal RL machines for unlimited problem depth that are time-optimal in various theoretical senses (Hutter, 2005, 2002; Schmidhuber, 2002, 2006b). In particular, the G¨odel Machine can be implemented on general computers such as RNNs and may improve any part of its software (including the learning algorithm itself) in a way that is provably time-optimal in a certain sense (Schmidhuber, 2006b). It can be initialized by an asymptotically optimal meta-method (Hutter, 2002) (also applicable to RNNs) which will solve any well-defined problem as quickly as the unknown fastest way of solving it, save for an additive constant overhead that becomes negligible as problem size grows. Note that most problems are large; only few are small. AI and DL researchers are still in business because many are interested in problems so small that it is worth trying to reduce the overhead through less general methods, including heuristics. Here I won't further discuss universal RL methods, which go beyond what is usually called DL.
Conclusion
Deep Learning (DL) in Neural Networks (NNs) is relevant for Supervised Learning (SL) (Sec. 5), Unsupervised Learning (UL) (Sec. 5), and Reinforcement Learning (RL) (Sec. 6). By alleviating problems with deep Credit Assignment Paths (CAPs, Sec. 3, 5.9), UL (Sec. 5.6.4) can not only facilitate SL of sequences(Sec. 5.10) and stationary patterns (Sec. 5.7, 5.15), but also RL (Sec. 6.4, 4.2). Dynamic Programming (DP, Sec. 4.1) is important for both deep SL (Sec. 5.5) and traditional RL with deep NNs (Sec. 6.2). A search for solution-computing, perturbation-resistant (Sec. 5.6.3, 5.15, 5.24), low-complexity NNs describable by few bits of information (Sec. 4.4) can reduce overfitting and improve deep SL & UL (Sec. 5.6.3, 5.6.4) as well as RL (Sec. 6.7), also in the case of partially observable environments (Sec. 6.3). Deep SL, UL, RL often create hierarchies of more and more abstract representations of stationary data (Sec. 5.3, 5.7, 5.15), sequential data (Sec. 5.10), or RL policies (Sec. 6.5). While UL can facilitate SL, pure SL for feedforward NNs(FNNs) (Sec. 5.5, 5.8, 5.16, 5.18) and recurrent NNs (RNNs) (Sec. 5.5, 5.13) did not only win early contests(Sec. 5.12, 5.14) but also most of the recent ones (Sec. 5.17–5.22). Especially DL in FNNs profited from
GPU implementations (Sec. 5.16–5.19). In particular, GPU-based (Sec. 5.19) Max-Pooling (Sec. 5.11)
Convolutional NNs (Sec. 5.4, 5.8, 5.16) won competitions not only in pattern recognition (Sec. 5.19–5.22) but also image segmentation (Sec. 5.21) and object detection (Sec. 5.21, 5.22). Unlike these systems, humans learn to actively perceive patterns by sequentially directing attention to relevant parts of the available data. Near future deep NNs may do so, too, extending previous work on learning selective attention through
RL of (a) motor actions such as saccade control (Sec. 6.1) and (b) internal actions controlling spotlights of attention within RNNs, thus closing the general sensorimotor loop through both external and internal feedback (e.g., Sec. 2, 5.21, 6.6, 6.7). The more distant future may belong to general purpose learning algorithms that improve themselves in provably optimal ways (Sec. 6.8), but these are not yet practical or commercially relevant.
Acknowledgments
Since 16 April 2014, drafts of this paper have undergone massive open online peer review through public mailing lists including connectionists@cs.cmu.edu, ml-news@googlegroups.com, compneuro@neuroinf.org, genetic programming@yahoogroups.com, rl-list@googlegroups.com, imageworld@diku.dk. Thanks to numerous NN / DL experts for valuable comments.
The contents of this paper may be used for educational and non-commercial purposes, including articles for Wikipedia and similar sites.
References
Aberdeen, D. (2003). Policy-Gradient Algorithms for Partially Observable Markov Decision Processes.
PhD thesis, Australian National University.
Abounadi, J., Bertsekas, D., and Borkar, V. S. (2002). Learning algorithms for Markov decision processes with average cost. SIAM Journal on Control and Optimization, 40(3):681–698.
Akaike, H. (1970). Statistical predictor identification. Ann. Inst. Statist. Math., 22:203–217.
Akaike, H. (1973). Information theory and an extension of the maximum likelihood principle. In Second
Intl. Symposium on Information Theory, pages 267–281. Akademinai Kiado.
Akaike, H. (1974). A new look at the statistical model identification. IEEE Transactions on Automatic
Control, 19(6):716–723.
Allender, A. (1992). Application of time-bounded Kolmogorov complexity in complexity theory. In Watanabe, O., editor, Kolmogorov complexity and computational complexity, pages 6–22. EATCS Monographs on Theoretical Computer Science, Springer.
Almeida, L. B. (1987). A learning rule for asynchronous perceptrons with feedback in a combinatorial environment. In IEEE 1st International Conference on Neural Networks, San Diego, volume 2, pages
609–618.
Almeida, L. B., Almeida, L. B., Langlois, T., Amaral, J. D., and Redol, R. A. (1997). On-line step size adaptation. Technical report, INESC, 9 Rua Alves Redol, 1000.
Amari, S. (1967). A theory of adaptive pattern classifiers. IEEE Trans. EC, 16(3):299–307.
Amari, S., Cichocki, A., and Yang, H. (1996). A new learning algorithm for blind signal separation.
In Touretzky, D. S., Mozer, M. C., and Hasselmo, M. E., editors, Advances in Neural Information
Processing Systems (NIPS), volume 8. The MIT Press.
Amari, S. and Murata, N. (1993). Statistical theory of learning curves under entropic loss criterion. Neural
Computation, 5(1):140–153.
Amari, S.-I. (1998). Natural gradient works efficiently in learning. Neural Computation, 10(2):251–276.
Amit, D. J. and Brunel, N. (1997). Dynamics of a recurrent network of spiking neurons before and following learning. Network: Computation in Neural Systems, 8(4):373–404.
An, G. (1996). The effects of adding noise during backpropagation training on a generalization performance. Neural Computation, 8(3):643–674.
Andrade, M. A., Chacon, P., Merelo, J. J., and Moran, F. (1993). Evaluation of secondary structure of proteins from UV circular dichroism spectra using an unsupervised learning neural network. Protein
Engineering, 6(4):383–390.
Andrews, R., Diederich, J., and Tickle, A. B. (1995). Survey and critique of techniques for extracting rules from trained artificial neural networks. Knowledge-Based Systems, 8(6):373–389.
Anguita, D. and Gomes, B. A. (1996). Mixing floating- and fixed-point formats for neural network learning on neuroprocessors. Microprocessing and Microprogramming, 41(10):757 – 769.
Anguita, D., Parodi, G., and Zunino, R. (1994). An efficient implementation of BP on RISC-based workstations. Neurocomputing, 6(1):57 – 65.
Arel, I., Rose, D. C., and Karnowski, T. P. (2010). Deep machine learning – a new frontier in artificial intelligence research. Computational Intelligence Magazine, IEEE, 5(4):13–18.
Ash, T. (1989).
Dynamic node creation in backpropagation neural networks.
Connection Science, 1(4):365–375.
Atick, J. J., Li, Z., and Redlich, A. N. (1992). Understanding retinal color coding from first principles.
Neural Computation, 4:559–572.
Atiya, A. F. and Parlos, A. G. (2000). New results on recurrent network training: unifying the algorithms and accelerating convergence. IEEE Transactions on Neural Networks, 11(3):697–709.
Ba, J. and Frey, B. (2013). Adaptive dropout for training deep neural networks. In Advances in Neural
Information Processing Systems (NIPS), pages 3084–3092.
Baird, H. (1990). Document image defect models. In Proceddings, IAPR Workshop on Syntactic and Structural Pattern Recognition, Murray Hill, NJ.
Baird, L. and Moore, A. W. (1999). Gradient descent for general reinforcement learning. In Advances in neural information processing systems 12 (NIPS), pages 968–974. MIT Press.
Baird, L. C. (1994). Reinforcement learning in continuous time: Advantage updating. In IEEE World
Congress on Computational Intelligence, volume 4, pages 2448–2453. IEEE.
Baird, L. C. (1995). Residual algorithms: Reinforcement learning with function approximation. In International Conference on Machine Learning, pages 30–37.
Bakker, B. (2002). Reinforcement learning with Long Short-Term Memory. In Dietterich, T. G., Becker, S., and Ghahramani, Z., editors, Advances in Neural Information Processing Systems 14, pages 1475–1482.
MIT Press, Cambridge, MA.
Bakker, B. and Schmidhuber, J. (2004). Hierarchical reinforcement learning based on subgoal discovery and subpolicy specialization. In et al., F. G., editor, Proc. 8th Conference on Intelligent Autonomous
Systems IAS-8, pages 438–445, Amsterdam, NL. IOS Press.
Bakker, B., Zhumatiy, V., Gruener, G., and Schmidhuber, J. (2003). A robot that reinforcement-learns to identify and memorize important previous observations. In Proceedings of the 2003 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2003, pages 430–435.
Baldi, P. (1995). Gradient descent learning algorithms overview: A general dynamical systems perspective.
IEEE Transactions on Neural Networks, 6(1):182–195.
Baldi, P. (2012). Autoencoders, unsupervised learning, and deep architectures. Journal of Machine Learning Research (Proc. 2011 ICML Workshop on Unsupervised and Transfer Learning), 27:37–50.
Baldi, P., Brunak, S., Frasconi, P., Pollastri, G., and Soda, G. (1999). Exploiting the past and the future in protein secondary structure prediction. Bioinformatics, 15:937–946.
Baldi, P. and Chauvin, Y. (1993).
Neural networks for fingerprint recognition.
Neural Computation, 5(3):402–418.
Baldi, P. and Chauvin, Y. (1996). Hybrid modeling, HMM/NN architectures, and protein applications.
Neural Computation, 8(7):1541–1565.
Baldi, P. and Hornik, K. (1989). Neural networks and principal component analysis: Learning from examples without local minima. Neural Networks, 2:53–58.
Baldi, P. and Hornik, K. (1994). Learning in linear networks: a survey. IEEE Transactions on Neural
Networks, 6(4):837–858. 1995.
Baldi, P. and Pollastri, G. (2003). The principled design of large-scale recursive neural network architectures – DAG-RNNs and the protein structure prediction problem. J. Mach. Learn. Res., 4:575–602.
Baldi, P. and Sadowski, P. (2014). The dropout learning algorithm. Artificial Intelligence, 210C:78–122.
Ballard, D. H. (1987). Modular learning in neural networks. In Proc. AAAI, pages 279–284.
Baluja, S. (1994). Population-based incremental learning: A method for integrating genetic search based function optimization and competitive learning. Technical Report CMU-CS-94-163, Carnegie Mellon
University.
Balzer, R. (1985). A 15 year perspective on automatic programming. IEEE Transactions on Software
Engineering, 11(11):1257–1268.
Barlow, H. B. (1989). Unsupervised learning. Neural Computation, 1(3):295–311.
Barlow, H. B., Kaushal, T. P., and Mitchison, G. J. (1989). Finding minimum entropy codes. Neural
Computation, 1(3):412–423.
Barrow, H. G. (1987). Learning receptive fields. In Proceedings of the IEEE 1st Annual Conference on
Neural Networks, volume IV, pages 115–121. IEEE.
Barto, A. G. and Mahadevan, S. (2003). Recent advances in hierarchical reinforcement learning. Discrete
Event Dynamic Systems, 13(4):341–379.
Barto, A. G., Singh, S., and Chentanez, N. (2004). Intrinsically motivated learning of hierarchical collections of skills. In Proceedings of International Conference on Developmental Learning (ICDL), pages
112–119. MIT Press, Cambridge, MA.
Barto, A. G., Sutton, R. S., and Anderson, C. W. (1983). Neuronlike adaptive elements that can solve difficult learning control problems. IEEE Transactions on Systems, Man, and Cybernetics, SMC-13:834–
Battiti, R. (1989). Accelerated backpropagation learning: two optimization methods. Complex Systems, 3(4):331–342.
Battiti, T. (1992). First- and second-order methods for learning: Between steepest descent and Newton's method. Neural Computation, 4(2):141–166.
Baum, E. B. and Haussler, D. (1989). What size net gives valid generalization?
Neural Computation, 1(1):151–160.
Baum, L. E. and Petrie, T. (1966). Statistical inference for probabilistic functions of finite state Markov chains. The Annals of Mathematical Statistics, pages 1554–1563.
Baxter, J. and Bartlett, P. (1999). Direct gradient-based reinforcement learning. Technical report, Research
School of Information Sciences and Engineering, Australian National University.
Baxter, J. and Bartlett, P. L. (2001).
Infinite-horizon policy-gradient estimation.
J. Artif. Int. Res., 15(1):319–350.
Bayer, J., Osendorfer, C., Chen, N., Urban, S., and van der Smagt, P. (2013). On fast dropout and its applicability to recurrent networks. arXiv preprint arXiv:1311.0701.
Bayer, J., Wierstra, D., Togelius, J., and Schmidhuber, J. (2009). Evolving memory cell structures for sequence learning. In Proc. ICANN (2), pages 755–764.
Bayes, T. (1763). An essay toward solving a problem in the doctrine of chances. Philosophical Transactions of the Royal Society of London, 53:370–418. Communicated by R. Price, in a letter to J. Canton.
Becker, S. (1991). Unsupervised learning procedures for neural networks. International Journal of Neural
Systems, 2(1 & 2):17–33.
Becker, S. and Le Cun, Y. (1989). Improving the convergence of back-propagation learning with second order methods. In Touretzky, D., Hinton, G., and Sejnowski, T., editors, Proc. 1988 Connectionist
Models Summer School, pages 29–37, Pittsburg 1988. Morgan Kaufmann, San Mateo.
Behnke, S. (2003). Hierarchical Neural Networks for Image Interpretation, volume LNCS 2766 of Lecture
Notes in Computer Science. Springer.
Bell, A. J. and Sejnowski, T. J. (1995). An information-maximization approach to blind separation and blind deconvolution. Neural Computation, 7(6):1129–1159.
Bellman, R. (1957). Dynamic Programming. Princeton University Press, Princeton, NJ, USA, 1st edition.
Belouchrani, A., Abed-Meraim, K., Cardoso, J.-F., and Moulines, E. (1997). A blind source separation technique using second-order statistics. IEEE Transactions on Signal Processing, 45(2):434–444.
Bengio, Y. (1991). Artificial Neural Networks and their Application to Sequence Recognition. PhD thesis, McGill University, (Computer Science), Montreal, Qc., Canada.
Bengio, Y. (2009). Learning Deep Architectures for AI. Foundations and Trends in Machine Learning, V2(1). Now Publishers.
Bengio, Y., Courville, A., and Vincent, P. (2013). Representation learning: A review and new perspectives.
Pattern Analysis and Machine Intelligence, IEEE Transactions on, 35(8):1798–1828.
Bengio, Y., Lamblin, P., Popovici, D., and Larochelle, H. (2007). Greedy layer-wise training of deep networks. In Cowan, J. D., Tesauro, G., and Alspector, J., editors, Advances in Neural Information
Processing Systems 19 (NIPS), pages 153–160. MIT Press.
Bengio, Y., Simard, P., and Frasconi, P. (1994). Learning long-term dependencies with gradient descent is difficult. IEEE Transactions on Neural Networks, 5(2):157–166.
Beringer, N., Graves, A., Schiel, F., and Schmidhuber, J. (2005).
Classifying unprompted speech by retraining LSTM nets. In Duch, W., Kacprzyk, J., Oja, E., and Zadrozny, S., editors, Artificial Neural
Networks: Biological Inspirations - ICANN 2005, LNCS 3696, pages 575–581. Springer-Verlag Berlin
Heidelberg.
Bertsekas, D. P. (2001). Dynamic Programming and Optimal Control. Athena Scientific.
Bertsekas, D. P. and Tsitsiklis, J. N. (1996). Neuro-dynamic Programming. Athena Scientific, Belmont, MA.
Bichot, N. P., Rossi, A. F., and Desimone, R. (2005). Parallel and serial neural mechanisms for visual search in macaque area V4. Science, 308:529–534.
Biegler-K¨onig, F. and B¨armann, F. (1993). A learning algorithm for multilayered neural networks based on linear least squares problems. Neural Networks, 6(1):127–131.
Bishop, C. M. (1993). Curvature-driven smoothing: A learning algorithm for feed-forward networks. IEEE
Transactions on Neural Networks, 4(5):882–884.
Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.
Blair, A. D. and Pollack, J. B. (1997). Analysis of dynamical recognizers. Neural Computation, 9(5):1127–
Blondel, V. D. and Tsitsiklis, J. N. (2000). A survey of computational complexity results in systems and control. Automatica, 36(9):1249–1274.
Bluche, T., Louradour, J., Knibbe, M., Moysset, B., Benzeghiba, F., and Kermorvant., C. (2014). The A2iA Arabic Handwritten Text Recognition System at the OpenHaRT2013 Evaluation. In International
Workshop on Document Analysis Systems.
Blum, A. L. and Rivest, R. L. (1992). Training a 3-node neural network is np-complete. Neural Networks, 5(1):117–127.
Blumer, A., Ehrenfeucht, A., Haussler, D., and Warmuth, M. K. (1987). Occam's razor. Information
Processing Letters, 24:377–380.
Bobrowski, L. (1978). Learning processes in multilayer threshold nets. Biological Cybernetics, 31:1–6.
Bod´en, M. and Wiles, J. (2000). Context-free and context-sensitive dynamics in recurrent neural networks.
Connection Science, 12(3-4):197–210.
Bodenhausen, U. and Waibel, A. (1991). The Tempo 2 algorithm: Adjusting time-delays by supervised learning. In Lippman, D. S., Moody, J. E., and Touretzky, D. S., editors, Advances in Neural Information
Processing Systems 3, pages 155–161. Morgan Kaufmann.
Bohte, S. M., Kok, J. N., and La Poutre, H. (2002). Error-backpropagation in temporally encoded networks of spiking neurons. Neurocomputing, 48(1):17–37.
Boltzmann, L. (1909). In Hasen¨ohrl, F., editor, Wissenschaftliche Abhandlungen (collection of Boltzmann's articles in scientific journals). Barth, Leipzig.
Bottou, L. (1991). Une approche th´eorique de l'apprentissage connexioniste; applications `a la reconnaissance de la parole. PhD thesis, Universit´e de Paris XI.
Bourlard, H. and Morgan, N. (1994). Connnectionist Speech Recognition: A Hybrid Approach. Kluwer
Academic Publishers.
Boutilier, C. and Poole, D. (1996). Computing optimal policies for partially observable Markov decision processes using compact representations. In Proceedings of the AAAI, Portland, OR.
Bradtke, S. J., Barto, A. G., and Kaelbling, L. P. (1996). Linear least-squares algorithms for temporal difference learning. In Machine Learning, pages 22–33.
Brafman, R. I. and Tennenholtz, M. (2002). R-MAX—a general polynomial time algorithm for nearoptimal reinforcement learning. Journal of Machine Learning Research, 3:213–231.
Brea, J., Senn, W., and Pfister, J.-P. (2013). Matching recall and storage in sequence learning with spiking neural networks. The Journal of Neuroscience, 33(23):9565–9575.
Breiman, L. (1996). Bagging predictors. Machine Learning, 24:123–140.
Brette, R., Rudolph, M., Carnevale, T., Hines, M., Beeman, D., Bower, J. M., Diesmann, M., Morrison, A., Goodman, P. H., Harris Jr, F. C., et al. (2007). Simulation of networks of spiking neurons: a review of tools and strategies. Journal of Computational Neuroscience, 23(3):349–398.
Breuel, T. M., Ul-Hasan, A., Al-Azawi, M. A., and Shafait, F. (2013). High-performance OCR for printed
English and Fraktur using LSTM networks. In 12th International Conference on Document Analysis and Recognition (ICDAR), pages 683–687. IEEE.
Bromley, J., Bentz, J. W., Bottou, L., Guyon, I., LeCun, Y., Moore, C., Sackinger, E., and Shah, R. (1993).
Signature verification using a Siamese time delay neural network.
International Journal of Pattern
Recognition and Artificial Intelligence, 7(4):669–688.
Broyden, C. G. et al. (1965). A class of methods for solving nonlinear simultaneous equations. Math.
Comp, 19(92):577–593.
Brunel, N. (2000). Dynamics of sparsely connected networks of excitatory and inhibitory spiking neurons.
Journal of Computational Neuroscience, 8(3):183–208.
Bryson, A. and Ho, Y. (1969). Applied optimal control: optimization, estimation, and control. Blaisdell
Pub. Co.
Bryson, A. E. (1961). A gradient method for optimizing multi-stage allocation processes. In Proc. Harvard
Univ. Symposium on digital computers and their applications.
Bryson, Jr., A. E. and Denham, W. F. (1961). A steepest-ascent method for solving optimum programming problems. Technical Report BR-1303, Raytheon Company, Missle and Space Division.
Buhler, J. (2001). Efficient large-scale sequence comparison by locality-sensitive hashing. Bioinformatics, 17(5):419–428.
Buntine, W. L. and Weigend, A. S. (1991). Bayesian back-propagation. Complex Systems, 5:603–643.
Burgess, N. (1994). A constructive algorithm that converges for real-valued input patterns. International
Journal of Neural Systems, 5(1):59–66.
Cardoso, J.-F. (1994). On the performance of orthogonal source separation algorithms. In Proc. EUSIPCO, pages 776–779.
Carreira-Perpinan, M. A. (2001). Continuous latent variable models for dimensionality reduction and sequential data reconstruction. PhD thesis, University of Sheffield UK.
Carter, M. J., Rudolph, F. J., and Nucci, A. J. (1990). Operational fault tolerance of CMAC networks. In
Touretzky, D. S., editor, Advances in Neural Information Processing Systems (NIPS) 2, pages 340–347.
San Mateo, CA: Morgan Kaufmann.
Caruana, R. (1997). Multitask learning. Machine Learning, 28(1):41–75.
Casey, M. P. (1996). The dynamics of discrete-time computation, with application to recurrent neural networks and finite state machine extraction. Neural Computation, 8(6):1135–1178.
Cauwenberghs, G. (1993). A fast stochastic error-descent algorithm for supervised learning and optimization. In Lippman, D. S., Moody, J. E., and Touretzky, D. S., editors, Advances in Neural Information
Processing Systems 5, pages 244–244. Morgan Kaufmann.
Chaitin, G. J. (1966). On the length of programs for computing finite binary sequences. Journal of the ACM, 13:547–569.
Chalup, S. K. and Blair, A. D. (2003). Incremental training of first order recurrent neural networks to predict a context-sensitive language. Neural Networks, 16(7):955–972.
Chellapilla, K., Puri, S., and Simard, P. (2006). High performance convolutional neural networks for document processing. In International Workshop on Frontiers in Handwriting Recognition.
Chen, K. and Salman, A. (2011). Learning speaker-specific characteristics with a deep neural architecture.
IEEE Transactions on Neural Networks, 22(11):1744–1756.
Cho, K. (2014). Foundations and Advances in Deep Learning. PhD thesis, Aalto University School of Science.
Cho, K., Ilin, A., and Raiko, T. (2012). Tikhonov-type regularization for restricted Boltzmann machines.
In Intl. Conf. on Artificial Neural Networks (ICANN) 2012, pages 81–88. Springer.
Cho, K., Raiko, T., and Ilin, A. (2013). Enhanced gradient for training restricted Boltzmann machines.
Neural Computation, 25(3):805–831.
Church, A. (1936). An unsolvable problem of elementary number theory. American Journal of Mathematics, 58:345–363.
Ciresan, D. C., Giusti, A., Gambardella, L. M., and Schmidhuber, J. (2012a).
Deep neural networks segment neuronal membranes in electron microscopy images. In Advances in Neural Information Processing Systems (NIPS), pages 2852–2860.
Ciresan, D. C., Giusti, A., Gambardella, L. M., and Schmidhuber, J. (2013). Mitosis detection in breast cancer histology images with deep neural networks. In Proc. MICCAI, volume 2, pages 411–418.
Ciresan, D. C., Meier, U., Gambardella, L. M., and Schmidhuber, J. (2010). Deep big simple neural nets for handwritten digit recogntion. Neural Computation, 22(12):3207–3220.
Ciresan, D. C., Meier, U., Masci, J., Gambardella, L. M., and Schmidhuber, J. (2011a). Flexible, high performance convolutional neural networks for image classification. In Intl. Joint Conference on Artificial
Intelligence IJCAI, pages 1237–1242.
Ciresan, D. C., Meier, U., Masci, J., and Schmidhuber, J. (2011b). A committee of neural networks for traffic sign classification. In International Joint Conference on Neural Networks (IJCNN), pages 1918–
Ciresan, D. C., Meier, U., Masci, J., and Schmidhuber, J. (2012b). Multi-column deep neural network for traffic sign classification. Neural Networks, 32:333–338.
Ciresan, D. C., Meier, U., and Schmidhuber, J. (2012c). Multi-column deep neural networks for image classification. In IEEE Conference on Computer Vision and Pattern Recognition CVPR 2012. Long preprint arXiv:1202.2745v1 [cs.CV].
Ciresan, D. C., Meier, U., and Schmidhuber, J. (2012d). Transfer learning for Latin and Chinese characters with deep neural networks. In International Joint Conference on Neural Networks (IJCNN), pages 1301–
Ciresan, D. C. and Schmidhuber, J. (2013). Multi-column deep neural networks for offline handwritten
Chinese character classification. Technical report, IDSIA. arXiv:1309.0261.
Cliff, D. T., Husbands, P., and Harvey, I. (1993). Evolving recurrent dynamical networks for robot control.
In Artificial Neural Nets and Genetic Algorithms, pages 428–435. Springer.
Clune, J., Mouret, J.-B., and Lipson, H. (2013). The evolutionary origins of modularity. Proceedings of the Royal Society B: Biological Sciences, 280(1755):20122863.
Clune, J., Stanley, K. O., Pennock, R. T., and Ofria, C. (2011). On the performance of indirect encoding across the continuum of regularity. Trans. Evol. Comp, 15(3):346–367.
Coates, A., Huval, B., Wang, T., Wu, D. J., Ng, A. Y., and Catanzaro, B. (2013). Deep learning with COTS
HPC systems. In Proc. International Conference on Machine learning (ICML'13).
Cochocki, A. and Unbehauen, R. (1993). Neural networks for optimization and signal processing. John
Wiley & Sons, Inc.
Collobert, R. and Weston, J. (2008). A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th International Conference on Machine
Learning (ICML), pages 160–167. ACM.
Comon, P. (1994). Independent component analysis – a new concept? Signal Processing, 36(3):287–314.
Connor, C. E., Brincat, S. L., and Pasupathy, A. (2007). Transformation of shape information in the ventral pathway. Current Opinion in Neurobiology, 17(2):140–147.
Connor, J., Martin, D. R., and Atlas, L. E. (1994). Recurrent neural networks and robust time series prediction. IEEE Transactions on Neural Networks, 5(2):240–254.
Cook, S. A. (1971). The complexity of theorem-proving procedures. In Proceedings of the 3rd Annual
ACM Symposium on the Theory of Computing (STOC'71), pages 151–158. ACM, New York.
Cramer, N. L. (1985). A representation for the adaptive generation of simple sequential programs. In
Grefenstette, J., editor, Proceedings of an International Conference on Genetic Algorithms and Their
Applications, Carnegie-Mellon University, July 24-26, 1985, Hillsdale NJ. Lawrence Erlbaum Associates.
Craven, P. and Wahba, G. (1979). Smoothing noisy data with spline functions: Estimating the correct degree of smoothing by the method of generalized cross-validation. Numer. Math., 31:377–403.
Cuccu, G., Luciw, M., Schmidhuber, J., and Gomez, F. (2011). Intrinsically motivated evolutionary search for vision-based reinforcement learning. In Proceedings of the 2011 IEEE Conference on Development and Learning and Epigenetic Robotics IEEE-ICDL-EPIROB, volume 2, pages 1–7. IEEE.
Dahl, G., Yu, D., Deng, L., and Acero, A. (2012). Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition. Audio, Speech, and Language Processing, IEEE Transactions on, 20(1):30–42.
Dahl, G. E., Sainath, T. N., and Hinton, G. E. (2013). Improving deep neural networks for LVCSR using rectified linear units and dropout. In IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP), pages 8609–8613. IEEE.
D'Ambrosio, D. B. and Stanley, K. O. (2007). A novel generative encoding for exploiting neural network sensor and output geometry. In Proceedings of the Conference on Genetic and Evolutionary Computation(GECCO), pages 974–981.
Datar, M., Immorlica, N., Indyk, P., and Mirrokni, V. S. (2004). Locality-sensitive hashing scheme based on p-stable distributions. In Proceedings of the 20th Annual Symposium on Computational Geometry, pages 253–262. ACM.
Dayan, P. and Hinton, G. (1993). Feudal reinforcement learning. In Lippman, D. S., Moody, J. E., and Touretzky, D. S., editors, Advances in Neural Information Processing Systems (NIPS) 5, pages 271–278.
Morgan Kaufmann.
Dayan, P. and Hinton, G. E. (1996). Varieties of Helmholtz machine. Neural Networks, 9(8):1385–1403.
Dayan, P., Hinton, G. E., Neal, R. M., and Zemel, R. S. (1995). The Helmholtz machine. Neural Computation, 7:889–904.
Dayan, P. and Zemel, R. (1995). Competition and multiple cause models. Neural Computation, 7:565–579.
De Freitas, J. F. G. (2003). Bayesian methods for neural networks. PhD thesis, University of Cambridge. de Souto, M. C., Souto, M. C. P. D., and Oliveira, W. R. D. (1999). The loading problem for pyramidal neural networks. In Electronic Journal on Mathematics of Computation.
De Valois, R. L., Albrecht, D. G., and Thorell, L. G. (1982). Spatial frequency selectivity of cells in macaque visual cortex. Vision Research, 22(5):545–559. de Vries, B. and Principe, J. C. (1991). A theory for neural networks with time delays. In Lippmann, R. P., Moody, J. E., and Touretzky, D. S., editors, Advances in Neural Information Processing Systems (NIPS)
3, pages 162–168. Morgan Kaufmann.
Deco, G. and Parra, L. (1997). Non-linear feature extraction by redundancy reduction in an unsupervised stochastic neural network. Neural Networks, 10(4):683–691.
Deco, G. and Rolls, E. T. (2005). Neurodynamics of biased competition and cooperation for attention: a model with spiking neurons. Journal of Neurophysiology, 94(1):295–313.
DeJong, G. and Mooney, R. (1986). Explanation-based learning: An alternative view. Machine Learning, 1(2):145–176.
DeMers, D. and Cottrell, G. (1993). Non-linear dimensionality reduction. In Hanson, S. J., Cowan, J. D., and Giles, C. L., editors, Advances in Neural Information Processing Systems (NIPS) 5, pages 580–587.
Morgan Kaufmann.
Dempster, A. P., Laird, N. M., and Rubin, D. B. (1977). Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, B, 39.
Deng, L. and Yu, D. (2014). Deep Learning: Methods and Applications. NOW Publishers.
Desimone, R., Albright, T. D., Gross, C. G., and Bruce, C. (1984). Stimulus-selective properties of inferior temporal neurons in the macaque. The Journal of Neuroscience, 4(8):2051–2062.
Deville, Y. and Lau, K. K. (1994). Logic program synthesis. Journal of Logic Programming, 19(20):321–
Di Lena, P., Nagata, K., and Baldi, P. (2012). Deep architectures for protein contact map prediction.
Bioinformatics, 28:2449–2457.
DiCarlo, J. J., Zoccolan, D., and Rust, N. C. (2012). How does the brain solve visual object recognition?
Neuron, 73(3):415–434.
Dickmanns, D., Schmidhuber, J., and Winklhofer, A. (1987).
Der genetische Algorithmus:
Eine Implementierung in Prolog. Technical Report, Inst. of Informatics, Tech. Univ. Munich. http://www.idsia.ch/˜juergen/geneticprogramming.html.
Dickmanns, E. D., Behringer, R., Dickmanns, D., Hildebrandt, T., Maurer, M., Thomanek, F., and Schiehlen, J. (1994). The seeing passenger car 'VaMoRs-P'. In Proc. Int. Symp. on Intelligent Vehicles '94, Paris, pages 68–73.
Dietterich, T. G. (2000a). Ensemble methods in machine learning. In Multiple classifier systems, pages
1–15. Springer.
Dietterich, T. G. (2000b). Hierarchical reinforcement learning with the MAXQ value function decomposition. J. Artif. Intell. Res. (JAIR), 13:227–303.
Director, S. W. and Rohrer, R. A. (1969). Automated network design - the frequency-domain case. IEEE
Trans. Circuit Theory, CT-16:330–337.
Dittenbach, M., Merkl, D., and Rauber, A. (2000). The growing hierarchical self-organizing map. In
IEEE-INNS-ENNS International Joint Conference on Neural Networks, volume 6, pages 6015–6015.
IEEE Computer Society.
Donahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E., and Darrell, T. (2013). DeCAF: A deep convolutional activation feature for generic visual recognition. arXiv preprint arXiv:1310.1531.
Dorffner, G. (1996). Neural networks for time series processing. In Neural Network World.
Doya, K., Samejima, K., ichi Katagiri, K., and Kawato, M. (2002). Multiple model-based reinforcement learning. Neural Computation, 14(6):1347–1369.
Dreyfus, S. E. (1962). The numerical solution of variational problems. Journal of Mathematical Analysis and Applications, 5(1):30–45.
Dreyfus, S. E. (1973). The computational solution of optimal control problems with time lag. IEEE
Transactions on Automatic Control, 18(4):383–385.
Duchi, J., Hazan, E., and Singer, Y. (2011). Adaptive subgradient methods for online learning and stochastic optimization. The Journal of Machine Learning, 12:2121–2159.
Egorova, A., Gloye, A., G¨oktekin, C., Liers, A., Luft, M., Rojas, R., Simon, M., Tenchio, O., and Wiesel, F. (2004). FU-Fighters Small Size 2004, Team Description. RoboCup 2004 Symposium: Papers and Team Description Papers. CD edition.
Elfwing, S., Otsuka, M., Uchibe, E., and Doya, K. (2010). Free-energy based reinforcement learning for vision-based navigation with high-dimensional sensory inputs. In Neural Information Processing.
Theory and Algorithms (ICONIP), volume 1, pages 215–222. Springer.
Eliasmith, C. (2013).
How to build a brain: A neural architecture for biological cognition.
Oxford
University Press, New York, NY.
Eliasmith, C., Stewart, T. C., Choo, X., Bekolay, T., DeWolf, T., Tang, Y., and Rasmussen, D. (2012). A large-scale model of the functioning brain. Science, 338(6111):1202–1205.
Elman, J. L. (1990). Finding structure in time. Cognitive Science, 14(2):179–211.
Erhan, D., Bengio, Y., Courville, A., Manzagol, P.-A., Vincent, P., and Bengio, S. (2010). Why does unsupervised pre-training help deep learning? J. Mach. Learn. Res., 11:625–660.
Escalante-B., A. N. and Wiskott, L. (2013). How to solve classification and regression problems on highdimensional data with a supervised extension of slow feature analysis. Journal of Machine Learning
Research, 14:3683–3719.
Eubank, R. L. (1988).
Spline smoothing and nonparametric regression.
In Farlow, S., editor, SelfOrganizing Methods in Modeling. Marcel Dekker, New York.
Euler, L. (1744). Methodus inveniendi.
Faggin, F. (1992).
Neural network hardware.
In International Joint Conference on Neural Networks(IJCNN), volume 1, page 153.
Fahlman, S. E. (1988). An empirical study of learning speed in back-propagation networks. Technical
Report CMU-CS-88-162, Carnegie-Mellon Univ.
Fahlman, S. E. (1991). The recurrent cascade-correlation learning algorithm. In Lippmann, R. P., Moody, J. E., and Touretzky, D. S., editors, Advances in Neural Information Processing Systems (NIPS) 3, pages
190–196. Morgan Kaufmann.
Falconbridge, M. S., Stamps, R. L., and Badcock, D. R. (2006). A simple Hebbian/anti-Hebbian network learns the sparse, independent components of natural images. Neural Computation, 18(2):415–429.
Farabet, C., Couprie, C., Najman, L., and LeCun, Y. (2013). Learning hierarchical features for scene labeling. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1915–1929.
Farlow, S. J. (1984). Self-organizing methods in modeling: GMDH type algorithms, volume 54. CRC
Press.
Feldkamp, L. A., Prokhorov, D. V., Eagen, C. F., and Yuan, F. (1998). Enhanced multi-stream Kalman filter training for recurrent networks. In Nonlinear Modeling, pages 29–53. Springer.
Feldkamp, L. A., Prokhorov, D. V., and Feldkamp, T. M. (2003). Simple and conditioned adaptive behavior from Kalman filter trained recurrent networks. Neural Networks, 16(5):683–689.
Feldkamp, L. A. and Puskorius, G. V. (1998). A signal processing framework based on dynamic neural networks with application to problems in adaptation, filtering, and classification. Proceedings of the IEEE, 86(11):2259–2277.
Felleman, D. J. and Van Essen, D. C. (1991). Distributed hierarchical processing in the primate cerebral cortex. Cerebral Cortex, 1(1):1–47.
Fern´andez, S., Graves, A., and Schmidhuber, J. (2007). An application of recurrent neural networks to discriminative keyword spotting. In Proc. ICANN (2), pages 220–229.
Fernandez, S., Graves, A., and Schmidhuber, J. (2007). Sequence labelling in structured domains with hierarchical recurrent neural networks. In Proceedings of the 20th International Joint Conference on
Artificial Intelligence (IJCAI).
Field, D. J. (1987). Relations between the statistics of natural images and the response properties of cortical cells. Journal of the Optical Society of America, 4:2379–2394.
Field, D. J. (1994). What is the goal of sensory coding? Neural Computation, 6:559–601.
Fieres, J., Schemmel, J., and Meier, K. (2008). Realizing biological spiking network models in a configurable wafer-scale hardware system. In IEEE International Joint Conference on Neural Networks, pages
969–976.
Fine, S., Singer, Y., and Tishby, N. (1998). The hierarchical hidden Markov model: Analysis and applications. Machine Learning, 32(1):41–62.
Fischer, A. and Igel, C. (2014). Training restricted Boltzmann machines: An introduction. Pattern Recognition, 47:25–39.
FitzHugh, R. (1961). Impulses and physiological states in theoretical models of nerve membrane. Biophysical Journal, 1(6):445–466.
Fletcher, R. and Powell, M. J. (1963). A rapidly convergent descent method for minimization. The Computer Journal, 6(2):163–168.
Fogel, D. B., Fogel, L. J., and Porto, V. (1990).
Evolving neural networks.
Biological Cybernetics, 63(6):487–493.
Fogel, L., Owens, A., and Walsh, M. (1966). Artificial Intelligence through Simulated Evolution. Wiley, New York.
F¨oldi´ak, P. (1990). Forming sparse representations by local anti-Hebbian learning. Biological Cybernetics, 64:165–170.
F¨oldi´ak, P. and Young, M. P. (1995). Sparse coding in the primate cortex. In Arbib, M. A., editor, The Handbook of Brain Theory and Neural Networks, pages 895–898. The MIT Press.
F¨orster, A., Graves, A., and Schmidhuber, J. (2007). RNN-based Learning of Compact Maps for Efficient
Robot Localization. In 15th European Symposium on Artificial Neural Networks, ESANN, pages 537–
542, Bruges, Belgium.
Franzius, M., Sprekeler, H., and Wiskott, L. (2007). Slowness and sparseness lead to place, head-direction, and spatial-view cells. PLoS Computational Biology, 3(8):166.
Friedman, J., Hastie, T., and Tibshirani, R. (2001). The elements of statistical learning, volume 1. Springer
Series in Statistics, New York.
Frinken, V., Zamora-Martinez, F., Espana-Boquera, S., Castro-Bleda, M. J., Fischer, A., and Bunke, H.(2012). Long-short term memory neural networks language modeling for handwriting recognition. In
Pattern Recognition (ICPR), 2012 21st International Conference on, pages 701–704. IEEE.
Fritzke, B. (1994). A growing neural gas network learns topologies. In Tesauro, G., Touretzky, D. S., and Leen, T. K., editors, NIPS, pages 625–632. MIT Press.
Fu, K. S. (1977). Syntactic Pattern Recognition and Applications. Berlin, Springer.
Fukada, T., Schuster, M., and Sagisaka, Y. (1999).
Phoneme boundary estimation using bidirectional recurrent neural networks and its applications. Systems and Computers in Japan, 30(4):20–30.
Fukushima, K. (1979). Neural network model for a mechanism of pattern recognition unaffected by shift in position - Neocognitron. Trans. IECE, J62-A(10):658–665.
Fukushima, K. (1980). Neocognitron: A self-organizing neural network for a mechanism of pattern recognition unaffected by shift in position. Biological Cybernetics, 36(4):193–202.
Fukushima, K. (2011). Increasing robustness against background noise: visual pattern recognition by a Neocognitron. Neural Networks, 24(7):767–778.
Fukushima, K. (2013a). Artificial vision by multi-layered neural networks: Neocognitron and its advances.
Neural Networks, 37:103–119.
Fukushima, K. (2013b). Training multi-layered neural network Neocognitron. Neural Networks, 40:18–31.
Gabor, D. (1946). Theory of communication. Part 1: The analysis of information. Electrical EngineersPart III: Journal of the Institution of Radio and Communication Engineering, 93(26):429–441.
Gallant, S. I. (1988). Connectionist expert systems. Communications of the ACM, 31(2):152–169.
Gauss, C. F. (1809). Theoria motus corporum coelestium in sectionibus conicis solem ambientium.
Gauss, C. F. (1821). Theoria combinationis observationum erroribus minimis obnoxiae (Theory of the combination of observations least subject to error).
Ge, S., Hang, C. C., Lee, T. H., and Zhang, T. (2010). Stable adaptive neural network control. Springer.
Geman, S., Bienenstock, E., and Doursat, R. (1992). Neural networks and the bias/variance dilemma.
Neural Computation, 4:1–58.
Gers, F. A. and Schmidhuber, J. (2000). Recurrent nets that time and count. In Neural Networks, 2000.
IJCNN 2000, Proceedings of the IEEE-INNS-ENNS International Joint Conference on, volume 3, pages
189–194. IEEE.
Gers, F. A. and Schmidhuber, J. (2001). LSTM recurrent networks learn simple context free and context sensitive languages. IEEE Transactions on Neural Networks, 12(6):1333–1340.
Gers, F. A., Schmidhuber, J., and Cummins, F. (2000). Learning to forget: Continual prediction with
LSTM. Neural Computation, 12(10):2451–2471.
Gers, F. A., Schraudolph, N., and Schmidhuber, J. (2002). Learning precise timing with LSTM recurrent networks. Journal of Machine Learning Research, 3:115–143.
Gerstner, W. and Kistler, W. K. (2002). Spiking Neuron Models. Cambridge University Press.
Gerstner, W. and van Hemmen, J. L. (1992). Associative memory in a network of spiking neurons. Network:
Computation in Neural Systems, 3(2):139–164.
Ghavamzadeh, M. and Mahadevan, S. (2003). Hierarchical policy gradient algorithms. In Proceedings of the Twentieth Conference on Machine Learning (ICML-2003), pages 226–233.
Gherrity, M. (1989). A learning algorithm for analog fully recurrent neural networks. In IEEE/INNS
International Joint Conference on Neural Networks, San Diego, volume 1, pages 643–644.
Girshick, R., Donahue, J., Darrell, T., and Malik, J. (2013). Rich feature hierarchies for accurate object detection and semantic segmentation. Technical Report arxiv.org/abs/1311.2524, UC Berkeley and ICSI.
Gisslen, L., Luciw, M., Graziano, V., and Schmidhuber, J. (2011). Sequential constant size compressor for reinforcement learning. In Proc. Fourth Conference on Artificial General Intelligence (AGI), Google, Mountain View, CA, pages 31–40. Springer.
Giusti, A., Ciresan, D. C., Masci, J., Gambardella, L. M., and Schmidhuber, J. (2013). Fast image scanning with deep max-pooling convolutional neural networks. In Proc. ICIP.
Glackin, B., McGinnity, T. M., Maguire, L. P., Wu, Q., and Belatreche, A. (2005). A novel approach for the implementation of large scale spiking neural networks on FPGA hardware. In Computational
Intelligence and Bioinspired Systems, pages 552–563. Springer.
Glasmachers, T., Schaul, T., Sun, Y., Wierstra, D., and Schmidhuber, J. (2010). Exponential natural evolution strategies. In Proceedings of the Genetic and Evolutionary Computation Conference (GECCO), pages 393–400. ACM.
Glorot, X., Bordes, A., and Bengio, Y. (2011). Deep sparse rectifier networks. In AISTATS, volume 15, pages 315–323.
Gloye, A., Wiesel, F., Tenchio, O., and Simon, M. (2005). Reinforcing the driving quality of soccer playing robots by anticipation. IT - Information Technology, 47(5).
G¨odel, K. (1931). ¨Uber formal unentscheidbare S¨atze der Principia Mathematica und verwandter Systeme
I. Monatshefte f¨ur Mathematik und Physik, 38:173–198.
Goldberg, D. E. (1989). Genetic Algorithms in Search, Optimization and Machine Learning. AddisonWesley, Reading, MA.
Goldfarb, D. (1970). A family of variable-metric methods derived by variational means. Mathematics of computation, 24(109):23–26.
Golub, G., Heath, H., and Wahba, G. (1979). Generalized cross-validation as a method for choosing a good ridge parameter. Technometrics, 21:215–224.
Gomez, F. J. (2003). Robust Nonlinear Control through Neuroevolution. PhD thesis, Department of Computer Sciences, University of Texas at Austin.
Gomez, F. J. and Miikkulainen, R. (2003). Active guidance for a finless rocket using neuroevolution. In
Proc. GECCO 2003, Chicago.
Gomez, F. J. and Schmidhuber, J. (2005). Co-evolving recurrent neurons learn deep memory POMDPs.
In Proc. of the 2005 conference on genetic and evolutionary computation (GECCO), Washington, D. C.
ACM Press, New York, NY, USA.
Gomez, F. J., Schmidhuber, J., and Miikkulainen, R. (2008). Accelerated neural evolution through cooperatively coevolved synapses. Journal of Machine Learning Research, 9(May):937–965.
Gomi, H. and Kawato, M. (1993). Neural network control for a closed-loop system using feedback-errorlearning. Neural Networks, 6(7):933–946.
Goodfellow, I., Mirza, M., Da, X., Courville, A., and Bengio, Y. (2014a). An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks. TR arXiv:1312.6211v2.
Goodfellow, I. J., Bulatov, Y., Ibarz, J., Arnoud, S., and Shet, V. (2014b). Multi-digit number recognition from street view imagery using deep convolutional neural networks. arXiv preprint arXiv:1312.6082 v4.
Goodfellow, I. J., Courville, A., and Bengio, Y. (2011). Spike-and-slab sparse coding for unsupervised feature discovery. In NIPS Workshop on Challenges in Learning Hierarchical Models.
Goodfellow, I. J., Courville, A. C., and Bengio, Y. (2012). Large-scale feature learning with spike-and-slab sparse coding. In Proceedings of the 29th International Conference on Machine Learning (ICML).
Goodfellow, I. J., Warde-Farley, D., Mirza, M., Courville, A., and Bengio, Y. (2013). Maxout networks. In
International Conference on Machine Learning (ICML).
Graves, A. (2011). Practical variational inference for neural networks. In Advances in Neural Information
Processing Systems (NIPS), pages 2348–2356.
Graves, A., Eck, D., Beringer, N., and Schmidhuber, J. (2003). Isolated digit recognition with LSTM recurrent networks. In First International Workshop on Biologically Inspired Approaches to Advanced
Information Technology, Lausanne.
Graves, A., Fernandez, S., Gomez, F. J., and Schmidhuber, J. (2006). Connectionist temporal classification:
Labelling unsegmented sequence data with recurrent neural nets. In ICML'06: Proceedings of the 23rd
International Conference on Machine Learning, pages 369–376.
Graves, A., Fernandez, S., Liwicki, M., Bunke, H., and Schmidhuber, J. (2008). Unconstrained on-line handwriting recognition with recurrent neural networks. In Platt, J., Koller, D., Singer, Y., and Roweis, S., editors, Advances in Neural Information Processing Systems (NIPS) 20, pages 577–584. MIT Press, Cambridge, MA.
Graves, A. and Jaitly, N. (2014). Towards end-to-end speech recognition with recurrent neural networks.
In Proc. 31st International Conference on Machine Learning (ICML), pages 1764–1772.
Graves, A., Liwicki, M., Fernandez, S., Bertolami, R., Bunke, H., and Schmidhuber, J. (2009). A novel connectionist system for improved unconstrained handwriting recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 31(5).
Graves, A., Mohamed, A.-R., and Hinton, G. E. (2013). Speech recognition with deep recurrent neural networks. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 6645–6649. IEEE.
Graves, A. and Schmidhuber, J. (2005). Framewise phoneme classification with bidirectional LSTM and other neural network architectures. Neural Networks, 18(5-6):602–610.
Graves, A. and Schmidhuber, J. (2009). Offline handwriting recognition with multidimensional recurrent neural networks. In Advances in Neural Information Processing Systems (NIPS) 21, pages 545–552.
MIT Press, Cambridge, MA.
Graziano, M. (2009). The Intelligent Movement Machine: An Ethological Perspective on the Primate
Motor System. Oxford University Press, USA.
Griewank, A. (2012). Documenta Mathematica - Extra Volume ISMP, pages 389–400.
Grondman, I., Busoniu, L., Lopes, G. A. D., and Babuska, R. (2012). A survey of actor-critic reinforcement learning: Standard and natural policy gradients. Systems, Man, and Cybernetics, Part C: Applications and Reviews, IEEE Transactions on, 42(6):1291–1307.
Grossberg, S. (1969). Some networks that can learn, remember, and reproduce any number of complicated space-time patterns, I. Journal of Mathematics and Mechanics, 19:53–91.
Grossberg, S. (1976a). Adaptive pattern classification and universal recoding, 1: Parallel development and coding of neural feature detectors. Biological Cybernetics, 23:187–202.
Grossberg, S. (1976b). Adaptive pattern classification and universal recoding, 2: Feedback, expectation, olfaction, and illusions. Biological Cybernetics, 23.
Gruau, F., Whitley, D., and Pyeatt, L. (1996). A comparison between cellular encoding and direct encoding for genetic neural networks. NeuroCOLT Technical Report NC-TR-96-048, ESPRIT Working Group in Neural and Computational Learning, NeuroCOLT 8556.
Gr¨unwald, P. D., Myung, I. J., and Pitt, M. A. (2005). Advances in minimum description length: Theory and applications. MIT Press.
Gr¨uttner, M., Sehnke, F., Schaul, T., and Schmidhuber, J. (2010). Multi-Dimensional Deep Memory AtariGo Players for Parameter Exploring Policy Gradients. In Proceedings of the International Conference on Artificial Neural Networks ICANN, pages 114–123. Springer.
Guyon, I., Vapnik, V., Boser, B., Bottou, L., and Solla, S. A. (1992). Structural risk minimization for character recognition. In Lippman, D. S., Moody, J. E., and Touretzky, D. S., editors, Advances in Neural Information Processing Systems (NIPS) 4, pages 471–479. Morgan Kaufmann.
Hadamard, J. (1908). M´emoire sur le probl`eme d'analyse relatif `a l'´equilibre des plaques ´elastiques encastr´ees.
M´emoires pr´esent´es par divers savants `a l'Acad´emie des sciences de l'Institut de France:
´Extrait. Imprimerie nationale.
Hadsell, R., Chopra, S., and LeCun, Y. (2006). Dimensionality reduction by learning an invariant mapping.
In Proc. Computer Vision and Pattern Recognition Conference (CVPR'06). IEEE Press.
Hansen, N., M¨uller, S. D., and Koumoutsakos, P. (2003). Reducing the time complexity of the derandomized evolution strategy with covariance matrix adaptation (CMA-ES). Evolutionary Computation, 11(1):1–18.
Hansen, N. and Ostermeier, A. (2001). Completely derandomized self-adaptation in evolution strategies.
Evolutionary Computation, 9(2):159–195.
Hanson, S. J. and Pratt, L. Y. (1989). Comparing biases for minimal network construction with backpropagation. In Touretzky, D. S., editor, Advances in Neural Information Processing Systems (NIPS) 1, pages 177–185. San Mateo, CA: Morgan Kaufmann.
Happel, B. L. and Murre, J. M. (1994). Design and evolution of modular neural network architectures.
Neural Networks, 7(6):985–1004.
Hashem, S. and Schmeiser, B. (1992). Improving model accuracy using optimal linear combinations of trained neural networks. IEEE Transactions on Neural Networks, 6:792–794.
Hassibi, B. and Stork, D. G. (1993). Second order derivatives for network pruning: Optimal brain surgeon. In Lippman, D. S., Moody, J. E., and Touretzky, D. S., editors, Advances in Neural Information
Processing Systems 5, pages 164–171. Morgan Kaufmann.
Hastie, T., Tibshirani, R., and Friedman, J. (2009). The elements of statistical learning. Springer Series in Statistics.
Hastie, T. J. and Tibshirani, R. J. (1990). Generalized additive models. Monographs on Statisics and Applied Probability, 43.
Hawkins, J. and George, D. (2006). Hierarchical Temporal Memory - Concepts, Theory, and Terminology.
Numenta Inc.
Haykin, S. S. (2001). Kalman filtering and neural networks. Wiley Online Library.
Hebb, D. O. (1949). The Organization of Behavior. Wiley, New York.
Hecht-Nielsen, R. (1989). Theory of the backpropagation neural network. In International Joint Conference on Neural Networks (IJCNN), pages 593–605. IEEE.
Heemskerk, J. N. (1995). Overview of neural hardware. Neurocomputers for Brain-Style Processing.
Design, Implementation and Application.
Heess, N., Silver, D., and Teh, Y. W. (2012). Actor-critic reinforcement learning with energy-based policies.
In Proc. European Workshop on Reinforcement Learning, pages 43–57.
Heidrich-Meisner, V. and Igel, C. (2009). Neuroevolution strategies for episodic reinforcement learning.
Journal of Algorithms, 64(4):152–168.
Herrero, J., Valencia, A., and Dopazo, J. (2001). A hierarchical unsupervised growing neural network for clustering gene expression patterns. Bioinformatics, 17(2):126–136.
Hertz, J., Krogh, A., and Palmer, R. (1991). Introduction to the Theory of Neural Computation. AddisonWesley, Redwood City.
Hestenes, M. R. and Stiefel, E. (1952). Methods of conjugate gradients for solving linear systems. Journal of research of the National Bureau of Standards, 49:409–436.
Hihi, S. E. and Bengio, Y. (1996). Hierarchical recurrent neural networks for long-term dependencies.
In Touretzky, D. S., Mozer, M. C., and Hasselmo, M. E., editors, Advances in Neural Information
Processing Systems 8, pages 493–499. MIT Press.
Hinton, G. and Salakhutdinov, R. (2006).
Reducing the dimensionality of data with neural networks.
Science, 313(5786):504–507.
Hinton, G. E. (1989). Connectionist learning procedures. Artificial intelligence, 40(1):185–234.
Hinton, G. E. (2002). Training products of experts by minimizing contrastive divergence. Neural Comp., 14(8):1771–1800.
Hinton, G. E., Dayan, P., Frey, B. J., and Neal, R. M. (1995). The wake-sleep algorithm for unsupervised neural networks. Science, 268:1158–1160.
Hinton, G. E., Deng, L., Yu, D., Dahl, G. E., Mohamed, A., Jaitly, N., Senior, A., Vanhoucke, V., Nguyen, P., Sainath, T. N., and Kingsbury, B. (2012a). Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. IEEE Signal Process. Mag., 29(6):82–97.
Hinton, G. E. and Ghahramani, Z. (1997). Generative models for discovering sparse distributed representations. Philosophical Transactions of the Royal Society B, 352:1177–1190.
Hinton, G. E., Osindero, S., and Teh, Y.-W. (2006). A fast learning algorithm for deep belief nets. Neural
Computation, 18(7):1527–1554.
Hinton, G. E. and Sejnowski, T. E. (1986). Learning and relearning in Boltzmann machines. In Parallel
Distributed Processing, volume 1, pages 282–317. MIT Press.
Hinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. R. (2012b). Improving neural networks by preventing co-adaptation of feature detectors. Technical Report arXiv:1207.0580.
Hinton, G. E. and van Camp, D. (1993). Keeping neural networks simple. In Proceedings of the International Conference on Artificial Neural Networks, Amsterdam, pages 11–18. Springer.
Hochreiter, S. (1991). Untersuchungen zu dynamischen neuronalen Netzen. Diploma thesis, Institut f¨ur
Informatik, Lehrstuhl Prof. Brauer, Technische Universit¨at M¨unchen. Advisor: J. Schmidhuber.
Hochreiter, S., Bengio, Y., Frasconi, P., and Schmidhuber, J. (2001a). Gradient flow in recurrent nets: the difficulty of learning long-term dependencies. In Kremer, S. C. and Kolen, J. F., editors, A Field Guide to Dynamical Recurrent Neural Networks. IEEE Press.
Hochreiter, S. and Obermayer, K. (2005). Sequence classification for protein analysis. In Snowbird Workshop, Snowbird, Utah. Computational and Biological Learning Society.
Hochreiter, S. and Schmidhuber, J. (1996). Bridging long time lags by weight guessing and "Long ShortTerm Memory". In Silva, F. L., Principe, J. C., and Almeida, L. B., editors, Spatiotemporal models in biological and artificial systems, pages 65–72. IOS Press, Amsterdam, Netherlands. Serie: Frontiers in Artificial Intelligence and Applications, Volume 37.
Hochreiter, S. and Schmidhuber, J. (1997a). Flat minima. Neural Computation, 9(1):1–42.
Hochreiter, S. and Schmidhuber, J. (1997b). Long Short-Term Memory. Neural Computation, 9(8):1735–
1780. Based on TR FKI-207-95, TUM (1995).
Hochreiter, S. and Schmidhuber, J. (1999). Feature extraction through LOCOCODE. Neural Computation, 11(3):679–714.
Hochreiter, S., Younger, A. S., and Conwell, P. R. (2001b). Learning to learn using gradient descent. In
Lecture Notes on Comp. Sci. 2130, Proc. Intl. Conf. on Artificial Neural Networks (ICANN-2001), pages
87–94. Springer: Berlin, Heidelberg.
Hodgkin, A. L. and Huxley, A. F. (1952). A quantitative description of membrane current and its application to conduction and excitation in nerve. The Journal of Physiology, 117(4):500.
Hoerzer, G. M., Legenstein, R., and Maass, W. (2014). Emergence of complex computational structures from chaotic neural networks through reward-modulated Hebbian learning. Cerebral Cortex, 24:677–
Holden, S. B. (1994). On the Theory of Generalization and Self-Structuring in Linearly Weighted Connectionist Networks. PhD thesis, Cambridge University, Engineering Department.
Holland, J. H. (1975). Adaptation in Natural and Artificial Systems. University of Michigan Press, Ann
Arbor.
Honavar, V. and Uhr, L. (1993). Generative learning structures and processes for generalized connectionist networks. Information Sciences, 70(1):75–108.
Honavar, V. and Uhr, L. M. (1988). A network of neuron-like units that learns to perceive by generation as well as reweighting of its links. In Touretzky, D., Hinton, G. E., and Sejnowski, T., editors, Proc. of the 1988 Connectionist Models Summer School, pages 472–484, San Mateo. Morgan Kaufman.
Hopfield, J. J. (1982).
Neural networks and physical systems with emergent collective computational abilities. Proc. of the National Academy of Sciences, 79:2554–2558.
Hornik, K., Stinchcombe, M., and White, H. (1989). Multilayer feedforward networks are universal approximators. Neural Networks, 2(5):359–366.
Hubel, D. H. and Wiesel, T. (1962). Receptive fields, binocular interaction, and functional architecture in the cat's visual cortex. Journal of Physiology (London), 160:106–154.
Hubel, D. H. and Wiesel, T. N. (1968). Receptive fields and functional architecture of monkey striate cortex. The Journal of Physiology, 195(1):215–243.
Huffman, D. A. (1952). A method for construction of minimum-redundancy codes. Proceedings IRE, 40:1098–1101.
Hung, C. P., Kreiman, G., Poggio, T., and DiCarlo, J. J. (2005). Fast readout of object identity from macaque inferior temporal cortex. Science, 310(5749):863–866.
Hutter, M. (2002). The fastest and shortest algorithm for all well-defined problems. International Journal of Foundations of Computer Science, 13(3):431–443. (On J. Schmidhuber's SNF grant 20-61847).
Hutter, M. (2005). Universal Artificial Intelligence: Sequential Decisions based on Algorithmic Probability. Springer, Berlin. (On J. Schmidhuber's SNF grant 20-61847).
Hyv¨arinen, A., Hoyer, P., and Oja, E. (1999). Sparse code shrinkage: Denoising by maximum likelihood estimation. In Kearns, M., Solla, S. A., and Cohn, D., editors, Advances in Neural Information
Processing Systems (NIPS) 12. MIT Press.
Hyv¨arinen, A., Karhunen, J., and Oja, E. (2001). Independent component analysis. John Wiley & Sons.
ICPR 2012 Contest on Mitosis Detection in Breast Cancer Histological Images (2012).
IPAL Laboratory and TRIBVN Company and Pitie-Salpetriere Hospital and CIALAB of Ohio State Univ., http://ipal.cnrs.fr/ICPR2012/.
Igel, C. (2003). Neuroevolution for reinforcement learning using evolution strategies. In Reynolds, R., Abbass, H., Tan, K. C., Mckay, B., Essam, D., and Gedeon, T., editors, Congress on Evolutionary
Computation (CEC 2003), volume 4, pages 2588–2595. IEEE.
Igel, C. and H¨usken, M. (2003). Empirical evaluation of the improved Rprop learning algorithm. Neurocomputing, 50(C):105–123.
Ikeda, S., Ochiai, M., and Sawaragi, Y. (1976). Sequential GMDH algorithm and its application to river flow prediction. IEEE Transactions on Systems, Man and Cybernetics, (7):473–479.
Indermuhle, E., Frinken, V., and Bunke, H. (2012). Mode detection in online handwritten documents using BLSTM neural networks. In Frontiers in Handwriting Recognition (ICFHR), 2012 International
Conference on, pages 302–307. IEEE.
Indermuhle, E., Frinken, V., Fischer, A., and Bunke, H. (2011). Keyword spotting in online handwritten documents containing text and non-text using BLSTM neural networks. In Document Analysis and Recognition (ICDAR), 2011 International Conference on, pages 73–77. IEEE.
Indiveri, G., Linares-Barranco, B., Hamilton, T. J., Van Schaik, A., Etienne-Cummings, R., Delbruck, T., Liu, S.-C., Dudek, P., H¨afliger, P., Renaud, S., et al. (2011). Neuromorphic silicon neuron circuits.
Frontiers in Neuroscience, 5(73).
Ivakhnenko, A. G. (1968).
The group method of data handling – a rival of the method of stochastic approximation. Soviet Automatic Control, 13(3):43–55.
Ivakhnenko, A. G. (1971). Polynomial theory of complex systems. IEEE Transactions on Systems, Man and Cybernetics, (4):364–378.
Ivakhnenko, A. G. (1995). The review of problems solvable by algorithms of the group method of data handling (GMDH). Pattern Recognition and Image Analysis / Raspoznavaniye Obrazov I Analiz Izobrazhenii, 5:527–535.
Ivakhnenko, A. G. and Lapa, V. G. (1965). Cybernetic Predicting Devices. CCM Information Corporation.
Ivakhnenko, A. G., Lapa, V. G., and McDonough, R. N. (1967). Cybernetics and forecasting techniques.
American Elsevier, NY.
Izhikevich, E. M. et al. (2003). Simple model of spiking neurons. IEEE Transactions on Neural Networks, 14(6):1569–1572.
Jaakkola, T., Singh, S. P., and Jordan, M. I. (1995). Reinforcement learning algorithm for partially observable Markov decision problems. In Tesauro, G., Touretzky, D. S., and Leen, T. K., editors, Advances in Neural Information Processing Systems (NIPS) 7, pages 345–352. MIT Press.
Jackel, L., Boser, B., Graf, H.-P., Denker, J., LeCun, Y., Henderson, D., Matan, O., Howard, R., and Baird, H. (1990). VLSI implementation of electronic neural networks: and example in character recognition.
In IEEE, editor, IEEE International Conference on Systems, Man, and Cybernetics, pages 320–322, Los
Angeles, CA.
Jacob, C., Lindenmayer, A., and Rozenberg, G. (1994). Genetic L-System Programming. In Parallel
Problem Solving from Nature III, Lecture Notes in Computer Science.
Jacobs, R. A. (1988). Increased rates of convergence through learning rate adaptation. Neural Networks, 1(4):295–307.
Jaeger, H. (2001). The "echo state" approach to analysing and training recurrent neural networks. Technical
Report GMD Report 148, German National Research Center for Information Technology.
Jaeger, H. (2002). Short term memory in echo state networks. GMD-Report 152, GMD - German National
Research Institute for Computer Science.
Jaeger, H. (2004). Harnessing nonlinearity: Predicting chaotic systems and saving energy in wireless communication. Science, 304:78–80.
Jain, V. and Seung, S. (2009).
Natural image denoising with convolutional networks.
In Koller, D., Schuurmans, D., Bengio, Y., and Bottou, L., editors, Advances in Neural Information Processing Systems(NIPS) 21, pages 769–776. Curran Associates, Inc.
Jameson, J. (1991). Delayed reinforcement learning with multiple time scale hierarchical backpropagated adaptive critics. In Neural Networks for Control.
Ji, S., Xu, W., Yang, M., and Yu, K. (2013). 3D convolutional neural networks for human action recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(1):221–231.
Jim, K., Giles, C. L., and Horne, B. G. (1995). Effects of noise on convergence and generalization in recurrent networks. In Tesauro, G., Touretzky, D., and Leen, T., editors, Advances in Neural Information
Processing Systems (NIPS) 7, page 649. San Mateo, CA: Morgan Kaufmann.
Jin, X., Lujan, M., Plana, L. A., Davies, S., Temple, S., and Furber, S. B. (2010). Modeling spiking neural networks on SpiNNaker. Computing in Science & Engineering, 12(5):91–97.
Jodogne, S. R. and Piater, J. H. (2007).
Closed-loop learning of visual control policies.
J. Artificial
Intelligence Research, 28:349–391.
Jones, J. P. and Palmer, L. A. (1987). An evaluation of the two-dimensional Gabor filter model of simple receptive fields in cat striate cortex. Journal of Neurophysiology, 58(6):1233–1258.
Jordan, M. I. (1986). Serial order: A parallel distributed processing approach. Technical Report ICS Report
8604, Institute for Cognitive Science, University of California, San Diego.
Jordan, M. I. (1988). Supervised learning and systems with excess degrees of freedom. Technical Report
COINS TR 88-27, Massachusetts Institute of Technology.
Jordan, M. I. (1997). Serial order: A parallel distributed processing approach. Advances in Psychology, 121:471–495.
Jordan, M. I. and Rumelhart, D. E. (1990). Supervised learning with a distal teacher. Technical Report
Occasional Paper #40, Center for Cog. Sci., Massachusetts Institute of Technology.
Jordan, M. I. and Sejnowski, T. J. (2001). Graphical models: Foundations of neural computation. MIT
Press.
Juang, C.-F. (2004). A hybrid of genetic algorithm and particle swarm optimization for recurrent network design. Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE Transactions on, 34(2):997–1006.
Judd, J. S. (1990). Neural network design and the complexity of learning. Neural network modeling and connectionism. MIT Press.
Jutten, C. and Herault, J. (1991). Blind separation of sources, part I: An adaptive algorithm based on neuromimetic architecture. Signal Processing, 24(1):1–10.
Kaelbling, L. P., Littman, M. L., and Cassandra, A. R. (1995). Planning and acting in partially observable stochastic domains. Technical report, Brown University, Providence RI.
Kaelbling, L. P., Littman, M. L., and Moore, A. W. (1996). Reinforcement learning: a survey. Journal of AI research, 4:237–285.
Kak, S., Chen, Y., and Wang, L. (2010). Data mining using surface and deep agents based on neural networks. AMCIS 2010 Proceedings.
Kalinke, Y. and Lehmann, H. (1998). Computation in recurrent neural networks: From counters to iterated function systems. In Antoniou, G. and Slaney, J., editors, Advanced Topics in Artificial Intelligence, Proceedings of the 11th Australian Joint Conference on Artificial Intelligence, volume 1502 of LNAI, Berlin, Heidelberg. Springer.
Kalman, R. E. (1960). A new approach to linear filtering and prediction problems. Journal of Basic
Engineering, 82(1):35–45.
Karhunen, J. and Joutsensalo, J. (1995). Generalizations of principal component analysis, optimization problems, and neural networks. Neural Networks, 8(4):549–562.
Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., and Fei-Fei, L. (2014). Large-scale video classification with convolutional neural networks. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR).
Kasabov, N. K. (2014). Neucube: A spiking neural network architecture for mapping, learning and understanding of spatio-temporal brain data. Neural Networks.
Kelley, H. J. (1960). Gradient theory of optimal flight paths. ARS Journal, 30(10):947–954.
Kempter, R., Gerstner, W., and Van Hemmen, J. L. (1999). Hebbian learning and spiking neurons. Physical
Review E, 59(4):4498.
Kerlirzin, P. and Vallet, F. (1993). Robustness in multilayer perceptrons. Neural Computation, 5(1):473–
Khan, M. M., Khan, G. M., and Miller, J. F. (2010). Evolution of neural networks using Cartesian Genetic
Programming. In IEEE Congress on Evolutionary Computation (CEC), pages 1–8.
Khan, M. M., Lester, D. R., Plana, L. A., Rast, A., Jin, X., Painkras, E., and Furber, S. B. (2008). SpiNNaker: mapping neural networks onto a massively-parallel chip multiprocessor. In International Joint
Conference on Neural Networks (IJCNN), pages 2849–2856. IEEE.
Khan, S. H., Bennamoun, M., Sohel, F., and Togneri, R. (2014). Automatic feature learning for robust shadow detection. In IEEE Conference on Computer Vision and Pattern Recognition CVPR.
Kimura, H., Miyazaki, K., and Kobayashi, S. (1997). Reinforcement learning in POMDPs with function approximation. In ICML, volume 97, pages 152–160.
Kistler, W. M., Gerstner, W., and van Hemmen, J. L. (1997). Reduction of the Hodgkin-Huxley equations to a single-variable threshold model. Neural Computation, 9(5):1015–1045.
Kitano, H. (1990). Designing neural networks using genetic algorithms with graph generation system.
Complex Systems, 4:461–476.
Klampfl, S. and Maass, W. (2013). Emergence of dynamic memory traces in cortical microcircuit models through STDP. The Journal of Neuroscience, 33(28):11515–11529.
Klapper-Rybicka, M., Schraudolph, N. N., and Schmidhuber, J. (2001). Unsupervised learning in LSTM recurrent neural networks. In Lecture Notes on Comp. Sci. 2130, Proc. Intl. Conf. on Artificial Neural
Networks (ICANN-2001), pages 684–691. Springer: Berlin, Heidelberg.
Kobatake, E. and Tanaka, K. (1994). Neuronal selectivities to complex object features in the ventral visual pathway of the macaque cerebral cortex. J. Neurophysiol., 71:856–867.
Kohl, N. and Stone, P. (2004). Policy gradient reinforcement learning for fast quadrupedal locomotion.
In Robotics and Automation, 2004. Proceedings. ICRA'04. 2004 IEEE International Conference on, volume 3, pages 2619–2624. IEEE.
Kohonen, T. (1972). Correlation matrix memories. Computers, IEEE Transactions on, 100(4):353–359.
Kohonen, T. (1982). Self-organized formation of topologically correct feature maps. Biological Cybernetics, 43(1):59–69.
Kohonen, T. (1988). Self-Organization and Associative Memory. Springer, second edition.
Koikkalainen, P. and Oja, E. (1990). Self-organizing hierarchical feature maps. In International Joint
Conference on Neural Networks (IJCNN), pages 279–284. IEEE.
Kolmogorov, A. N. (1965a). On the representation of continuous functions of several variables by superposition of continuous functions of one variable and addition.
Doklady Akademii. Nauk USSR,, 114:679–681.
Kolmogorov, A. N. (1965b). Three approaches to the quantitative definition of information. Problems of Information Transmission, 1:1–11.
Kompella, V. R., Luciw, M. D., and Schmidhuber, J. (2012). Incremental slow feature analysis: Adaptive low-complexity slow feature updating from high-dimensional input streams. Neural Computation, 24(11):2994–3024.
Kondo, T. (1998). GMDH neural network algorithm using the heuristic self-organization method and its application to the pattern identification problem. In Proceedings of the 37th SICE Annual Conference
SICE'98, pages 1143–1148. IEEE.
Kondo, T. and Ueno, J. (2008). Multi-layered GMDH-type neural network self-selecting optimum neural network architecture and its application to 3-dimensional medical image recognition of blood vessels.
International Journal of Innovative Computing, Information and Control, 4(1):175–187.
Kord´ık, P., N´aplava, P., Snorek, M., and Genyk-Berezovskyj, M. (2003). Modified GMDH method and models quality evaluation by visualization. Control Systems and Computers, 2:68–75.
Korkin, M., de Garis, H., Gers, F., and Hemmi, H. (1997). CBM (CAM-Brain Machine) - a hardware tool which evolves a neural net module in a fraction of a second and runs a million neuron artificial brain in real time.
Kosko, B. (1990). Unsupervised learning in noise. IEEE Transactions on Neural Networks, 1(1):44–57.
Koutn´ık, J., Cuccu, G., Schmidhuber, J., and Gomez, F. (July 2013). Evolving large-scale neural networks for vision-based reinforcement learning. In Proceedings of the Genetic and Evolutionary Computation
Conference (GECCO), pages 1061–1068, Amsterdam. ACM.
Koutn´ık, J., Gomez, F., and Schmidhuber, J. (2010). Evolving neural networks in compressed weight space. In Proceedings of the 12th Annual Conference on Genetic and Evolutionary Computation, pages
619–626.
Koutn´ık, J., Greff, K., Gomez, F., and Schmidhuber, J. (2014).
A Clockwork RNN.
In Proceedings of the 31th International Conference on Machine Learning (ICML), volume 32, pages 1845–1853. arXiv:1402.3511 [cs.NE].
Koza, J. R. (1992). Genetic Programming – On the Programming of Computers by Means of Natural
Selection. MIT Press.
Kramer, M. (1991). Nonlinear principal component analysis using autoassociative neural networks. AIChE
Journal, 37:233–243.
Kremer, S. C. and Kolen, J. F. (2001). Field guide to dynamical recurrent networks. Wiley-IEEE Press.
Kriegeskorte, N., Mur, M., Ruff, D. A., Kiani, R., Bodurka, J., Esteky, H., Tanaka, K., and Bandettini, P. A.(2008). Matching categorical object representations in inferior temporal cortex of man and monkey.
Neuron, 60(6):1126–1141.
Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems (NIPS 2012), page 4.
Krogh, A. and Hertz, J. A. (1992). A simple weight decay can improve generalization. In Lippman, D. S., Moody, J. E., and Touretzky, D. S., editors, Advances in Neural Information Processing Systems 4, pages
950–957. Morgan Kaufmann.
Kruger, N., Janssen, P., Kalkan, S., Lappe, M., Leonardis, A., Piater, J., Rodriguez-Sanchez, A., and Wiskott, L. (2013). Deep hierarchies in the primate visual cortex: What can we learn for computer vision? IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1847–1871.
Kullback, S. and Leibler, R. A. (1951). On information and sufficiency. The Annals of Mathematical
Statistics, pages 79–86.
Kurzweil, R. (2012). How to Create a Mind: The Secret of Human Thought Revealed.
Lagoudakis, M. G. and Parr, R. (2003). Least-squares policy iteration. JMLR, 4:1107–1149.
Lampinen, J. and Oja, E. (1992). Clustering properties of hierarchical self-organizing maps. Journal of Mathematical Imaging and Vision, 2(2-3):261–272.
Lang, K., Waibel, A., and Hinton, G. E. (1990). A time-delay neural network architecture for isolated word recognition. Neural Networks, 3:23–43.
Lange, S. and Riedmiller, M. (2010). Deep auto-encoder neural networks in reinforcement learning. In
Neural Networks (IJCNN), The 2010 International Joint Conference on, pages 1–8.
Lapedes, A. and Farber, R. (1986). A self-optimizing, nonsymmetrical neural net for content addressable memory and pattern recognition. Physica D, 22:247–259.
Laplace, P. (1774). M´emoire sur la probabilit´e des causes par les ´ev`enements. M´emoires de l'Academie
Royale des Sciences Present´es par Divers Savan, 6:621–656.
Larraanaga, P. and Lozano, J. A. (2001). Estimation of Distribution Algorithms: A New Tool for Evolutionary Computation. Kluwer Academic Publishers, Norwell, MA, USA.
Le, Q. V., Ranzato, M., Monga, R., Devin, M., Corrado, G., Chen, K., Dean, J., and Ng, A. Y. (2012).
Building high-level features using large scale unsupervised learning. In Proc. ICML'12.
LeCun, Y. (1985). Une proc´edure d'apprentissage pour r´eseau `a seuil asym´etrique. Proceedings of Cognitiva 85, Paris, pages 599–604.
LeCun, Y. (1988).
A theoretical framework for back-propagation.
In Touretzky, D., Hinton, G., and Sejnowski, T., editors, Proceedings of the 1988 Connectionist Models Summer School, pages 21–28, CMU, Pittsburgh, Pa. Morgan Kaufmann.
LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., and Jackel, L. D. (1989).
Back-propagation applied to handwritten zip code recognition. Neural Computation, 1(4):541–551.
LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., and Jackel, L. D. (1990a).
Handwritten digit recognition with a back-propagation network. In Touretzky, D. S., editor, Advances in Neural Information Processing Systems 2, pages 396–404. Morgan Kaufmann.
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324.
LeCun, Y., Denker, J. S., and Solla, S. A. (1990b). Optimal brain damage. In Touretzky, D. S., editor, Advances in Neural Information Processing Systems 2, pages 598–605. Morgan Kaufmann.
LeCun, Y., Muller, U., Cosatto, E., and Flepp, B. (2006). Off-road obstacle avoidance through end-to-end learning. In Advances in Neural Information Processing Systems (NIPS 2005).
LeCun, Y., Simard, P., and Pearlmutter, B. (1993).
Automatic learning rate maximization by on-line estimation of the Hessian's eigenvectors. In Hanson, S., Cowan, J., and Giles, L., editors, Advances in Neural Information Processing Systems (NIPS 1992), volume 5. Morgan Kaufmann Publishers, San
Mateo, CA.
Lee, H., Battle, A., Raina, R., and Ng, A. Y. (2007a). Efficient sparse coding algorithms. In Advances in Neural Information Processing Systems (NIPS) 19, pages 801–808.
Lee, H., Ekanadham, C., and Ng, A. Y. (2007b). Sparse deep belief net model for visual area V2. In
Advances in Neural Information Processing Systems (NIPS), volume 7, pages 873–880.
Lee, H., Grosse, R., Ranganath, R., and Ng, A. Y. (2009a). Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. In Proceedings of the 26th International
Conference on Machine Learning (ICML), pages 609–616.
Lee, H., Pham, P. T., Largman, Y., and Ng, A. Y. (2009b). Unsupervised feature learning for audio classification using convolutional deep belief networks. In Proc. NIPS, volume 9, pages 1096–1104.
Lee, L. (1996). Learning of context-free languages: A survey of the literature. Technical Report TR-12-96, Center for Research in Computing Technology, Harvard University, Cambridge, Massachusetts.
Lee, S. and Kil, R. M. (1991). A Gaussian potential function network with hierarchically self-organizing learning. Neural Networks, 4(2):207–224.
Legendre, A. M. (1805). Nouvelles m´ethodes pour la d´etermination des orbites des cometes. F. Didot.
Legenstein, R., Wilbert, N., and Wiskott, L. (2010). Reinforcement learning on slow features of highdimensional input streams. PLoS Computational Biology, 6(8).
Legenstein, R. A. and Maass, W. (2002). Neural circuits for pattern recognition with small total wire length. Theor. Comput. Sci., 287(1):239–249.
Leibniz, G. W. (1676). Memoir using the chain rule (cited in TMME 7:2&3 p 321-332, 2010).
Leibniz, G. W. (1684). Nova methodus pro maximis et minimis, itemque tangentibus, quae nec fractas, nec irrationales quantitates moratur, et singulare pro illis calculi genus. Acta Eruditorum, pages 467–473.
Lenat, D. B. (1983). Theory formation by heuristic search. Machine Learning, 21.
Lenat, D. B. and Brown, J. S. (1984). Why AM an EURISKO appear to work. Artificial Intelligence, 23(3):269–294.
Lennie, P. and Movshon, J. A. (2005). Coding of color and form in the geniculostriate visual pathway.
Journal of the Optical Society of America A, 22(10):2013–2033.
Levenberg, K. (1944). A method for the solution of certain problems in least squares. Quarterly of applied mathematics, 2:164–168.
Levin, A. U., Leen, T. K., and Moody, J. E. (1994). Fast pruning using principal components. In Advances in Neural Information Processing Systems 6, page 35. Morgan Kaufmann.
Levin, A. U. and Narendra, K. S. (1995). Control of nonlinear dynamical systems using neural networks. ii. observability, identification, and control. IEEE Transactions on Neural Networks, 7(1):30–42.
Levin, L. A. (1973a). On the notion of a random sequence. Soviet Math. Dokl., 14(5):1413–1416.
Levin, L. A. (1973b).
Universal sequential search problems.
Problems of Information Transmission, 9(3):265–266.
Lewicki, M. S. and Olshausen, B. A. (1998). Inferring sparse, overcomplete image codes using an efficient coding framework. In Jordan, M. I., Kearns, M. J., and Solla, S. A., editors, Advances in Neural
Information Processing Systems (NIPS) 10, pages 815–821.
L'Hˆopital, G. F. A. (1696). Analyse des infiniment petits, pour l'intelligence des lignes courbes. Paris:
L'Imprimerie Royale.
Li, M. and Vit´anyi, P. M. B. (1997). An Introduction to Kolmogorov Complexity and its Applications (2nd edition). Springer.
Li, R., Zhang, W., Suk, H.-I., Wang, L., Li, J., Shen, D., and Ji, S. (2014). Deep learning based imaging data completion for improved brain disease diagnosis. In Proc. MICCAI. Springer.
Lin, L. (1993). Reinforcement Learning for Robots Using Neural Networks. PhD thesis, Carnegie Mellon
University, Pittsburgh.
Lin, T., Horne, B., Tino, P., and Giles, C. (1996). Learning long-term dependencies in NARX recurrent neural networks. IEEE Transactions on Neural Networks, 7(6):1329–1338.
Lindenmayer, A. (1968). Mathematical models for cellular interaction in development. J. Theoret. Biology, 18:280–315.
Lindst¨adt, S. (1993a). Comparison of two unsupervised neural network models for redundancy reduction.
In Mozer, M. C., Smolensky, P., Touretzky, D. S., Elman, J. L., and Weigend, A. S., editors, Proc. of the 1993 Connectionist Models Summer School, pages 308–315. Hillsdale, NJ: Erlbaum Associates.
Lindst¨adt, S. (1993b). Comparison of unsupervised neural networks for redundancy reduction. Master's thesis, Dept. of Comp. Sci., University of Colorado at Boulder. Advisor: J. S.
Linnainmaa, S. (1970). The representation of the cumulative rounding error of an algorithm as a Taylor expansion of the local rounding errors. Master's thesis, Univ. Helsinki.
Linnainmaa, S. (1976). Taylor expansion of the accumulated rounding error. BIT Numerical Mathematics, 16(2):146–160.
Linsker, R. (1988). Self-organization in a perceptual network. IEEE Computer, 21:105–117.
Littman, M. L. (1996). Algorithms for Sequential Decision Making. PhD thesis, Brown University.
Littman, M. L., Cassandra, A. R., and Kaelbling, L. P. (1995). Learning policies for partially observable environments: Scaling up. In Prieditis, A. and Russell, S., editors, Machine Learning: Proceedings of the Twelfth International Conference, pages 362–370. Morgan Kaufmann Publishers, San Francisco, CA.
Liu, S.-C., Kramer, J., Indiveri, G., Delbr¨uck, T., Burg, T., Douglas, R., et al. (2001). Orientation-selective aVLSI spiking neurons. Neural Networks, 14(6-7):629–643.
Ljung, L. (1998). System identification. Springer.
Logothetis, N. K., Pauls, J., and Poggio, T. (1995). Shape representation in the inferior temporal cortex of monkeys. Current Biology, 5(5):552–563.
Loiacono, D., Cardamone, L., and Lanzi, P. L. (2011). Simulated car racing championship competition software manual. Technical report, Dipartimento di Elettronica e Informazione, Politecnico di Milano, Italy.
Loiacono, D., Lanzi, P. L., Togelius, J., Onieva, E., Pelta, D. A., Butz, M. V., L¨onneker, T. D., Cardamone, L., Perez, D., S´aez, Y., Preuss, M., and Quadflieg, J. (2009). The 2009 simulated car racing championship.
Lowe, D. (1999). Object recognition from local scale-invariant features. In The Proceedings of the Seventh
IEEE International Conference on Computer Vision (ICCV), volume 2, pages 1150–1157.
Lowe, D. (2004). Distinctive image features from scale-invariant key-points. Intl. Journal of Computer
Vision, 60:91–110.
Luciw, M., Kompella, V. R., Kazerounian, S., and Schmidhuber, J. (2013). An intrinsic value system for developing multiple invariant representations with incremental slowness learning. Frontiers in Neurorobotics, 7(9).
Lusci, A., Pollastri, G., and Baldi, P. (2013). Deep architectures and deep learning in chemoinformatics: the prediction of aqueous solubility for drug-like molecules.
Journal of Chemical Information and Modeling, 53(7):1563–1575.
Maas, A. L., Hannun, A. Y., and Ng, A. Y. (2013). Rectifier nonlinearities improve neural network acoustic models. In International Conference on Machine Learning (ICML).
Maass, W. (1996). Lower bounds for the computational power of networks of spiking neurons. Neural
Computation, 8(1):1–40.
Maass, W. (1997). Networks of spiking neurons: the third generation of neural network models. Neural
Networks, 10(9):1659–1671.
Maass, W. (2000). On the computational power of winner-take-all. Neural Computation, 12:2519–2535.
Maass, W., Natschl¨ager, T., and Markram, H. (2002). Real-time computing without stable states: A new framework for neural computation based on perturbations. Neural Computation, 14(11):2531–2560.
MacKay, D. J. C. (1992). A practical Bayesian framework for backprop networks. Neural Computation, 4:448–472.
MacKay, D. J. C. and Miller, K. D. (1990). Analysis of Linsker's simulation of Hebbian rules. Neural
Computation, 2:173–187.
Maclin, R. and Shavlik, J. W. (1993). Using knowledge-based neural networks to improve algorithms:
Refining the Chou-Fasman algorithm for protein folding. Machine Learning, 11(2-3):195–215.
Maclin, R. and Shavlik, J. W. (1995). Combining the predictions of multiple classifiers: Using competitive learning to initialize neural networks. In Proc. IJCAI, pages 524–531.
Madala, H. R. and Ivakhnenko, A. G. (1994). Inductive learning algorithms for complex systems modeling.
CRC Press, Boca Raton.
Madani, O., Hanks, S., and Condon, A. (2003). On the undecidability of probabilistic planning and related stochastic optimization problems. Artificial Intelligence, 147(1):5–34.
Maei, H. R. and Sutton, R. S. (2010). GQ(λ): A general gradient algorithm for temporal-difference prediction learning with eligibility traces. In Proceedings of the Third Conference on Artificial General
Intelligence, volume 1, pages 91–96.
Maex, R. and Orban, G. (1996). Model circuit of spiking neurons generating directional selectivity in simple cells. Journal of Neurophysiology, 75(4):1515–1545.
Mahadevan, S. (1996). Average reward reinforcement learning: Foundations, algorithms, and empirical results. Machine Learning, 22:159.
Maniezzo, V. (1994). Genetic evolution of the topology and weight distribution of neural networks. IEEE
Transactions on Neural Networks, 5(1):39–53.
Manolios, P. and Fanelli, R. (1994). First-order recurrent neural networks and deterministic finite state automata. Neural Computation, 6:1155–1173.
Markram, H. (2012). The human brain project. Scientific American, 306(6):50–55.
Marquardt, D. W. (1963). An algorithm for least-squares estimation of nonlinear parameters. Journal of the Society for Industrial & Applied Mathematics, 11(2):431–441.
Martens, J. (2010). Deep learning via Hessian-free optimization. In F¨urnkranz, J. and Joachims, T., editors, Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 735–742, Haifa, Israel. Omnipress.
Martens, J. and Sutskever, I. (2011). Learning recurrent neural networks with Hessian-free optimization.
In Proceedings of the 28th International Conference on Machine Learning (ICML), pages 1033–1040.
Martinetz, T. M., Ritter, H. J., and Schulten, K. J. (1990). Three-dimensional neural net for learning visuomotor coordination of a robot arm. IEEE Transactions on Neural Networks, 1(1):131–136.
Masci, J., Giusti, A., Ciresan, D. C., Fricout, G., and Schmidhuber, J. (2013). A fast learning algorithm for image segmentation with max-pooling convolutional networks. In International Conference on Image
Processing (ICIP13), pages 2713–2717.
Matsuoka, K. (1992). Noise injection into inputs in back-propagation learning. IEEE Transactions on
Systems, Man, and Cybernetics, 22(3):436–440.
Mayer, H., Gomez, F., Wierstra, D., Nagy, I., Knoll, A., and Schmidhuber, J. (2008).
A system for robotic heart surgery that learns to tie knots using recurrent neural networks. Advanced Robotics, 22(1314):1521–1537.
McCallum, R. A. (1996). Learning to use selective attention and short-term memory in sequential tasks.
In Maes, P., Mataric, M., Meyer, J.-A., Pollack, J., and Wilson, S. W., editors, From Animals to Animats
4: Proceedings of the Fourth International Conference on Simulation of Adaptive Behavior, Cambridge, MA, pages 315–324. MIT Press, Bradford Books.
McCulloch, W. and Pitts, W. (1943). A logical calculus of the ideas immanent in nervous activity. Bulletin of Mathematical Biophysics, 7:115–133.
Melnik, O., Levy, S. D., and Pollack, J. B. (2000). RAAM for infinite context-free languages. In Proc.
IJCNN (5), pages 585–590.
Memisevic, R. and Hinton, G. E. (2010).
Learning to represent spatial transformations with factored higher-order Boltzmann machines. Neural Computation, 22(6):1473–1492.
Menache, I., Mannor, S., and Shimkin, N. (2002). Q-cut – dynamic discovery of sub-goals in reinforcement learning. In Proc. ECML'02, pages 295–306.
Mesnil, G., Dauphin, Y., Glorot, X., Rifai, S., Bengio, Y., Goodfellow, I., Lavoie, E., Muller, X., Desjardins, G., Warde-Farley, D., Vincent, P., Courville, A., and Bergstra, J. (2011). Unsupervised and transfer learning challenge: a deep learning approach. In JMLR W&CP: Proc. Unsupervised and Transfer Learning, volume 7.
Meuleau, N., Peshkin, L., Kim, K. E., and Kaelbling, L. P. (1999). Learning finite state controllers for partially observable environments. In 15th International Conference of Uncertainty in AI, pages 427–
Miglino, O., Lund, H., and Nolfi, S. (1995). Evolving mobile robots in simulated and real environments.
Artificial Life, 2(4):417–434.
Miller, G., Todd, P., and Hedge, S. (1989). Designing neural networks using genetic algorithms. In Proceedings of the 3rd International Conference on Genetic Algorithms, pages 379–384. Morgan Kauffman.
Miller, J. F. and Harding, S. L. (2009). Cartesian genetic programming. In Proceedings of the 11th Annual
Conference Companion on Genetic and Evolutionary Computation Conference: Late Breaking Papers, pages 3489–3512. ACM.
Miller, J. F. and Thomson, P. (2000). Cartesian genetic programming. In Genetic Programming, pages
121–132. Springer.
Miller, K. D. (1994). A model for the development of simple cell receptive fields and the ordered arrangement of orientation columns through activity-dependent competition between on- and off-center inputs.
Journal of Neuroscience, 14(1):409–441.
Miller, W. T., Werbos, P. J., and Sutton, R. S. (1995). Neural networks for control. MIT Press.
Minai, A. A. and Williams, R. D. (1994). Perturbation response in feedforward networks. Neural Networks, 7(5):783–796.
Minsky, M. (1963).
Steps toward artificial intelligence.
In Feigenbaum, E. and Feldman, J., editors, Computers and Thought, pages 406–450. McGraw-Hill, New York.
Minsky, M. and Papert, S. (1969). Perceptrons. Cambridge, MA: MIT Press.
Minton, S., Carbonell, J. G., Knoblock, C. A., Kuokka, D. R., Etzioni, O., and Gil, Y. (1989). Explanationbased learning: A problem solving perspective. Artificial Intelligence, 40(1):63–118.
Mitchell, T. (1997). Machine Learning. McGraw Hill.
Mitchell, T. M., Keller, R. M., and Kedar-Cabelli, S. T. (1986). Explanation-based generalization: A unifying view. Machine Learning, 1(1):47–80.
Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M. (Dec
2013). Playing Atari with deep reinforcement learning. Technical Report arXiv:1312.5602 [cs.LG], Deepmind Technologies.
Mohamed, A., Dahl, G. E., and Hinton, G. E. (2009). Deep belief networks for phone recognition. In
NIPS'22 workshop on deep learning for speech recognition.
Mohamed, A. and Hinton, G. E. (2010). Phone recognition using restricted Boltzmann machines. In IEEE
International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 4354–4357.
Molgedey, L. and Schuster, H. G. (1994). Separation of independent signals using time-delayed correlations. Phys. Reviews Letters, 72(23):3634–3637.
Møller, M. F. (1993). Exact calculation of the product of the Hessian matrix of feed-forward network error functions and a vector in O(N) time. Technical Report PB-432, Computer Science Department, Aarhus
University, Denmark.
Montana, D. J. and Davis, L. (1989). Training feedforward neural networks using genetic algorithms. In
Proceedings of the 11th International Joint Conference on Artificial Intelligence (IJCAI) - Volume 1, IJCAI'89, pages 762–767, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.
Montavon, G., Orr, G., and M¨uller, K. (2012). Neural Networks: Tricks of the Trade. Number LNCS 7700 in Lecture Notes in Computer Science Series. Springer Verlag.
Moody, J. E. (1989). Fast learning in multi-resolution hierarchies. In Touretzky, D. S., editor, Advances in Neural Information Processing Systems (NIPS) 1, pages 29–39. Morgan Kaufmann.
Moody, J. E. (1992). The effective number of parameters: An analysis of generalization and regularization in nonlinear learning systems. In Lippman, D. S., Moody, J. E., and Touretzky, D. S., editors, Advances in Neural Information Processing Systems (NIPS) 4, pages 847–854. Morgan Kaufmann.
Moody, J. E. and Utans, J. (1994). Architecture selection strategies for neural networks: Application to corporate bond rating prediction. In Refenes, A. N., editor, Neural Networks in the Capital Markets.
John Wiley & Sons.
Moore, A. and Atkeson, C. (1995). The parti-game algorithm for variable resolution reinforcement learning in multidimensional state-spaces. Machine Learning, 21(3):199–233.
Moore, A. and Atkeson, C. G. (1993). Prioritized sweeping: Reinforcement learning with less data and less time. Machine Learning, 13:103–130.
Moriarty, D. E. (1997). Symbiotic Evolution of Neural Networks in Sequential Decision Tasks. PhD thesis, Department of Computer Sciences, The University of Texas at Austin.
Moriarty, D. E. and Miikkulainen, R. (1996). Efficient reinforcement learning through symbiotic evolution.
Machine Learning, 22:11–32.
Morimoto, J. and Doya, K. (2000). Robust reinforcement learning. In Leen, T. K., Dietterich, T. G., and Tresp, V., editors, Advances in Neural Information Processing Systems (NIPS) 13, pages 1061–1067.
MIT Press.
Mosteller, F. and Tukey, J. W. (1968). Data analysis, including statistics. In Lindzey, G. and Aronson, E., editors, Handbook of Social Psychology, Vol. 2. Addison-Wesley.
Mozer, M. C. (1989). A focused back-propagation algorithm for temporal sequence recognition. Complex
Systems, 3:349–381.
Mozer, M. C. (1991). Discovering discrete distributed representations with iterative competitive learning. In Lippmann, R. P., Moody, J. E., and Touretzky, D. S., editors, Advances in Neural Information
Processing Systems 3, pages 627–634. Morgan Kaufmann.
Mozer, M. C. (1992). Induction of multiscale temporal structure. In Lippman, D. S., Moody, J. E., and Touretzky, D. S., editors, Advances in Neural Information Processing Systems (NIPS) 4, pages 275–282.
Morgan Kaufmann.
Mozer, M. C. and Smolensky, P. (1989). Skeletonization: A technique for trimming the fat from a network via relevance assessment. In Touretzky, D. S., editor, Advances in Neural Information Processing
Systems (NIPS) 1, pages 107–115. Morgan Kaufmann.
Muller, U. A., Gunzinger, A., and Guggenb¨uhl, W. (1995). Fast neural net simulation with a DSP processor array. IEEE Transactions on Neural Networks, 6(1):203–213.
Munro, P. W. (1987). A dual back-propagation scheme for scalar reinforcement learning. Proceedings of the Ninth Annual Conference of the Cognitive Science Society, Seattle, WA, pages 165–176.
Murray, A. F. and Edwards, P. J. (1993). Synaptic weight noise during MLP learning enhances faulttolerance, generalisation and learning trajectory. In S. J. Hanson, J. D. C. and Giles, C. L., editors, Advances in Neural Information Processing Systems (NIPS) 5, pages 491–498. San Mateo, CA: Morgan
Kaufmann.
Nadal, J.-P. and Parga, N. (1994). Non-linear neurons in the low noise limit: a factorial code maximises information transfer. Network, 5:565–581.
Nagumo, J., Arimoto, S., and Yoshizawa, S. (1962). An active pulse transmission line simulating nerve axon. Proceedings of the IRE, 50(10):2061–2070.
Nair, V. and Hinton, G. E. (2010). Rectified linear units improve restricted Boltzmann machines. In
International Conference on Machine Learning (ICML).
Narendra, K. S. and Parthasarathy, K. (1990). Identification and control of dynamical systems using neural networks. Neural Networks, IEEE Transactions on, 1(1):4–27.
Narendra, K. S. and Thathatchar, M. A. L. (1974). Learning automata – a survey. IEEE Transactions on
Systems, Man, and Cybernetics, 4:323–334.
Neal, R. M. (1995). Bayesian learning for neural networks. PhD thesis, University of Toronto.
Neal, R. M. (2006). Classification with Bayesian neural networks. In Quinonero-Candela, J., Magnini, B., Dagan, I., and D'Alche-Buc, F., editors, Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Textual Entailment, volume 3944 of Lecture Notes in Computer Science, pages 28–32. Springer.
Neal, R. M. and Zhang, J. (2006). High dimensional classification with Bayesian neural networks and Dirichlet diffusion trees. In Guyon, I., Gunn, S., Nikravesh, M., and Zadeh, L. A., editors, Feature
Extraction: Foundations and Applications, Studies in Fuzziness and Soft Computing, pages 265–295.
Springer.
Neftci, E., Das, S., Pedroni, B., Kreutz-Delgado, K., and Cauwenberghs, G. (2014). Event-driven contrastive divergence for spiking neuromorphic systems. Frontiers in Neuroscience, 7(272).
Neil, D. and Liu, S.-C. (2014). Minitaur, an event-driven FPGA-based spiking network accelerator. IEEE
Transactions on Very Large Scale Integration (VLSI) Systems, PP(99):1–8.
Nessler, B., Pfeiffer, M., Buesing, L., and Maass, W. (2013).
Bayesian computation emerges in generic cortical microcircuits through spike-timing-dependent plasticity. PLoS Computational Biology, 9(4):e1003037.
Neti, C., Schneider, M. H., and Young, E. D. (1992). Maximally fault tolerant neural networks. In IEEE
Transactions on Neural Networks, volume 3, pages 14–23.
Neuneier, R. and Zimmermann, H.-G. (1996). How to train neural networks. In Orr, G. B. and M¨uller, K.R., editors, Neural Networks: Tricks of the Trade, volume 1524 of Lecture Notes in Computer Science, pages 373–423. Springer.
Newton, I. (1687). Philosophiae naturalis principia mathematica. William Dawson & Sons Ltd., London.
Nguyen, N. and Widrow, B. (1989). The truck backer-upper: An example of self learning in neural networks. In Proceedings of the International Joint Conference on Neural Networks, pages 357–363. IEEE
Press.
Nilsson, N. J. (1980). Principles of artificial intelligence. Morgan Kaufmann, San Francisco, CA, USA.
Nolfi, S., Floreano, D., Miglino, O., and Mondada, F. (1994a). How to evolve autonomous robots: Different approaches in evolutionary robotics. In Brooks, R. A. and Maes, P., editors, Fourth International
Workshop on the Synthesis and Simulation of Living Systems (Artificial Life IV), pages 190–197. MIT.
Nolfi, S., Parisi, D., and Elman, J. L. (1994b). Learning and evolution in neural networks. Adaptive
Behavior, 3(1):5–28.
Nowak, E., Jurie, F., and Triggs, B. (2006). Sampling strategies for bag-of-features image classification.
In Proc. ECCV 2006, pages 490–503. Springer.
Nowlan, S. J. and Hinton, G. E. (1992). Simplifying neural networks by soft weight sharing. Neural
Computation, 4:173–193.
O'Connor, P., Neil, D., Liu, S.-C., Delbruck, T., and Pfeiffer, M. (2013). Real-time classification and sensor fusion with a spiking deep belief network. Frontiers in Neuroscience, 7(178).
Oh, K.-S. and Jung, K. (2004). GPU implementation of neural networks. Pattern Recognition, 37(6):1311–
Oja, E. (1989). Neural networks, principal components, and subspaces. International Journal of Neural
Systems, 1(1):61–68.
Oja, E. (1991). Data compression, feature extraction, and autoassociation in feedforward neural networks.
In Kohonen, T., M¨akisara, K., Simula, O., and Kangas, J., editors, Artificial Neural Networks, volume 1, pages 737–745. Elsevier Science Publishers B.V., North-Holland.
Olshausen, B. A. and Field, D. J. (1996). Emergence of simple-cell receptive field properties by learning a sparse code for natural images. Nature, 381(6583):607–609.
Omlin, C. and Giles, C. L. (1996). Extraction of rules from discrete-time recurrent neural networks. Neural
Networks, 9(1):41–52.
Oquab, M., Bottou, L., Laptev, I., and Sivic, J. (2013). Learning and transferring mid-level image representations using convolutional neural networks. Technical Report hal-00911179.
O'Reilly, R. (2003). Making working memory work: A computational model of learning in the prefrontal cortex and basal ganglia. Technical Report ICS-03-03, ICS.
O'Reilly, R. C. (1996). Biologically plausible error-driven learning using local activation differences: The generalized recirculation algorithm. Neural Computation, 8(5):895–938.
Orr, G. and M¨uller, K. (1998). Neural Networks: Tricks of the Trade. Number LNCS 1524 in Lecture
Notes in Computer Science Series. Springer Verlag.
Ostrovskii, G. M., Volin, Y. M., and Borisov, W. W. (1971). ¨Uber die Berechnung von Ableitungen. Wiss.
Z. Tech. Hochschule f¨ur Chemie, 13:382–384.
Otsuka, M. (2010). Goal-Oriented Representation of the External World: A Free-Energy-Based Approach.
PhD thesis, Nara Institute of Science and Technology.
Otsuka, M., Yoshimoto, J., and Doya, K. (2010). Free-energy-based reinforcement learning in a partially observable environment. In Proc. ESANN.
Otte, S., Krechel, D., Liwicki, M., and Dengel, A. (2012). Local feature based online mode detection with recurrent neural networks. In Proceedings of the 2012 International Conference on Frontiers in Handwriting Recognition, pages 533–537. IEEE Computer Society.
Oudeyer, P.-Y., Baranes, A., and Kaplan, F. (2013). Intrinsically motivated learning of real world sensorimotor skills with developmental constraints. In Baldassarre, G. and Mirolli, M., editors, Intrinsically
Motivated Learning in Natural and Artificial Systems. Springer.
OReilly, R. C., Wyatte, D., Herd, S., Mingus, B., and Jilk, D. J. (2013). Recurrent processing during object recognition. Frontiers in Psychology, 4:124.
Pachitariu, M. and Sahani, M. (2013). Regularization and nonlinearities for neural language models: when are they needed? arXiv preprint arXiv:1301.5650.
Palm, G. (1980). On associative memory. Biological Cybernetics, 36.
Palm, G. (1992).
On the information storage capacity of local learning rules.
Neural Computation, 4(2):703–711.
Pan, S. J. and Yang, Q. (2010). A survey on transfer learning. IEEE Transactions on Knowledge and Data
Engineering, 22(10):1345–1359.
Parekh, R., Yang, J., and Honavar, V. (2000). Constructive neural network learning algorithms for multicategory pattern classification. IEEE Transactions on Neural Networks, 11(2):436–451.
Parker, D. B. (1985). Learning-logic. Technical Report TR-47, Center for Comp. Research in Economics and Management Sci., MIT.
Pascanu, R., Gulcehre, C., Cho, K., and Bengio, Y. (2013a). How to construct deep recurrent neural networks. arXiv preprint arXiv:1312.6026.
Pascanu, R., Mikolov, T., and Bengio, Y. (2013b). On the difficulty of training recurrent neural networks.
In ICML'13: JMLR: W&CP volume 28.
Pasemann, F., Steinmetz, U., and Dieckman, U. (1999). Evolving structure and function of neurocontrollers. In Angeline, P. J., Michalewicz, Z., Schoenauer, M., Yao, X., and Zalzala, A., editors, Proceedings of the Congress on Evolutionary Computation, volume 3, pages 1973–1978, Mayflower Hotel, Washington D.C., USA. IEEE Press.
Pearlmutter, B. A. (1989). Learning state space trajectories in recurrent neural networks. Neural Computation, 1(2):263–269.
Pearlmutter, B. A. (1994). Fast exact multiplication by the Hessian. Neural Computation, 6(1):147–160.
Pearlmutter, B. A. (1995). Gradient calculations for dynamic recurrent neural networks: A survey. IEEE
Transactions on Neural Networks, 6(5):1212–1228.
Pearlmutter, B. A. and Hinton, G. E. (1986). G-maximization: An unsupervised learning procedure for discovering regularities. In Denker, J. S., editor, Neural Networks for Computing: American Institute of Physics Conference Proceedings 151, volume 2, pages 333–338.
Peng, J. and Williams, R. J. (1996). Incremental multi-step Q-learning. Machine Learning, 22:283–290.
P´erez-Ortiz, J. A., Gers, F. A., Eck, D., and Schmidhuber, J. (2003). Kalman filters improve LSTM network performance in problems unsolvable by traditional recurrent nets. Neural Networks, (16):241–250.
Perrett, D., Hietanen, J., Oram, M., Benson, P., and Rolls, E. (1992). Organization and functions of cells responsive to faces in the temporal cortex [and discussion]. Philosophical Transactions of the Royal
Society of London. Series B: Biological Sciences, 335(1273):23–30.
Perrett, D., Rolls, E., and Caan, W. (1982). Visual neurones responsive to faces in the monkey temporal cortex. Experimental Brain Research, 47(3):329–342.
Peters, J. (2010). Policy gradient methods. Scholarpedia, 5(11):3698.
Peters, J. and Schaal, S. (2008a). Natural actor-critic. Neurocomputing, 71:1180–1190.
Peters, J. and Schaal, S. (2008b). Reinforcement learning of motor skills with policy gradients. Neural
Network, 21(4):682–697.
Pham, V., Kermorvant, C., and Louradour, J. (2013). Dropout Improves Recurrent Neural Networks for
Handwriting Recognition. arXiv preprint arXiv:1312.4569.
Pineda, F. J. (1987). Generalization of back-propagation to recurrent neural networks. Physical Review
Letters, 19(59):2229–2232.
Plate, T. A. (1993). Holographic recurrent networks. In S. J. Hanson, J. D. C. and Giles, C. L., editors, Advances in Neural Information Processing Systems (NIPS) 5, pages 34–41. Morgan Kaufmann.
Plumbley, M. D. (1991). On information theory and unsupervised neural networks. Dissertation, published as technical report CUED/F-INFENG/TR.78, Engineering Department, Cambridge University.
Pollack, J. B. (1988). Implications of recursive distributed representations. In Proc. NIPS, pages 527–536.
Pollack, J. B. (1990). Recursive distributed representation. Artificial Intelligence, 46:77–105.
Pontryagin, L. S., Boltyanskii, V. G., Gamrelidze, R. V., and Mishchenko, E. F. (1961). The Mathematical
Theory of Optimal Processes.
Poon, H. and Domingos, P. (2011). Sum-product networks: A new deep architecture. In IEEE International
Conference on Computer Vision (ICCV) Workshops, pages 689–690. IEEE.
Post, E. L. (1936). Finite combinatory processes-formulation 1. The Journal of Symbolic Logic, 1(3):103–
Prasoon, A., Petersen, K., Igel, C., Lauze, F., Dam, E., and Nielsen, M. (2013). Voxel classification based on triplanar convolutional neural networks applied to cartilage segmentation in knee MRI. In Medical
Image Computing and Computer Assisted Intervention (MICCAI), volume 8150 of LNCS, pages 246–
253. Springer.
Precup, D., Sutton, R. S., and Singh, S. (1998). Multi-time models for temporally abstract planning. In
Advances in Neural Information Processing Systems (NIPS), pages 1050–1056. Morgan Kaufmann.
Prokhorov, D. (2010). A convolutional learning system for object classification in 3-D LIDAR data. IEEE
Transactions on Neural Networks, 21(5):858–863.
Prokhorov, D., Puskorius, G., and Feldkamp, L. (2001). Dynamical neural networks for control. In Kolen, J. and Kremer, S., editors, A field guide to dynamical recurrent networks, pages 23–78. IEEE Press.
Prokhorov, D. and Wunsch, D. (1997). Adaptive critic design. IEEE Transactions on Neural Networks, 8(5):997–1007.
Prokhorov, D. V., Feldkamp, L. A., and Tyukin, I. Y. (2002). Adaptive behavior with fixed weights in RNN: an overview. In Proceedings of the IEEE International Joint Conference on Neural Networks (IJCNN), pages 2018–2023.
Puskorius, G. V. and Feldkamp, L. A. (1994). Neurocontrol of nonlinear dynamical systems with Kalman filter trained recurrent networks. IEEE Transactions on Neural Networks, 5(2):279–297.
Raiko, T., Valpola, H., and LeCun, Y. (2012). Deep learning made easier by linear transformations in perceptrons. In International Conference on Artificial Intelligence and Statistics, pages 924–932.
Raina, R., Madhavan, A., and Ng, A. (2009). Large-scale deep unsupervised learning using graphics processors. In Proceedings of the 26th Annual International Conference on Machine Learning (ICML), pages 873–880. ACM.
Ramacher, U., Raab, W., Anlauf, J., Hachmann, U., Beichter, J., Bruels, N., Wesseling, M., Sicheneder, E., Maenner, R., Glaess, J., and Wurz, A. (1993).
Multiprocessor and memory architecture of the neurocomputer SYNAPSE-1. International Journal of Neural Systems, 4(4):333–336.
Ranzato, M., Poultney, C., Chopra, S., and LeCun, Y. (2006). Efficient learning of sparse representations with an energy-based model. In et al., J. P., editor, Advances in Neural Information Processing Systems(NIPS 2006). MIT Press.
Ranzato, M. A., Huang, F., Boureau, Y., and LeCun, Y. (2007). Unsupervised learning of invariant feature hierarchies with applications to object recognition. In Proc. Computer Vision and Pattern Recognition
Conference (CVPR'07), pages 1–8. IEEE Press.
Rauber, A., Merkl, D., and Dittenbach, M. (2002). The growing hierarchical self-organizing map: exploratory analysis of high-dimensional data. IEEE Transactions on Neural Networks, 13(6):1331–1341.
Razavian, A. S., Azizpour, H., Sullivan, J., and Carlsson, S. (2014).
CNN features off-the-shelf: an astounding baseline for recognition. arXiv preprint arXiv:1403.6382.
Rechenberg, I. (1971). Evolutionsstrategie - Optimierung technischer Systeme nach Prinzipien der biologischen Evolution. Dissertation. Published 1973 by Fromman-Holzboog.
Redlich, A. N. (1993). Redundancy reduction as a strategy for unsupervised learning. Neural Computation, 5:289–304.
Refenes, N. A., Zapranis, A., and Francis, G. (1994). Stock performance modeling using neural networks: a comparative study with regression models. Neural Networks, 7(2):375–388.
Rezende, D. J. and Gerstner, W. (2014). Stochastic variational learning in recurrent spiking networks.
Frontiers in Computational Neuroscience, 8:38.
Riedmiller, M. (2005). Neural fitted Q iteration—first experiences with a data efficient neural reinforcement learning method. In Proc. ECML-2005, pages 317–328. Springer-Verlag Berlin Heidelberg.
Riedmiller, M. and Braun, H. (1993). A direct adaptive method for faster backpropagation learning: The Rprop algorithm. In Proc. IJCNN, pages 586–591. IEEE Press.
Riedmiller, M., Lange, S., and Voigtlaender, A. (2012). Autonomous reinforcement learning on raw visual input data in a real world application. In International Joint Conference on Neural Networks (IJCNN), pages 1–8, Brisbane, Australia.
Riesenhuber, M. and Poggio, T. (1999). Hierarchical models of object recognition in cortex. Nat. Neurosci., 2(11):1019–1025.
Rifai, S., Vincent, P., Muller, X., Glorot, X., and Bengio, Y. (2011). Contractive auto-encoders: Explicit invariance during feature extraction. In Proceedings of the 28th International Conference on Machine
Learning (ICML-11), pages 833–840.
Ring, M., Schaul, T., and Schmidhuber, J. (2011). The two-dimensional organization of behavior. In
Proceedings of the First Joint Conference on Development Learning and on Epigenetic Robotics ICDLEPIROB, Frankfurt.
Ring, M. B. (1991). Incremental development of complex behaviors through automatic construction of sensory-motor hierarchies. In Birnbaum, L. and Collins, G., editors, Machine Learning: Proceedings of the Eighth International Workshop, pages 343–347. Morgan Kaufmann.
Ring, M. B. (1993). Learning sequential tasks by incrementally adding higher orders. In S. J. Hanson, J.
D. C. and Giles, C. L., editors, Advances in Neural Information Processing Systems 5, pages 115–122.
Morgan Kaufmann.
Ring, M. B. (1994). Continual Learning in Reinforcement Environments. PhD thesis, University of Texas at Austin, Austin, Texas 78712.
Risi, S. and Stanley, K. O. (2012). A unified approach to evolving plasticity and neural geometry. In
International Joint Conference on Neural Networks (IJCNN), pages 1–8. IEEE.
Rissanen, J. (1986). Stochastic complexity and modeling. The Annals of Statistics, 14(3):1080–1100.
Ritter, H. and Kohonen, T. (1989). Self-organizing semantic maps. Biological Cybernetics, 61(4):241–254.
Robinson, A. J. and Fallside, F. (1987). The utility driven dynamic error propagation network. Technical
Report CUED/F-INFENG/TR.1, Cambridge University Engineering Department.
Robinson, T. and Fallside, F. (1989).
Dynamic reinforcement driven error propagation networks with application to game playing. In Proceedings of the 11th Conference of the Cognitive Science Society, Ann Arbor, pages 836–843.
Rodriguez, P. and Wiles, J. (1998). Recurrent neural networks can learn to implement symbol-sensitive counting. In Advances in Neural Information Processing Systems (NIPS), volume 10, pages 87–93. The MIT Press.
Rodriguez, P., Wiles, J., and Elman, J. (1999). A recurrent neural network that learns to count. Connection
Science, 11(1):5–40.
Roggen, D., Hofmann, S., Thoma, Y., and Floreano, D. (2003). Hardware spiking neural network with runtime reconfigurable connectivity in an autonomous robot. In Proc. NASA/DoD Conference on Evolvable
Hardware, 2003, pages 189–198. IEEE.
Rohwer, R. (1989). The 'moving targets' training method. In Kindermann, J. and Linden, A., editors, Proceedings of 'Distributed Adaptive Neural Information Processing', St.Augustin, 24.-25.5,. Oldenbourg.
Rosenblatt, F. (1958). The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review, 65(6):386.
Rosenblatt, F. (1962). Principles of Neurodynamics. Spartan, New York.
Roux, L., Racoceanu, D., Lomenie, N., Kulikova, M., Irshad, H., Klossa, J., Capron, F., Genestie, C., Naour, G. L., and Gurcan, M. N. (2013). Mitosis detection in breast cancer histological images - an
ICPR 2012 contest. J. Pathol. Inform., 4:8.
Rubner, J. and Schulten, K. (1990). Development of feature detectors by self-organization: A network model. Biological Cybernetics, 62:193–199.
Rubner, J. and Tavan, P. (1989). A self-organization network for principal-component analysis. Europhysics Letters, 10:693–698.
R¨uckstieß, T., Felder, M., and Schmidhuber, J. (2008). State-Dependent Exploration for policy gradient methods. In et al., W. D., editor, European Conference on Machine Learning (ECML) and Principles and Practice of Knowledge Discovery in Databases 2008, Part II, LNAI 5212, pages 234–249.
Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1986). Learning internal representations by error propagation. In Rumelhart, D. E. and McClelland, J. L., editors, Parallel Distributed Processing, volume 1, pages 318–362. MIT Press.
Rumelhart, D. E. and Zipser, D. (1986). Feature discovery by competitive learning. In Parallel Distributed
Processing, pages 151–193. MIT Press.
Rummery, G. and Niranjan, M. (1994). On-line Q-learning using connectionist sytems. Technical Report
CUED/F-INFENG-TR 166, Cambridge University, UK.
Russell, S. J., Norvig, P., Canny, J. F., Malik, J. M., and Edwards, D. D. (1995). Artificial Intelligence: a Modern Approach, volume 2. Englewood Cliffs: Prentice Hall.
Saito, K. and Nakano, R. (1997). Partial BFGS update and efficient step-length calculation for three-layer neural networks. Neural Computation, 9(1):123–141.
Salakhutdinov, R. and Hinton, G. (2009). Semantic hashing. Int. J. Approx. Reasoning, 50(7):969–978.
Sallans, B. and Hinton, G. (2004). Reinforcement learning with factored states and actions. Journal of Machine Learning Research, 5:1063–1088.
Sałustowicz, R. P. and Schmidhuber, J. (1997). Probabilistic incremental program evolution. Evolutionary
Computation, 5(2):123–141.
Samejima, K., Doya, K., and Kawato, M. (2003). Inter-module credit assignment in modular reinforcement learning. Neural Networks, 16(7):985–994.
Samuel, A. L. (1959). Some studies in machine learning using the game of checkers. IBM Journal on
Research and Development, 3:210–229.
Sanger, T. D. (1989). An optimality principle for unsupervised learning. In Touretzky, D. S., editor, Advances in Neural Information Processing Systems (NIPS) 1, pages 11–19. Morgan Kaufmann.
Santamar´ıa, J. C., Sutton, R. S., and Ram, A. (1997). Experiments with reinforcement learning in problems with continuous state and action spaces. Adaptive Behavior, 6(2):163–217.
Saravanan, N. and Fogel, D. B. (1995). Evolving neural control systems. IEEE Expert, pages 23–27.
Saund, E. (1994). Unsupervised learning of mixtures of multiple causes in binary data. In Cowan, J. D., Tesauro, G., and Alspector, J., editors, Advances in Neural Information Processing Systems (NIPS) 6, pages 27–34. Morgan Kaufmann.
Schaback, R. and Werner, H. (1992). Numerische Mathematik, volume 4. Springer.
Sch¨afer, A. M., Udluft, S., and Zimmermann, H.-G. (2006). Learning long term dependencies with recurrent neural networks. In Kollias, S. D., Stafylopatis, A., Duch, W., and Oja, E., editors, ICANN (1), volume 4131 of Lecture Notes in Computer Science, pages 71–80. Springer.
Schapire, R. E. (1990). The strength of weak learnability. Machine Learning, 5:197–227.
Schaul, T. and Schmidhuber, J. (2010). Metalearning. Scholarpedia, 6(5):4650.
Schaul, T., Zhang, S., and LeCun, Y. (2013). No more pesky learning rates. In Proc. 30th International
Conference on Machine Learning (ICML).
Schemmel, J., Grubl, A., Meier, K., and Mueller, E. (2006). Implementing synaptic plasticity in a VLSI spiking neural network model. In International Joint Conference on Neural Networks (IJCNN), pages
1–6. IEEE.
Scherer, D., M¨uller, A., and Behnke, S. (2010). Evaluation of pooling operations in convolutional architectures for object recognition. In Proc. International Conference on Artificial Neural Networks (ICANN), pages 92–101.
Schmidhuber, J. (1987). Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... hook. Inst. f. Inf., Tech. Univ. Munich. http://www.idsia.ch/˜juergen/diploma.html.
Schmidhuber, J. (1989a).
Accelerated learning in back-propagation nets.
In Pfeifer, R., Schreter, Z., Fogelman, Z., and Steels, L., editors, Connectionism in Perspective, pages 429 – 438. Amsterdam:
Elsevier, North-Holland.
Schmidhuber, J. (1989b). A local learning algorithm for dynamic feedforward and recurrent networks.
Connection Science, 1(4):403–412.
Schmidhuber, J. (1990a). Dynamische neuronale Netze und das fundamentale raumzeitliche Lernproblem.(Dynamic neural nets and the fundamental spatio-temporal credit assignment problem.) Dissertation, Inst. f. Inf., Tech. Univ. Munich.
Schmidhuber, J. (1990b). Learning algorithms for networks with internal and external feedback. In Touretzky, D. S., Elman, J. L., Sejnowski, T. J., and Hinton, G. E., editors, Proc. of the 1990 Connectionist
Models Summer School, pages 52–61. Morgan Kaufmann.
Schmidhuber, J. (1990c). The Neural Heat Exchanger. Talks at TU Munich (1990), University of Colorado at Boulder (1992), and Z. Li's NIPS*94 workshop on unsupervised learning. Also published at the Intl.
Conference on Neural Information Processing (ICONIP'96), vol. 1, pages 194-197, 1996.
Schmidhuber, J. (1990d). An on-line algorithm for dynamic reinforcement learning and planning in reactive environments. In Proc. IEEE/INNS International Joint Conference on Neural Networks, San Diego, volume 2, pages 253–258.
Schmidhuber, J. (1991a). Curious model-building control systems. In Proceedings of the International
Joint Conference on Neural Networks, Singapore, volume 2, pages 1458–1463. IEEE press.
Schmidhuber, J. (1991b). Learning to generate sub-goals for action sequences. In Kohonen, T., M¨akisara, K., Simula, O., and Kangas, J., editors, Artificial Neural Networks, pages 967–972. Elsevier Science
Publishers B.V., North-Holland.
Schmidhuber, J. (1991c). Reinforcement learning in Markovian and non-Markovian environments. In
Lippman, D. S., Moody, J. E., and Touretzky, D. S., editors, Advances in Neural Information Processing
Systems 3 (NIPS 3), pages 500–506. Morgan Kaufmann.
Schmidhuber, J. (1992a). A fixed size storage O(n3) time complexity learning algorithm for fully recurrent continually running networks. Neural Computation, 4(2):243–248.
Schmidhuber, J. (1992b). Learning complex, extended sequences using the principle of history compression. Neural Computation, 4(2):234–242. (Based on TR FKI-148-91, TUM, 1991).
Schmidhuber, J. (1992c). Learning factorial codes by predictability minimization. Neural Computation, 4(6):863–879.
Schmidhuber, J. (1993a). An introspective network that can learn to run its own weight change algorithm.
In Proc. of the Intl. Conf. on Artificial Neural Networks, Brighton, pages 191–195. IEE.
Schmidhuber, J. (1993b). Netzwerkarchitekturen, Zielfunktionen und Kettenregel. (Network architectures, objective functions, and chain rule.) Habilitation Thesis, Inst. f. Inf., Tech. Univ. Munich.
Schmidhuber, J. (1997). Discovering neural nets with low Kolmogorov complexity and high generalization capability. Neural Networks, 10(5):857–873.
Schmidhuber, J. (2002). The Speed Prior: a new simplicity measure yielding near-optimal computable predictions. In Kivinen, J. and Sloan, R. H., editors, Proceedings of the 15th Annual Conference on
Computational Learning Theory (COLT 2002), Lecture Notes in Artificial Intelligence, pages 216–228.
Springer, Sydney, Australia.
Schmidhuber, J. (2004). Optimal ordered problem solver. Machine Learning, 54:211–254.
Schmidhuber, J. (2006a). Developmental robotics, optimal artificial curiosity, creativity, music, and the fine arts. Connection Science, 18(2):173–187.
Schmidhuber, J. (2006b). G¨odel machines: Fully self-referential optimal universal self-improvers. In
Goertzel, B. and Pennachin, C., editors, Artificial General Intelligence, pages 199–226. Springer Verlag.
Variant available as arXiv:cs.LO/0309048.
Schmidhuber, J. (2007). Prototype resilient, self-modeling robots. Science, 316(5825):688.
Schmidhuber, J. (2012).
Self-delimiting neural networks.
Technical Report IDSIA-08-12, arXiv:1210.0118v1 [cs.NE], The Swiss AI Lab IDSIA.
Schmidhuber, J. (2013a). My first Deep Learning system of 1991 + Deep Learning timeline 1962-2013.
Technical Report arXiv:1312.5548v1 [cs.NE], The Swiss AI Lab IDSIA.
Schmidhuber, J. (2013b). POWERPLAY: Training an Increasingly General Problem Solver by Continually
Searching for the Simplest Still Unsolvable Problem. Frontiers in Psychology.
Schmidhuber, J., Ciresan, D., Meier, U., Masci, J., and Graves, A. (2011). On fast deep nets for AGI vision.
In Proc. Fourth Conference on Artificial General Intelligence (AGI), Google, Mountain View, CA, pages
243–246.
Schmidhuber, J., Eldracher, M., and Foltin, B. (1996). Semilinear predictability minimization produces well-known feature detectors. Neural Computation, 8(4):773–786.
Schmidhuber, J. and Huber, R. (1991). Learning to generate artificial fovea trajectories for target detection.
International Journal of Neural Systems, 2(1 & 2):135–141.
Schmidhuber, J., Mozer, M. C., and Prelinger, D. (1993). Continuous history compression. In H¨uning, H., Neuhauser, S., Raus, M., and Ritschel, W., editors, Proc. of Intl. Workshop on Neural Networks, RWTH
Aachen, pages 87–95. Augustinus.
Schmidhuber, J. and Prelinger, D. (1992). Discovering predictable classifications. Technical Report CUCS-626-92, Dept. of Comp. Sci., University of Colorado at Boulder. Published in Neural Computation
5(4):625-635 (1993).
Schmidhuber, J. and Wahnsiedler, R. (1992). Planning simple trajectories using neural subgoal generators.
In Meyer, J. A., Roitblat, H. L., and Wilson, S. W., editors, Proc. of the 2nd International Conference on
Simulation of Adaptive Behavior, pages 196–202. MIT Press.
Schmidhuber, J., Wierstra, D., Gagliolo, M., and Gomez, F. J. (2007). Training recurrent networks by
Evolino. Neural Computation, 19(3):757–779.
Schmidhuber, J., Zhao, J., and Schraudolph, N. (1997a). Reinforcement learning with self-modifying policies. In Thrun, S. and Pratt, L., editors, Learning to learn, pages 293–309. Kluwer.
Schmidhuber, J., Zhao, J., and Wiering, M. (1997b). Shifting inductive bias with success-story algorithm, adaptive Levin search, and incremental self-improvement. Machine Learning, 28:105–130.
Sch¨olkopf, B., Burges, C. J. C., and Smola, A. J., editors (1998). Advances in Kernel Methods - Support
Vector Learning. MIT Press, Cambridge, MA.
Schraudolph, N. and Sejnowski, T. J. (1993). Unsupervised discrimination of clustered data via optimization of binary information gain. In Hanson, S. J., Cowan, J. D., and Giles, C. L., editors, Advances in Neural Information Processing Systems, volume 5, pages 499–506. Morgan Kaufmann, San Mateo.
Schraudolph, N. N. (2002). Fast curvature matrix-vector products for second-order gradient descent. Neural Computation, 14(7):1723–1738.
Schraudolph, N. N. and Sejnowski, T. J. (1996). Tempering backpropagation networks: Not all weights are created equal. In Touretzky, D. S., Mozer, M. C., and Hasselmo, M. E., editors, Advances in Neural
Information Processing Systems (NIPS), volume 8, pages 563–569. The MIT Press, Cambridge, MA.
Schrauwen, B., Verstraeten, D., and Van Campenhout, J. (2007). An overview of reservoir computing: theory, applications and implementations. In Proceedings of the 15th European Symposium on Artificial
Neural Networks. p. 471-482 2007, pages 471–482.
Schuster, H. G. (1992). Learning by maximization the information transfer through nonlinear noisy neurons and "noise breakdown". Phys. Rev. A, 46(4):2131–2138.
Schuster, M. (1999). On supervised learning from sequential data with applications for speech recognition.
PhD thesis, Nara Institute of Science and Technolog, Kyoto, Japan.
Schuster, M. and Paliwal, K. K. (1997). Bidirectional recurrent neural networks. IEEE Transactions on
Signal Processing, 45:2673–2681.
Schwartz, A. (1993). A reinforcement learning method for maximizing undiscounted rewards. In Proc.
ICML, pages 298–305.
Schwefel, H. P. (1974). Numerische Optimierung von Computer-Modellen. Dissertation. Published 1977 by Birkh¨auser, Basel.
Segmentation of Neuronal Structures in EM Stacks Challenge (2012). IEEE International Symposium on
Biomedical Imaging (ISBI), http://tinyurl.com/d2fgh7g.
Sehnke, F., Osendorfer, C., R¨uckstieß, T., Graves, A., Peters, J., and Schmidhuber, J. (2010). Parameterexploring policy gradients. Neural Networks, 23(4):551–559.
Sermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., and LeCun, Y. (2013). OverFeat: Integrated recognition, localization and detection using convolutional networks. arXiv preprint arXiv:1312.6229.
Sermanet, P. and LeCun, Y. (2011). Traffic sign recognition with multi-scale convolutional networks. In
Proceedings of International Joint Conference on Neural Networks (IJCNN'11), pages 2809–2813.
Serrano-Gotarredona, R., Oster, M., Lichtsteiner, P., Linares-Barranco, A., Paz-Vicente, R., G´omezRodr´ıguez, F., Camu˜nas-Mesa, L., Berner, R., Rivas-P´erez, M., Delbruck, T., et al. (2009). Caviar:
A 45k neuron, 5m synapse, 12g connects/s AER hardware sensory–processing–learning–actuating system for high-speed visual object recognition and tracking. IEEE Transactions on Neural Networks, 20(9):1417–1438.
Serre, T., Riesenhuber, M., Louie, J., and Poggio, T. (2002). On the role of object-specific features for real world object recognition in biological vision. In Biologically Motivated Computer Vision, pages
387–397.
Seung, H. S. (2003). Learning in spiking neural networks by reinforcement of stochastic synaptic transmission. Neuron, 40(6):1063–1073.
Shan, H. and Cottrell, G. (2014). Efficient visual coding: From retina to V2. In Proc. International
Conference on Learning Representations (ICLR). arXiv preprint arXiv:1312.6077.
Shan, H., Zhang, L., and Cottrell, G. W. (2007). Recursive ICA. Advances in Neural Information Processing Systems (NIPS), 19:1273.
Shanno, D. F. (1970). Conditioning of quasi-Newton methods for function minimization. Mathematics of computation, 24(111):647–656.
Shannon, C. E. (1948). A mathematical theory of communication (parts I and II). Bell System Technical
Journal, XXVII:379–423.
Shao, L., Wu, D., and Li, X. (2014).
Learning deep and wide: A spectral method for learning deep networks. IEEE Transactions on Neural Networks and Learning Systems.
Shavlik, J. W. (1994). Combining symbolic and neural learning. Machine Learning, 14(3):321–331.
Shavlik, J. W. and Towell, G. G. (1989). Combining explanation-based and neural learning: An algorithm and empirical results. Connection Science, 1(3):233–255.
Siegelmann, H. (1992). Theoretical Foundations of Recurrent Neural Networks. PhD thesis, Rutgers, New
Brunswick Rutgers, The State of New Jersey.
Siegelmann, H. T. and Sontag, E. D. (1991). Turing computability with neural nets. Applied Mathematics
Letters, 4(6):77–80.
Silva, F. M. and Almeida, L. B. (1990). Speeding up back-propagation. In Eckmiller, R., editor, Advanced
Neural Computers, pages 151–158, Amsterdam. Elsevier.
S´ıma, J. (1994). Loading deep networks is hard. Neural Computation, 6(5):842–850.
S´ıma, J. (2002). Training a single sigmoidal neuron is hard. Neural Computation, 14(11):2709–2728.
Simard, P., Steinkraus, D., and Platt, J. (2003). Best practices for convolutional neural networks applied to visual document analysis. In Seventh International Conference on Document Analysis and Recognition, pages 958–963.
Sims, K. (1994). Evolving virtual creatures.
In Glassner, A., editor, Proceedings of SIGGRAPH '94(Orlando, Florida, July 1994), Computer Graphics Proceedings, Annual Conference, pages 15–22. ACM
SIGGRAPH, ACM Press. ISBN 0-89791-667-0.
Simsek, ¨O. and Barto, A. G. (2008). Skill characterization based on betweenness. In NIPS'08, pages
1497–1504.
Singh, S., Barto, A. G., and Chentanez, N. (2005). Intrinsically motivated reinforcement learning. In
Advances in Neural Information Processing Systems 17 (NIPS). MIT Press, Cambridge, MA.
Singh, S. P. (1994). Reinforcement learning algorithms for average-payoff Markovian decision processes.
In National Conference on Artificial Intelligence, pages 700–705.
Smith, S. F. (1980). A Learning System Based on Genetic Adaptive Algorithms,. PhD thesis, Univ. Pittsburgh.
Smolensky, P. (1986). Parallel distributed processing: Explorations in the microstructure of cognition, vol. 1. chapter Information Processing in Dynamical Systems: Foundations of Harmony Theory, pages
194–281. MIT Press, Cambridge, MA, USA.
Solla, S. A. (1988). Accelerated learning in layered neural networks. Complex Systems, 2:625–640.
Solomonoff, R. J. (1964). A formal theory of inductive inference. Part I. Information and Control, 7:1–22.
Solomonoff, R. J. (1978). Complexity-based induction systems. IEEE Transactions on Information Theory, IT-24(5):422–432.
Soloway, E. (1986). Learning to program = learning to construct mechanisms and explanations. Communications of the ACM, 29(9):850–858.
Song, S., Miller, K. D., and Abbott, L. F. (2000). Competitive Hebbian learning through spike-timingdependent synaptic plasticity. Nature Neuroscience, 3(9):919–926.
Speelpenning, B. (1980). Compiling Fast Partial Derivatives of Functions Given by Algorithms. PhD thesis, Department of Computer Science, University of Illinois, Urbana-Champaign.
Srivastava, R. K., Masci, J., Kazerounian, S., Gomez, F., and Schmidhuber, J. (2013). Compete to compute.
In Advances in Neural Information Processing Systems (NIPS), pages 2310–2318.
Stallkamp, J., Schlipsing, M., Salmen, J., and Igel, C. (2011). The German traffic sign recognition benchmark: A multi-class classification competition. In International Joint Conference on Neural Networks(IJCNN 2011), pages 1453–1460. IEEE Press.
Stallkamp, J., Schlipsing, M., Salmen, J., and Igel, C. (2012). Man vs. computer: Benchmarking machine learning algorithms for traffic sign recognition. Neural Networks, 32:323–332.
Stanley, K. O., D'Ambrosio, D. B., and Gauci, J. (2009). A hypercube-based encoding for evolving largescale neural networks. Artificial Life, 15(2):185–212.
Stanley, K. O. and Miikkulainen, R. (2002). Evolving neural networks through augmenting topologies.
Evolutionary Computation, 10:99–127.
Steijvers, M. and Grunwald, P. (1996). A recurrent network that performs a contextsensitive prediction task. In Proceedings of the 18th Annual Conference of the Cognitive Science Society. Erlbaum.
Steil, J. J. (2007). Online reservoir adaptation by intrinsic plasticity for backpropagation–decorrelation and echo state learning. Neural Networks, 20(3):353–364.
Stemmler, M. (1996). A single spike suffices: the simplest form of stochastic resonance in model neurons.
Network: Computation in Neural Systems, 7(4):687–716.
Stoianov, I. and Zorzi, M. (2012). Emergence of a 'visual number sense' in hierarchical generative models.
Nature Neuroscience, 15(2):194–6.
Stone, M. (1974). Cross-validatory choice and assessment of statistical predictions. Roy. Stat. Soc., 36:111–
Stoop, R., Schindler, K., and Bunimovich, L. (2000). When pyramidal neurons lock, when they respond chaotically, and when they like to synchronize. Neuroscience research, 36(1):81–91.
Stratonovich, R. (1960).
Conditional Markov processes.
Theory of Probability And Its Applications, 5(2):156–178.
Sun, G., Chen, H., and Lee, Y. (1993a). Time warping invariant neural networks. In S. J. Hanson, J. D. C. and Giles, C. L., editors, Advances in Neural Information Processing Systems (NIPS) 5, pages 180–187.
Morgan Kaufmann.
Sun, G. Z., Giles, C. L., Chen, H. H., and Lee, Y. C. (1993b). The neural network pushdown automaton:
Model, stack and learning simulations. Technical Report CS-TR-3118, University of Maryland, College
Park.
Sun, Y., Gomez, F., Schaul, T., and Schmidhuber, J. (2013). A Linear Time Natural Evolution Strategy for
Non-Separable Functions. In Proceedings of the Genetic and Evolutionary Computation Conference, page 61, Amsterdam, NL. ACM.
Sun, Y., Wierstra, D., Schaul, T., and Schmidhuber, J. (2009). Efficient natural evolution strategies. In
Proc. 11th Genetic and Evolutionary Computation Conference (GECCO), pages 539–546.
Sutskever, I., Hinton, G. E., and Taylor, G. W. (2008). The recurrent temporal restricted Boltzmann machine. In NIPS, volume 21, page 2008.
Sutton, R. and Barto, A. (1998). Reinforcement learning: An introduction. Cambridge, MA, MIT Press.
Sutton, R. S., McAllester, D. A., Singh, S. P., and Mansour, Y. (1999a). Policy gradient methods for reinforcement learning with function approximation. In Advances in Neural Information Processing
Systems (NIPS) 12, pages 1057–1063.
Sutton, R. S., Precup, D., and Singh, S. P. (1999b). Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning. Artif. Intell., 112(1-2):181–211.
Sutton, R. S., Szepesv´ari, C., and Maei, H. R. (2008). A convergent O(n) algorithm for off-policy temporaldifference learning with linear function approximation. In Advances in Neural Information Processing
Systems (NIPS'08), volume 21, pages 1609–1616.
Szab´o, Z., P´oczos, B., and L˝orincz, A. (2006). Cross-entropy optimization for independent process analysis. In Independent Component Analysis and Blind Signal Separation, pages 909–916. Springer.
Szegedy, C., Toshev, A., and Erhan, D. (2013). Deep neural networks for object detection. pages 2553–
Taylor, G. W., Spiro, I., Bregler, C., and Fergus, R. (2011). Learning invariance through imitation. In
Conference on Computer Vision and Pattern Recognition (CVPR), pages 2729–2736. IEEE.
Tegge, A. N., Wang, Z., Eickholt, J., and Cheng, J. (2009). NNcon: improved protein contact map prediction using 2D-recursive neural networks. Nucleic Acids Research, 37(Suppl 2):W515–W518.
Teichmann, M., Wiltschut, J., and Hamker, F. (2012). Learning invariance from natural images inspired by observations in the primary visual cortex. Neural Computation, 24(5):1271–1296.
Teller, A. (1994). The evolution of mental models. In Kenneth E. Kinnear, J., editor, Advances in Genetic
Programming, pages 199–219. MIT Press.
Tenenberg, J., Karlsson, J., and Whitehead, S. (1993). Learning via task decomposition. In Meyer, J. A., Roitblat, H., and Wilson, S., editors, From Animals to Animats 2: Proceedings of the Second International Conference on Simulation of Adaptive Behavior, pages 337–343. MIT Press.
Tesauro, G. (1994). TD-gammon, a self-teaching backgammon program, achieves master-level play. Neural Computation, 6(2):215–219.
Tieleman, T. and Hinton, G. (2012). Lecture 6.5—RmsProp: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning.
Tikhonov, A. N., Arsenin, V. I., and John, F. (1977). Solutions of ill-posed problems. Winston.
Ting, K. M. and Witten, I. H. (1997). Stacked generalization: when does it work? In in Proc. International
Joint Conference on Artificial Intelligence (IJCAI).
Tiˇno, P. and Hammer, B. (2004). Architectural bias in recurrent neural networks: Fractal analysis. Neural
Computation, 15(8):1931–1957.
Tonkes, B. and Wiles, J. (1997). Learning a context-free task with a recurrent neural network: An analysis of stability. In Proceedings of the Fourth Biennial Conference of the Australasian Cognitive Science
Society.
Towell, G. G. and Shavlik, J. W. (1994). Knowledge-based artificial neural networks. Artificial Intelligence, 70(1):119–165.
Tsitsiklis, J. N. and van Roy, B. (1996). Feature-based methods for large scale dynamic programming.
Machine Learning, 22(1-3):59–94.
Tsodyks, M., Pawelzik, K., and Markram, H. (1998). Neural networks with dynamic synapses. Neural
Computation, 10(4):821–835.
Tsodyks, M. V., Skaggs, W. E., Sejnowski, T. J., and McNaughton, B. L. (1996). Population dynamics and theta rhythm phase precession of hippocampal place cell firing: a spiking neuron model. Hippocampus, 6(3):271–280.
Turaga, S. C., Murray, J. F., Jain, V., Roth, F., Helmstaedter, M., Briggman, K., Denk, W., and Seung, H. S.(2010). Convolutional networks can learn to generate affinity graphs for image segmentation. Neural
Computation, 22(2):511–538.
Turing, A. M. (1936). On computable numbers, with an application to the Entscheidungsproblem. Proceedings of the London Mathematical Society, Series 2, 41:230–267.
Turner, A. J. and Miller, J. F. (2013). Cartesian Genetic Programming encoded artificial neural networks:
A comparison using three benchmarks. In Proceedings of the Conference on Genetic and Evolutionary
Computation (GECCO), pages 1005–1012.
Ueda, N. (2000). Optimal linear combination of neural networks for improving classification performance.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 22(2):207–215.
Urlbe, A. P. (1999). Structure-adaptable digital neural networks. PhD thesis, Universidad del Valle.
Utgoff, P. E. and Stracuzzi, D. J. (2002). Many-layered learning. Neural Computation, 14(10):2497–2529.
Vahed, A. and Omlin, C. W. (2004). A machine learning method for extracting symbolic knowledge from recurrent neural networks. Neural Computation, 16(1):59–71.
Vaillant, R., Monrocq, C., and LeCun, Y. (1994). Original approach for the localisation of objects in images. IEE Proc on Vision, Image, and Signal Processing, 141(4):245–250. van den Berg, T. and Whiteson, S. (2013). Critical factors in the performance of HyperNEAT. In GECCO
2013: Proceedings of the Genetic and Evolutionary Computation Conference, pages 759–766. van Hasselt, H. (2012). Reinforcement learning in continuous state and action spaces. In Wiering, M. and van Otterlo, M., editors, Reinforcement Learning, pages 207–251. Springer.
Vapnik, V. (1992). Principles of risk minimization for learning theory. In Lippman, D. S., Moody, J. E., and Touretzky, D. S., editors, Advances in Neural Information Processing Systems (NIPS) 4, pages 831–838.
Morgan Kaufmann.
Vapnik, V. (1995). The Nature of Statistical Learning Theory. Springer, New York.
Versino, C. and Gambardella, L. M. (1996). Learning fine motion by using the hierarchical extended
Kohonen map. In Proc. Intl. Conf. on Artificial Neural Networks (ICANN), pages 221–226. Springer.
Veta, M., Viergever, M., Pluim, J., Stathonikos, N., and van Diest, P. J. (2013). MICCAI 2013 Grand
Challenge on Mitosis Detection.
Vieira, A. and Barradas, N. (2003). A training algorithm for classification of high-dimensional data. Neurocomputing, 50:461–472.
Vincent, P., Hugo, L., Bengio, Y., and Manzagol, P.-A. (2008). Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th international conference on Machine learning, ICML '08, pages 1096–1103, New York, NY, USA. ACM.
Vlassis, N., Littman, M. L., and Barber, D. (2012). On the computational complexity of stochastic controller optimization in POMDPs. ACM Transactions on Computation Theory, 4(4):12.
Vogl, T., Mangis, J., Rigler, A., Zink, W., and Alkon, D. (1988). Accelerating the convergence of the back-propagation method. Biological Cybernetics, 59:257–263. von der Malsburg, C. (1973). Self-organization of orientation sensitive cells in the striate cortex. Kybernetik, 14(2):85–100.
Waldinger, R. J. and Lee, R. C. T. (1969). PROW: a step toward automatic program writing. In Walker, D. E. and Norton, L. M., editors, Proceedings of the 1st International Joint Conference on Artificial
Intelligence (IJCAI), pages 241–252. Morgan Kaufmann.
Wallace, C. S. and Boulton, D. M. (1968). An information theoretic measure for classification. Computer
Journal, 11(2):185–194.
Wan, E. A. (1994). Time series prediction by using a connectionist network with internal delay lines.
In Weigend, A. S. and Gershenfeld, N. A., editors, Time series prediction: Forecasting the future and understanding the past, pages 265–295. Addison-Wesley.
Wang, C., Venkatesh, S. S., and Judd, J. S. (1994). Optimal stopping and effective machine complexity in learning. In Advances in Neural Information Processing Systems (NIPS'6), pages 303–310. Morgan
Kaufmann.
Wang, S. and Manning, C. (2013). Fast dropout training. In Proceedings of the 30th International Conference on Machine Learning (ICML-13), pages 118–126.
Watanabe, O. (1992). Kolmogorov complexity and computational complexity. EATCS Monographs on
Theoretical Computer Science, Springer.
Watanabe, S. (1985). Pattern Recognition: Human and Mechanical. Willey, New York.
Watkins, C. J. C. H. (1989). Learning from Delayed Rewards. PhD thesis, King's College, Oxford.
Watkins, C. J. C. H. and Dayan, P. (1992). Q-learning. Machine Learning, 8:279–292.
Watrous, R. L. and Kuhn, G. M. (1992). Induction of finite-state automata using second-order recurrent networks. In Moody, J. E., Hanson, S. J., and Lippman, R. P., editors, Advances in Neural Information
Processing Systems 4, pages 309–316. Morgan Kaufmann.
Waydo, S. and Koch, C. (2008). Unsupervised learning of individuals and categories from images. Neural
Computation, 20(5):1165–1178.
Weigend, A. S. and Gershenfeld, N. A. (1993). Results of the time series prediction competition at the Santa Fe Institute. In Neural Networks, 1993., IEEE International Conference on, pages 1786–1793.
IEEE.
Weigend, A. S., Rumelhart, D. E., and Huberman, B. A. (1991). Generalization by weight-elimination with application to forecasting. In Lippmann, R. P., Moody, J. E., and Touretzky, D. S., editors, Advances in Neural Information Processing Systems (NIPS) 3, pages 875–882. San Mateo, CA: Morgan Kaufmann.
Weiss, G. (1994). Hierarchical chunking in classifier systems. In Proceedings of the 12th National Conference on Artificial Intelligence, volume 2, pages 1335–1340. AAAI Press/The MIT Press.
Weng, J., Ahuja, N., and Huang, T. S. (1992). Cresceptron: a self-organizing neural network which grows adaptively. In International Joint Conference on Neural Networks (IJCNN), volume 1, pages 576–581.
IEEE.
Weng, J. J., Ahuja, N., and Huang, T. S. (1997). Learning recognition and segmentation using the cresceptron. International Journal of Computer Vision, 25(2):109–143.
Werbos, P. J. (1974). Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences.
PhD thesis, Harvard University.
Werbos, P. J. (1981). Applications of advances in nonlinear sensitivity analysis. In Proceedings of the 10th
IFIP Conference, 31.8 - 4.9, NYC, pages 762–770.
Werbos, P. J. (1987). Building and understanding adaptive systems: A statistical/numerical approach to factory automation and brain research. IEEE Transactions on Systems, Man, and Cybernetics, 17.
Werbos, P. J. (1988). Generalization of backpropagation with application to a recurrent gas market model.
Neural Networks, 1.
Werbos, P. J. (1989a). Backpropagation and neurocontrol: A review and prospectus. In IEEE/INNS International Joint Conference on Neural Networks, Washington, D.C., volume 1, pages 209–216.
Werbos, P. J. (1989b). Neural networks for control and system identification. In Proceedings of IEEE/CDC
Tampa, Florida.
Werbos, P. J. (1992). Neural networks, system identification, and control in the chemical industries. In
D. A. White, D. A. S., editor, Handbook of Intelligent Control: Neural, Fuzzy, and Adaptive Approaches, pages 283–356. Thomson Learning.
Werbos, P. J. (2006). Backwards differentiation in AD and neural nets: Past links and new opportunities.
In Automatic Differentiation: Applications, Theory, and Implementations, pages 15–34. Springer.
West, A. H. L. and Saad, D. (1995). Adaptive back-propagation in on-line learning of multilayer networks.
In Touretzky, D. S., Mozer, M., and Hasselmo, M. E., editors, NIPS, pages 323–329. MIT Press.
White, H. (1989). Learning in artificial neural networks: A statistical perspective. Neural Computation, 1(4):425–464.
Whitehead, S. (1992). Reinforcement Learning for the adaptive control of perception and action. PhD thesis, University of Rochester.
Whiteson, S. (2012). Evolutionary computation for reinforcement learning. In Wiering, M. and van Otterlo, M., editors, Reinforcement Learning, pages 325–355. Springer, Berlin, Germany.
Whiteson, S., Kohl, N., Miikkulainen, R., and Stone, P. (2005). Evolving keepaway soccer players through task decomposition. Machine Learning, 59(1):5–30.
Whiteson, S. and Stone, P. (2006). Evolutionary function approximation for reinforcement learning. Journal of Machine Learning Research, 7:877–917.
Widrow, B. and Hoff, M. (1962). Associative storage and retrieval of digital information in networks of adaptive neurons. Biological Prototypes and Synthetic Systems, 1:160.
Widrow, B., Rumelhart, D. E., and Lehr, M. A. (1994). Neural networks: Applications in industry, business and science. Commun. ACM, 37(3):93–105.
Wieland, A. P. (1991). Evolving neural network controllers for unstable systems. In International Joint
Conference on Neural Networks (IJCNN), volume 2, pages 667–673. IEEE.
Wiering, M. and Schmidhuber, J. (1996). Solving POMDPs with Levin search and EIRA. In Saitta, L., editor, Machine Learning: Proceedings of the Thirteenth International Conference, pages 534–542.
Morgan Kaufmann Publishers, San Francisco, CA.
Wiering, M. and Schmidhuber, J. (1998a). HQ-learning. Adaptive Behavior, 6(2):219–246.
Wiering, M. and van Otterlo, M. (2012). Reinforcement Learning. Springer.
Wiering, M. A. and Schmidhuber, J. (1998b). Fast online Q(λ). Machine Learning, 33(1):105–116.
Wierstra, D., Foerster, A., Peters, J., and Schmidhuber, J. (2007). Solving deep memory POMDPs with recurrent policy gradients. In ICANN (1), volume 4668 of Lecture Notes in Computer Science, pages
697–706. Springer.
Wierstra, D., Foerster, A., Peters, J., and Schmidhuber, J. (2010). Recurrent policy gradients. Logic Journal of IGPL, 18(2):620–634.
Wierstra, D., Schaul, T., Peters, J., and Schmidhuber, J. (2008). Natural evolution strategies. In Congress of Evolutionary Computation (CEC 2008).
Wiesel, D. H. and Hubel, T. N. (1959). Receptive fields of single neurones in the cat's striate cortex. J.
Physiol., 148:574–591.
Wiles, J. and Elman, J. (1995).
Learning to count without a counter: A case study of dynamics and activation landscapes in recurrent networks. In In Proceedings of the Seventeenth Annual Conference of the Cognitive Science Society, pages pages 482 – 487, Cambridge, MA. MIT Press.
Wilkinson, J. H., editor (1965). The Algebraic Eigenvalue Problem. Oxford University Press, Inc., New
York, NY, USA.
Williams, R. J. (1986). Reinforcement-learning in connectionist networks: A mathematical analysis. Technical Report 8605, Institute for Cognitive Science, University of California, San Diego.
Williams, R. J. (1988). Toward a theory of reinforcement-learning connectionist systems. Technical Report
NU-CCS-88-3, College of Comp. Sci., Northeastern University, Boston, MA.
Williams, R. J. (1989). Complexity of exact gradient computation algorithms for recurrent neural networks. Technical Report Technical Report NU-CCS-89-27, Boston: Northeastern University, College of Computer Science.
Williams, R. J. (1992a). Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8:229–256.
Williams, R. J. (1992b). Training recurrent networks using the extended Kalman filter. In International
Joint Conference on Neural Networks (IJCNN), volume 4, pages 241–246. IEEE.
Williams, R. J. and Peng, J. (1990). An efficient gradient-based algorithm for on-line training of recurrent network trajectories. Neural Computation, 4:491–501.
Williams, R. J. and Zipser, D. (1988). A learning algorithm for continually running fully recurrent networks. Technical Report ICS Report 8805, Univ. of California, San Diego, La Jolla.
Williams, R. J. and Zipser, D. (1989a). Experimental analysis of the real-time recurrent learning algorithm.
Connection Science, 1(1):87–111.
Williams, R. J. and Zipser, D. (1989b). A learning algorithm for continually running fully recurrent networks. Neural Computation, 1(2):270–280.
Willshaw, D. J. and von der Malsburg, C. (1976). How patterned neural connections can be set up by self-organization. Proc. R. Soc. London B, 194:431–445.
Windisch, D. (2005). Loading deep networks is hard: The pyramidal case. Neural Computation, 17(2):487–
Wiskott, L. and Sejnowski, T. (2002). Slow feature analysis: Unsupervised learning of invariances. Neural
Computation, 14(4):715–770.
Witczak, M., Korbicz, J., Mrugalski, M., and Patton, R. J. (2006). A GMDH neural network-based approach to robust fault diagnosis: Application to the DAMADICS benchmark problem. Control Engineering Practice, 14(6):671–683.
Wolpert, D. H. (1992). Stacked generalization. Neural Networks, 5(2):241–259.
Wolpert, D. H. (1994). Bayesian backpropagation over i-o functions rather than weights. In Cowan, J. D., Tesauro, G., and Alspector, J., editors, Advances in Neural Information Processing Systems (NIPS) 6, pages 200–207. Morgan Kaufmann.
Wu, D. and Shao, L. (2014). Leveraging hierarchical parametric networks for skeletal joints based action segmentation and recognition. In Proc. Conference on Computer Vision and Pattern Recognition(CVPR).
Wu, L. and Baldi, P. (2008). Learning to play Go using recursive neural networks. Neural Networks, 21(9):1392–1400.
Wyatte, D., Curran, T., and O'Reilly, R. (2012). The limits of feedforward vision: Recurrent processing promotes robust object recognition when objects are degraded.
Journal of Cognitive Neuroscience, 24(11):2248–2261.
Yamauchi, B. M. and Beer, R. D. (1994). Sequential behavior and learning in evolved dynamical neural networks. Adaptive Behavior, 2(3):219–246.
Yamins, D., Hong, H., Cadieu, C., and DiCarlo, J. J. (2013). Hierarchical modular optimization of convolutional networks achieves representations similar to macaque IT and human ventral stream. Advances in Neural Information Processing Systems (NIPS), pages 1–9.
Yang, M., Ji, S., Xu, W., Wang, J., Lv, F., Yu, K., Gong, Y., Dikmen, M., Lin, D. J., and Huang, T. S.(2009). Detecting human actions in surveillance videos. In TREC Video Retrieval Evaluation Workshop.
Yao, X. (1993). A review of evolutionary artificial neural networks. International Journal of Intelligent
Systems, 4:203–222.
Yin, F., Wang, Q.-F., Zhang, X.-Y., and Liu, C.-L. (2013). ICDAR 2013 Chinese handwriting recognition competition. In 12th International Conference on Document Analysis and Recognition (ICDAR), pages
1464–1470.
Young, S., Davis, A., Mishtal, A., and Arel, I. (2014). Hierarchical spatiotemporal feature extraction using recurrent online clustering. Pattern Recognition Letters, 37:115–123.
Yu, X.-H., Chen, G.-A., and Cheng, S.-X. (1995). Dynamic learning rate optimization of the backpropagation algorithm. IEEE Transactions on Neural Networks, 6(3):669–677.
Zamora-Martnez, F., Frinken, V., Espaa-Boquera, S., Castro-Bleda, M., Fischer, A., and Bunke, H. (2014).
Neural network language models for off-line handwriting recognition. Pattern Recognition, 47(4):1642–
Zeiler, M. D. (2012). ADADELTA: An Adaptive Learning Rate Method. CoRR, abs/1212.5701.
Zeiler, M. D. and Fergus, R. (2013). Visualizing and understanding convolutional networks. Technical
Report arXiv:1311.2901 [cs.CV], NYU.
Zemel, R. S. (1993). A minimum description length framework for unsupervised learning. PhD thesis, University of Toronto.
Zemel, R. S. and Hinton, G. E. (1994). Developing population codes by minimizing description length.
In Cowan, J. D., Tesauro, G., and Alspector, J., editors, Advances in Neural Information Processing
Systems 6, pages 11–18. Morgan Kaufmann.
Zeng, Z., Goodman, R., and Smyth, P. (1994). Discrete recurrent neural networks for grammatical inference. IEEE Transactions on Neural Networks, 5(2).
Zimmermann, H.-G., Tietz, C., and Grothmann, R. (2012). Forecasting with recurrent neural networks: 12 tricks. In Montavon, G., Orr, G. B., and M¨uller, K.-R., editors, Neural Networks: Tricks of the Trade(2nd ed.), volume 7700 of Lecture Notes in Computer Science, pages 687–707. Springer.
Zipser, D., Kehoe, B., Littlewort, G., and Fuster, J. (1993). A spiking network model of short-term active memory. The Journal of Neuroscience, 13(8):3406–3420.Li Deng and Dong Yu
Microsoft Research
One Microsoft Way
Redmond, WA 98052
NOW PUBLISHERS, 2014
DEEP LEARNING:
METHODS AND APPLICATIONS
Table of Contents
Chapter 1 Introduction.................................................................................................................... 5
Definitions and Background............................................................................................. 5
Organization of This Book............................................................................................... 8
Chapter 2 Some Historical Context of Deep Learning................................................................ 11
Chapter 3 Three Classes of Deep Learning Networks................................................................. 18
A Three-Way Categorization......................................................................................... 18
Deep Networks for Unsupervised or Generative Learning............................................ 21
Deep Networks for Supervised Learning....................................................................... 24
Hybrid Deep Networks................................................................................................... 26
Chapter 4 Deep Autoencoders --- Unsupervised Learning........................................................... 29
Introduction.................................................................................................................... 29
Use of Deep Autoencoders to Extract Speech Features................................................. 30
Stacked Denoising Autoencoders................................................................................... 35
Transforming Autoencoders........................................................................................... 35
Chapter 5 Pre-Trained Deep Neural Networks --- A Hybrid...................................................... 37
Restricted Boltzmann Machines..................................................................................... 37
Unsupervised Layer-wise Pretraining............................................................................ 40
Interfacing DNNs with HMMs...................................................................................... 42
Chapter 6 Deep Stacking Networks and Variants --- Supervised Learning................................ 44
Introduction.................................................................................................................... 44
A Basic Architecture of the Deep Stacking Network.................................................... 45
A Method for Learning the DSN Weights..................................................................... 46
The Tensor Deep Stacking Network.............................................................................. 48
The Kernelized Deep Stacking Network........................................................................ 50
Chapter 7 Selected Applications in Speech and Audio Processing............................................. 53
7.1 Acoustic Modeling for Speech Recognition................................................................... 53
7.1.1 Back to primitive spectral features of speech................................................................. 54
7.1.2 The DNN-HMM architecture vs. use of DNN-derived features.................................... 56
7.1.3 Noise robustness by deep learning................................................................................. 59
7.1.4 Output representations in the DNN................................................................................ 60
7.1.5 Adaptation of the DNN-based speech recognizers........................................................ 62
7.1.6 Better architectures and nonlinear units......................................................................... 63
7.1.7 Better optimization and regularization …………………………………………………67
Speech Synthesis............................................................................................................ 70
Audio and Music Processing.......................................................................................... 71
Chapter 8 Selected Applications in Language Modeling and Natural Language Processing...... 73
Language Modeling........................................................................................................ 73
Natural Language Processing......................................................................................... 77
Chapter 9 Selected Applications in Information Retrieval.......................................................... 84
A Brief Introduction to Information Retrieval............................................................... 84
Semantic Hashing with Deep Autoencoders for Document Indexing and Retrieval..... 85
Deep-Structured Semantic Modeling for Document Retrieval...................................... 86
Use of Deep Stacking Networks for Information Retrieval........................................... 91
Chapter 10 Selected Applications in Object Recognition and Computer Vision........................ 92
10.1 Unsupervised or Generative Feature Learning............................................................... 92
10.2 Supervised Feature Learning and Classification............................................................ 94
Chapter 11 Selected Applications in Multi-modal and Multi-task Learning............................. 101
11.1 Multi-Modalities: Text and Image............................................................................... 101
11.2 Multi-Modalities: Speech and Image........................................................................... 104
11.3 Multi-Task Learning within the Speech, NLP or Image Domain................................ 106
Chapter 12 Epilogues................................................................................................................. 110
BIBLIOGRAPHY....................................................................................................................... 114
Abstract
This book is aimed to provide an overview of general deep learning methodology and its applications to a variety of signal and information processing tasks. The application areas are chosen with the following three criteria: 1) expertise or knowledge of the authors; 2) the application areas that have already been transformed by the successful use of deep learning technology, such as speech recognition and computer vision; and 3) the application areas that have the potential to be impacted significantly by deep learning and that have gained concentrated research efforts, including natural language and text processing, information retrieval, and multimodal information processing empowered by multi-task deep learning.
In Chapter 1, we provide the background of deep learning, as intrinsically connected to the use of multiple layers of nonlinear transformations to derive features from the sensory signals such as speech and visual images. In the most recent literature, deep learning is embodied also as representation learning, which involves a hierarchy of features or concepts where higher-level representations of them are defined from lower-level ones and where the same lower-level representations help to define higher-level ones. In Chapter 2, a brief historical account of deep learning is presented. In particular, selected chronological development of speech recognition is used to illustrate the recent impact of deep learning that has become a dominant technology in speech recognition industry within only a few years since the start of a collaboration between academic and industrial researchers in applying deep learning to speech recognition. In Chapter 3, a three-way classification scheme for a large body of work in deep learning is developed. We classify a growing number of deep learning techniques into unsupervised, supervised, and hybrid categories, and present qualitative descriptions and a literature survey for each category. From
Chapter 4 to Chapter 6, we discuss in detail three popular deep networks and related learning methods, one in each category. Chapter 4 is devoted to deep autoencoders as a prominent example of the unsupervised deep learning techniques. Chapter 5 gives a major example in the hybrid deep network category, which is the discriminative feed-forward neural network for supervised learning with many layers initialized using layer-by-layer generative, unsupervised pre-training. In Chapter
6, deep stacking networks and several of the variants are discussed in detail, which exemplify the discriminative or supervised deep learning techniques in the three-way categorization scheme.
In Chapters 7-11, we select a set of typical and successful applications of deep learning in diverse areas of signal and information processing and of applied artificial intelligence. In Chapter 7, we review the applications of deep learning to speech and audio processing, with emphasis on speech recognition organized according to several prominent themes. In Chapters 8, we present recent results of applying deep learning to language modeling and natural language processing. Chapter
9 is devoted to selected applications of deep learning to information retrieval including Web search.
In Chapter 10, we cover selected applications of deep learning to image object recognition in computer vision. Selected applications of deep learning to multi-modal processing and multi-task learning are reviewed in Chapter 11. Finally, an epilogue is given in Chapter 12 to summarize what we presented in earlier chapters and to discuss future challenges and directions.
CHAPTER 1
INTRODUCTION
1.1 Definitions and Background
Since 2006, deep structured learning, or more commonly called deep learning or hierarchical learning, has emerged as a new area of machine learning research (Hinton et al., 2006; Bengio, 2009). During the past several years, the techniques developed from deep learning research have already been impacting a wide range of signal and information processing work within the traditional and the new, widened scopes including key aspects of machine learning and artificial intelligence; see overview articles in (Bengio, 2009; Arel et al., 2010; Yu and Deng, 2011; Deng, 2011, 2013; Hinton et al., 2012; Bengio et al., 2013a), and also the media coverage of this progress in (Markoff, 2012; Anthes, 2013). A series of workshops, tutorials, and special issues or conference special sessions in recent years have been devoted exclusively to deep learning and its applications to various signal and information processing areas. These include:
 2008 NIPS Deep Learning Workshop;
 2009 NIPS Workshop on Deep Learning for Speech Recognition and Related Applications;
 2009 ICML Workshop on Learning Feature Hierarchies;
 2011 ICML Workshop on Learning Architectures, Representations, and Optimization for
Speech and Visual Information Processing;
 2012 ICASSP Tutorial on Deep Learning for Signal and Information Processing;
 2012 ICML Workshop on Representation Learning;
 2012 Special Section on Deep Learning for Speech and Language Processing in IEEE
Transactions on Audio, Speech, and Language Processing (T-ASLP, January);
 2010, 2011, and 2012 NIPS Workshops on Deep Learning and Unsupervised Feature
Learning;
 2013 NIPS Workshops on Deep Learning and on Output Representation Learning;
 2013 Special Issue on Learning Deep Architectures in IEEE Transactions on Pattern
Analysis and Machine Intelligence (T-PAMI, September).
 2013 International Conference on Learning Representations;
 2013 ICML Workshop on Representation Learning Challenges;
 2013 ICML Workshop on Deep Learning for Audio, Speech, and Language Processing;
 2013 ICASSP Special Session on New Types of Deep Neural Network Learning for Speech
Recognition and Related Applications.
The authors have been actively involved in deep learning research and in organizing or providing several of the above events, tutorials, and editorials. In particular, they gave tutorials and invited lectures on this topic at various places. Part of this book is based on their tutorials and lecture material.
Before embarking on describing details of deep learning, let's provide necessary definitions. Deep learning has various closely related definitions or high-level descriptions:

Definition 1: A class of machine learning techniques that exploit many layers of non-linear information processing for supervised or unsupervised feature extraction and transformation, and for pattern analysis and classification.

Definition 2: "A sub-field within machine learning that is based on algorithms for learning multiple levels of representation in order to model complex relationships among data.
Higher-level features and concepts are thus defined in terms of lower-level ones, and such a hierarchy of features is called a deep architecture. Most of these models are based on unsupervised learning of representations." (Wikipedia on "Deep Learning" around March

Definition 3: "A sub-field of machine learning that is based on learning several levels of representations, corresponding to a hierarchy of features or factors or concepts, where higherlevel concepts are defined from lower-level ones, and the same lower-level concepts can help to define many higher-level concepts. Deep learning is part of a broader family of machine learning methods based on learning representations. An observation (e.g., an image) can be represented in many ways (e.g., a vector of pixels), but some representations make it easier to learn tasks of interest (e.g., is this the image of a human face?) from examples, and research in this area attempts to define what makes better representations and how to learn them."(Wikipedia on "Deep Learning" around February 2013.)

Definition 4: "Deep learning is a set of algorithms in machine learning that attempt to learn in multiple levels, corresponding to different levels of abstraction. It typically uses artificialneural networks. The levels in these learned statistical models correspond to distinct levels of concepts, where higher-level concepts are defined from lower-level ones, and the same lower-level concepts can help to define many higher-level concepts." See Wikipedia http://en.wikipedia.org/wiki/Deep_learning on "Deep Learning" as of this most recent update in October 2013.

Definition 5: "Deep Learning is a new area of Machine Learning research, which has been introduced with the objective of moving Machine Learning closer to one of its original goals:
Artificial Intelligence. Deep Learning is about learning multiple levels of representation and abstraction that help to make sense of data such as images, sound, and text." See https://github.com/lisa-lab/DeepLearningTutorials
Note that the deep learning that we discuss in this book is about learning in deep architectures for signal and information processing. It is not about deep understanding of the signal or information, although in many cases they may be related. It should also be distinguished from the overloaded term in educational psychology: "Deep learning describes an approach to learning that is characterized by active engagement, intrinsic motivation, and a personal search for meaning." http://www.blackwellreference.com/public/tocnode?id=g9781405161251_chunk_g97814051612
516_ss1-1
Common among the various high-level descriptions of deep learning above are two key aspects:
1) models consisting of multiple layers or stages of nonlinear information processing; and 2) methods for supervised or unsupervised learning of feature representation at successively higher, more abstract layers. Deep learning is in the intersections among the research areas of neural networks, artificial intelligence, graphical modeling, optimization, pattern recognition, and signal processing. Three important reasons for the popularity of deep learning today are the drastically increased chip processing abilities (e.g., general-purpose graphical processing units or GPGPUs), the significantly lowered cost of computing hardware, and the recent advances in machine learning and signal/information processing research. These advances have enabled the deep learning methods to effectively exploit complex, compositional nonlinear functions, to learn distributed and hierarchical feature representations, and to make effective use of both labeled and unlabeled data.
Active researchers in this area include those at University of Toronto, New York University, University of Montreal, Stanford University, Microsoft Research (since 2009), Google (since about 2011), IBM Research (since about 2011), Baidu (since 2012), Facebook (since 2013), UCBerkeley, UC-Irvine, IDIAP, IDSIA, University College London, University of Michigan, Massachusetts Institute of Technology, University of Washington, and numerous other places; see http://deeplearning.net/deep-learning-research-groups-and-labs/ for a more detailed list. These researchers have demonstrated empirical successes of deep learning in diverse applications of computer vision, phonetic recognition, voice search, conversational speech recognition, speech and image feature coding, semantic utterance classification, natural language understanding, handwriting recognition, audio processing, information retrieval, robotics, and even in the analysis of molecules that may lead to discovery of new drugs as reported recently by Markoff (2012).
In addition to the reference list provided at the end of this book, which may be outdated not long after the publication of this book, there are a number of excellent and frequently updated reading lists, tutorials, software, and video lectures online at:
 http://deeplearning.net/reading-list/
 http://ufldl.stanford.edu/wiki/index.php/UFLDL_Recommended_Readings
 http://www.cs.toronto.edu/~hinton/
 http://deeplearning.net/tutorial/
 http://ufldl.stanford.edu/wiki/index.php/UFLDL_Tutorial
1.2 Organization of This Book
The rest of the book is organized as follows:
In Chapter 2, we provide a brief historical account of deep learning, mainly from the perspective of how speech recognition technology has been hugely impacted by deep learning, and how the revolution got started and has gained and sustained immense momentum.
In Chapter 3, a three-way categorization scheme for a majority of the work in deep learning is developed. They include unsupervised, supervised, and hybrid deep learning networks, where in the latter category unsupervised learning (or pre-training) is exploited to assist the subsequent stage of supervised learning when the final tasks pertain to classification. The supervised and hybrid deep networks often have the same type of architectures or the structures in the deep networks, but the unsupervised deep networks tend to have different architectures from the others.
Chapters 4-6 are devoted, respectively, to three popular types of deep architectures, one from each of the classes in the three-way categorization scheme reviewed in Chapter 3. In Chapter 4, we discuss in detail deep autoencoders as a prominent example of the unsupervised deep learning networks. No class labels are used in the learning, although supervised learning methods such as back-propagation are cleverly exploited when the input signal itself, instead of any label information of interest to possible classification tasks, is treated as the "supervised" signal.
In Chapter 5, as a major example in the hybrid deep network category, we present in detail the deep neural networks with unsupervised and largely generative pre-training to boost the effectiveness of supervised training. This benefit is found critical when the training data are limited and no other appropriate regularization ways (i.e., dropout) are exploited. The particular pretraining method based on restricted Boltzmann machines and the related deep belief networks described in this chapter has been historically significant as it ignited the intense interest in the early applications of deep learning to speech recognition and other information processing tasks.
In addition to this retrospective review, subsequent development and different paths from the more recent perspective are discussed.
In Chapter 6, the basic deep stacking networks and their several extensions are discussed in detail, which exemplify the discriminative, supervised deep learning networks in the three-way classification scheme. This group of deep networks operate in many ways that are distinct from the deep neural networks. Most notably, they use target labels in constructing each of many layers or modules in the overall deep networks. Assumptions made about part of the networks, such as linear output units in each of the modules, simplify the learning algorithms and enable a much wider variety of network architectures to be constructed and learned than the networks discussed in Chapters 4 and 5.
In Chapters 7-11, we select a set of typical and successful applications of deep learning in diverse areas of signal and information processing. In Chapter 7, we review the applications of deep learning to speech recognition, speech synthesis, and audio processing. Subsections surrounding the main subject of speech recognition are created based on several prominent themes on the topic in the literature.
In Chapters 8, we present recent results of applying deep learning to language modeling and natural language processing, where we highlight the key recent development in embedding symbolic entities such as words into low-dimensional, continuous-valued vectors.
Chapter 9 is devoted to selected applications of deep learning to information retrieval including web search.
In Chapter 10, we cover selected applications of deep learning to image object recognition in computer vision. The chapter is divided to two main classes of deep learning approaches: 1) unsupervised feature learning, and 2) supervised learning for end-to-end and joint feature learning and classification.
Selected applications to multi-modal processing and multi-task learning are reviewed in Chapter
11, divided into three categories according to the nature of the multi-modal data as inputs to the deep learning systems. For single-modality data of speech, text, or image, a number of recent multi-task learning studies based on deep learning methods are reviewed in the literature.
Finally, an epilogue is given in Chapter 12 to summarize the book and to discuss future challenges and directions.
This short monograph contains the material expanded from two tutorials that the authors gave, one at APSIPA in October 2011 and the other at ICASSP in March 2012. Substantial updates have been made based on the literature up to January 2014 (including the materials presented at NIPS2013 and at IEEE-ASRU-2013 both held in December of 2013), focusing on practical aspects in the fast development of deep learning research and technology during the interim years.
CHAPTER 2
SOME HISTORICAL CONTEXT OF DEEP
LEARNING
Until recently, most machine learning and signal processing techniques had exploited shallowstructured architectures. These architectures typically contain at most one or two layers of nonlinear feature transformations. Examples of the shallow architectures are Gaussian mixture models (GMMs), linear or nonlinear dynamical systems, conditional random fields (CRFs), maximum entropy (MaxEnt) models, support vector machines (SVMs), logistic regression, kernel regression, multi-layer perceptrons (MLPs) with a single hidden layer including extreme learning machines (ELMs). For instance, SVMs use a shallow linear pattern separation model with one or zero feature transformation layer when the kernel trick is used or otherwise. (Notable exceptions are the recent kernel methods that have been inspired by and integrated with deep learning; e.g.
Cho and Saul, 2009; Deng et al., 2012; Vinyals et al., 2012; Aslan et al., 2013). Shallow architectures have been shown effective in solving many simple or well-constrained problems, but their limited modeling and representational power can cause difficulties when dealing with more complicated real-world applications involving natural signals such as human speech, natural sound and language, and natural image and visual scenes.
Human information processing mechanisms (e.g., vision and audition), however, suggest the need of deep architectures for extracting complex structure and building internal representation from rich sensory inputs. For example, human speech production and perception systems are both equipped with clearly layered hierarchical structures in transforming the information from the waveform level to the linguistic level (Baker et al., 2009, 2009a; Deng, 1999, 2003). In a similar vein, the human visual system is also hierarchical in nature, mostly in the perception side but interestingly also in the "generation" side (George, 2008; Bouvrie, 2009; Poggio, 2007). It is natural to believe that the state-of-the-art can be advanced in processing these types of natural signals if efficient and effective deep learning algorithms can be developed.
Historically, the concept of deep learning originated from artificial neural network research.(Hence, one may occasionally hear the discussion of "new-generation neural networks".) Feedforward neural networks or MLPs with many hidden layers, which are often referred to as deep neural networks (DNNs), are good examples of the models with a deep architecture. Backpropagation (BP), popularized in 1980's, has been a well-known algorithm for learning the parameters of these networks. Unfortunately back-propagation alone did not work well in practice then for learning networks with more than a small number of hidden layers (see a review and analysis in (Bengio, 2009; Glorot and Bengio, 2010). The pervasive presence of local optima and other optimization challenges in the non-convex objective function of the deep networks are the main source of difficulties in the learning. Back-propagation is based on local gradient information, and starts usually at some random initial points. It often gets trapped in poor local optima when the batch-mode or even stochastic gradient descent BP algorithm is used. The severity increasessignificantly as the depth of the networks increases. This difficulty is partially responsible for steering away most of the machine learning and signal processing research from neural networks to shallow models that have convex loss functions (e.g., SVMs, CRFs, and MaxEnt models), for which the global optimum can be efficiently obtained at the cost of reduced modeling power, although there had been continuing work on neural networks with limited scale and impact (e.g., Hochreiter and Schmidhuber, 1997; LeCun et al., 1998; Bourlard and Morgan, 1993; Deng et al., 1994s; Bridle et al., 1998; Robinson, 1994; Morgan, et al., 2005).
The optimization difficulty associated with the deep models was empirically alleviated when a reasonably efficient, unsupervised learning algorithm was introduced in the two seminar papers(Hinton et al., 2006; Hinton and Salakhutdinov, 2006). In these papers, a class of deep generative models, called deep belief network (DBN), was introduced. A DBN is composed of a stack of restricted Boltzmann machines (RBMs). A core component of the DBN is a greedy, layer-by-layer learning algorithm which optimizes DBN weights at time complexity linear to the size and depth of the networks. Separately and with some surprise, initializing the weights of an MLP with a correspondingly configured DBN often produces much better results than that with the random weights. As such, MLPs with many hidden layers, or deep neural networks (DNN), which are learned with unsupervised DBN pre-training followed by back-propagation fine-tuning is sometimes also called DBNs in the literature (e.g., Dahl et al., 2011; Mohamed et al., 2010, 2012).
More recently, researchers have been more careful in distinguishing DNNs from DBNs (Dahl et al., 2012; Hinton et al., 2012), and when DBN is used to initialize the training of a DNN, the resulting network is sometimes called the DBN-DNN (Hinton et al., 2012).
Independently of the RBM development, in 2006 two alternative, non-probabilistic, nongenerative, unsupervised deep models were published. One is an autoencoder variant with greedy layer-wise training much like the DBN training (Bengio et al., 2006). Another is an energy-based model with unsupervised learning of sparse over-complete representations (Ranzato et al., 2006).
They both can be effectively used to pre-train a deep neural network, much like the DBN.
In addition to the supply of good initialization points, the DBN comes with additional attractive properties. First, the learning algorithm makes effective use of unlabeled data. Second, it can be interpreted as Bayesian probabilistic generative model. Third, the over-fitting problem, which is often observed in the models with millions of parameters such as DBNs, and the under-fitting problem, which occurs often in deep networks, can be effectively addressed by the generative pretraining step. An insightful analysis on what speech information DBNs can capture is provided in(Mohamed et al. 2012a).
Using hidden layers with many neurons in a DNN significantly improves the modeling power of the DNN and creates many closely optimal configurations. Even if parameter learning is trapped into a local optimum, the resulting DNN can still perform quite well since the chance of having a poor local optimum is lower than when a small number of neurons are used in the network. Using deep and wide neural networks, however, would cast great demand to the computational power during the training process and this is one of the reasons why it is not until recent years that researchers have started exploring both deep and wide neural networks in a serious manner.
Better learning algorithms and different nonlinearities also contributed to the success of DNNs.
Stochastic gradient descend (SGD) algorithms are the most efficient algorithm when the training set is large and redundant as is the case for most applications (Bottou and LeCun, 2004). Recently, SGD is shown to be effective for parallelizing over many machines with an asynchronous mode(Dean et al., 2012) or over multiple GPUs through pipelined BP (Chen et al., 2012). Further, SGD can often allow the training to jump out of local optima due to the noisy gradients estimated from a single or a small batch of samples. Other learning algorithms such as Hessian free (Martens 2010, Kingsbury et al., 2012) or Krylov subspace methods (Vinyals and Povey, 2011) have shown a similar ability.
For the highly non-convex optimization problem of DNN learning, it is obvious that better parameter initialization techniques will lead to better models since optimization starts from these initial models. What was not obvious, however, is how to efficiently and effectively initialize DNN parameters and how the use of very large amounts of training data can alleviate the learning problem until more recently (Hinton et al. 2006; Hinton and Salakhutdinov, 2006; Bengio, 2009;
Vincent et al., 2010; Deng et al., 2010; Yu et al., 2010c; Dahl et al., 2010, 2012; Seide et al. 2011;
Hinton et al., 2012). The DNN parameter initialization technique that attracted the most attention is the unsupervised pretraining technique proposed in (Hinton et al. 2006; Hinton and Salakhutdinov, 2006) discussed earlier.
The DBN pretraining procedure is not the only one that allows effective initialization of DNNs.
An alternative unsupervised approach that performs equally well is to pretrain DNNs layer by layer by considering each pair of layers as a de-noising autoencoder regularized by setting a random subset of the input nodes to zero (Bengio, 2009; Vincent et al., 2010). Another alternative is to use contractive autoencoders for the same purpose by favoring representations that are more robust to the input variations, i.e., penalizing the gradient of the activities of the hidden units with respect to the inputs (Rifai et al., 2011). Further, Ranzato et al. (2007) developed the Sparse Encoding
Symmetric Machine (SESM), which has a very similar architecture to RBMs as building blocks of a DBN. The SESM may also be used to effectively initialize the DNN training. In addition to unsupervised pretraining using greedy layer-wise procedures (Hinton and Salakhutdinov, 2006;
Bengio et al., 2006; Ranzato et al., 2007), the supervised pretraining, or sometimes called discriminative pretraining, has also been shown to be effective (Seide et al., 2011; Yu et al., 2011;
Hinton et al., 2012) and in cases where labeled training data are abundant performs better than the unsupervised pretraining techniques. The idea of the discriminative pretraining is to start from a one-hidden-layer MLP trained with the BP algorithm. Every time when we want to add a new hidden layer we replace the output layer with a randomly initialized new hidden and output layer and train the whole new MLP (or DNN) using the BP algorithm. Different from the unsupervised pretraining techniques, the discriminative pretraining technique requires labels.
Researchers who apply deep learning to speech and vision analyzed what DNNs capture in speech and images. For example, Mohamed et al. (2012a) applied a dimensionality reduction method to visualize the relationship among the feature vectors learned by the DNN. They found that the DNN's hidden activity vectors preserve the similarity structure of the feature vectors at multiple scales, and that this is especially true for the filterbank features. A more elaborated visualization method, based on a top-down generative process in the reverse direction of the classification network, was recently developed by Zeiler and Fergus (2013) for examining what features the deepconvolutional networks capture from the image data. The power of the deep networks is shown to be their ability to extract appropriate features and do discrimination jointly (LeCun, 2012).
As another way to concisely introduce the DNN, we can review the history of artificial neural networks using a "Hype Cycle", which is a graphic representation of the maturity, adoption and social application of specific technologies. The 2012 version of the Hype Cycles graph compiled by Gartner is shown in Figure 2.1. It intends to show how a technology or application will evolve over time (according to five phases: technology trigger, peak of inflated expectations, trough of disillusionment, slope of enlightenment, and plateau of production), and to provide a source of insight to manage its deployment.
Figure 2.1. Gartner Hyper Cycle graph representing five phases of a technology(http://en.wikipedia.org/wiki/Hype_cycle)
Applying the Gartner Hyper Cycle to the artificial neural network development, we created Figure
2.2 to align different generations of the neural network with the various phases designated in the Hype Cycle. The peak activities ("expectations" or "media hype" on the vertical axis) occurred in late 1980's and early 1990's, corresponding to the height of what is often referred to as the "second generation" of neural networks. The deep belief network (DBN) and a fast algorithm for training it were invented in 2006 (Hinton and Salakhudinov, 2006; Hinton et al., 2006). When the DBN was used to initialize the DNN, the learning became highly effective and this has inspired the subsequent fast growing research ("enlightenment" phase shown in Figure 2.2). Applications of the DBN and DNN to industry-scale speech feature extraction and speech recognition started in 2009 when leading academic and industrial researchers with both deep learning and speech expertise collaborated; see reviews in (Hinton et al., 2012; Deng et al., 2013b). This collaboration fast expanded the work of speech recognition using deep learning methods to increasingly larger successes (Yu et al., 2010c; Seide et al., 2011; Hinton et al., 2012; Deng et al., 2013a), many of which will be covered in the remainder of this book. The height of the "plateau of productivity" phase, not yet reached in our opinion, is expected to be higher than in the stereotypical curve(circled with a question mark in Figure 2.2), and is marked by the dashed line that moves straight up.
Figure 2.2: Applying Gartner Hyper Cycle graph to analyzing the history of artificial neural network technology (We thank our colleague John Platt during 2012 for bringing this type of "Hyper Cycle" graph to our attention for concisely analyzing the neural network history).
We show in Figure 2.3 the history of speech recognition, which has been compiled by NIST, organized by plotting the word error rate (WER) as a function of time for a number of increasingly difficult speech recognition tasks. Note all WER results were obtained using the GMM-HMM technology. When one particularly difficult task (Switchboard) is extracted from Figure 2.3, we see a flat curve over many years using the GMM-HMM technology but after the DNN technology is used the WER drops sharply (marked by the red star in Figure 2.4).
Figure 2.3: The famous NIST plot showing the historical speech recognition error rates achieved by the GMM-HMM approach for a number of increasingly difficult speech recognition tasks. Data source: http://itl.nist.gov/iad/mig/publications/ASRhistory/index.html
Figure 2.4. Extracting WERs of one task from Figure 2.3 and adding the significantly lower WER(marked by the star) achieved by the DNN technology approach.
In the next Chapter, an overview is provided on the various architectures of deep learning, followed by more detailed expositions of a few widely studied architectures and methods and by selected applications in signal and information processing including speech and audio, natural language, information retrieval, vision, and multi-modal processing.
CHAPTER 3
THREE CLASSES OF DEEP LEARNING
NETWORKS
3.1 A Three-Way Categorization
As described earlier, deep learning refers to a rather wide class of machine learning techniques and architectures, with the hallmark of using many layers of non-linear information processing that are hierarchical in nature. Depending on how the architectures and techniques are intended for use, e.g., synthesis/generation or recognition/classification, one can broadly categorize most of the work in this area into three major classes:
1) Deep networks for unsupervised or generative learning, which are intended to capture high-order correlation of the observed or visible data for pattern analysis or synthesis purposes when no information about target class labels is available. Unsupervised feature or representation learning in the literature refers to this category of the deep networks.
When used in the generative mode, may also be intended to characterize joint statistical distributions of the visible data and their associated classes when available and being treated as part of the visible data. In the latter case, the use of Bayes rule can turn this type of generative networks into a discriminative one for learning.
2) Deep networks for supervised learning, which are intended to directly provide discriminative power for pattern classification purposes, often by characterizing the posterior distributions of classes conditioned on the visible data. Target label data are always available in direct or indirect forms for such supervised learning. They are also called discriminative deep networks.
3) Hybrid deep networks, where the goal is discrimination which is assisted, often in a significant way, with the outcomes of generative or unsupervised deep networks. This can be accomplished by better optimization or/and regularization of the deep networks in category 2). The goal can also be accomplished when discriminative criteria for supervised learning are used to estimate the parameters in any of the deep generative or unsupervised deep networks in category 1) above.
Note the use of "hybrid" in 3) above is different from that used sometimes in the literature, which refers to the hybrid systems for speech recognition feeding the output probabilities of a neural network into an HMM (Bengio, 1991; Bengio et al., 1992; Bourlard and Morgan, 1993; Morgan, By the commonly adopted machine learning tradition (e.g., Chapter 28 in Murphy, 2012; Deng and Li, 2013), it may be natural to just classify deep learning techniques into deep discriminative models (e.g., deep neural networks or DNNs, recurrent neural networks or RNNs, convolutional neural networks or CNNs, etc.) and generative/unsupervised models (e.g., restricted Boltzmann machine or RBMs, deep belief networks or DBNs, deep Boltzmann machines (DBMs), regularized autoencoders, etc.). This two-way classification scheme, however, misses a key insight gained in deep learning research about how generative or unsupervised-learning models can greatly improve the training of DNNs and other deep discriminative or supervised-learning models via better regularization or optimization. Also, deep networks for unsupervised learning may not necessarily need to be probabilistic or be able to meaningfully sample from the model (e.g., traditional autoencoders, sparse coding networks, etc.). We note here that more recent studies have generalized the traditional denoising autoencoders so that they can be efficiently sampled from and thus have become generative models (Alain and Bengio, 2013; Bengio et al., 2013, 2013b).
Nevertheless, the traditional two-way classification indeed points to several key differences between deep networks for unsupervised and supervised learning. Compared between the two, deep supervised-learning models such as DNNs are usually more efficient to train and test, more flexible to construct, and more suitable for end-to-end learning of complex systems (e.g., no approximate inference and learning such as loopy belief propagation). On the other hand, the deep unsupervised-learning models, especially the probabilistic generative ones, are easier to interpret, easier to embed domain knowledge, easier to compose, and easier to handle uncertainty, but they are typically intractable in inference and learning for complex systems. These distinctions are retained also in the proposed three-way classification which is hence adopted throughout this book.
Below we review representative work in each of the above three categories, where several basic definitions are summarized in Table 3.1. Applications of these deep architectures, with varied ways of learning including supervised, unsupervised, or hybrid, are deferred to Chapters 7-11.
TABLE 3.1. BASIC DEEP LEARNING TERMINOLOGIES
Deep Learning: a class of machine learning techniques, where many layers of information processing stages in hierarchical architectures are exploited for unsupervised feature learning and for pattern analysis/classification. The essence of deep learning is to compute hierarchical features or representations of the observational data, where the higher-level features or factors are defined from lower-level ones. The family of deep learning methods have been growing increasingly richer, encompassing those of neural networks, hierarchical probabilistic models, and a variety of unsupervised and supervised feature learning algorithms.
Deep belief network (DBN): probabilistic generative models composed of multiple layers of stochastic, hidden variables. The top two layers have undirected, symmetric connections between them. The lower layers receive top-down, directed connections from the layer above.
Boltzmann machine (BM): a network of symmetrically connected, neuron-like units that make stochastic decisions about whether to be on or off.
Restricted Boltzmann machine (RBM): a special type of BM consisting of a layer of visible units and a layer of hidden units with no visible-visible or hidden-hidden connections.
Deep neural network (DNN): a multilayer perceptron with many hidden layers, whose weights are fully connected and are often initialized using either an unsupervised or a supervised pretraining technique. (In the literature prior to 2012, a DBN was often used incorrectly to mean a DNN.)
Deep autoencoder: a "discriminative" DNN whose output targets are the data input itself rather than class labels; hence an unsupervised learning model. When trained with a denoising criterion, a deep autoencoder is also a generative model and can be sampled from.
Distributed representation: an internal representation of the observed data in such a way that they are modeled as being explained by the interactions of many hidden factors. A particular factor learned from configurations of other factors can often generalize well to new configurations. Distributed representations naturally occur in a "connectionist" neural network, where a concept is represented by a pattern of activity across a number of many units and where at the same time a unit typically contributes to many concepts. One key advantage of such manyto-many correspondence is that they provide robustness in representing the internal structure of the data in terms of graceful degradation and damage resistance. Another key advantage is that they facilitate generalizations of concepts and relations, thus enabling reasoning abilities.
3.2 Deep Networks for Unsupervised or Generative
Learning
Unsupervised learning refers to no use of task specific supervision information (e.g., target class labels) in the learning process. Many deep networks in this category can be used to meaningfully generate samples by sampling from the networks, with examples of RBMs, DBNs, DBMs, and generalized denoising autoencoders (Bengio et al., 2013), and are thus generative models. Some networks in this category, however, cannot be easily sampled, with examples of sparse coding networks and the original forms of deep autoencoders, and are thus not generative in nature.
Among the various subclasses of generative or unsupervised deep networks, the energy-based deep models are the most common (e.g., Bengio at al., 2006; LeCun et al., 2007; Ngiam et al., 2011;
Bengio, 2009). The original form of the deep auto encoder (Hinton and Salakhutdinov, 2006;
Bengio at al., 2006; Deng et al., 2010), which we will give more detail about in Chapter 4, is a typical example of this unsupervised model category. Most other forms of deep autoencoders are also unsupervised in nature, but with quite different properties and implementations. Examples are transforming autoencoders (Hinton et al., 2011), predictive sparse coders and their stacked version, and de-noising autoencoders and their stacked versions (Vincent et al., 2010).
Specifically, in de-noising autoencoders, the input vectors are first corrupted by, for example, randomly selecting a percentage of the inputs and setting them to zeros or adding Gaussian noise to them. Then the parameters are adjusted for the hidden encoding nodes to reconstruct the original, uncorrupted input data using criteria such as mean square reconstruction error and KL divergence between the original inputs and the reconstructed inputs. The encoded representations transformed from the uncorrupted data are used as the inputs to the next level of the stacked de-noising autoencoder.
Another prominent type of deep unsupervised models with generative capability is the deep
Boltzmann machine or DBM (Salakhutdinov and Hinton, 2009, 2012; Srivastava and Salakhutdinov, 2012; Goodfellow et al., 2013). A DBM contains many layers of hidden variables, and has no connections between the variables within the same layer. This is a special case of the general Boltzmann machine (BM), which is a network of symmetrically connected units that are on or off based on a stochastic mechanism. While having a simple learning algorithm, the general
BMs are very complex to study and very slow to train. In a DBM, each layer captures complicated, higher-order correlations between the activities of hidden features in the layer below. DBMs have the potential of learning internal representations that become increasingly complex, highly desirable for solving object and speech recognition problems. Further, the high-level representations can be built from a large supply of unlabeled sensory inputs and very limited labeled data can then be used to only slightly fine-tune the model for a specific task at hand.
When the number of hidden layers of DBM is reduced to one, we have restricted Boltzmann machine (RBM). Like DBM, there are no hidden-to-hidden and no visible-to-visible connections in the RBM. The main virtue of RBM is that via composing many RBMs, many hidden layers can be learned efficiently using the feature activations of one RBM as the training data for the next.
Such composition leads to deep belief network (DBN), which we will describe in more detail, together with RBMs, in Chapter 5.
The standard DBN has been extended to the factored higher-order Boltzmann machine in its bottom layer, with strong results for phone recognition obtained (Dahl et. al., 2010). This model, called the mean-covariance RBM or mcRBM, recognizes the limitation of the standard RBM in its ability to represent the covariance structure of the data. However, it is difficult to train mcRBMs and to use them at the higher levels of the deep architecture. Further, the strong results published are not easy to reproduce. In the architecture described by Dahl et al. (2010), the mcRBM parameters in the full DBN are not fine-tuned using the discriminative information, which is used for fine tuning the higher layers of RBMs, due to the high computational cost.
Another representative deep generative network that can be used for unsupervised (as well as supervised) learning is the sum-product network or SPN (Poon and Domingo, 2011; Gens and Domingo, 2012). An SPN is a directed acyclic graph with the observed variables as leaves, and with sum and product operations as internal nodes in the deep network. The "sum" nodes give mixture models, and the "product" nodes build up the feature hierarchy. Properties of "completeness" and "consistency" constrain the SPN in a desirable way. The learning of SPNs is carried out using the EM algorithm together with back-propagation. The learning procedure starts with a dense SPN. It then finds an SPN structure by learning its weights, where zero weights indicate removed connections. The main difficulty in learning SPNs is that the learning signal (i.e., the gradient) quickly dilutes when it propagates to deep layers. Empirical solutions have been found to mitigate this difficulty as reported in (Poon and Domingo, 2011). It was pointed out in that early paper that despite the many desirable generative properties in the SPN, it is difficult to fine tune the parameters using the discriminative information, limiting its effectiveness in classification tasks. However, this difficulty has been overcome in the subsequent work reported in (Gens and Domingo, 2012), where an efficient backpropagation-style discriminative training algorithm for SPN was presented. Importantly, the standard gradient descent, based on the derivative of the conditional likelihood, suffers from the same gradient diffusion problem well known in the regular DNNs. The trick to alleviate this problem in learning SPNs is to replace the marginal inference with the most probable state of the hidden variables and to propagate gradients through this "hard" alignment only. Excellent results on small-scale image recognition tasks were reported by Gens and Domingo (2012).
Recurrent neural networks (RNNs) can be considered as another class of deep networks for unsupervised (as well as supervised) learning, where the depth can be as large as the length of the input data sequence. In the unsupervised learning mode, the RNN is used to predict the data sequence in the future using the previous data samples, and no additional class information is used for learning. The RNN is very powerful for modeling sequence data (e.g., speech or text), but until recently they had not been widely used partly because they are difficult to train to capture longterm dependencies, giving rise to gradient vanishing or gradient explosion problems. These problems can now be dealt with more easily (Bengio et al., 2013a; Pascanu et al., 2013; Chen and Deng, 2013). Recent advances in Hessian-free optimization (Martens, 2010) have also partially overcome this difficulty using approximated second-order information or stochastic curvature estimates. In the more recent work (Martens and Sutskever, 2011), RNNs that are trained with
Hessian-free optimization are used as a generative deep network in the character-level languagemodeling tasks, where gated connections are introduced to allow the current input characters to predict the transition from one latent state vector to the next. Such generative RNN models are demonstrated to be well capable of generating sequential text characters. More recently, Bengio et al. (2013) and Sutskever (2013) have explored variations of stochastic gradient descent optimization algorithms in training generative RNNs and shown that these algorithms can outperform Hessian-free optimization methods. Molotov et al. (2010) have reported excellent results on using RNNs for language modeling. More recently, Mesnil et al. (2013) and Yao et al.(2013) reported the success of RNNs in spoken language understanding. We will review this set of work in Chapter 8.
There has been a long history in speech recognition research where human speech production mechanisms are exploited to construct dynamic and deep structure in probabilistic generative models; for a comprehensive review, see the book by Deng (2006). Specifically, the early work described in (Deng 1992, 1993; Deng et al., 1994; Ostendorf et al., 1996, Deng and Sameti, 1996;
Deng and Aksmanovic, 1997) generalized and extended the conventional shallow and conditionally independent HMM structure by imposing dynamic constraints, in the form of polynomial trajectory, on the HMM parameters. A variant of this approach has been more recently developed using different learning techniques for time-varying HMM parameters and with the applications extended to speech recognition robustness (Yu and Deng, 2009; Yu et al., 2009a).
Similar trajectory HMMs also form the basis for parametric speech synthesis (Zen et al., 2011;
Zen et al., 2012; Ling et al., 2013; Shannon et al., 2013). Subsequent work added a new hidden layer into the dynamic model to explicitly account for the target-directed, articulatory-like properties in human speech generation (Deng and Ramsay, 1997; Deng, 1998; Bridle et al., 1998;
Deng, 1999; Picone et al., 1999; Deng, 2003; Minami et al., 2002; Deng and Huang, 2004; Deng and Ma, 2000; Ma and Deng, 2000, 2003, 2004). More efficient implementation of this deep architecture with hidden dynamics is achieved with non-recursive or finite impulse response (FIR) filters in more recent studies (Deng et. al., 2006, 2006a, Deng and Yu, 2007). The above deepstructured generative models of speech can be shown as special cases of the more general dynamic network model and even more general dynamic graphical models (Bilmes and Bartels, 2005;
Bilmes, 2010). The graphical models can comprise many hidden layers to characterize the complex relationship between the variables in speech generation. Armed with powerful graphical modeling tool, the deep architecture of speech has more recently been successfully applied to solve the very difficult problem of single-channel, multi-talker speech recognition, where the mixed speech is the visible variable while the un-mixed speech becomes represented in a new hidden layer in the deep generative architecture (Rennie et al., 2010; Wohlmayr et al., 2011). Deep generative graphical models are indeed a powerful tool in many applications due to their capability of embedding domain knowledge. However, they are often used with inappropriate approximations in inference, learning, prediction, and topology design, all arising from inherent intractability in these tasks for most real-world applications. This problem has been addressed in the recent work of Stoyanov et al. (2011), which provides an interesting direction for making deep generative graphical models potentially more useful in practice in the future. An even more drastic way to deal with this intractability was proposed recently by Bengio et al. (2013b), where the need to marginalize latent variables is avoided altogether.
The standard statistical methods used for large-scale speech recognition and understanding combine (shallow) hidden Markov models for speech acoustics with higher layers of structurerepresenting different levels of natural language hierarchy. This combined hierarchical model can be suitably regarded as a deep generative architecture, whose motivation and some technical detail may be found in Chapter 7 of the recent book (Kurzweil, 2012) on "Hierarchical HMM" or HHMM.
Related models with greater technical depth and mathematical treatment can be found in (Fine et al., 1998) for HHMM and (Oliver et al., 2004) for Layered HMM. These early deep models were formulated as directed graphical models, missing the key aspect of "distributed representation" embodied in the more recent deep generative networks of the DBN and DBM discussed earlier in this chapter. Filling in this missing aspect would help improve these generative models.
Finally, dynamic or temporally recursive generative models based on neural network architectures can be found in (Taylor et al., 2007) for human motion modeling, and in (Socher et al., 2011, 2012) for natural language and natural scene parsing. The latter model is particularly interesting because the learning algorithms are capable of automatically determining the optimal model structure. This contrasts with other deep architectures such as DBN where only the parameters are learned while the architectures need to be pre-defined. Specifically, as reported in (Socher et al., 2011), the recursive structure commonly found in natural scene images and in natural language sentences can be discovered using a max-margin structure prediction architecture. It is shown that the units contained in the images or sentences are identified, and the way in which these units interact with each other to form the whole is also identified.
3.3 Deep Networks for Supervised Learning
Many of the discriminative techniques for supervised learning in signal and information processing are shallow architectures such as HMMs (e.g., Juang et al., 1997; Chengalvarayan and Deng, 1998;
Povey and Woodland, 2002; Yu et al., 2007; He et al., 2008; Jiang and Li, 2010; Xiao and Deng, 2010; Gibson and Hain, 2010) and conditional random fields (CRFs) (e.g., Yang and Furui, 2009;
Yu et al., 2010; Hifny and Renals, 2009; Heintz et al., 2009; Zweig and Nguyen, 2009; Peng et al., 2009). A CRF is intrinsically a shallow discriminative architecture, characterized by the linear relationship between the input features and the transition features. The shallow nature of the CRF is made most clear by the equivalence established between the CRF and the discriminatively trained Gaussian models and HMMs (Heigold et al., 2011). More recently, deep-structured CRFs have been developed by stacking the output in each lower layer of the CRF, together with the original input data, onto its higher layer (Yu et al., 2010a). Various versions of deep-structured
CRFs are successfully applied to phone recognition (Yu and Deng, 2010), spoken language identification (Yu et al., 2010a), and natural language processing (Yu et al., 2010). However, at least for the phone recognition task, the performance of deep-structured CRFs, which are purely discriminative (non-generative), has not been able to match that of the hybrid approach involving
DBN, which we will take on shortly.
Morgan (2012) gives an excellent review on other major existing discriminative models in speech recognition based mainly on the traditional neural network or MLP architecture using backpropagation learning with random initialization. It argues for the importance of both the increased width of each layer of the neural networks and the increased depth. In particular, a class of deep neural network models forms the basis of the popular "tandem" approach (Morgan et al., 2005), where the output of the discriminatively learned neural network is treated as part of the observationvariable in HMMs. For some representative recent work in this area, see (Pinto et al., 2011;
Ketabdar and Bourlard, 2010).
In the most recent work of (Deng et. al, 2011; Deng et al., 2012a; Tur et al., 2012; Lena et al., 2012; Vinyals et al., 2012), a new deep learning architecture, sometimes called Deep Stacking
Network (DSN), together with its tensor variant (Hutchinson et al, 2012, 2013) and its kernel version (Deng et al., 2012), are developed that all focus on discrimination with scalable, parallelizable learning relying on little or no generative component. We will describe this type of discriminative deep architecture in detail in Chapter 6.
As discussed in the preceding section, recurrent neural networks (RNNs) have been used as a generative model; see also the neural predictive model (Deng et al., 1994a) with a similar
"generative" mechanism. RNNs can also be used as a discriminative model where the output is a label sequence associated with the input data sequence. Note that such discriminative RNNs or sequence models were applied to speech a long time ago with limited success. In (Bengio, 1991), an HMM was trained jointly with the neural networks, with a discriminative probabilistic training criterion. In (Robinson, 1994), a separate HMM was used to segment the sequence during training, and the HMM was also used to transform the RNN classification results into label sequences.
However, the use of the HMM for these purposes does not take advantage of the full potential of RNNs.
A set of new models and methods were proposed more recently in (Graves et al., 2006; Graves, 2012, Graves et al., 2013, 2013a) that enable the RNNs themselves to perform sequence classification while embedding the long-short-term memory into the model, removing the need for pre-segmenting the training data and for post-processing the outputs. Underlying this method is the idea of interpreting RNN outputs as the conditional distributions over all possible label sequences given the input sequences. Then, a differentiable objective function can be derived to optimize these conditional distributions over the correct label sequences, where the segmentation of the data is performed automatically by the algorithm. The effectiveness of this method has been demonstrated in handwriting recognition tasks and in a small speech task (Graves et al., 2013, 2013a) to be discussed in more detail in Chapter 7 of this book.
Another type of discriminative deep architecture is the convolutional neural network (CNN), in which each module consists of a convolutional layer and a pooling layer. These modules are often stacked up with one on top of another, or with a DNN on top of it, to form a deep model. The convolutional layer shares many weights, and the pooling layer subsamples the output of the convolutional layer and reduces the data rate from the layer below. The weight sharing in the convolutional layer, together with appropriately chosen pooling schemes, endows the CNN with some "invariance" properties (e.g., translation invariance). It has been argued that such limited
"invariance" or equi-variance is not adequate for complex pattern recognition tasks and more principled ways of handling a wider range of invariance may be needed (Hinton et al., 2011).
Nevertheless, CNNs have been found highly effective and been commonly used in computer vision and image recognition (Bengio and LeCun, 1995; LeCun et al., 1998; Ciresan et al., 2010, 2011, 2012, 2012a; Le et al., 2012; Dean et al., 2012; Krizhevsky et al., 2012, Zeiler, 2014). More recently, with appropriate changes from the CNN designed for image analysis to that taking intoaccount speech-specific properties, the CNN is also found effective for speech recognition (AbdelHamid et al., 2012, 2013, 2013a; Sainath et al., 2013; Deng et al., 2013). We will discuss such applications in more detail in Chapter 7 of this book.
It is useful to point out that the time-delay neural network (TDNN, Lang et al., 1990; Waibel et al., 1989) developed for early speech recognition is a special case and predecessor of the CNN when weight sharing is limited to one of the two dimensions, i.e., time dimension, and there is no pooling layer. It was not until recently that researchers have discovered that the time-dimension invariance is less important than the frequency-dimension invariance for speech recognition (Abdel-Hamid et al., 2012, 2013; Deng et al., 2013). A careful analysis on the underlying reasons is described in(Deng et al., 2013), together with a new strategy for designing the CNN's pooling layer demonstrated to be more effective than all previous CNNs in phone recognition.
It is also useful to point out that the model of hierarchical temporal memory (HTM, Hawkins and Blakeslee, 2004; Hawkins et al., 2010; George, 2008) is another variant and extension of the CNN.
The extension includes the following aspects: 1) Time or temporal dimension is introduced to serve as the "supervision" information for discrimination (even for static images); 2) Both bottom-up and top-down information flows are used, instead of just bottom-up in the CNN; and 3) A Bayesian probabilistic formalism is used for fusing information and for decision making.
Finally, the learning architecture developed for bottom-up, detection-based speech recognition proposed in (Lee, 2004) and developed further since 2004, notably in (Yu et al., 2012a; Siniscalchi et al., 2013, 2013a) using the DBN-DNN technique, can also be categorized in the discriminative or supervised-learning deep architecture category. There is no intent and mechanism in this architecture to characterize the joint probability of data and recognition targets of speech attributes and of the higher-level phone and words. The most current implementation of this approach is based on the DNN, or neural networks with many layers using back-propagation learning. One intermediate neural network layer in the implementation of this detection-based framework explicitly represents the speech attributes, which are simplified entities from the "atomic" units of speech developed in the early work of (Deng and Sun, 1994; Sun and Deng, 2002). The simplification lies in the removal of the temporally overlapping properties of the speech attributes or articulatory-like features. Embedding such more realistic properties in the future work is expected to improve the accuracy of speech recognition further.
3.4 Hybrid Deep Networks
The term "hybrid" for this third category refers to the deep architecture that either comprises or makes use of both generative and discriminative model components. In the existing hybrid architectures published in the literature, the generative component is mostly exploited to help with discrimination, which is the final goal of the hybrid architecture. How and why generative modeling can help with discrimination can be examined from two viewpoints (Erhan et al., 2010):

The optimization viewpoint where generative models trained in an unsupervised fashion can provide excellent initialization points in highly nonlinear parameter estimation problems(The commonly used term of "pre-training" in deep learning has been introduced for this reason); and/or

The regularization perspective where the unsupervised-learning models can effectively provide a prior on the set of functions representable by the model.
The study reported in (Erhan et al., 2010) provided an insightful analysis and experimental evidence supporting both of the viewpoints above.
The DBN, a generative, deep network for unsupervised learning discussed in Chapter 3.2, can be converted to and used as the initial model of a DNN for supervised learning with the same network structure, which is further discriminatively trained or fine-tuned using the target labels provided.
When the DBN is used in this way we consider this DBN-DNN model as a hybrid deep model, where the model trained using unsupervised data helps to make the discriminative model effective for supervised learning. We will review details of the discriminative DNN for supervised learning in the context of RBM/DBN generative, unsupervised pre-training in Chapter 5.
Another example of the hybrid deep network is developed in (Mohamed et al., 2010), where the DNN weights are also initialized from a generative DBN but are further fine-tuned with a sequence-level discriminative criterion, which is the conditional probability of the label sequence given the input feature sequence, instead of the frame-level criterion of cross-entropy commonly used. This can be viewed as a combination of the static DNN with the shallow discriminative architecture of CRF. It can be shown that such a DNN-CRF is equivalent to a hybrid deep architecture of DNN and HMM whose parameters are learned jointly using the full-sequence maximum mutual information (MMI) criterion between the entire label sequence and the input feature sequence. A closely related full-sequence training method designed and implemented for much larger tasks is carried out more recently with success for a shallow neural network(Kingsbury, 2009) and for a deep one (Kingsbury et al., 2012; Su et al., 2013). We note that the origin of the idea for joint training of the sequence model (e.g., the HMM) and of the neural network came from the early work of (Bengio, 1991; Bengio et al., 1992), where shallow neural networks were trained with small amounts of training data and with no generative pre-training.
Here, it is useful to point out a connection between the above pretraining/fine-tuning strategy associated with hybrid deep networks and the highly popular minimum phone error (MPE) training technique for the HMM (Povey and Woodland, 2002; and He et al., 2008 for an overview). To make MPE training effective, the parameters need to be initialized using an algorithm (e.g., BaumWelch algorithm) that optimizes a generative criterion (e.g., maximum likelihood). This type of methods, which uses maximum-likelihood trained parameters to assist in the discriminative HMM training can be viewed as a "hybrid" approach to train the shallow HMM model.
Along the line of using discriminative criteria to train parameters in generative models as in the above HMM training example, we here discuss the same method applied to learning other hybrid deep networks. In (Larochelle and Bengio, 2008), the generative model of RBM is learned using the discriminative criterion of posterior class-label probabilities. Here the label vector is concatenated with the input data vector to form the combined visible layer in the RBM. In thisway, RBM can serve as a stand-alone solution to classification problems and the authors derived a discriminative learning algorithm for RBM as a shallow generative model. In the more recent work by Ranzato et al. (2011), the deep generative model of DBN with gated Markov random field(MRF) at the lowest level is learned for feature extraction and then for recognition of difficult image classes including occlusions. The generative ability of the DBN facilitates the discovery of what information is captured and what is lost at each level of representation in the deep model, as demonstrated in (Ranzato et al., 2011). A related study on using the discriminative criterion of empirical risk to train deep graphical models can be found in (Stoyanov et al., 2011).
A further example of hybrid deep networks is the use of generative models of DBNs to pre-train deep convolutional neural networks (deep CNNs) (Lee et al., 2009, 2010, 2011). Like the fully connected DNN discussed earlier, pre-training also helps to improve the performance of deep
CNNs over random initialization. Pre-training DNNs or CNNs using a set of regularized deep autoencoders (Bengio et al., 2013a), including denoising autoencoders, contractive autoencoders, and sparse autoencoders, is also a similar example of the category of hybrid deep networks.
The final example given here for hybrid deep networks is based on the idea and work of (Ney, 1999; He and Deng, 2011), where one task of discrimination (e.g., speech recognition) produces the output (text) that serves as the input to the second task of discrimination (e.g., machine translation). The overall system, giving the functionality of speech translation – translating speech in one language into text in another language – is a two-stage deep architecture consisting of both generative and discriminative elements. Both models of speech recognition (e.g., HMM) and of machine translation (e.g., phrasal mapping and non-monotonic alignment) are generative in nature, but their parameters are all learned for discrimination of the ultimate translated text given the speech data. The framework described in (He and Deng, 2011) enables end-to-end performance optimization in the overall deep architecture using the unified learning framework initially published in (He et al., 2008). This hybrid deep learning approach can be applied to not only speech translation but also all speech-centric and possibly other information processing tasks such as speech information retrieval, speech understanding, cross-lingual speech/text understanding and retrieval, etc. (e.g., Yamin et al., 2008; Tur et al., 2012; He and Deng, 2012, 2013; Deng et al., 2012; Deng et al., 2013a; He et al., 2013).
In the next three chapters, we will elaborate on three prominent types of models for deep learning, one from each of the three classes reviewed in this chapter. These are chosen to serve the tutorial purpose, given their simplicity of the architectural and mathematical descriptions. The three architectures described in the following three chapters may not be interpreted as the most representative and influential work in each of the three classes.
CHAPTER 4
DEEP AUTOENCODERS --UNSUPERVISED LEARNING
This chapter and the next two will each select one prominent example deep network for each of the three categories outlined in Chapter 3. Here we begin with the category of the deep models designed mainly for unsupervised learning.
4.1 Introduction
The deep autoencoder is a special type of the DNN (with no class labels), whose output vectors have the same dimensionality as the input vectors. It is often used for learning a representation or effective encoding of the original data, in the form of input vectors, at hidden layers. Note that the autoencoder is a nonlinear feature extraction method without using class labels. As such, the features extracted aim at conserving and better representing information instead of performing classification tasks, although sometimes these two goals are correlated.
An autoencoder typically has an input layer which represents the original data or input feature vectors (e.g., pixels in image or spectra in speech), one or more hidden layers that represent the transformed feature, and an output layer which matches the input layer for reconstruction. When the number of hidden layers is greater than one, the autoencoder is considered to be deep. The dimension of the hidden layers can be either smaller (when the goal is feature compression) or larger (when the goal is mapping the feature to a higher-dimensional space) than the input dimension.
An autoencoder is often trained using one of the many back-propagation variants, typically the stochastic gradient descent method. Though often reasonably effective, there are fundamental problems when using back-propagation to train networks with many hidden layers. Once the errors get back-propagated to the first few layers, they become minuscule, and training becomes quite ineffective. Though more advanced back-propagation methods help with this problem to some degree, it still results in slow learning and poor solutions, especially with limited amounts of training data. As mentioned in the previous chapters, the problem can be alleviated by pre-training each layer as a simple autoencoder (Hinton et al, 2006; Bengio et al., 2006). This strategy has been applied to construct a deep autoencoder to map images to short binary code for fast, content-based image retrieval, to encode documents (called semantic hashing), and to encode spectrogram-like speech features which we review below.
4.2 Use of Deep Autoencoders to Extract Speech
Features
Here we review a set of work, some of which was published in (Deng et al., 2010), in developing an autoencoder for extracting binary speech codes using unlabeled speech data only. The discrete representations in terms of a binary code extracted by this model can be used in speech information retrieval or as bottleneck features for speech recognition.
A deep generative model of patches of spectrograms that contain 256 frequency bins and 1, 3, 9, or 13 frames is illustrated in Figure 4.1. An undirected graphical model called a GaussianBernoulli RBM is built that has one visible layer of linear variables with Gaussian noise and one hidden layer of 500 to 3000 binary latent variables. After learning the Gaussian- Bernoulli RBM, the activation probabilities of its hidden units are treated as the data for training another BernoulliBernoulli RBM. These two RBM's can then be composed to form a deep belief net (DBN) in which it is easy to infer the states of the second layer of binary hidden units from the input in a single forward pass. The DBN used in this work is illustrated on the left side of Figure 4.1, where the two RBMs are shown in separate boxes. (See more detailed discussions on RBM and DBN in Chapter 5).
Figure 4.1. The architecture of the deep autoencoder used in (Deng et al., 2010) for extracting binary speech codes from high-resolution spectrograms. [after (Deng et. al., 2010), @Elsevier]
The deep autoencoder with three hidden layers is formed by "unrolling" the DBN using its weight matrices. The lower layers of this deep autoencoder use the matrices to encode the input and the upper layers use the matrices in reverse order to decode the input. This deep autoencoder is then fine-tuned using error back-propagation to minimize the reconstruction error, as shown on the right side of Figure 4.1. After learning is complete, any variable-length spectrogram can be encoded and reconstructed as follows. First, N consecutive overlapping frames of 256-point log power spectra are each normalized to zero-mean and unit-variance across samples per feature to provide the input to the deep autoencoder. The first hidden layer then uses the logistic function to compute realvalued activations. These real values are fed to the next, coding layer to compute "codes". The real-valued activations of hidden units in the coding layer are quantized to be either zero or one with 0.5 as the threshold. These binary codes are then used to reconstruct the original spectrogram, where individual fixed-frame patches are reconstructed first using the two upper layers of network weights. Finally, the standard overlap-and-add technique in signal processing is used to reconstruct the full-length speech spectrogram from the outputs produced by applying the deep autoencoder to every possible window of N consecutive frames. We show some illustrative encoding and reconstruction examples below.
At the top of Figure 4.2 is the original, un-coded speech, followed by the speech utterances reconstructed from the binary codes (zero or one) at the 312 unit bottleneck code layer with encoding window lengths of N=1, 3, 9, and 13, respectively. The lower reconstruction errors for
N=9 and N=13 are clearly seen.
Figure 4.2. Top to Bottom: The original spectrogram; reconstructions using input window sizes of N= 1, 3, 9, and 13 while forcing the coding units to take values of zero or one (i.e., a binary code). [after (Deng et. al., 2010), @Elsevier]
Encoding error of the deep autoencoder is qualitatively examined in comparison with the more traditional codes via vector quantization (VQ). Figure 3 shows various aspects of the encoding errors. At the top is the original speech utterance's spectrogram. The next two spectrograms are the blurry reconstruction from the 312-bit VQ and the much more faithful reconstruction from the 312-bit deep autoencoder. Coding errors from both coders, plotted as a function of time, are shown below the spectrograms, demonstrating that the autoencoder (red curve) is producing lower errors than the VQ coder (blue curve) throughout the entire span of the utterance. The final two spectrograms show detailed coding error distributions over both time and frequency bins.
Figures 4.4 to 4.10 show additional examples (unpublished) for the original un-coded speech spectrograms and their reconstructions using the deep autoencoder. They give a diverse number of binary codes for either a single or three consecutive frames in the spectrogram samples.
Figure 4.3. Top to bottom: The original spectrogram from the test set; reconstruction from the 312-bit VQ coder; reconstruction from the 312-bit autoencoder; coding errors as a function of time for the VQ coder (blue) and autoencoder (red); spectrogram of the VQ coder residual; spectrogram of the deep autoencoder's residual. [after (Deng et. al., 2010), @Elsevier]
Figure 4.4. The original speech spectrogram and the reconstructed counterpart. A total of 312 binary codes are with one for each single frame.
Figure 4.5. Same as Figure 4.4 but with a different TIMIT speech utterance.
Figure 4.6. The original speech spectrogram and the reconstructed counterpart. A total of 936 binary codes are used for three adjacent frames.
Figure 4.7. Same as Figure 4.6 but with a different TIMIT speech utterance.
Figure 4.8. Same as Figure 4.6 but with yet another TIMIT speech utterance.
Figure 4.9. The original speech spectrogram and the reconstructed counterpart. A total of 2000 binary codes with one for each single frame.
Figure 4.10. Same as Figure 4.9 but with a different TIMIT speech utterance.
4.3 Stacked Denoising Autoencoders
In early years of autoencoder research, the encoding layer had smaller dimensions than the input layer. However, in some applications, it is desirable that the encoding layer is wider than the input layer, in which case techniques are needed to prevent the neural network from learning the trivial identity mapping function. One of the reasons for using a higher dimension in the hidden or encoding layers than the input layer is that it allows the autoencoder to capture a rich input distribution.
The trivial mapping problem discussed above can be prevented by methods such as using sparseness constraints, or using the "dropout" trick by randomly forcing certain values to be zero and thus introducing distortions at the input data (Vincent, et al., 2010; Vincent, 2011) or at the hidden layers (Hinton et al., 2012a). For example, in the stacked denoising autoencoder detailed in (Vincent, et al., 2010), random noises are added to the input data. This serves several purposes.
First, by forcing the output to match the original undistorted input data the model can avoid learning the trivial identity solution. Second, since the noises are added randomly, the model learned would be robust to the same kind of distortions in the test data. Third, since each distorted input sample is different, it greatly increases the training set size and thus can alleviate the overfitting problem.
It is interesting to note that when the encoding and decoding weights are forced to be the transpose of each other, such denoising autoencoder with a single sigmoidal hidden layer is strictly equivalent to a particular Gaussian RBM, but instead of training it by the technique of contrastive divergence (CD) or persistent CD, it is trained by a score matching principle, where the score is defined as the derivative of the log-density with respect to the input (Vincent, 2011). Furthermore, Alain and Bengio (2013) generalized this result to any parameterization of the encoder and decoder with squared reconstruction error and Gaussian corruption noise. They show that as the amount of noise approaches zero, such models estimate the true score of the underlying data generating distribution. Finally, Bengio et al (2013b) show that any denoising autoencoder is a consistent estimator of the underlying data generating distribution within some family of distributions. This is true for any parameterization of the autoencoder, for any type of information-destroying corruption process with no constraint on the noise level except being positive, and for any reconstruction loss expressed as a conditional log-likelihood. The consistency of the estimator is achieved by associating the denoising autoencoder with a Markov chain whose stationary distribution is the distribution estimated by the model, and this Markov chain can be used to sample from the denoising autoencoder.
4.4 Transforming Autoencoders
The deep autoencoder described above can extract faithful codes for feature vectors due to many layers of nonlinear processing. However, the code extracted in this way is transformation-variant.
In other words, the extracted code would change in ways chosen by the learner when the input feature vector is transformed. Sometimes, it is desirable to have the code change predictably to reflect the underlying transformation-invariant property of the perceived content. This is the goal of the transforming autoencoder proposed in (Hinton et al., 2011) for image recognition.
The building block of the transforming autoencoder is a "capsule", which is an independent subnetwork that extracts a single parameterized feature representing a single entity, be it visual or audio. A transforming autoencoder receives both an input vector and a target output vector, which is transformed from the input vector through a simple global transformation mechanism; e.g. translation of an image and frequency shift of speech (the latter due to the vocal tract length difference). An explicit representation of the global transformation is assumed known. The coding layer of the transforming autoencoder consists of the outputs of several capsules.
During the training phase, the different capsules learn to extract different entities in order to minimize the error between the final output and the target.
In addition to the deep autoencoder architectures described here, there are many other types of generative architectures in the literature, all characterized by the use of data alone (i.e., free of classification labels) to automatically derive higher-level features.
CHAPTER 5
PRE-TRAINED DEEP NEURAL
NETWORKS --- A HYBRID
In this chapter, we present the most widely used hybrid deep architecture – the pre-trained deep neural network (DNN), and discuss the related techniques and building blocks including the RBM and DBN. We discuss the DNN example here in the category of hybrid deep networks before the examples in the category of deep networks for supervised learning (Chapter 6). This is partly due to the natural flow from the unsupervised learning models to the DNN as a hybrid model. The discriminative nature of artificial neural networks for supervised learning has been widely known, and thus would not be required for understanding the hybrid nature of the DNN that uses unsupervised pre-training to facilitate the subsequent discriminative fine tuning.
Part of the review in this chapter is based on recent publications in (Hinton et al., 2012), (Yu and Deng, 2011), and (Dahl et al., 2012).
5.1 Restricted Boltzmann Machines
An RBM is a special type of Markov random field that has one layer of (typically Bernoulli) stochastic hidden units and one layer of (typically Bernoulli or Gaussian) stochastic visible or observable units. RBMs can be represented as bipartite graphs, where all visible units are connected to all hidden units, and there are no visible-visible or hidden-hidden connections.
In an RBM, the joint distribution p(𝐯, 𝐡; θ) over the visible units 𝐯 and hidden units 𝐡, given the model parameters θ, is defined in terms of an energy function E(𝐯, 𝐡; θ) of p(𝐯, 𝐡; θ) = 𝑒𝑥𝑝 (−E(𝐯, 𝐡; θ))
𝑍, where 𝑍 = ∑ ∑ 𝑒𝑥𝑝(−E(𝐯, 𝐡; θ)) 𝐡 𝐯 is a normalization factor or partition function, and the marginal probability that the model assigns to a visible vector 𝐯 is p(𝐯; θ) =
∑ 𝑒𝑥𝑝(−E(𝐯, 𝐡; θ)) 𝒉
𝑍
For a Bernoulli (visible)-Bernoulli (hidden) RBM, the energy function is defined as
E(𝐯, 𝐡; θ) = − ∑ ∑ 𝑤𝑖𝑗
𝐽 𝑗=1 𝑣𝑖ℎ𝑗
𝐼 𝑖=1
− ∑ 𝑏𝑖𝑣𝑖
𝐼 𝑖=1
− ∑ 𝑎𝑗ℎ𝑗
𝐽 𝑗=1, where 𝑤𝑖𝑗 represents the symmetric interaction term between visible unit 𝑣𝑖 and hidden unit ℎ𝑗, 𝑏𝑖 and 𝑎𝑗 the bias terms, and 𝐼 and 𝐽 are the numbers of visible and hidden units. The conditional probabilities can be efficiently calculated as 𝑝(ℎ𝑗 = 1|𝐯; θ) = 𝜎 (∑ 𝑤𝑖𝑗
𝐼 𝑖=1 𝑣𝑖 + 𝑎𝑗), 𝑝(𝑣𝑖 = 1|𝐡; θ) = 𝜎 (∑ 𝑤𝑖𝑗
𝐽 𝑗=1 ℎ𝑗 + 𝑏𝑖), where 𝜎(𝑥) = 1 (1 + 𝑒𝑥𝑝(−𝑥))
⁄
Similarly, for a Gaussian (visible)-Bernoulli (hidden) RBM, the energy is E(𝐯, 𝐡; θ) = − ∑ ∑ 𝑤𝑖𝑗
𝐽 𝑗=1 𝑣𝑖ℎ𝑗
𝐼 𝑖=1
− 1
2 ∑(𝑣𝑖 − 𝑏𝑖)2
𝐼 𝑖=1
− ∑ 𝑎𝑗ℎ𝑗
𝐽 𝑗=1, The corresponding conditional probabilities become 𝑝(ℎ𝑗 = 1|𝐯; θ) = 𝜎 (∑ 𝑤𝑖𝑗
𝐼 𝑖=1 𝑣𝑖 + 𝑎𝑗), 𝑝(𝑣𝑖|𝐡; θ) = 𝒩 (∑ 𝑤𝑖𝑗
𝐽 𝑗=1 ℎ𝑗 + 𝑏𝑖, 1), where 𝑣𝑖 takes real values and follows a Gaussian distribution with mean ∑ 𝑤𝑖𝑗
𝐽 𝑗=1 ℎ𝑗 + 𝑏𝑖 and variance one. Gaussian-Bernoulli RBMs can be used to convert real-valued stochastic variables to binary stochastic variables, which can then be further processed using the Bernoulli-Bernoulli
RBMs.
The above discussion used two of the most common conditional distributions for the visible data in the RBM – Gaussian (for continuous-valued data) and binomial (for binary data). More generaltypes of distributions in the RBM can also be used. See (Welling et al., 2005) for the use of general exponential-family distributions for this purpose.
Taking the gradient of the log likelihood log 𝑝(𝐯; θ) we can derive the update rule for the RBM weights as:
∆𝑤𝑖𝑗 = 𝐸𝑑𝑎𝑡𝑎(𝑣𝑖ℎ𝑗) − 𝐸𝑚𝑜𝑑𝑒𝑙(𝑣𝑖ℎ𝑗), where 𝐸𝑑𝑎𝑡𝑎(𝑣𝑖ℎ𝑗) is the expectation observed in the training set (with ℎ𝑗 sampled given 𝑣𝑖 according to the model), and 𝐸𝑚𝑜𝑑𝑒𝑙(𝑣𝑖ℎ𝑗) is that same expectation under the distribution defined by the model. Unfortunately, 𝐸𝑚𝑜𝑑𝑒𝑙(𝑣𝑖ℎ𝑗) is intractable to compute. The contrastive divergence (CD) approximation to the gradient was the first efficient method proposed to approximate this expected value, where 𝐸𝑚𝑜𝑑𝑒𝑙(𝑣𝑖ℎ𝑗) is replaced by running the Gibbs sampler initialized at the data for one or more steps. The steps in approximating 𝐸𝑚𝑜𝑑𝑒𝑙(𝑣𝑖ℎ𝑗) is summarized as follows:

Initialize 𝐯𝟎 at data

Sample 𝐡𝟎 ∼ 𝒑(𝐡|𝐯𝟎)

Sample 𝐯𝟏 ∼ 𝒑(𝐯|𝐡𝟎)

Sample 𝐡𝟏 ∼ 𝒑(𝐡|𝐯𝟏)
Here, (𝐯𝟏, 𝐡𝟏) is a sample from the model, as a very rough estimate of 𝐸𝑚𝑜𝑑𝑒𝑙(𝑣𝑖ℎ𝑗). The use of(𝐯𝟏, 𝐡𝟏) to approximate 𝐸𝑚𝑜𝑑𝑒𝑙(𝑣𝑖ℎ𝑗) gives rise to the algorithm of CD-1. The sampling process can be pictorially depicted in Figure 5.1 below.
Figure 5.1. A pictorial view of sampling from a RBM during RBM learning (courtesy of Geoff
Hinton).
Note that CD-k generalizes this to more steps of the Markov chain. There are other techniques for estimating the log-likelihood gradient of RBMs, in particular the stochastic maximum likelihood or persistent contrastive divergence (PCD) (Younes 1999; Tieleman, 2008). Both work better than
CD when using the RBM as a generative model.
Careful training of RBMs is essential to the success of applying RBM and related deep learning techniques to solve practical problems. See Technical Report (Hinton 2010) for a very useful practical guide for training RBMs.
The RBM discussed above is both a generative and an unsupervised model, which characterizes the input data distribution using hidden variables and there is no label information involved.
However, when the label information is available, it can be used together with the data to form the concatenated "data" set. Then the same CD learning can be applied to optimize the approximate
"generative" objective function related to data likelihood. Further, and more interestingly, a "discriminative" objective function can be defined in terms of conditional likelihood of labels.
This discriminative RBM can be used to "fine tune" RBM for classification tasks (Larochelle and Bengio, 2008).
Ranzato et al. (2007) proposed an unsupervised learning algorithm called Sparse Encoding
Symmetric Machine (SESM), which is quite similar to RBM. They both have a symmetric encoder and decoder, and a logistic non-linearity on the top of the encoder. The main difference is that whereas the RBM is trained using (very approximate) maximum likelihood, SESM is trained by simply minimizing the average energy plus an additional code sparsity term. SESM relies on the sparsity term to prevent flat energy surfaces, while RBM relies on an explicit contrastive term in the loss, an approximation of the log partition function. Another difference is in the coding strategy in that the code units are "noisy" and binary in the RBM, while they are quasi-binary and sparse in SESM.
5.2 Unsupervised Layer-wise Pre-training
Here we describe how to stack up RBMs just described to form a DBN as the basis for DNN's pretraining. Before delving into details, we first note that this procedure, proposed by Hinton and Salakhutdinov (2006) is a more general technique of unsupervised layer-wise pretraining. That is, not only RBMs can be stacked to form deep generative (or discriminative) networks, but other types of networks can also do the same, such as autoencoder variants as proposed by Bengio et al.
Stacking a number of the RBMs learned layer by layer from bottom up gives rise to a DBN, an example of which is shown in Figure 5.2. The stacking procedure is as follows. After learning a Gaussian-Bernoulli RBM (for applications with continuous features such as speech) or BernoulliBernoulli RBM (for applications with nominal or binary features such as black-white image or coded text), we treat the activation probabilities of its hidden units as the data for training the Bernoulli-Bernoulli RBM one layer up. The activation probabilities of the second-layer BernoulliBernoulli RBM are then used as the visible data input for the third-layer Bernoulli-Bernoulli RBM, and so on. Some theoretical justification of this efficient layer-by-layer greedy learning strategy is given in (Hinton et al., 2006), where it is shown that the stacking procedure above improves a variational lower bound on the likelihood of the training data under the composite model. That is, the greedy procedure above achieves approximate maximum likelihood learning. Note that this learning procedure is unsupervised and requires no class label.
Figure 5.2. An illustration of the DBN-DNN architecture.
When applied to classification tasks, the generative pre-training can be followed by or combined with other, typically discriminative, learning procedures that fine-tune all of the weights jointly to improve the performance of the network. This discriminative fine-tuning is performed by adding a final layer of variables that represent the desired outputs or labels provided in the training data.
Then, the back-propagation algorithm can be used to adjust or fine-tune the network weights in the same way as for the standard feed-forward neural network. What goes to the top, label layer of this DNN depends on the application. For speech recognition applications, the top layer, denoted by "l1, l2,… lj,… lL," in Figure 5.2, can represent either syllables, phones, sub-phones, phone states, or other speech units used in the HMM-based speech recognition system.
The generative pre-training described above has produced better phone and speech recognition results than random initialization on a wide variety of tasks, which will be surveyed in Chapter 7.
Further research has also shown the effectiveness of other pre-training strategies. As an example, greedy layer-by-layer training may be carried out with an additional discriminative term to the generative cost function at each level. And without generative pre-training, purely discriminative training of DNNs from random initial weights using the traditional stochastic gradient decent method has been shown to work very well when the scales of the initial weights are set carefully and the mini-batch sizes, which trade off noisy gradients with convergence speed, used instochastic gradient decent are adapted prudently (e.g., with an increasing size over training epochs).
Also, randomization order in creating mini-batches needs to be judiciously determined.
Importantly, it was found effective to learn a DNN by starting with a shallow neural network with a single hidden layer. Once this has been trained discriminatively (using early stops to avoid overfitting), a second hidden layer is inserted between the first hidden layer and the labeled softmax output units and the expanded deeper network is again trained discriminatively. This can be continued until the desired number of hidden layers is reached, after which a full backpropagation "fine tuning" is applied. This discriminative "pre-training" procedure is found to work well in practice (e.g., Seide et al., 2011; Yu et al., 2011), especially with a reasonably large amount of training data. When the amount of training data is increased even more, then some carefully designed random initialization methods can work well also without using the above pretraining schemes.
In any case, pre-training based on the use of RBMs to stack up in forming the DBN has been found to work well in most cases, regardless of a large or small amount of training data. It is useful to point out that there are other ways to perform pre-training in addition to the use of RBMs and DBNs. For example, denoising autoencoders have now been shown to be consistent estimators of the data generating distribution (Bengio et al., 2013b). Like RBMs, they are also shown to be generative models from which one can sample. Unlike RBMs, however, an unbiased estimator of the gradient of the training objective function can be obtained by the denoising autoencoders, avoiding the need for MCMC or variational approximations in the inner loop of training. Therefore, the greedy layer-wise pre-training may be performed as effectively by stacking the denoising autoencoders as by stacking the RBMs each as a single-layer learner.
Further, a general framework for layer-wise pre-training can be found in many deep learning papers; e.g., Section 2 of (Bengio, 2012). This includes, as a special case, the use of RBMs as the single-layer building block as discussed in this section. The more general framework can cover the RBM/DBN as well as any other unsupervised feature extractor. It can also cover the case of unsupervised pre-training of the representation only followed by a separate stage of learning a classifier on top of the unsupervised, pre-trained features (Lee et al., 2009, 2010, 2011).
5.3 Interfacing DNNs with HMMs
The pre-trained DNN as a prominent example of the hybrid deep networks discussed so far in this chapter is a static classifier with input vectors having a fixed dimensionality. However, many practical pattern recognition and information processing problems, including speech recognition, machine translation, natural language understanding, video processing and bio-information processing, require sequence recognition. In sequence recognition, sometimes called classification with structured input/output, the dimensionality of both inputs and outputs are variable.
Figure 5.3. Interface between DBN/DNN and HMM to form a DNN-HMM. This architecture, developed at Microsoft, has been successfully used in speech recognition experiments reported in(Dahl et al., 2011, 2012). [after (Dahl et. al., 2011, 2012), @IEEE]
The HMM, based on dynamic programing operations, is a convenient tool to help port the strength of a static classifier to handle dynamic or sequential patterns. Thus, it is natural to combine feedforward neural networks and HMMs to bridge the gap between the static and sequence pattern recognition, as was done in the early days of neural networks for speech recognition (Bengio, 1991;
Bengio et al., 1992; Bourlard and Morgan, 1993). A popular architecture to fulfill this role with the use of the DNN is shown in 5.3. This architecture has been successfully used in speech recognition experiments as reported in (Dahl et al., 2011, 2012).
It is important to note that the unique elasticity of temporal dynamics of speech as elaborated in(Deng et al., 1997; Bridle et al., 1998; Deng, 1998, 2006) would require temporally-correlated models more powerful than HMMs for the ultimate success of speech recognition. Integrating such dynamic models that have realistic co-articulatory properties with the DNN and possibly other deep learning models to form the coherent dynamic deep architecture is a challenging new research direction.
CHAPTER 6
DEEP STACKING NETWORKS AND
VARIANTS --- SUPERVISED LEARNING
6.1 Introduction
While the DNN just reviewed has been shown to be extremely powerful in connection with performing recognition and classification tasks including speech recognition and image classification, training a DNN has proven to be difficult computationally. In particular, conventional techniques for training DNNs at the fine tuning phase involve the utilization of a stochastic gradient descent learning algorithm, which is difficult to parallelize across machines.
This makes learning at large scale non-trivial. For example, it has been possible to use one single, very powerful GPU machine to train DNN-based speech recognizers with dozens to a few hundreds or thousands of hours of speech training data with remarkable results. It is less clear, however, to scale up this success with many thousands or more hours of training data. See (Dean et al., 2012) for recent work in this direction.
Here we describe a new deep learning architecture, the deep stacking network (DSN), which was originally designed with the learning scalability problem in mind. This chapter is based in part on the recent publications of (Deng and Yu, 2011; Deng et al., 2012a; Hutchinson et al., 2012, 2013) with expanded discussions.
The central idea of the DSN design relates to the concept of stacking, as proposed originally in(Wolpert, 1992), where simple modules of functions or classifiers are composed first and then they are "stacked" on top of each other in order to learn complex functions or classifiers. Various ways of implementing stacking operations have been developed in the past, typically making use of supervised information in the simple modules. The new features for the stacked classifier at a higher level of the stacking architecture often come from concatenation of the classifier output of a lower module and the raw input features. In (Cohen and de Carvalho, 2005), the simple module used for stacking was a conditional random field (CRF). This type of deep architecture was further developed with hidden states added for successful natural language and speech recognition applications where segmentation information in unknown in the training data (Yu et al., 2010).
Convolutional neural networks, as in (Jarrett, et al., 2009), can also be considered as a stacking architecture but the supervision information is typically not used until in the final stacking module.
The DSN architecture was originally presented in (Deng and Yu, 2011) and was referred as deep convex network or DCN to emphasize the convex nature of a major portion of the algorithm used for learning the network. The DSN makes use of supervision information for stacking each of the basic modules, which takes the simplified form of multilayer perceptron. In the basic module, the output units are linear and the hidden units are sigmoidal nonlinear. The linearity in the outputunits permits highly efficient, parallelizable, and closed-form estimation (a result of convex optimization) for the output network weights given the hidden units' activities. Due to the closedform constraints between the input and output weights, the input weights can also be elegantly estimated in an efficient, parallelizable, batch-mode manner, which we will describe in some detail in Section 6.3.
The name "convex" used in (Deng and Yu, 2011) accentuates the role of convex optimization in learning the output network weights given the hidden units' activities in each basic module. It also points to the importance of the closed-form constraints, derived from the convexity, between the input and output weights. Such constraints make the learning of the remaining network parameters(i.e., the input network weights) much easier than otherwise, enabling batch-mode learning of the DSN that can be distributed over CPU clusters. And in more recent publications, the DSN was used when the key operation of stacking is emphasized.
6.2 A Basic Architecture of the Deep Stacking
Network
A DSN, as shown in Figure 6.1, includes a variable number of layered modules, wherein each module is a specialized neural network consisting of a single hidden layer and two trainable sets of weights. In Figure 6.1, only four such modules are illustrated, where each module is shown with a separate color. In practice, up to a few hundreds of modules have been efficiently trained and used in image and speech classification experiments.
The lowest module in the DSN comprises a linear layer with a set of linear input units, a hidden non-linear layer with a set of non-linear units, and a second linear layer with a set of linear output units. A sigmoidal nonlinearity is typically used in the hidden layer. However, other nonlinearities can also be used. If the DSN is utilized in connection with recognizing an image, the input units can correspond to a number of pixels (or extracted features) in the image, and can be assigned values based at least in part upon intensity values, RGB values, or the like corresponding to the respective pixels. If the DSN is utilized in connection with speech recognition, the set of input units may correspond to samples of speech waveform, or the extracted features from speech waveforms, such as power spectra or cepstral coefficients. The output units in the linear output layer represent the targets of classification. For instance, if the DSN is configured to perform digit recognition, then the output units may be representative of the values 0, 1, 2, 3, and so forth up to
9 with a 0-1 coding scheme. If the DSN is configured to perform speech recognition, then the output units may be representative of phones, HMM states of phones, or context-dependent HMM states of phones.
The lower-layer weight matrix, which we denote by W, connects the linear input layer and the hidden nonlinear layer. The upper-layer weight matrix, which we denote by U, connects the nonlinear hidden layer with the linear output layer. The weight matrix U can be determined through a closed-form solution given the weight matrix W when the mean square error training criterion is used.
As indicated above, the DSN includes a set of serially connected, overlapping, and layered modules, wherein each module has the same architecture – a linear input layer followed by a nonlinear hidden layer, which is connected to a linear output layer. Note that the output units of a lower module are a subset of the input units of an adjacent higher module in the DSN. More specifically, in a second module that is directly above the lowest module in the DSN, the input units can include the output units of the lowest module and optionally the raw input feature.
This pattern of including output units in a lower module as a portion of the input units in an adjacent higher module and thereafter learning a weight matrix that describes connection weights between hidden units and linear output units via convex optimization can continue for many modules. A resultant learned DSN may then be deployed in connection with an automatic classification task such as frame-level speech phone or state classification. Connecting the DSN's output to an HMM or any dynamic programming device enables continuous speech recognition and other forms of sequential pattern recognition.
Figure 6.1. A DSN architecture using input-output stacking. Four modules are illustrated, each with a distinct color. Dashed lines denote copying layers. [after (Tur et. al., 2012), @IEEE]
6.3 A Method for Learning the DSN Weights
W2
U2
W1
U1
Wrand
W3
Wrand
U3
Wrand
W4
U4
Here, we provide some technical detail on how the use of linear output units in the DSN facilitates the learning of the DSN weights. A single module is used to illustrate the advantage for simplicity reasons. First, it is clear that the upper layer weight matrix U can be efficiently learned once the activity matrix H over all training samples in the hidden layer is known. Let's denote the training vectors by 𝑿 = [𝒙1, ⋯, 𝒙𝑖, ⋯, 𝒙𝑁], in which each vector is denoted by 𝒙𝑖 = [𝑥1𝑖, ⋯, 𝑥𝑗𝑖, ⋯, 𝑥𝐷𝑖]
𝑇 where D is the dimension of the input vector, which is a function of the block, and 𝑁 is the total number of training samples. Denote by 𝐿 the number of hidden units and by 𝐶 the dimension of the output vector. Then the output of a DSN block is 𝒚𝑖 = 𝑼𝑇𝒉𝑖, where 𝒉𝑖 = 𝜎(𝑾𝑇𝒙𝑖) is the hidden-layer vector for sample i, 𝑼 is an 𝐿 × 𝐶 weight matrix at the upper layer of a block. 𝑾 is a 𝐷 × 𝐿 weight matrix at the lower layer of a block, and σ(∙) is a sigmoid function. Bias terms are implicitly represented in the above formulation if 𝒙𝑖 and 𝒉𝑖 are augmented with ones.
Given target vectors in the full training set with a total of N samples, 𝑻 = [𝒕1, ⋯, 𝒕𝑖, ⋯, 𝒕𝑁], where each vector is 𝒕𝑖 = [𝑡1𝑖, ⋯, 𝑡𝑗𝑖, ⋯, 𝑡𝐶𝑖]
𝑇, the parameters 𝑼 and 𝑾 are learned so as to minimize the average of the total square error below:
E = 1
2 ∑ ||𝒚𝑖 − 𝑖 𝒕𝑖||2 = 1
2 Tr[(𝐘 − 𝐓)(𝐘 − 𝐓)T], where the output of the network is 𝒚𝑖 = 𝑼𝑇𝒉𝑖 = 𝑼𝑇𝜎(𝑾𝑇𝒙𝑖) = 𝐺𝑖(𝑼, 𝑾)which depends on both weight matrices, as in the standard neural net. Assuming 𝑯 =
[𝒉1, ⋯, 𝒉𝑖, ⋯, 𝒉𝑁] is known, or equivalently, 𝑾 is known. Then, setting the error derivative with respective to U to zero gives
𝑼 = (𝑯𝑯𝑻)−1𝑯𝑻𝑇 = F(𝑾), where 𝒉𝑖 = 𝜎(𝑾𝑇𝒙𝑖).
This provides an explicit constraint between 𝑼 and 𝑾which were treated independently in the conventional backpropagation algorithm.
Now, given the equality constraint 𝑼 = F(𝑾), let's use Lagrangian multiplier method to solve the optimization problem in learning 𝑾. Optimizing the Lagrangian:
𝐸 =
2 ∑ ||𝐺𝑖(𝑼, 𝑾) − 𝑖 𝒕𝑖||2 + 𝜆 ||U − F(𝑾)||we can derive batch-mode gradient descent learning algorithm where the gradient takes the following form (Deng and Yu, 2011; Yu and Deng, 2012):
𝜕𝐸
𝜕𝑾 = 𝟐𝑿 [𝑯𝑇 ∘ (𝟏 − 𝑯)𝑇 ∘ [𝑯†(𝑯𝑻𝑇)(𝑻𝑯†) − 𝑻𝑇(𝑻𝑯†)]]where 𝑯† = 𝑯𝑇(𝑯𝑯𝑇)−𝟏 is pseudo-inverse of 𝑯 and symbol ∘ denotes element-wise multiplication.
Compared with conventional backpropagation, the above method has less noise in gradient computation due to the exploitation of the explicit constraint 𝑼 = F(𝑾). As such, it was found experimentally that, unlike backpropagation, batch training is effective, which aids parallel learning of the DSN.
6.4 The Tensor Deep Stacking Network
The above DSN architecture has recently been generalized to its tensorized version, which we call the tensor DSN (TDSN) (Hutchinson et al., 2012, 2013). It has the same scalability as the DSN in terms of parallelizability in learning, but it generalizes the DSN by providing higher-order feature interactions missing in the DSN.
The architecture of the TDSN is similar to that of the DSN in the way that stacking operation is carried out. That is, modules of the TDSN are stacked up in a similar way to form a deep architecture. The differences between the TDSN and the DSN lie mainly in how each module is constructed. In the DSN, we have one set of hidden units forming a hidden layer, as denoted at the left panel of Figure 6.2. In contrast, each module of a TDSN contains two independent hidden layers, denoted as "Hidden 1" and "Hidden 2" in the middle and right panels of Figure 6.2. As a result of this difference, the upper-layer weights, denoted by "U" in Figure 6.2, changes from a matrix (a two dimensional array) in the DSN to a tensor (a three dimensional array) in the TDSN, shown as a cube labeled by "U" in the middle panel.
Figure 6.2. Comparisons of a single module of a DSN (left) and that of a tensor DSN (TDSN).
Two equivalent forms of a TDSN module are shown to the right. [after (Hutchinson et. al., 2012), @IEEE]
The tensor U has a three-way connection, one to the prediction layer and the remaining to the two separate hidden layers. An equivalent form of this TDSN module is shown in the right panel of Figure 6.2, where the implicit hidden layer is formed by expanding the two separate hidden layersinto their outer product. The resulting large vector contains all possible pair-wise products for the two sets of hidden-layer vectors. This turns tensor U into a matrix again whose dimensions are 1) size of the prediction layer; and 2) product of the two hidden layers' sizes. Such equivalence enables the same convex optimization for learning U developed for the DSN to be applied to learning tensor U. Importantly, higher-order hidden feature interactions are enabled in the TDSN via the outer product construction for the large, implicit hidden layer.
Stacking the TDSN modules to form a deep architecture pursues in a similar way to the DSN by concatenating various vectors. Two examples are shown in Figure 6.3 and Figure 6.4. Note stacking by concatenating hidden layers with input (Figure 6.4) would be difficult for the DSN since its hidden layer tends to be too large for practical purposes.
Figure 6.3. Stacking of TDSN modules by concatenating prediction vector with input vector. [after(Hutchinson et. al., 2012), @IEEE]
Figure 6.4. Stacking of TDSN modules by concatenating two hidden-layers' vectors with the input vector.
6.5 The Kernelized Deep Stacking Network
The DSN architecture has also recently been generalized to its kernelized version, which we call the kernel-DSN (K-DSN) (Deng et al., 2012; Huang et al, 2013). The motivation of the extension is to increase the size of the hidden units in each DSN module, yet without increasing the size of the free parameters to learn. This goal can be easily accomplished using the kernel trick, resulting in the K-DSN which we describe below.
In the DSN architecture reviewed above optimizing the weight matrix U given the hidden layers' outputs in each module is a convex optimization problem. However, the problem of optimizing weight matrix 𝑾 and thus the whole network is non-convex. In a recent extension of DSN, a tensor structure was imposed, shifting most of the non-convex learning burden for 𝑾 to the convex optimization of U (Hutchinson et al, 2012; 2013). In the new K-DSN extension, we completely eliminate non-convex learning for 𝑾 using the kernel trick.
To derive the K-DSN architecture and the associated learning algorithm, we first take the bottom module of DSN as an example and generalize the sigmoidal hidden layer 𝒉𝑖 = 𝜎(𝑾𝑇𝒙𝑖) in the DSN module into a generic nonlinear mapping function 𝑮(𝑿) from the raw input feature 𝑿, with high dimensionality in 𝑮(𝑿) (possibly infinite) determined only implicitly by a kernel function to be chosen. Second, we formulate the constrained optimization problem ofminimize
2 Tr[𝑬𝑬T] +
𝐶
2 𝑼T𝑼subject to 𝐓-𝑼𝑇𝑮(𝑿) = E
Third, we make use of dual representations of the above constrained optimization problem to obtain U = 𝐆T𝒂, where vector 𝒂 takes the following forma = (𝐶𝑰 + 𝑲)−1𝑻and 𝑲 = 𝑮(𝑿)𝑮T(𝑿) is a symmetric kernel matrix with elements Knm = gT(xn)𝑔(xm).
Finally, for each new input vector x in the test or dev set, we obtain the K-DSN (bottom) module's prediction asy(x) = UT𝒈(𝒙) = 𝐚T𝑮(𝑿) 𝒈(𝒙)= 𝒌T(𝒙)(𝐶 𝑰 + 𝑲)−1𝑻where the kernel vector 𝒌(𝒙) is so defined that its elements have values of 𝑘𝑛(𝒙) = 𝑘(𝒙𝑛, 𝒙) in which 𝒙𝑛 is a training sample and 𝒙 is the current test sample.
For l-th module in K-DCN where 𝑙 ≥ 2, the kernel matrix is modified to
𝑲 = 𝑮 ([𝑿| 𝒀(𝑙−1)| 𝒀(𝑙−2)|.. 𝒀(1)]) 𝑮T ([𝑿| 𝒀(𝑙−1)| 𝒀(𝑙−2)|.. 𝒀(1)]).
The key advantages of K-DSN can be analyzed as follows. First, unlike DSN which needs to compute hidden units' output, the K-DSN does not need to explicitly compute hidden units' output
𝑮(𝑿) or 𝑮([𝑿| 𝒀(𝑙−1)| 𝒀(𝑙−2)|.. 𝒀(1)]). When Gaussian kernels are used, kernel trick equivalently gives us an infinite number of hidden units without the need to compute them explicitly. Further, we no longer need to learn the lower-layer weight matrix 𝑾 in DSN as described in (Deng et al, 2012) and the kernel parameter (e.g., the single variance parameter 𝜎 in the Gaussian kernel) makes K-DSN much less subject to overfitting than DSN. Figure 6.5 illustrates the basic architecture of a K-DSN using the Gaussian kernel and using three modules.
Figure 6.5. An example architecture of the K-DSN with three modules each of which uses a Gaussian kernel with different kernel parameters. [after (Deng. al., 2012), @IEEE]
The entire K-DSN with Gaussian kernels is characterized by two sets of module-dependent hyperparameters: 𝜎(𝑙) and 𝐶(𝑙), the kernel smoothing parameter and regularization parameter, respectively. While both parameters are intuitive and their tuning (via line search or leave-one-out cross validation) is straightforward for a single bottom module, tuning the full network with all the modules is more difficult. For example, if the bottom module is tuned too well, then adding more modules would not benefit much. In contrast, when the lower modules are loosely tuned (i.e., relaxed from the results obtained from straightforward methods), the overall K-DSN often performs much better. The experimental results reported by Deng et al. (2012) are obtained using a set of empirically determined tuning schedules to adaptively regularize the K-DSN from bottom to top modules.
The K-DSN described here has a set of highly desirable properties from the machine learning and pattern recognition perspectives. It combines the power of deep learning and kernel learning in a principled way and unlike the basic DSN there is no longer non-convex optimization problem( ) = ( ( )) T( ( ))( ) = ( ) T( );(2) = ( (2)) T( (2))involved in training the K-DSN. The computation steps make the K-DSN easier to scale up for parallel computing in distributed servers than the DSN and tensor-DSN. There are many fewer parameters in the K-DSN to tune than in the DSN, T-DSN, and DNN, and there is no need for pretraining. It is found in the study of (Deng et al., 2012) that regularization plays a much more important role in the K-DSN than in the basic DSN and Tensor-DSN. Further, effective regularization schedules developed for learning the K-DSN weights can be motivated by intuitive insight from useful optimization tricks such as the heuristic in Rprop or resilient backpropagation algorithm (Riedmiller and Braun, 1993).
However, as inherent in any kernel method, the scalability becomes an issue also for the K-DSN as the training and testing samples become very large. A solution is provided in the study by Huang et al. (2013), based on the use of random Fourier features, which possess the strong theoretical property of approximating the Gaussian kernel while rendering efficient computation in both training and evaluation of the K-DSN with large training samples. It is empirically demonstrated that just like the conventional K-DSN exploiting rigorous Gaussian kernels, the use of random
Fourier features also enables successful stacking of kernel modules to form a deep architecture.
CHAPTER 7
SELECTED APPLICATIONS IN SPEECH
AND AUDIO PROCESSING
7.1 Acoustic Modeling for Speech Recognition
As discussed in Chapter 2, speech recognition is the very first successful application of deep learning methods at an industry scale. This success is a result of close academic-industrial collaboration, initiated at Microsoft Research, with the involved researchers identifying and acutely attending to the industrial need for large-scale deployment (Deng et al., 2009; Yu et al., 2010c; Seide et al., 2011; Hinton et al, 2012; Dahl et al., 2012; Deng et al., 2013b). It is also a result of carefully exploiting the strengths of the deep learning and the then-state-of-the-art speech recognition technology, including notably the highly efficient decoding techniques.
Speech recognition has long been dominated by the GMM-HMM method, with an underlying shallow or flat generative model of context-dependent GMMs and HMMs (e.g., Rabiner, 1989;
Juang et al., 1986; Deng et al., 1990, 1991). Neural networks once were a popular approach but had not been competitive with the GMM-HMM (Waibel et al., 1989; Bourlard and Morgan, 1993;
Deng et al., 1994; Morgan, 2012). Generative models with deep hidden dynamics likewise have also not been clearly competitive (e.g., Picone et al., 1999; Deng, 1998; Bridle et al. 1998; Deng et al., 2006).
Deep learning and the DNN started making their impact in speech recognition in 2010, after close collaborations between academic and industrial researchers; see reviews in (Hinton et al., 2012;
Deng et al., 2013c). The collaborative work started in phone recognition tasks (Mohamed et al., 2009, 2010, 2012; Deng et al., 2010, 2013; Sivaram and Hermansky, 2012; Graves et al., 2013, 2013a; Sainath et al., 2011, 2013), demonstrating the power of hybrid DNN architectures discussed in Chapter 5 and of subsequent new architectures with convolutional and recurrent structure. The work also showed the importance of raw speech features of spectrogram --- back from the longpopular MFCC features toward but not yet reaching the raw speech-waveform level (e.g., Sheikhzadeh and Deng, 1994; Jaitly and Hinton, 2011). The collaboration continued to large vocabulary tasks with more convincing, highly positive results (Yu et al., 2010c; Dahl et al., 2011, 2012; Seide et al., 2011; Kubo et al., 2012; Hinton et al., 2012; Kingsbury et al., 2012; Deng et al., 2013a, 2013b; Su et al., 2013; Yan et al., 2013; Liao et al., 2013). The success in large vocabulary speech recognition is in large part attributed to the use of a very large DNN output layer structured in the same way as the GMM-HMM speech units (senones), motivated initially by the speech researchers' desires to take advantage of the context-dependent phone modeling techniques that have been proven to work well in the GMM-HMM framework, and to keep the change of the already highly efficient decoder software's infrastructure developed for the GMM-HMM systems to a minimum. In the meantime, this body of work also demonstrated the possibility to reduce the need for the DBN-like pre-training in effective learning of DNNs when a large amount of labeleddata is available. A combination of three factors helped to quickly spread the success of deep learning in speech recognition to the entire speech industry and academia: 1) minimal decoder changes required to deploy the new DNN-based speech recognizer due to the use of senones as the DNN output; 2) significantly lowered errors compared with the then-state-of-the-art GMM-HMM systems; and 3) reduced system complexity empowered by the DNN's strong modeling power. By the ICASSP-2013 timeframe, at least 15 major speech recognition groups worldwide confirmed experimentally the success of DNNs with very large tasks and with the use of raw speech spectral features other than MFCCs. The most notable groups include major industrial speech labs worldwide: Microsoft (Seide et al, 2011; Chen et al., 2012; Deng et al., 2013b, 2013c; Yan et al.
2013; Yu et al., 2013b), IBM (Sainath et al., 2011, 2013, 2013b; Kingsbury et al., 2012; Saon et al., 2013), Google (Jaitly et al., 2012; Dean et al., 2012; Heigold et al., 2013; Liao et al., 2013), iFlyTek, and Baidu. Their results represent a new state-of-the-art in speech recognition widely deployed in these companies' voice products and services with extensive media coverage in recent years.
In the remainder of this chapter, we review a wide range of speech recognition work based on deep learning methods according to several major themes expressed in the section titles.
7.1.1 Back to primitive spectral features of speech
Deep learning, also referred as representation learning or (unsupervised) feature learning, sets an important goal of automatic discovery of powerful features from raw input data independent of application domains. For speech feature learning and for speech recognition, this goal is condensed to the use of primitive spectral or possibly waveform features. Over the past 30 years or so, largely
"hand-crafted" transformations of speech spectrogram have led to significant accuracy improvements in the GMM-based HMM systems, despite the known loss of information from the raw speech data. The most successful transformation is the non-adaptive cosine transform, which gave rise to Mel-frequency cepstral coefficients (MFCC) features. The cosine transform approximately de-correlates feature components, which is important for the use of GMMs with diagonal covariance matrices. However, when GMMs are replaced by deep learning models such as DNNs, deep belief nets (DBNs), or deep autoencoders, such de-correlation becomes irrelevant due to the very strength of the deep learning methods in modeling data correlation. As discussed in detail in Chapter 4, early work of (Deng et al., 2010) demonstrated this strength and in particular the benefit of spectrograms over MFCCs in effective coding of bottleneck speech features using autoencoders in an unsupervised manner.
The pipeline from speech waveforms (raw speech features) to MFCCs and their temporal differences goes through intermediate stages of log-spectra and then (Mel-warped) filter-banks, with learned parameters based on the data. An important character of deep learning is to move away from separate design of feature representations and of classifiers. This idea of jointly learning classifier and feature transformation for speech recognition was already explored in early studies on the GMM-HMM based systems; e.g., (Chengalvarayan and Deng, 1997; 1997a; Rathinavalu and Deng, 1997). However, greater speech recognition performance gain is obtained only recently in the recognizers empowered by deep learning methods. For example, Li et al., (2012) and Deng et al., (2013a) showed significantly lowered speech recognition errors using large-scale DNNswhen moving from the MFCC features back to more primitive (Mel-scaled) filter-bank features.
These results indicate that DNNs can learn a better transformation than the original fixed cosine transform from the Mel-scaled filter-bank features.
Compared with MFCCs, "raw" spectral features not only retain more information, but also enable the use of convolution and pooling operations to represent and handle some typical speech invariance and variability --- e.g., vocal tract length differences across speakers, distinct speaking styles causing formant undershoot or overshoot, etc. --- expressed explicitly in the frequency domain. For example, the convolutional neural network (CNN) can only be meaningfully and effectively applied to speech recognition (Abdel-Hamid et al., 2012; 2013, 2013a; Deng et al., 2013) when spectral features, instead of MFCC features, are used.
More recently, Sainath et al. (2013b) went one step further toward raw features by learning the parameters that define the filter-banks on power spectra. That is, rather than using Mel-warped filter-bank features as the input features as in (Abdel-Hamid et al., 2012; 2013; Li et al., 2012;
Chengalvarayan and Deng, 1997), the weights corresponding to the Mel-scale filters are only used to initialize the parameters, which are subsequently learned together with the rest of the deep network as the classifier. The overall architecture of the jointly learned feature generator and classifier is shown in Figure 7.1. Substantial speech recognition error reduction is reported in(Sainath et al., 2013b).
Figure 7.1. Illustration of the joint learning of filter parameters and the rest of the deep network.
Adopted from [after (Sainath et al., 2013b), @IEEE].
It has been shown that not only learning the spectral aspect of the features are beneficial for speech recognition, learning the temporal aspect of the features is also helpful (Siniscalchi et al. 2013).
Further, Yu et al. (2013a) carefully analyzed the properties of different layers in the DNN as the layer-wise extracted features starting from the lower raw filter-bank features. They found that the improved speech recognition accuracy achieved by the DNNs partially attributes to DNN's ability to extract discriminative internal representations that are robust to the many sources of variability in speech signals. They also show that these representations become increasingly insensitive to small perturbations in the input at higher layers, which helps to achieve better speech recognition accuracy.
To the extreme end, deep learning would promote to use the lowest level of raw features of speech, i.e., speech sound waveforms, for speech recognition, and learn the transformation automatically.
As an initial attempt toward this goal the study carried out by Jaitly and Hinton (2011) makes use of speech sound waves as the raw input feature to an RBM with a convolutional structure as the classifier. With the use of rectified linear units in the hidden layer (Glorot et al., 2011), it is possible, to a limited extent, to automatically normalize the amplitude variation in the waveform signal.
Although the final results are disappointing, the work shows that much work is needed along this direction. For example, just as demonstrated by Sainath et al. (2013b) that the use of raw spectra as features requires additional attention in normalization than MFCCs, the use of speech waveforms demands even more attention (e.g., Sheikhzadeh and Deng, 1994). This is true for both
GMM-based and deep learning based methods.
7.1.2 The DNN-HMM architecture vs. use of DNN-derived features
Another major theme in the recent studies reported in the literature on applying deep learning methods to speech recognition is two disparate ways of using the DNN: 1) Direct applications of the DNN-HMM architecture as discussed in Chapter 5.3 to perform speech recognition; and 2)
The use of DNNs to extract or derive features, which are then fed into a separate sequence classifier.
In the speech recognition literature (e.g., Bourlard and Morgan, 1993), a system, in which a neural network's output is directly used to estimate the emission probabilities of an HMM, is often called an ANN/HMM hybrid system. This should be distinguished from the use of "hybrid" in Chapter
5 and throughout this book, where a hybrid of unsupervised pre-training and of supervised fine tuning is exploited to learn the parameters of DNNs.
The DNN-HMM architecture as a recognizer
An early DNN-HMM architecture (Mohamed et al., 2009) was presented at the NIPS Workshop(Deng, Yu, Hinton, 2009), developed, analyzed, and assisted by University of Toronto and MSR speech researchers. In this work, a five-layer DNN (called the DBN in the paper) was used to replace the Gaussian mixture models in the GMM-HMM system, and the monophone state was used as the modeling unit. Although monophones are generally accepted as a weaker phonetic representation than triphones, the DNN-HMM approach with monophones was shown to achieve higher phone recognition accuracy than the state-of-the-art triphone GMM-HMM systems. Further, the DNN results were found to be slightly superior to the then-best-performing single system based on the generative hidden trajectory model (HTM) in the literature (Deng et al., 2006, 2007) evaluated on the same, commonly used TIMIT task by many speech researchers (e.g., Ostendorf et al., 1996; Deng et al., 2006; Sainath et al., 2011a). At MSR, Redmond, the error patterns produced by these two separate systems (the DNN vs. the HTM) were carefully analyzed and found to be very different, reflecting distinct core capabilities of the two approaches and igniting intensive further studies on the DNN-HMM approach described below.
MSR and University of Toronto researchers (Yu et al., 2010c; Dahl et al., 2011, 2012) extended the DNN-HMM system from the monophone phonetic representation of the DNN outputs to the triphone or context-dependent counterpart and from phone recognition to large vocabulary speech recognition. Experiments conducted at MSR on the 24-hr and 48-hr Bing mobile voice search datasets collected under the real usage scenario demonstrate that the context-dependent DNNHMM significantly outperforms the state-of-the-art HMM system. Three factors, in addition to the use of the DNN, contribute to the success: the use of triphones as the DNN modeling units, the use of the best available tri-phone GMM-HMM to generate the tri-phone state alignment, and the effective exploitation of a long window of input features. Experiments also indicate that the decoding time of a five-layer DNN-HMM is almost the same as that of the state-of-the-art triphone
GMM-HMM.
The success was quickly extended to large vocabulary speech recognition tasks with hundreds and even thousands of hours of training set and with thousands of tri-phone states, including the Switchboard and Broadcast News databases, and Google's voice search and YouTube tasks (Seide et al., 2011; Sainath et al., 2011; Jaitly et al., 2012; Hinton et al., 2012; Deng et al., 2013a; Sainath et al, 2013). For example, on the Switchboard benchmark, the context-dependent DNN-HMM(CD-DNN-HMM) is shown to cut error by one third compared to the state-of-the-art GMM-HMM system (Seide et al., 2011). As a summary, we show in Table 7.1 some quantitative recognition error rates produced by the DNN-HMM architecture in comparison with those by the previous state of the art systems based on the generative models. Note from sub-tables A to D, the training data are increased approximately one order of magnitude from one task to the next. Not only the computation scales up well (i.e., almost linearly) with the training size, but most importantly the relative error rate reduction increases substantially with increasing amounts of training data --from approximately 10% to 20%, and then to 30%. This set of results highlight the strongly desirable properties of the DNN-based methods, despite the conceptual simplicity of the overall
DNN-HMM architecture and some known weaknesses.
Table 7.1. Comparisons of the DNN-HMM architecture with the generative model (e.g., the GMMHMM) in terms of phone or word recognition error rates. From sub-tables A to D, the training data are increased approximately three orders of magnitudes.
The use of DNN-derived features in a separate recognizer
One clear weakness of the above DNN-HMM architecture for speech recognition is that much of the highly effective techniques for the GMM-HMM systems, including discriminative training (in both feature space and model space), unsupervised speaker adaptation, noise robustness, and scalable batch training tools for big training data, developed over the past 20 some years may not be directly applicable to the new systems although similar techniques have been recently developed for DNN-HMMs. To remedy this problem, the "tandem" approach, developed originally by Hermansky et al. (2000), has been adopted, where the output of the neural networks in the form of posterior probabilities of the phone classes, are used, often in conjunction with the acoustic features to form new augmented input features, in a separate GMM-HMM system.
This tandem approach is used by Vinyals and Ravuri (2011) where a DNN's outputs are extracted to serve as the features for mismatched noisy speech. It is reported that DNNs outperform the neural networks with a single hidden layer under the clean condition, but the gains slowly diminish as the noise level is increased. Furthermore, using MFCCs in conjunction with the posteriors computed from DNNs outperforms using the DNN features alone in low to moderate noise conditions with the tandem architecture. Comparisons of such tandem approach with the direct
DNN-HMM approach are made by Tüske et al. (2012) and Imseng et al. (2013).
An alternative way of extracting the DNN features is to use the "bottleneck" layer, which is narrower than other layers in the DNN, to restrict the capacity of the network. Then, such bottleneck features are fed to a GMM-HMM system, often in conjunction with the original acoustic features and some dimensionality reduction techniques. The bottleneck features derived from the DNN are believed to capture information complementary to conventional acoustic features derived from the short-time spectra of the input. A speech recognizer based on the above bottleneck feature approach is built by Yu and Seltzer (2011), with the overall architecture shown in Figure 7.2.
Several variants of the DNN-based bottleneck-feature approach have been explored; see details in(Bell, et al., 2013; Lal, et al., 2013; Sainath et al., 2012; Tüske et al., 2012; Plahl et al., 2010).
Figure 7.2. Illustration of the use of bottleneck (BN) features extracted from a DNN in a GMMHMM speech recognizer. [after (Yu and Seltzer, 2011), @IEEE].
Yet another method to derive the features from the DNN is to feed its top-most hidden layer as the new features for a separate speech recognizer. In (Yan et al., 2013), a GMM-HMM is used as such a recognizer, and the high-dimensional, DNN-derived features are subject to dimensionality reduction before feeding them into the recognizer. More recently, a recurrent neural network (RNN) is used as the "backend" recognizer receiving the high-dimensional, DNN-derived features as the input without dimensionality reduction (Chen and Deng, 2013; Deng and Chen, 2014). These studies also show that the use of the top-most hidden layer of the DNN as features is better than other hidden layers and also better than the output layer in terms of recognition accuracy for the RNN sequence classifier.
7.1.3 Noise robustness by deep learning
The study of noise robustness in speech recognition has a long history, mostly before the recent rise of deep learning. One major contributing factor to the often observed brittleness of speech recognition technology is the inability of the standard GMM-HMM-based acoustic model to accurately model noise-distorted speech test data that differs in character from the training data, which may or may not be distorted by noise. A wide range of noise-robust techniques developed over past 30 years can be analyzed and categorized using five different criteria: 1) feature-domain vs. model-domain processing, 2) the use of prior knowledge about the acoustic environmentdistortion, 3) the use of explicit environment-distortion models, 4) deterministic vs. uncertainty processing, and 5) the use of acoustic models trained jointly with the same feature enhancement or model adaptation process used in the testing stage. See a comprehensive review in (Li et al., 2014) and some additional review literature or original work in (Gales, 2011; Lu et al., 2013;
Yoshioka and Nakatani, 2013; Wang and Gales, 2012; Zhao and Juang, 2012; Hain et al., 2012; van Dalen, et al., 2011; Yu et al., 2009; Acero et al., 2000; Deng et al., 2000).
Many of the model-domain techniques developed for GMM-HMMs (e.g., model-domain noise robustness techniques surveyed by Li et al. (2014) and Gales (2011)), are not directly applicable to the new deep learning models for speech recognition. The feature-domain techniques, however, can be directly applied to the DNN system. A detailed investigation of the use of DNNs for noise robust speech recognition in the feature domain is reported by Seltzer et al. (2013), who apply the C-MMSE (Yu et al., 2008) feature enhancement algorithm on the input feature used in the DNN.
By processing both the training and testing data with the same algorithm, any consistent errors or artifacts introduced by the enhancement algorithm can be learned by the DNN-HMM recognizer.
This study also successfully explores the use of the noise aware training paradigm for training the DNN, where each observation is augmented with an estimate of the noise. Strong results are obtained on the Aurora4 task. More recently, Kashiwagi et al. (2013) applies the SPLICE feature enhancement technique (Deng et al., 2000, 2001) to a DNN speech recognizer. In that study the DNN's output layer is determined on the clean data instead of the noisy data as in the study by
Seltzer et al. (2013).
Besides DNN, other deep architectures have also been proposed to perform feature enhancement and noise-robust speech recognition. For example, Mass et al. (2012) applied a deep recurrent auto encoder neural network to remove noise in the input features for robust speech recognition. The model is trained on stereo (noisy and clean) speech features to predict clean features given noisy input, similar to the SPLICE setup but using a deep model instead of a GMM. Vinyals and Ravuri(2011) investigated the tandem approaches to noise-robust speech recognition, where DNNs are trained directly with noisy speech to generate posterior features.
7.1.4 Output representations in the DNN
Most deep learning methods for speech recognition and other information processing applications have focused on learning representations from input acoustic features without paying attention to output representations. The recent 2013 NIPS Workshop on Learning Output Representations(http://nips.cc/Conferences/2013/Program/event.php?ID=3714) was dedicated to bridging this gap.
For example, the Deep Visual-Semantic Embedding Model described in (Frome et al., 2013, to be discussed more in Chapter 11) exploits continuous-valued output representations obtained from the text embeddings to assist in the branch of the deep network for classifying images. For speech recognition, importance of designing effective linguistic representations for the output layers of deep networks is highlighted in (Deng, 2013).
Most current DNN systems use a high-dimensional output representation to match the context-dependent phonetic states in the HMMs. For this reason, the output layer evaluation can cost 1/3 of the total computation time. To improve the decoding speed, techniques such as low-rank approximation is typically applied to the output layer. In (Sainath et al., 2013c)and (Xue et al., 2013), the DNN with high-dimensional output layer was trained first. The singular value decomposition (SVD)-based dimension reduction technique was then performed on the large output-layer matrix. The resulting matrices are further combined and as the result the original large weight matrix is approximated by a product of two much smaller matrices. This technique in essence converts the original large output layer to two layers – a bottleneck linear layer and a nonlinear output layer --- both with smaller weight matrices. The converted DNN with reduced dimensionality in is further refined. The experimental results show that no speech recognition accuracy reduction was observed even when the size is cut to 1/3, while the run-time computation is significantly reduced.
The output representations for speech recognition can benefit from the structured design of the symbolic or phonological units of speech as presented in (Deng, 2013). The rich phonological structure of symbolic nature in human speech has been well known for many years. Likewise, it has also been well understood for a long time that the use of phonetic or its finer state sequences, even with contextual dependency, in engineering speech recognition systems, is inadequate in representing such rich structure (e.g., Deng and Erler, 1992; Ostendorf, 1999; Sun and Deng, 2002), and thus leaving a promising open direction to improve the speech recognition systems' performance. Basic theories about the internal structure of speech sounds and their relevance to speech recognition technology in terms of the specification, design, and learning of possible output representations of the underlying speech model for speech target sequences are surveyed in (Deng and O'Shaughnessy, 2003) and more recently in (Deng, 2013).
There has been a growing body of deep learning work in speech recognition with their focus placed on designing output representations related to linguistic structure. In (Wang and Sim, 2013; 2014), a limitation of the output representation design, based on the context-dependent phone units as proposed by Dahl et al. (2012), is recognized and a solution is offered. The root cause of this limitation is that all context-dependent phone states within a cluster created by the decision tree share the same set of parameters and this reduces its resolution power for fine-grained states during the decoding phase. The solution proposed formulates output representations of the contextdependent DNN as an instance of the canonical state modeling technique, making use of broad phonetic classes. First, triphones are clustered into multiple sets of shorter bi-phones using broad phone contexts. Then, the DNN is trained to discriminate the bi-phones within each set. Logistic regression is used to transform the canonical states into the detailed triphone state output probabilities. That is, the overall design of the output representation of the context-dependent DNN is hierarchical in nature, solving both the data sparseness and low-resolution problems at the same time.
Related work on designing the output linguistic representations for speech recognition can be found in (Ko and Mak, 2013) and in (McGraw et al., 2013). While the designs are in the context of GMM-HMM-based speech recognition systems, they both can be extended to deep learning models.
7.1.5 Adaptation of the DNN-based speech recognizers
The DNN-HMM is an advanced version of the artificial neural network and HMM hybrid system developed in 1990s, for which several adaptation techniques have been developed. Most of these techniques are based on linear transformation of the network weights of either input or output layers. Some initial work on DNN adaptation makes use of the same or related linear transformation methods (e.g., Yao et al., 2012; 2013a). However, compared with the earlier narrower and shallower neural network systems, the DNN-HMM has significantly more parameters due to wider and deeper hidden layers used and the much larger output layer designed to model context dependent phones and states. This difference casts additional challenges to adapting the DNN-HMM, especially when the adaptation data is small. Here we discuss three recent studies on overcoming such challenges in adapting the large-sized DNN weights in three distinct ways.
Yu et al. (2013b) proposed a regularized adaptation technique for DNNs. It adapts the DNN weights conservatively by forcing the distribution estimated from the adapted model to be close to that estimated from those before the adaptation. This constraint is realized by adding Kullback–
Leibler divergence (KLD) regularization to the adaptation criterion. This type of regularization is shown to be equivalent to a modification of the target distribution in the conventional backpropagation algorithm and thus the training of the DNN remains largely unchanged. The new target distribution is derived to be a linear interpolation of the distribution estimated from the model before adaptation and the ground truth alignment of the adaptation data. This interpolation prevents overtraining by keeping the adapted model from straying too far from the speakerindependent model. This type of adaptation differs from L2 regularization which constrains the model parameters themselves rather than the output probabilities.
In (Siniscalchi et al., 2013a), adaptation of the DNN is applied not on the conventional network weights by on the hidden activation functions. In this way, the main limitation of current adaptation techniques based on adaptable linear transformation of the network weights in either the input or the output layer is effectively overcome, since the new method only needs to adapt limited hidden activation function.
Most recently, Saon et al. (2013) explore a new and highly effective method in adapting DNNs for speech recognition. The method combines I-vector features with fMLLR (feature-domain MaxLikelihood Linear Regression) features as the input into a DNN. I-vectors or (speaker) identity vectors are commonly used for speaker verification and speaker recognition applications, as they encapsulate relevant information about a speaker's identity in a low-dimensional feature vector.
The fMLLR is an effective adaptation technique developed for GMM-HMM systems. Since Ivectors do not obey locality in frequency, they must be combined carefully with the fMLLR features that obey locality. The architecture of multi-scale CNN-DNN is shown to be effective for the combination of these two different types of features. During both training and decoding, the speaker-specific I-vector is appended to the frame-based fMLLR features.
7.1.6 Better architectures and nonlinear units
Over recent years, since the success of the (fully-connected) DNN-HMM hybrid system was demonstrated in (Mohamed et al., 2009, 2012; Deng et al., 2009; Yu et al., 2010; Dahl et al., 2011, 2012; Seide et al., 2011; Sainath et al., 2011, 2012; Hinton et al., 2012), many new architectures and nonlinear units have been proposed and evaluated for speech recognition. Here we provide an overview of this progress, extending the overview provided in (Deng et al., 2013b).
The tensor version of the DNN is reported by Yu et al. (2012c, 2013), which extends the conventional DNN by replacing one or more of its layers with a double-projection layer and a tensor layer. In the double-projection layer, each input vector is projected into two nonlinear subspaces. In the tensor layer, two subspace projections interact with each other and jointly predict the next layer in the overall deep architecture. An approach is developed to map the tensor layers to the conventional sigmoid layers so that the former can be treated and trained in a similar way to the latter. With this mapping the tensor version of the DNN can be treated as the DNN augmented with double-projection layers so that the backpropagation learning algorithm can be cleanly derived and relatively easily implemented.
A related architecture to the above is the tensor version of the DSN described in Chapter 6, also usefully applied to speech classification and recognition (Hutchinson et al., 2012, 2013). The same approach applies to mapping the tensor layers (i.e., the upper layer in each of the many modules in the DSN context) to the conventional sigmoid layers. Again, this mapping simplifies the training algorithm so that it becomes not so far apart from that for the DSN.
As discussed in Chapter 3.2, the concept of convolution in time was originated in the TDNN (timedelay neural network) as a shallow neural network (Lang et al., 1990; Waibel et al., 1989) developed during early days of speech recognition. Only recently and when deep architectures (e.g. deep Convolutional Neural Network or deep CNN) were used, it has been found that frequencydimension weight sharing is more effective for high-performance phone recognition, when the HMM is used to handle the time variability, than time-domain weight sharing as in the previous
TDNN in which the HMM was not used (Abdel-Hamid et al., 2012, 2013, 2013a; Deng et al., 2013). These studies also show that designing the pooling in the deep CNN to properly trade-off between invariance to vocal tract length and discrimination between speech sounds, together with a regularization technique of "dropout" (Hinton et al., 2012a), leads to even better phone recognition performance. This set of work further points to the direction of trading-off between trajectory discrimination and invariance expressed in the whole dynamic pattern of speech defined in mixed time and frequency domains using convolution and pooling. Moreover, the most recent studies reported in (Sainath et al., 2013, 2013a; 2013e) show that CNNs also benefit large vocabulary continuous speech recognition. They further demonstrate that multiple convolutional layers provide even more improvement when the convolutional layers use a large number of convolution kernels or feature maps. In particular, Sainath et al. (2013e) extensively explored many variants of the deep CNN. In combination with several novel methods the deep CNN is shown to produce state of the art results in a few large vocabulary speech recognition tasks.
In addition to the DNN, CNN, and DSN, as well as their tensor versions, other deep models have also been developed and reported in the literature for speech recognition. For example, the deepstructured CRF, which stacks many layers of CRFs, have been usefully applied to the task of language identification (Yu et al., 2010), phone recognition (Yu and Deng, 2010), sequential labeling in natural language processing (Yu et al., 2010a), and confidence calibration in speech recognition (Yu et al., 2010b). More recently, Demuynck and Triefenbach (2013) developed the deep GMM architecture, where the aspects of DNNs that lead to strong performance are extracted and applied to build hierarchical GMMs. They show that by going "deep and wide" and feeding windowed probabilities of a lower layer of GMMs to a higher layer of GMMs, the performance of the deep-GMM system can be made comparable to a DNN. One advantage of staying in the GMM space is that the decades of work in GMM adaptation and discriminative learning remains applicable.
Perhaps the most notable deep architecture among all is the recurrent neural network (RNN) as well as its stacked or deep version (Graves et al., 2013, 2013a; Hermans and Schrauwen, 2013).
While the RNN saw its early success in phone recognition (Robinson, 1994), it was not easy to duplicate due to the intricacy in training, let alone to scale up for larger speech recognition tasks.
Learning algorithms for the RNN have been dramatically improved since then, and much better results have been obtained recently using the RNN (Graves, et al, 2006; Maas et al., 2012; Chen and Deng, 2013), especially when the bi-directional LSTM (long short-term memory) is used(Graves et al., 2013, 2013a). The basic information flow in the bi-directional RNN and a cell of LSTM is shown in Figures 7.3 and 7.4 respectively.
Figure 7.3. Information flow in the bi-directional RNN, with both diagrammatic and mathematical descriptions. W's are weight matrices, not shown but can be easily inferred in the diagram. [after(Graves et al., 2013), @IEEE].
Figure 7.4. Information flow in an LSTM unit of the RNN, with both diagrammatic and mathematical descriptions. W's are weight matrices, not shown but can easily be inferred in the diagram. [after (Graves et al., 2013), @IEEE].
Learning the RNN parameters is known to be difficult due to vanishing or exploding gradients(Pascanu et al., 2013). Chen and Deng (2013) and Deng and Chen (2014) developed a primal-dual training method that formulates the learning of the RNN as a formal optimization problem, where cross entropy is maximized subject to the condition that the infinity norm of the recurrent matrix of the RNN is less than a fixed value to guarantee the stability of RNN dynamics. Experimental results on phone recognition demonstrate: 1) the primal-dual technique is highly effective in learning RNNs, with superior performance to the earlier heuristic method of truncating the size of the gradient; 2) The use of a DNN to compute high-level features of speech data to feed into the RNN gives much higher accuracy than without using the DNN; and 3) The accuracy drops progressively as the DNN features are extracted from higher to lower hidden layers of the DNN.
A special case of the RNN is reservoir models or echo state networks, where the output layers are fixed to be linear instead of nonlinear as in the regular RNN, and where the recurrent matrices are carefully designed but not learned. The input matrices are also fixed and not learned, due partly to the difficulty of learning. Only the weight matrices between the hidden and output layers are learned. Since the output layer is linear, the learning is very efficient and with global optimumachievable by a closed-form solution. But due to the fact that many parameters are not learned, the hidden layer needs to be very large in order to obtain good results. Triefenbach et al. (2013) applied such models to phone recognition, with reasonably good accuracy obtained.
Palangi et al. (2013a) presented an improved version of the reservoir model by learning both the input and recurrent matrices which were fixed in the previous model that makes use of the linear output (or readout) units to simplify the learning of only the output matrix in the RNN. Rather, a special technique is devised that takes advantage of the linearity in the output units in the reservoir model to learn the input and recurrent matrices. Compared with the backpropagation through time(BPTT) algorithm commonly used in learning the general RNNs, the proposed technique makes use of the linearity in the output units to provide constraints among various matrices in the RNN, enabling the computation of the gradients as the learning signal in an analytical form instead of by recursion as in the BPTT.
In addition to the recent innovations in better architectures of deep learning models for speech recognition reviewed above, there is also a growing body of work on developing and implementing better nonlinear units. Although sigmoidal and tanh functions are the most commonly used nonlinear types in DNNs their limitations are well known. For example, it is slow to learn the whole network due to weak gradients when the units are close to saturation in both directions.
Jaitly and Hinton (2011) appear to be the first to apply the rectified linear units (ReLU) in the DNNs to speech recognition to overcome the weakness of the sigmoidal units. ReLU refers to the units in a neural network that use the activation function of𝑓(𝑥) = max(0, 𝑥). Dahl et al. (2013) and Mass et al. (2013) successfully applied ReLU to large vocabulary speech recognition, with the best accuracy obtained when combining ReLU with the "Dropout" regularization technique.
Another new type of DNN units demonstrated more recently to be useful for speech recognition is the "maxout" units, which were used for forming the deep maxout network as described in (Miao et al., 2013). A deep maxout network consists of multiple layers which generate hidden activations via the maximum or "maxout" operation over a fixed number of weighted inputs called a "group".
This is the same operation as the max pooling used in the CNN as discussed earlier for both speech recognition and computer vision. The maximal value within each group is taken as the output from the previous layer. Most recently, Zheng et al. (2014) generalize the above "maxout" units to two new types. The "soft-maxout" type of units replace the original max operation with the soft-max function. The second, p-norm type of units used the nonlinearity of y = ||x||p. It is shown experimentally that the p-norm units with p=2 perform consistently better than the maxout, tanh, and ReLU units.
Finally, Srivastava et al. (2013) propose yet another new type of nonlinear units, called winnertake-all units. Here, local competition among neighboring neurons are incorporated into the otherwise regular feed-forward architecture, which is then trained via backpropagation with different gradients than the normal one. Winner-take-all is an interesting new form of nonlinearity, and it forms groups of (typically two) neurons where all the neurons in a group are made zerovalued except the one with the largest value. Experiments show that the network does not forget as much as networks with standard sigmoidal nonlinearity. This new type of nonlinear units are yet to be evaluated in speech recognition tasks.
7.1.7 Better optimization and regularization
Another area where significant advances are made recently in applying deep learning to acoustic model for speech recognition is on optimization criteria and methods, as well as on the related regularization techniques to help prevent overfitting during the deep network training.
One of the early studies on DNNs for speech recognition, conducted at Microsoft Research and reported in (Mohamed et al., 2010), first recognizes the mismatch between the desired error rate and the cross-entropy training criterion in the conventional DNN training. The solution is provided by replacing the frame-based, cross-entropy training criterion with the full-sequence-based maximum mutual information optimization objective. Equivalently, this amounts to putting the model of conditional random field (CRF) at the top of the DNN, replacing the original softmax layer which naturally leads to cross entropy. (Note the DNN was called the DBN in the paper).
This new sequential discriminative learning technique is developed to jointly optimize the DNN weights, CRF transition weights, and bi-phone language model. Importantly, the speech task is defined in TIMIT, with the use of a simple bi-phone-gram "language" model. The simplicity of the bi-gram language model enables the full-sequence training to carry out without the need to use lattices, drastically reducing the training complexity.
As another way to motivate the full-sequence training method of (Mohamed et al., 2010), we note that the earlier DNN phone recognition experiments made use of the standard frame-based objective function in static pattern classification, cross-entropy, to optimize the DNN weights. The transition parameters and language model scores were obtained from an HMM and were trained independently of the DNN weights. However, it has been known during the long history of the HMM research that sequence classification criteria can be very helpful in improving speech and phone recognition accuracy. This is because the sequence classification criteria are more directly correlated with the performance measure (e.g., the overall word or phone error rate) than framelevel criteria. More specifically, the use of frame-level cross entropy to train the DNN for phone sequence recognition does not explicitly take into account the fact that the neighboring frames have smaller distances between the assigned probability distributions over phone class labels. To overcome this deficiency, one can optimize the conditional probability of the whole sequence of labels, given the whole visible feature utterance or equivalent the hidden feature sequence extracted by DNN. To optimize the log conditional probability on the training data, the gradient can be taken over the activation parameters, transition parameters and lower-layer weights, and then pursue back-propagation of the error defined at the sentence level. We remark that in a much earlier study (LeCun et al., 1998), combining a neural network with a CRF-like structure was done, where the mathematical formulation appears to include CRFs as a special case. Also, the benefit of using the full-sequence classification criteria was shown earlier on shallow neural networks in (Kingsbury 2009; Prabhavalkar and Fosler-Lussier, 2010).
In implementing the above full-sequence learning algorithm for the DNN system as described in(Mohamed et al., 2010), the DNN weights are initialized using the frame-level cross entropy as the objective. The transition parameters are initialized from the combination of the HMM transition matrices and the "bi-phone language" model scores, and are then further optimized by tuning the transition features while fixing the DNN weights before the joint optimization. Usingjoint optimization with careful scheduling to reduce overfitting, it is shown that the full-sequence training outperforms the DNN trained with frame-level cross entropy by approximately 5% relative (Mohamed et al., 2010). Without the effort to reduce overfitting, it is found that the DNN trained with MMI is much more prone to overfitting than that trained with frame-level cross entropy. This is because the correlations across frames in speech tend to be different among the training, development, and test data. Importantly, such differences do not show when frame-based objective functions are used for training.
For large vocabulary speech recognition where more complex language models are in use, the optimization methods for full-sequence training of the DNN-HMM are much more sophisticated.
Kingsbury et al. (2012) reported the first success of such training using parallel, second-order, Hessian-free optimization techniques, which are carefully implemented for large vocabulary speech recognition. Sainath et al. (2013d) improved and speeded up the Hessian-free techniques by reducing the number of Krylov subspace solver iterations (Vinyals and Povey, 2012), which are used for implicit estimation of the Hessian. They also use sampling methods to decrease the amount of training data to speed up the training. While the batch-mode, second-order Hessian-free techniques prove successful for full-sequence training of large-scale DNN-HMM systems, the success of the first-order stochastic gradient descent methods is also reported recently (Su et al., 2013). It is found that heuristics are needed to handle the problem of lattice sparseness. That is, the DNN must be adjusted to the updated numerator lattices by additional iterations of frame-based cross-entropy training. Further, artificial silence arcs need to be added to the denominator lattices, or the maximum mutual information objective function needs to be smoothed with the frame-based cross entropy objective. The conclusion is that for large vocabulary speech recognition tasks with sparse lattices, the implementation of the sequence training requires much greater engineering skills than the small tasks such as reported in (Mohamed et al., 2010), although the objective function as well as the gradient derivation are essentially the same. Similar conclusions are reached by Vesely et al. (2013) when carrying out full-sequence training of DNN-HMMs for largevocabulary speech recognition. However, different heuristics from (Su et al., 2013) are shown to be effective in the training. Separately, Wiesler et al. (2013) investigated the Hessian-free optimization method for training the DNN with the cross-entropy objective and empirically analyzed the properties of the method. And finally, Dognin and Goel (2013) combined stochastic average gradient and Hessian-free optimization for sequence training of deep neural networks with success in that the training procedure converges in about half the time compared with the full
Hessian-free sequence training.
For large DNN-HMM systems with either frame-level or sequence-level optimization objectives, speeding up the training is essential to take advantage of large amounts of training data and of large model sizes. In addition to the methods described above, Dean et al. (2012) reported the use of the asynchronous stochastic gradient descent (ASGD) method, the adaptive gradient descent(Adagrad) method, and the large-scale limited-memory BFGS (L-BFGS) method for very large vocabulary speech recognition. Sainath et al. (2013) provided a review of a wide range of optimization methods for speeding up the training of DNN-based systems for large speech recognition tasks.
In addition to the advances described above focusing on optimization with the fully supervised learning paradigm, where all training data contain the label information, the semi-supervisedtraining paradigm is also exploited for learning DNN-HMM systems for speech recognition. Liao et al. (2013) reported the exploration of using semi-supervised training on the DNN-HMM system for the very challenging task of recognizing YouTube speech. The main technique is based on the use of "island of confidence" filtering heuristics to select useful training segments. Separately, semi-supervised training of DNNs is explored by Vesely et al. (2013), where self-training strategies are used as the basis for data selection using both the utterance-level and frame-level confidences. Frame-selection based on per-frame confidences derived from confusion in a lattice is found beneficial. Huang et al. (2013) reported another variant of semi-supervised training technique in which multi-system combination and confidence recalibration is applied to select the training data. Further, Thomas et al. (2013) overcome the problem of lacking sufficient training data for acoustic modeling in a number of low-resource scenarios. They make use of transcribed multilingual data and semi-supervised training to build the proposed feature front-ends for subsequent speech recognition.
Finally, we see important progress in deep learning based speech recognition in recent years with the introduction of new regularization methods based on "dropout" originally proposed by Hinton et al., (2012a). Overfitting is very common in DNN training and co-adaptation is prevalent within the DNN with multiple activations adapting together to explain input acoustic data. Dropout is a technique to limit co-adaptation. It operates as follows. On each training instance, each hidden unit is randomly omitted with a fixed probability (e.g., p=0.5). Then, decoding is done normally except with straightforward scaling of the DNN weights (by a factor of 1-p). Alternatively, the scaling of the DNN weights can be done during training [by a factor of 1/(1-p)] rather than in decoding.
The benefits of dropout regularization for training DNNs are to make a hidden unit in the DNN act strongly by itself without relying on others, and to serve a way to do model averaging of different networks. These benefits are most pronounced when the training data is limited, or when the DNN size is disproportionally large with respect to the size of the training data. Dahl et al.(2013) applied dropout in conjunction with the ReLU units and to only the top few layers of a fully-connected DNN. Seltzer and Yu (2013) applied it to noise robust speech recognition. Deng et al. (2013), on the other hand, applied dropout to all layers of a deep convolutional neural network, including both the top fully-connected DNN layers and the bottom locally-connected CNN layer and the pooling layer. It is found that the dropout rate need to be substantially smaller for the convolutional layer.
Subsequent work on applying dropout includes the study by Miao and Metze (2013), where DNNbased speech recognition is constrained by low resources with sparse training data. Most recently, Sainath et al. (2013e) combined dropout with a number of novel techniques described in this section (including the use of deep CNNs, Hessian-free sequence learning, the use of ReLU units, and the use of joint fMLLR and filterbank features, etc.) to obtain state of the art results on several large vocabulary speech recognition tasks.
As a summary, the initial success of deep learning methods for speech analysis and recognition reported around 2010 has come a long way over the past three years. An explosive growth in the work and publications on this topic has been observed, and huge excitement has been ignited within the speech recognition community. We expect that the growth in the research on deep learning based speech recognition will continue, at least in the near future. It is also fair to say that the continuing large-scale success of deep learning in speech recognition as surveyed in thischapter (up to the ASRU-2013 time frame) is a key stimulant to the large-scale exploration and applications of the deep learning methods to other areas, which we will survey in Chapters 8-11.
7.2 Speech Synthesis
In addition to speech recognition, the impact of deep learning has recently spread to speech synthesis, aimed to overcome the limitations of the conventional approach in statistical parametric synthesis based on Gaussian-HMM and decision-tree-based model clustering. The goal of speech synthesis is to generate speech sounds directly from text and possibly with additional information.
The first set of papers appeared at ICASSP, May 2013, where four different deep learning approaches are reported to improve the traditional HMM-based statistical parametric speech synthesis systems built based on "shallow" speech models, which we briefly review here after providing appropriate background information.
Statistical parametric speech synthesis emerged in the mid-1990s, and is currently the dominant technology in speech synthesis. See a recent overview in (Tokuda et al., 2013). In this approach, the relationship between texts and their acoustic realizations are modeled using a set of stochastic generative acoustic models. Decision tree-clustered context-dependent HMMs with a Gaussian distribution as the output of an HMM state are the most popular generative acoustic model used.
In such HMM-based speech synthesis systems, acoustic features including the spectra, excitation and segment durations of speech are modeled simultaneously within a unified context-dependent
HMM framework. At the synthesis time, a text analysis module extracts a sequence of contextual factors including phonetic, prosodic, linguistic, and grammatical descriptions from an input text to be synthesized. Given the sequence of contextual factors, a sentence-level context-dependent
HMM corresponding to the input text is composed, where its model parameters are determined by traversing the decision trees. The acoustic features are predicted so as to maximize their output probabilities from the sentence HMM under the constraints between static and dynamic features.
Finally, the predicted acoustic features are sent to a waveform synthesis module to reconstruct the speech waveforms. It has been known for many years that the speech sounds generated by this standard approach are often muffled compared with natural speech. The inadequacy of acoustic modeling based on the shallow-structured HMM is conjectured to be one of the reasons. Several very recent studies have adopted deep learning approaches to overcome such deficiency. One significant advantage of deep learning techniques is their strong ability to represent the intrinsic correlation or mapping relationship among the units of a high-dimensional stochastic vector using a generative (e.g., the RBM and DBN discussed in Chapter 3.2) or discriminative (e.g., the DNN discussed in Chapter 3.3) modeling framework. The deep learning techniques are thus expected to help the acoustic modeling aspect of speech synthesis in overcoming the limitations of the conventional shallow modeling approach.
A series of studies are carried out recently on ways of overcoming the above limitations using deep learning methods, inspired partly by the intrinsically hierarchical processes in human speech production and the successful applications of a number of deep learning methods in speech recognition as reviewed earlier in this chapter. In Ling et al. (2013, 2013a), the RBM and DBN as generative models are used to replace the traditional Gaussian models, achieving significant quality improvement, in both subjective and objective measures, of the synthesized voice. In the approach developed in (Kang et al., 2013), the DBN as a generative model is used to representjoint distribution of linguistic and acoustic features. Both the decision trees and Gaussian models are replaced by the DBN. The method is very similar to that used for generating digit images by the DBN, where the issue of temporal sequence modeling specific to speech (non-issue for image) is by-passed via the use of the relatively large, syllable-sized units in speech synthesis. On the other hand, in contrast to the generative deep models (RBMs and DBNs) exploited above, the study reported in (Zen et al., 2013) makes use of the discriminative model of the DNN to represent the conditional distribution of the acoustic features given the linguistic features. Finally, in(Fernandez et al., 2013), the discriminative model of the DNN is used as a feature extractor that summarizes high-level structure from the raw acoustic features. Such DNN features are then used as the input for the second stage for the prediction of prosodic contour targets from contextual features in the full speech synthesis system.
The application of deep learning to speech synthesis is in its infancy, and much more work is expected from that community in the near future.
7.3 Audio and Music Processing
Similar to speech recognition but to a less extent, in the area of audio and music processing, deep learning has also become of intense interest but only quite recently. As an example, the first major event of deep learning for speech recognition took place in 2009, followed by a series of events including a comprehensive tutorial on the topic at ICASSP-2012 and with the special issue at IEEE
Transactions on Audio, Speech, and Language Processing, the premier publication for speech recognition, in the same year. The first major event of deep learning for audio and music processing appears to be the special session at ICASSP-2014, titled Deep Learning for Music (Battenberg et al., 2014).
In the general field of audio and music processing, the impacted areas by deep learning include mainly music signal processing and music information retrieval (e.g., Bengio et al., 2013;
Humphrey et al., 2012, 2012a, 2013; Battenberg and Wessel, 2012; Schmidt and Kim, 2011;
Hamel and Eck, 2010). Deep learning presents a unique set of challenges in these areas. Music audio signals are time series where events are organized in musical time, rather than in real time, which changes as a function of rhythm and expression. The measured signals typically combine multiple voices that are synchronized in time and overlapping in frequency, mixing both shortterm and long-term temporal dependencies. The influencing factors include musical tradition, style, composer and interpretation. The high complexity and variety give rise to the signal representation problems well-suited to the high levels of abstraction afforded by the perceptually and biologically motivated processing techniques of deep learning.
In the early work on audio signals as reported by Lee et al. (2009) and their follow-up work, the convolutional structure is imposed on the RBM while building up a DBN. Convolution is made in time by sharing weights between hidden units in an attempt to detect the same "invariant" feature over different times. Then a max-pooling operation is performed where the maximal activations over small temporal neighborhoods of hidden units are obtained, inducing some local temporal invariance. The resulting convolutional DBN is applied to audio as well as speech data for anumber of tasks including music artist and genre classification, speaker identification, speaker gender classification, and phone classification, with promising results presented.
The RNN has also been recently applied to music processing applications (Bengio et al., 2013;
Boulanger-Lewandowski, et al., 2013), where the use of ReLU hidden units instead of logistic or tanh nonlinearities are explored in the RNN. As reviewed in Chapter 7.2, ReLU units compute y
= max(x, 0), and lead to sparser gradients, less diffusion of credit and blame in the RNN, and faster training. The RNN is applied to the task of automatic recognition of chords from audio music, an active area of research in music information retrieval. The motivation of using the RNN architecture is its power in modeling dynamical systems. The RNN incorporates an internal memory, or hidden state, represented by a self-connected hidden layer of neurons. This property makes them well suited to model temporal sequences, such as frames in a magnitude spectrogram or chord labels in a harmonic progression. When well trained, the RNN is endowed with the power to predict the output at the next time step given the previous ones. Experimental results show that the RNN-based automatic chord recognition system is competitive with existing state-of-the-art approaches (e.g., Oudre et al., 2011). The RNN is capable of learning basic musical properties such as temporal continuity, harmony and temporal dynamics. It can also efficiently search for the most musically plausible chord sequences when the audio signal is ambiguous, noisy or weakly discriminative.
A recent review article by Humphrey et al. (2013) provides a detailed analysis on content-based music informatics, and in particular on why the progress is decelerating throughout the field. The analysis concludes that hand-crafted feature design is sub-optimal and unsustainable, that the power of shallow architectures is fundamentally limited, and that short-time analysis cannot encode musically meaningful structure. These conclusions motivate the use of deep learning methods aimed at automatic feature learning. By embracing feature learning, it becomes possible to optimize a music retrieval system's internal feature representation or discovering it directly, since deep architectures are especially well-suited to characterize the hierarchical nature of music.
Finally, we review the very recent work by van den Oord, et al. (2013) on content-based music recommendation using deep learning methods. Automatic music recommendation has become an increasingly significant and useful technique in practice. Most recommender systems rely on collaborative filtering, suffering from the cold start problem where it fails when no usage data is available. Thus, collaborative filtering is not effective for recommending new and unpopular songs.
Deep learning methods power the latent factor model for recommendation, which predicts the latent factors from music audio when they cannot be obtained from usage data. A traditional approach using a bag-of-words representation of the audio signals is compared with deep CNNs with rigorous evaluation made. The results show highly sensible recommendations produced by the predicted latent factors using deep CNNs. The study demonstrates that a combination of convolutional neural networks and richer audio features lead to such promising results for contentbased music recommendation.
Like speech recognition and speech synthesis, much more work is expected from the music and audio signal processing community in the near future.
CHAPTER 8
SELECTED APPLICATIONS IN
LANGUAGE MODELING AND NATURAL
LANGUAGE PROCESSING
Research in language, document, and text processing has seen increasing popularity recently in the signal processing community, and has been designated as one of the main focus areas by the IEEE
Signal Processing Society's Speech and Language Processing Technical Committee. Applications of deep learning to this area started with language modeling (LM), where the goal is to provide a probability to any arbitrary sequence of words or other linguistic symbols (e.g., letters, characters, phones, etc.). Natural language processing (NLP) or computational linguistics also deals with sequences of words or other linguistic symbols, but the tasks are much more diverse (e.g., translation, parsing, text classification, etc.), not focusing on providing probabilities for linguistic symbols. The connection is that LM is often an important and very useful component of NLP systems. Applications to NLP is currently one of the most active areas in deep learning research, and deep learning is also considered as one promising direction by the NLP research community.
However, the intersection between the deep learning and NLP researchers is so far not nearly as large as that for the application areas of speech or vision. This is partly because the hard evidence for the superiority of deep learning over the current state of the art NLP methods has not been as strong as speech or visual object recognition.
8.1 Language Modeling
Language models (LMs) are crucial part of many successful applications, such as speech recognition, text information retrieval, statistical machine translation and other tasks of NLP.
Traditional techniques for estimating the parameters in LMs are based on N-gram counts. Despite known weaknesses of N-grams and huge efforts of research communities across many fields, Ngrams remained the state-of-the-art until neural network and deep learning based methods were shown to significantly lower the perplexity of LMs, one common (but not ultimate) measure of the LM quality, over several standard benchmark tasks (Mikolov, 2012; Mikolov et al., 2010, 2011).
Before we discuss neural network based LMs, we note the use of hierarchical Bayesian priors in building up deep and recursive structure for LMs (Huang and Renals, 2010). Specifically, PitmanYor process is exploited as the Bayesian prior, from which a deep (four layers) probabilistic generative model is built. It offers a principled approach to LM smoothing by incorporating the power-law distribution for natural language. As discussed in Chapter 3, this type of prior knowledge embedding is more readily achievable in the generative probabilistic modeling setup than in the discriminative neural network based setup. The reported results on LM perplexity reduction are not nearly as strong as that achieved by the neural network based LMs, which we discuss next.
There has been a long history (e.g., Bengio et al., 2001; 2003; Zamora et al., 2009) of using(shallow) feed-forward neural networks in LMs, called the NNLM. An LM is a function that captures the salient statistical characteristics of the distribution of sequences of words in natural language. It allows one to make probabilistic predictions of the next word given preceding ones.
An NNLM is one that exploits the neural network's ability to learn distributed representations in order to reduce the impact of the curse of dimensionality. The original NNLM, with a feed-forward neural network structure works as follows: the input of the N-gram NNLM is formed by using a fixed length history of N-1 words. Each of the previous N-1 words is encoded using the very sparse
1-of-V coding, where V is the size of the vocabulary. Then, this 1-of-V orthogonal representation of words is projected linearly to a lower dimensional space, using the projection matrix shared among words at different positions in the history. After the projection layer, a hidden layer with non-linear activation function, which is either a hyperbolic tangent or a logistic sigmoid, is used.
An output layer of the neural network then follows the hidden layer, with the number of output units equal to the size of the full vocabulary. After the network is trained, the output layer activations represent the "N-gram" LM's probability distribution.
The main advantage of NNLMs over the traditional counting-based N-gram LMs is that history is no longer seen as exact sequence of N-1 words, but rather as a projection of the entire history into some lower dimensional space. This leads to a reduction of the total number of parameters in the model that have to be trained, resulting in automatic clustering of similar histories. Compared with the class-based N-gram LMs, the NNLMs are different in that they project all words into the same low dimensional space, in which there can be many degrees of similarity between words. On the other hand, NNLMs have much larger computational complexity than N-gram LMs.
Let's look at the strengths of the NNLMs again from the viewpoint of distributed representations.
A distributed representation of a symbol is a vector of features which characterize the meaning of the symbol. Each element in the vector participates in representing the meaning. With an NNLM, one relies on the learning algorithm to discover meaningful, continuous-valued features. The basic idea is to learn to associate each word in the dictionary with a continuous-valued vector representation, which in the literature is called a word embedding, where each word corresponds to a point in a feature space. One can imagine that each dimension of that space corresponds to a semantic or grammatical characteristic of words. The hope is that functionally similar words get to be closer to each other in that space, at least along some directions. A sequence of words can thus be transformed into a sequence of these learned feature vectors. The neural network learns to map that sequence of feature vectors to the probability distribution over the next word in the sequence. The distributed representation approach to LMs has the advantage that it allows the model to generalize well to sequences that are not in the set of training word sequences, but that are similar in terms of their features, i.e., their distributed representation. Because neural networks tend to map nearby inputs to nearby outputs, the predictions corresponding to word sequences with similar features are mapped to similar predictions.
The above ideas of NNLMs have been implemented in various studies, some involving deep architectures. In (Mnih and Hinton, 2007), the temporally factored RBM was used for language modeling. Unlike the traditional N-gram model, the factored RBM uses distributed representationsnot only for context words but also for the words being predicted. This approach is generalized to deeper structures as reported in (Mnih and Hinton, 2008).
Subsequent work on NNLM with "deep" architectures can be found in (Le et al., 2010, 2011, 2013;
Mikolov et al., 2010; Mikolov et al., 2011; Mikolov, 2012). As an example, Le et al. (2013) describes an NNLM with structured output layer (SOUL-NNLM) where the processing depth in the LM is focused in the neural network's output representation. Figure 1 illustrates the SOULNNLM architecture with hierarchical structure in the output layers of the neural network, which shares the same architecture with the conventional NNLM up to the hidden layer. The hierarchical structure for the network's output vocabulary is in the form of a clustering tree, shown to the right of Figure 8.1, where each word belongs to only one class and ends in a single leaf node of the tree.
As a result of the hierarchical structure, the SOUL-NNLM enables the training of the NNLM with a full, very large vocabulary. This gives advantages over the traditional NNLM which requires shortlists of words in order to carry out the efficient computation in training.
Figure 8.1. The SOUL-NNLM architecture with hierarchical structure in the output layers of the neural network [after (Le et al., 2013), @IEEE].
As another example neural-network-based LMs, the work described in (Mikolov et al., 2010, 2011) and (Mikolov, 2012) makes use of RNNs to build large scale language models, called RNNLMs.
The main difference between the feed-forward and the recurrent architecture for LMs is different ways of representing the word history. For feed-forward NNLM, the history is still just previous several words. But for the RNNLM, an effective representation of history is learned from the data during training. The hidden layer of RNN represents all previous history and not just N-1 previous words, thus the model can theoretically represent long context patterns. A further important advantage of the RNNLM over the feed-forward counterpart is the possibility to represent more advanced patterns in the word sequence. For example, patterns that rely on words that could have occurred at variable positions in the history can be encoded much more efficiently with the recurrent architecture. That is, the RNNLM can simply remember some specific word in the stateof the hidden layer, while the feed-forward NNLM would need to use parameters for each specific position of the word in the history.
The RNNLM is trained using the algorithm of back-propagation through time; see details in(Mikolov, 2012), which provided Figure 8.2 to show during training how the RNN unfolds as a deep feed-forward network (with three time steps back in time).
Figure 8.2. During the training of RNNLMs, the RNN unfolds into a deep feed-forward network; based on Figure 3.2 of (Mikolov, 2012).
The training of the RNNLM achieves stability and fast convergence, helped by capping the growing gradient in training RNNs. Adaptation schemes for the RNNLM are also developed by sorting the training data with respect to their relevance and by training the model during processing of the test data. Empirical comparisons with other state-of-the-art counting-based N-gram LMs show much better performance of RNNLM in the perplexity measure, as reported in (Mikolov et al., 2010, 2011) and (Mikolov, 2012).
A separate work on applying RNN as an LM on the unit of characters instead of words can be found in (Sutskever et al., 2011; Hermans et al., 2013). Many interesting properties such as predicting long-term dependencies (e.g., making open and closing quotes in a paragraph) are demonstrated. However, the usefulness of characters instead of words as units in practical applications is not clear because the word is such a powerful representation for natural language.
Changing words to characters in LMs may limit most practical application scenarios and the training become more difficult. Word-level models currently remain superior.
In the most recent work, Mnih and Teh (2012) and Mnih and Kavukcuoglu (2013) have developed a fast and simple training algorithm for NNLMs. Despite their superior performance, NNLMs have been used less widely than standard N-gram LMs due to the much longer training time. The reported algorithm makes use of a method called noise-contrastive estimation or NCE (Gutmann and Hyvarinen, 2012) to achieve much faster training for NNLMs, with time complexity independent of the vocabulary size; hence a flat instead of tree-structured output layer in the NNLM is used. The idea behind NCE is to perform nonlinear logistic regression to discriminate between the observed data and some artificially generated noise. That is, to estimate parameters in a density model of observed data, we can learn to discriminate between samples from the data distribution and samples from a known noise distribution. As an important special case, NCE is particularly attractive for unnormalized distributions (i.e., free from partition functions in the denominator). In order to apply NCE to train NNLMs efficiently, Mnih and Teh (2012) and Mnih and Kavukcuoglu (2013) first formulate the learning problem as one which takes the objective function as the distribution of the word in terms of a scoring function. The NNLM then can be viewed as a way to quantify the compatibility between the word history and a candidate next word using the scoring function. The objective function for training the NNLM thus becomes exponentiation of the scoring function, normalized by the same constant over all possible words.
Removing the costly normalization factor, NCE is shown to speed up the NNLM training over an order of magnitude.
A similar concept to NCE is used in the recent work of (Mikolov et al., 2013), which is called negative sampling. This is applied to a simplified version of an NNLM, for the purpose of constructing word embedding instead of computing probabilities of word sequences. Word embedding is an important concept for NLP applications, which we discuss next.
8.2 Natural Language Processing
Machine learning has been a dominant tool in NLP for many years. However, the use of machine learning in NLP has been mostly limited to numerical optimization of weights for human designed representations and features from the text data. The goal of deep or representation learning is to automatically develop features or representations from the raw text material appropriate for a wide range of NLP tasks.
Recently, neural network based deep learning methods have been shown to perform well on various NLP tasks such as language modeling, machine translation, part-of-speech tagging, named entity recognition, sentiment analysis, and paraphrase detection. The most attractive aspect of deep learning methods is their ability to perform these tasks without external hand-designed resources or time-intensive feature engineering. To this end, deep learning develops and makes use an important concept called "embedding", which refers to the representation of symbolic information in natural language text at word-level, phrase-level, and even sentence-level in terms of continuous-valued vectors.
The early work highlighting the importance of word embedding came from (Collobert and Weston, 2008), (Turian et al., 2010), and (Collobert et al., 2011), although the original form came from(Bengio et al., 2000) as a side product of language modeling. Raw symbolic word representations are transformed from the sparse vectors via 1-of-V coding with a very high dimension (i.e., the vocabulary size V or its square or even its cubic) into low-dimensional, real-valued vectors via a neural network and then used for processing by subsequent neural network layers. The key advantage of using the continuous space to represent words (or phrases) is its distributed nature, which enables sharing or grouping the representations of words with a similar meaning. Such sharing is not possible in the original symbolic space, constructed by 1-of-V coding with a very high dimension, for representing words. Unsupervised learning is used where "context" of the word is used as the learning signal in neural networks. Excellent tutorials were recently given by
Socher et al. (2012-2013) to explain how the neural network is trained to perform word embedding.
More recent work proposes new ways of learning word embeddings that better capture the semantics of words by incorporating both local and global document contexts and better account for homonymy and polysemy by learning multiple embeddings per word (Huang et al., 2012).
Also, there is strong evidence that the use of RNNs can also provide empirically good performance in learning word embeddings (Mikolov, 2012). While the use of NNLMs, whose aim is to predict the future words in context, also induces word embeddings as its by-product, much simpler ways of achieving the embeddings are possible without the need to do word prediction. As shown by
Collobert and Weston (2008), the neural networks used for creating word embeddings need much smaller output units than the huge size typically required for NNLMs.
In the same early paper on word embedding, Collobert and Weston (2008) developed and employed a convolutional network as the common model to simultaneously solve a number of classic problems including part-of-speech tagging, chunking, named entity tagging, semantic role identification, and similar word identification. More recent work reported in (Collobert, 2011) further developed a fast, purely discriminative approach for parsing based on the deep recurrent convolutional architecture. Collobert et al., (2011) provide a comprehensive review on ways of applying unified neural network architectures and related deep learning algorithms to solve NLP problems from "scratch", meaning that no traditional NLP methods are used to extract features.
The theme of this line of work is to avoid task-specific, "man-made" feature engineering while providing versatility and unified features constructed automatically from deep learning applicable to all natural language processing tasks. The systems described in (Collobert et al., 2011) automatically learn internal representations or word embedding from vast amounts of mostly unlabeled training data while performing a wide range of NLP tasks.
The recent work by Mikolov et al. (2013a) derives word embeddings by simplifying the NNLM described in Section 8.1 of this chapter. It is found that the NNLM can be successfully trained in two steps. First, continuous word vectors are learned using a simple model which eliminates the nonlinearity in the upper neural network layer and share the projection layer for all words. And second, the N-gram NNLM is trained on top of the word vectors. So, after removing the second step in the NNLM, the simple model is used to learn word embeddings, where the simplicity allows the use of very large amount of data. This gives rise to a word embedding model called Continuous
Bag-of-Words Model (CBOW), as shown in Fig. 8.3a. Further, since the goal is no longer computing probabilities of word sequences as in LMs, the word embedding system here is made more effective by not only to predict the current word based on the context but also to performinverse prediction known as "Skip-gram" model, as shown in Fig. 8.3b. In the follow-up work(Mikolov et al., 2013) by the same authors, this word embedding system including the Skip-gram model is extended by a much faster learning method called negative sampling, similar to NCE discussed in Chapter 8.1.
Figure 8.3. The CBOW architecture (a) on the left, and the Skip-gram architecture (b) on the right.
[after (Mikolov et al., 2013a), @ICLR].
In parallel with the above development, Mnih and Kavukcuoglu (2013) demonstrate that NCE training of lightweight word embedding models is a highly efficient way of learning high-quality word representations, much like the somewhat earlier lightweight LMs developed by Mnih and Teh (2012) described in Section 8.1. Consequently, results that used to require very considerable hardware and software infrastructure can now be obtained on a single desktop with minimal programming effort and using less time and data. This most recent work also shows that for representation learning, only five noise samples in NCE can be sufficient for obtaining strong results for word embedding, much fewer than that required for LMs. The authors also used an
"inversed language model" for computing word embeddings, similar to the way in which the Skipgram model is used in (Mikolov et al., 2013).
Huang et al. (2012) recognized the limitation of the earlier work on word embeddings in that these models were built with only local context and one representation per word. They extended the local context models to one that can incorporate global context from full sentences or the entire document. This extended models accounts for homonymy and polysemy by learning multipleembeddings for each word. An illustration of this model is shown in Figure 8.4. In the earlier work by the same research group (Socher et al., 2011), a recursive neural network with local context was developed to build a deep architecture. The network, despite missing global context, was already shown to be capable of successful merging of natural language words based on the learned semantic transformations of their original features. This deep learning approach provided an excellent performance on natural language parsing. The same approach was also demonstrated to be reasonably successful in parsing natural scene images. In related studies, a similar recursive deep architecture is used for paraphrase detection (Socher et al., 2011a), and for predicting sentiment distributions from text (Socher et al., 2011b).
Figure 8.4. The extended word-embedding model using a recursive neural network that takes into account not only local context but also global context. The global context is extracted from the document and put in the form of a global semantic vector, as part of the input into the original word-embedding model with local context. Taken from Figure 1 of (Huang et al., 2012). [after(Huang et al., 2012), @ACL].
We now turn to selected applications of deep learning methods including the use of neural network architectures and word embeddings to practically useful NLP tasks. Machine translation is one of such tasks, pursued by NLP researchers for many years based typically on shallow statistical models. The work described in (Schwenk, et al., 2012) are perhaps the first comprehensive report on the successful application of neural-network-based language models with word embeddings, trained on a GPU, for large machine translation tasks. They address the problem of high computation complexity, and provide a solution that allows training 500 million words with 20 hours. Strong results are reported, with perplexity down from 71 to 60 in LMs and the corresponding BLEU score gained by 1.8 points using the neural-network-based language models with word embeddings compared with the best back-off LM.
A more recent study on applying deep learning methods to machine translation appears in (Gao et al., 2013), where the phrase-translation component, rather than the LM component in the machine translation system is replaced by the neural network models with semantic word embeddings. As shown in Figure 8.5 for the architecture of this approach, a pair of source (denoted by f) and target(denoted by e) phrases are projected into continuous-valued vector representations in a lowdimensional latent semantic space (denoted by the two y vectors).Then their translation score iscomputed by the distance between the pair in this new space. The projection is performed by two deep neural networks (not shown here) whose weights are learned on parallel training data. The learning is aimed to directly optimize the quality of end-to-end machine translation results.
Experimental evaluation has been performed on two standard Europarl translation tasks used by the NLP community, English-French and German-English. The results show that the new semantic-based phrase translation model significantly improves the performance of a state-of-theart phrase-based statistical machine translation system, leading to a gain close to1.0 BLEU point.
Figure 8.5. Illustration of the basic approach reported in (Gao et al., 2013) for machine translation. Parallel pairs of source (denoted by f) and target (denoted by e) phrases are projected into continuous-valued vector representations (denoted by the two y vectors), and their translation score is computed by the distance between the pair in this continuous space. The projection is performed by deep neural networks (denoted by the two arrows) whose weights are learned on parallel training data. [after (Gao et al., 2013), @NIPS].
A related approach to machine translation was developed by Schwenk (2012). The estimation of the translation model probabilities of a phrase-based machine translation system is carried out using neural networks. The translation probability of phrase pairs is learned using continuousspace representations induced by neural networks. A simplification is made that decomposes the translation probability of a phrase or a sentence to a product of n-gram probabilities as in a standard n-gram language model. No joint representations of a phrase in the source language and the translated version in the target language are exploited as in the approach reported by Gao et al.
Yet another deep learning approach to machine translation appeared in (Mikolov et al., 2013b). As in other approaches, a corpus of words in one language are compared with the same corpus of words translated into another, and words and phrases in such bilingual data that share similar statistical properties are considered equivalent. A new technique is proposed that automatically generates dictionaries and phrase tables that convert one language into another. It does not rely on versions of the same document in different languages. Instead, it uses data mining techniques to model the structure of a source language and then compares it to the structure of the targetlanguage. The technique is shown to translate missing word and phrase entries by learning language structures based on large monolingual data and mapping between languages from small bilingual data. It is based on vector-valued word embeddings as discussed earlier in this chapter and it learns a linear mapping between vector spaces of source and target languages.
An earlier study on applying deep learning techniques with DBNs was provided in (Deselaers et al., 2009) to attack a machine transliteration problem, a much easier task than machine translation.
This type of deep architectures and learning may be generalized to the more difficult machine translation problem but no follow-up work has been reported. As another early NLP application, Sarikaya et al. (2011) applied DNNs (called DBNs in the paper) to perform a natural language call–routing task. The DNNs use unsupervised learning to discover multiple layers of features that are then used to optimize discrimination. Unsupervised feature discovery is found to make DBNs far less prone to overfitting than the neural networks initialized with random weights.
Unsupervised learning also makes it easier to train neural networks with many hidden layers.
DBNs are found to produce better classification results than several other widely used learning techniques, e.g., maximum entropy and boosting based classifiers.
One most interesting NLP task recently tackled by deep learning methods is that of knowledge base (ontology) completion, which is instrumental in question-answering and many other NLP applications. An early work in this space came from (Bordes et al., 2011), where a process is introduced to automatically learn structured distributed embeddings of knowledge bases. The proposed representations in the continuous-valued vector space are compact and can be efficiently learned from large-scale data of entities and relations. A specialized neural network architecture, a generalization of "Siamese" network, is used. In the follow-up work that focuses on multirelational data (Bordes et al., 2013), the semantic matching energy model is proposed to learn vector representations for both entities and relations. More recent work (Socher et al., 2013) adopts an alternative approach, based on the use of neural tensor networks, to attack the problem of reasoning over a large joint knowledge graph for relation classification. The knowledge graph is represented as triples of a relation between two entities, and the authors aim to develop a neural network model suitable for inference over such relationships. The model they presented is a neural tensor network, with one layer only. The network is used to represent entities in a fixeddimensional vectors, which are created separately by averaging pre-trained word embedding vectors. It then learn the tensor with the newly added relationship element that describes the interactions among all the latent components in each of the relationships. The neural tensor network can be visualized in Figure 8.6, where each dashed box denotes one of the two slices of the tensor. Experimentally, the paper of (Socher et al., 2013) shows that this tensor model can effectively classify unseen relationships in WordNet and FreeBase.
Figure 8.6. Illustration of the neural tensor network described in (Socher et al., 2013), with two relationships shown as two slices in the tensor. The tensor is denoted by W[1:2]. The network contains a bilinear tensor layer that directly relates the two entity vectors (shown as e1 and e2 ) across three dimensions. Each dashed box denotes one of the two slices of the tensor. [after(Socher et al., 2013), @NIPS].
As the final example of deep learning applied successfully to NLP, we discuss here sentiment analysis applications based on recursive deep models published recently by Socher et al. (2013a).
Sentiment analysis is a task that is aimed to estimate the positive or negative opinion by an algorithm based on input text information. As we discussed earlier in this chapter, word embeddings in the semantic space achieved by neural network models have been very useful but it is difficult for them to express the meaning of longer phrases in a principled way. For sentiment analysis with the input data from typically many words and phrases, the embedding model requires the compositionality properties. To this end, Socher et al. (2013a) developed the recursive neural tensor network, where each layer is constructed similarly to that of the neural tensor network described in (Socher et al., 2013) with an illustration shown in Figure 8.6. The recursive construction of the full network exhibiting properties of compositionality follows that of (Socher et al., 2011) for the regular, non-tensor network. When trained on a carefully constructed sentiment analysis database, the recursive neural tensor network is shown to outperform all previous methods on several metrics. The new model pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag-of-features baselines.
CHAPTER 9
SELECTED APPLICATIONS IN
INFORMATION RETRIEVAL
9.1 A Brief Introduction to Information Retrieval
Information retrieval (IR) is a process whereby a user enters a query into the automated computer system that contains a collection of many documents with the goal of obtaining a set of most relevant documents. Queries are formal statements of information needs, such as search strings in web search engines. In IR, a query does not uniquely identify a single document in the collection.
Instead, several documents may match the query with different degrees of relevancy.
A document, sometimes called an object as a more general term which may include not only a text document but also an image, audio (music or speech), or video, is an entity that contains information and represented as an entry in a database. In this chapter, we limit the "object" to only text documents. User queries in IR are matched against the documents' representation stored in the database. Documents themselves often are not kept or stored directly in the IR system. Rather, they are represented in the system by metadata. Typical IR systems compute a numeric score on how well each document in the database matches the query, and rank the objects according to this value. The top-ranking documents from the system are then shown to the user. The process may then be iterated if the user wishes to refine the query.
Based partly on (Manning et al., 2009), common IR methods consist of several categories:
 Boolean retrieval, where a document either matches a query or it does not.
 Algebraic approaches to retrieval, where models are used to represent documents and queries as vectors, matrices, or tuples. The similarity of the query vector and document vector is represented as a scalar value. This value can be used to produce a list of documents that are rank-ordered for a query. Common models and methods include vector space model, topic-based vector space model, extended Boolean model, and latent semantic analysis.
 Probabilistic approaches to retrieval, where the process of IR is treated as a probabilistic inference. Similarities are computed as probabilities that a document is relevant for a given query, and the probability value is then used as the score in ranking documents. Common models and methods include binary Independence model, probabilistic relevance model with the BM25 relevance function, methods of inference with uncertainty, probabilistic, language modeling, and the technique of latent Dirichlet allocation.
 Feature-based approaches to retrieval, where documents are viewed as vectors of values of feature functions. Principled methods of "learning to rank" are devised to combine these features into a single relevance score. Feature functions are arbitrary functions of document and query, and as such Feature-based approaches can easily incorporate almost any other retrieval model as just yet another feature.
Deep learning applications to IR are rather recent. The approaches in the literature so far belong mostly to the category of feature-based approaches. The use of deep networks is mainly for extracting semantically meaningful features for subsequent document ranking stages. We will review selected studies in the recent literature in the remainder of this chapter below.
9.2 Semantic Hashing with Deep Autoencoders for
Document Indexing and Retrieval
Here we discuss the "semantic hashing" approach for the application of deep autoencoders to document indexing and retrieval as published in (Salakhutdinov and Hinton, 2007; Hinton and Salakhutdinov, 2010). It is shown that the hidden variables in the final layer of a DBN not only are easy to infer after using an approximation based on feed-forward propagation, but they also give a better representation of each document, based on the word-count features, than the widely used latent semantic analysis and the traditional TF-IDF approach for information retrieval. Using the compact code produced by deep autoencoders, documents are mapped to memory addresses in such a way that semantically similar text documents are located at nearby addresses to facilitate rapid document retrieval. The mapping from a word-count vector to its compact code is highly efficient, requiring only a matrix multiplication and a subsequent sigmoid function evaluation for each hidden layer in the encoder part of the network.
A deep generative model of DBN is exploited for the above purpose as discussed in (Hinton and Salakhutdinov, 2010). Briefly, the lowest layer of the DBN represents the word-count vector of a document and the top layer represents a learned binary code for that document. The top two layers of the DBN form an undirected associative memory and the remaining layers form a Bayesian(also called belief) network with directed, top-down connections. This DBN, composed of a set of stacked RBMs as we reviewed in Chapter 5, produces a feed-forward "encoder" network that converts word-count vectors to compact codes. By composing the RBMs in the opposite order, a "decoder" network is constructed that maps compact code vectors into reconstructed word-count vectors. Combining the encoder and decoder, one obtains a deep autoencoder (subject to further fine-tuning as discussed in Chapter 4) for document coding and subsequent retrieval.
After the deep model is trained, the retrieval process starts with mapping each query into a 128-bit binary code by performing a forward pass through the model with thresholding. Then the Hamming distance between the query binary code and all the documents' 128-bit binary codes, especially those of the "neighboring" documents defined in the semantic space, are computed extremely efficiently. The efficiency is accomplished by looking up the neighboring bit vectors in the hash table. The same idea as discussed here for coding text documents for information retrievalhas been explored for audio document retrieval and speech feature coding problems with some initial exploration reported in (Deng et al., 2010), discussed in Chapter 4 in detail.
9.3 Deep-Structured Semantic Modeling (DSSM) for
Document Retrieval
Here we discuss the more advanced and recent approach to large-scale document retrieval (Web search) based on a specialized deep architecture, called deep-structured semantic model or deep semantic similarity model (DSSM), as published in (Huang et al., 2013), and its convolutional version (C-DSSM), as published in (Shen et al., 2014).
Modern search engines retrieve Web documents mainly by matching keywords in documents with those in a search query. However, lexical matching can be inaccurate due to the fact that a concept is often expressed using different vocabularies and language styles in documents and queries.
Latent semantic models are able to map a query to its relevant documents at the semantic level where lexical-matching often fails (Manning et al., 2009). These models address the language discrepancy between Web documents and search queries by grouping different terms that occur in a similar context into the same semantic cluster. Thus, a query and a document, represented as two vectors in the lower-dimensional semantic space, can still have a high similarity even if they do not share any term. Probabilistic topic models such as probabilistic latent semantic models and latent Dirichlet allocation models have been proposed for semantic matching to partially overcome such difficulties. However, the improvement on IR tasks has not been as significant as originally expected because of two main factors: 1) most state-of-the-art latent semantic models are based on linear projection, and thus are inadequate in capturing effectively the complex semantic properties of documents; and 2) these models are often trained in an unsupervised manner using an objective function that is only loosely coupled with the evaluation metric for the retrieval task. In order to improve semantic matching for IR, two lines of research have been conducted to extend the above latent semantic models. The first is the semantic hashing approach reviewed in Section 9.1 above in this chapter based on the use of deep autoencoders (Salakhutdinov and Hinton, 2007; Hinton and Salakhutdinov, 2010). While the hierarchical semantic structure embedded in the query and the document can be extracted via deep learning, the deep learning approach used for their models still adopts an unsupervised learning method where the model parameters are optimized for the reconstruction of the documents rather than for differentiating the relevant documents from the irrelevant ones for a given query. As a result, the deep neural network models do not significantly outperform strong baseline IR models that are based on lexical matching. In the second line of research, click-through data, which consists of a list of queries and the corresponding clicked documents, is exploited for semantic modeling so as to bridge the language discrepancy between search queries and Web documents in recent studies (Gao et al., 2010, 2011). These models are trained on click-through data using objectives that tailor to the document ranking task. However, these click-through-based models are still linear, suffering from the issue of expressiveness. As a result, these models need to be combined with the keyword matching models (such as BM25) in order to obtain a significantly better performance than baselines.
The DSSM approach reported in (Huang et al., 2013) aims to combine the strengths of the above two lines of work while overcoming their weaknesses. It uses the DNN architecture to capture complex semantic properties of the query and the document, and to rank a set of documents for a given query. Briefly, a non-linear projection is performed first to map the query and the documents to a common semantic space. Then, the relevance of each document given the query is calculated as the cosine similarity between their vectors in that semantic space. The DNNs are trained using the click-through data such that the conditional likelihood of the clicked document given the query is maximized. Different from the previous latent semantic models that are learned in an unsupervised fashion, the DSSM is optimized directly for Web document ranking, and thus gives superior performance. Furthermore, to deal with large vocabularies in Web search applications, a new word hashing method is developed, through which the high-dimensional term vectors of queries or documents are projected to low-dimensional letter based n-gram vectors with little information loss.
Figure 9.1 illustrates the DNN part in the DSSM architecture. The DNN is used to map highdimensional sparse text features into low-dimensional dense features in a semantic space. The first hidden layer, with 30k units, accomplishes word hashing. The word-hashed features are then projected through multiple layers of non-linear projections. The final layer's neural activities in this DNN form the feature in the semantic space.
Figure 9.1. The DNN component of the DSSM architecture for computing semantic features. The DNN uses multiple layers to map high-dimensional sparse text features, for both Queries and Documents into low-dimensional dense features in a semantic space. [after (Huang et al., 2013), @CIKM]
To show the computational steps in the various layers of the DNN in Figure 9.1, we denote 𝑥 as the input term vector, 𝑦 as the output vector, 𝑙𝑖, 𝑖 = 1, …, 𝑁 − 1, as the intermediate hidden layers, 𝑊𝑖 as the i-th projection matrix, and 𝑏𝑖 as the 𝑖-th bias vector, we have
𝑙1 = 𝑊1𝑥, 𝑙𝑖 = 𝑓(𝑊𝑖𝑙𝑖−1 + 𝑏𝑖), 𝑖 > 1 𝑦 = 𝑓(𝑊𝑁𝑙𝑁−1 + 𝑏𝑁)where 𝑡𝑎𝑛ℎ function is used at the output layer and the hidden layers 𝑙𝑖, 𝑖 = 2, …, 𝑁 − 1:
𝑓(𝑥) = 1 − 𝑒−2𝑥
1 + 𝑒−2𝑥
The semantic relevance score between a query 𝑄 and a document 𝐷 can then be computed as the consine distance
𝑅(𝑄, 𝐷) = cosine(𝑦𝑄, 𝑦𝐷) = 𝑦𝑄
𝑇𝑦𝐷
‖𝑦𝑄‖‖𝑦𝐷‖where 𝑦𝑄 and 𝑦𝐷 are the concept vectors of the query and the document, respectively. In Web search, given the query, the documents can be sorted by their semantic relevance scores.
Learning of the DNN weights 𝑊𝑖 and 𝑏𝑖 shown in Figure 9.1 is an important contribution of the study of (Huang et al., 2013). Compared with the DNNs used in speech recognition where the targets or labels of the training data are readily available, the DNN in the DSSM does not have such label information well defined. That is, rather than using the common cross entropy or mean square errors as the training objective function, IR-centric loss functions need to be developed in order to train the DNN weights in the DSSM using the available data such as click-through logs.
The click-through logs consist of a list of queries and their clicked documents. A query is typically more relevant to the documents that are clicked on than those that are not. This weak supervision information can be exploited to train the DSSM. More specifically, the weight matrices in the DSSM, 𝑊𝑖, is learned to maximize the posterior probability of the clicked documents given the queries
𝑃(𝐷|𝑄) = exp(𝛾𝑅(𝑄, 𝐷))
∑ exp(𝛾𝑅(𝑄, 𝐷′))
𝐷′ 𝑫defined on the semantic relevance score 𝑅(𝑄, 𝐷) between the Query (Q) and the Document (D), where 𝛾 is a smoothing factor set empirically on a held-out data set, and 𝑫 denotes the set of candidate documents to be ranked. Ideally, 𝑫 should contain all possible documents, as in the maximum mutual information training for speech recognition where all possible negative candidates may be considered (He and Deng, 2008). However in this case 𝑫 is of Web scale and thus is intractable in practice. In the implementation of DSSM learning described in (Huang et al., 2013), a subset of the negative candidates are used, following the common practice adopted in MCE (Minimum Classification Error) training in speech recognition (Chengalvarayan and Deng, 1998; Yu and Deng, 2007; Yu et al., 2008; Fu et al., 2007). In other words, for each query and clicked-document pair, denoted by (𝑄, 𝐷+) where 𝑄 is a query and 𝐷+ is the clicked document, the set of D is approximated by including 𝐷+ and only four randomly selected unclickeddocuments, denote by {𝐷𝑗
−; 𝑗 = 1, …,4}. In the study reported in (Huang et al., 2013), no significant difference was found when different sampling strategies were used to select the unclicked documents.
With the above simplification the DSSM parameters are estimated to maximize the approximate likelihood of the clicked documents given the queries across the training set
𝐿(Λ) = log
∏
𝑃(𝐷+|𝑄)(𝑄,𝐷+,𝐷𝑗
−)where Λ denotes the parameter set of the DNN weights {𝑊𝑖} in the DSSM. In Figure 9.2, we show the overall DSSM architecture that contains several DNNs. All these DNNs share the same weights but take different documents (one positive and several negatives) as inputs when training the DSSM parameters. Details of the gradient computation of this approximate loss function with respect to the DNN weights tied across documents and queries can be found in (Huang et al., 2013) and are not elaborated here.
Figure 9.2. Architectural illustration of the DSSM for document retrieval (from Huang et al., 2013). All DNNs shown have shared weights. A set of n documents are shown here to illustrate the random negative sampling discussed in the text for simplifying the training procedure for the DSSM. [after (Huang et al., 2013), @CIKM]
Most recently, the DSSM described above has been extended to its convolutional version, or CDSSM (Shen et al., 2014). In the C-DSSM, semantically similar words within context are projected to vectors that are close to each other in the contextual feature space through a convolutional structure. The overall semantic meaning of a sentence is found to be determined by a few key words in the sentence, and thus the C-DSSM uses an additional max pooling layer to extract the most salient local features to form a fixed-length global feature vector. The global feature vectoris then fed to the remaining nonlinear DNN layer(s) to map it to a point in the shared semantic space.
The convolutional neural network component of the C-DSSM is shown in Figure 9.3, where a window size of three is illustrated for the convolutional layer. The overall C-DSSM architecture is similar to the DSSM architecture shown in Figure 9.2 except that the fully-connected DNNs are replaced by the convolutional neural networks with locally-connected tied weights and additional max-pooling layers. The model component shown in Figure 9.3 contains 1) a word hashing layer to transform words into letter-tri-gram count vectors in the same way as the DSSM; 2) a convolutional layer to extract local contextual features for each context window; 3) a max-pooling layer to extract and combine salient local contextual features to form a global feature vector; and 4) a semantic layer to represent the high-level semantic information of the input word sequence.
The main motivation for using the convolutional structure in the C-DSSM is its ability to map a variable-length word sequence to a low-dimensional vector in a latent semantic space. Unlike most previous models that treat a query or a document as a bag of words, a query or a document in the C-DSSM is viewed as a sequence of words with contextual structures. By using the convolutional structure, local contextual information at the word n-gram level is modeled first. Then, salient local features in a word sequence are combined to form a global feature vector. Finally, the high-level semantic information of the word sequence is extracted to form a global vector representation.
Like the DSSM just described, the C-DSSM is also trained on click-through data by maximizing the conditional likelihood of the clicked documents given a query using the back-propagation algorithm.
30k
30k
30k
30k
30k
300 max max... max
Word hashing layer: ft
Convolutional layer: ht
Max pooling layer: v
Semantic layer: y
<s> w1 w2 … wT <s>
Word sequence: xt
Word hashing matrix: Wf
Convolution matrix: Wc
Max pooling operation
Affine projection matrix: Ws
Figure 9.3. The convolutional neural network component of the C-DSSM, with the window size of three is illustrated for the convolutional layer. [after (Shen et al., 2014), @WWW].
9.4 Use of Deep Stacking Networks for Information
Retrieval
In parallel with the IR studies reviewed above, the deep stacking network (DSN) discussed in Chapter 6 has also been explored recently for IR with insightful results (Deng et al., 2013c). The experimental results suggest that the classification error rate using the binary decision of "relevant" vs. "non-relevant" from the DSN, which is closely correlated with the DSN training objective, is also generally correlated well with the NDCG (normalized discounted cumulative gain) as the most common IR quality measure. The exception is found in the region of high IR quality.
As described in Chapter 6, the simplicity of the DSN's training objective, the mean square error(MSE), drastically facilitates its successful applications to image recognition, speech recognition, and speech understanding. The MSE objective and classification error rate have been shown to be well correlated in these speech or image applications. For information retrieval (IR) applications, however, the inconsistency between the MSE objective and the desired objective (e.g., NDCG) is much greater than that for the above classification-focused applications. For example, the NDCG as a desirable IR objective function is a highly non-smooth function of the parameters to be learned, with a very different nature from the nonlinear relationship between MSE and classification error rate. Thus, it is of interest to understand to what extent the NDCG is reasonably well correlated with classification rate or MSE where the relevance level in IR is used as the DSN prediction target. Further, can the advantage of learning simplicity in the DSN be applied to improve IR quality measures such as the NDCG? Our experimental results presented in (Deng et al., 2013c) provide largely positive answers to both of the above questions. In addition, special care that need to be taken in implementing DSN learning algorithms when moving from classification to IR applications are addressed.
The IR task in the experiments of (Deng et al., 2013c) is the sponsored search related to ad placement. In addition to the organic web search results, commercial search engines also provide supplementary sponsored results in response to the user's query. The sponsored search results are selected from a database pooled by advertisers who bid to have their ads displayed on the search result pages. Given an input query, the search engine will retrieve relevant ads from the database, rank them, and display them at the proper place on the search result page; e.g., at the top or right hand side of the web search results. Finding relevant ads to a query is quite similar to common web search. For instance, although the documents come from a constrained database, the task resembles typical search ranking that targets on predicting document relevance to the input query.
The experiments conducted for this task are the first with the use of deep learning techniques(based on the DSN architecture) on the ad-related IR problem. The preliminary results from the experiments are the close correlation between the MSE as the DSN training objective with the NDCG as the IR quality measure over a wide NDCG range.
CHAPTER 10
SELECTED APPLICATIONS IN OBJECT
RECOGNITION AND COMPUTER VISION
Over the past two years or so, tremendous progress has been made in applying deep learning techniques to computer vision, especially in the field of object recognition. The success of deep learning in this area is now commonly accepted by the computer vision community. It is the second area in which the application of deep learning techniques is successful, following the speech recognition area as we reviewed and analyzed in Chapters 2 and 7.
Excellent surveys on the recent progress of deep learning for computer vision are available in the NIPS-2013 tutorial (https://nips.cc/Conferences/2013/Program/event.php?ID=4170 with video recording at http://research.microsoft.com/apps/video/default.aspx?id=206976&l=i ) and slides at http://cs.nyu.edu/~fergus/presentations/nips2013_final.pdf, and also in the CVPR-2012 tutorial(http://cs.nyu.edu/~fergus/tutorials/deep_learning_cvpr12). The reviews provided in this chapter below are based partly on these tutorials, in connection with the earlier deep learning material in this book. Another excellent source which this chapter draws upon is the most recent Ph.D. thesis on the topic of deep learning for computer vision (Zeiler, 2014).
Over many years, object recognition in computer vision has been relying on hand-designed features such as SIFT (Scale Invariant Feature Transform) and HOG (Histogram of Oriented
Gradients), akin to the reliance of speech recognition on hand-designed features such as MFCC and PLP. However, features like SIFT and HOG only capture low-level edge information. The design of features to effectively capture mid-level information such as edge intersections or highlevel representation such as object parts becomes much more difficult. Deep learning aims to overcome such challenges by automatically learning hierarchies of visual features in both unsupervised and supervised manners directly from data. The review below categorizes the many deep learning methods applied to computer vision into two classes: 1) unsupervised feature learning where the deep learning is used to extract features only, which may be subsequently fed to relatively simple machine learning algorithm for classification or other tasks; and 2) supervised learning methods where end-to-end learning is adopted to jointly optimize feature extractor and classifier components of the full system when large amounts of labeled training data are available.
10.1 Unsupervised or Generative Feature Learning
When labeled data are relatively scarce, unsupervised learning algorithms have been shown to learn useful visual feature hierarchies. In fact, prior to the demonstration of remarkable successes of CNN architectures with supervised learning in the 2012 ImageNet competition, much of the work in applying deep learning methods to computer vision had been on unsupervised feature learning. The original unsupervised deep autoencoder that exploits DBN pre-training wasdeveloped and demonstrated by Hinton and Salakhutdinov (2006) with success on the image recognition and dimensionality reduction (coding) tasks of MNIST with only 60,000 samples in the training set; see details of this task in http://yann.lecun.com/exdb/mnist/ and an analysis in(Deng, 2012). It is interesting to note that the gain of coding efficiency using the DBN-based autoencoder on the image data over the conventional method of principal component analysis as demonstrated in (Hinton and Salakhutdinov, 2006) is very similar to the gain reported in (Deng et al., 2010) and described in Chapter 4 of this book on the speech data over the traditional technique of vector quantization. Also, Nair and Hinton (2009) developed a modified DBN where the toplayer model uses a third-order Boltzmann machine. This type of DBN is applied to the NORB database – a three-dimensional object recognition task. An error rate close to the best published result on this task is reported. In particular, it is shown that the DBN substantially outperforms shallow models such as SVMs. In (Tang and Eliasmith, 2010), two strategies to improve the robustness of the DBN are developed. First, sparse connections in the first layer of the DBN are used as a way to regularize the model. Second, a probabilistic de-noising algorithm is developed.
Both techniques are shown to be effective in improving robustness against occlusion and random noise in a noisy image recognition task. DBNs have also been successfully applied to create compact but meaningful representations of images (Tarralba et al., 2008) for retrieval purposes.
On this large collection image retrieval task, deep learning approaches also produced strong results.
Further, the use of a temporally conditional DBN for video sequence and human motion synthesis were reported in (Taylor et al., 2007). The conditional RBM and DBN make the RBM and DBN weights associated with a fixed time window conditioned on the data from previous time steps.
The computational tool offered in this type of temporal DBN and the related recurrent networks may provide the opportunity to improve the DBN-HMMs towards efficient integration of temporal-centric human speech production mechanisms into DBN-based speech production model.
Deep learning methods have a rich family, including hierarchical probabilistic and generative models (neural networks or otherwise). One most recent example of this type developed and applied to facial expression datasets is the stochastic feed-forward neural networks that can be learned efficiently and that can induce a rich multiple-mode distribution in the output space not possible with the standard, deterministic neural networks (Tang and Salakhutdinov, 2013). In
Figure 10.1, we show the architecture of a typical stochastic feed-forward neural network with four hidden layers with mixed deterministic and stochastic neurons (left) used to model multimode distributions illustrated on the right. The stochastic network here is a deep, directed graphical model, where the generation process starts from input x, a neural face, and generates the output y, the facial expression. In face expression classification experiments, the learned unsupervised hidden features generated from this stochastic network are appended to the image pixels and helped to obtain superior accuracy to the baseline classifier based on the conditional RBM/DBN (Taylor et al., 2007).
Figure 10.1. Left: A typical architecture of the stochastic feed-forward neural network with four hidden layers. Right: Illustration of how the network can produce a distribution with two distinct modes and use them to represent two or more different facial expressions y given a neutral face x.
[after (Tang and Salakhutdinov, 2013), @NIPS].
Perhaps the most notable work in the category of unsupervised deep feature learning for computer vision (prior to the recent surge of the work on CNNs) is that of (Le et al., 2012), a nine-layer locally connected sparse autoencoder with pooling and local contrast normalization. The model has one billion connections, trained on the dataset with 10 million images downloaded from the Internet. The unsupervised feature learning methods allow the system to train a face detector without having to label images as containing a face or not. And the control experiments show that this feature detector is robust not only to translation but also to scaling and out-of-plane rotation.
Another set of popular studies on unsupervised deep feature learning for computer vision are based on deep sparse coding models (Yu et al., 2011; Lin et al., 2011). This type of deep models produced state-of-the-art accuracy results on the ImageNet object recognition tasks prior to the rise of the CNN architectures armed with supervised learning to perform joint feature learning and classification, which we turn to now.
10.2 Supervised Feature Learning and Classification
The origin of the applications of deep learning to object recognition tasks can be traced to the convolutional neural networks (CNNs) in the early 90s; see a comprehensive overview in (LeCun et al., 1998). The CNN-based architectures in the supervised learning mode have captured intense interest in computer vision since October 2012 shortly after the ImageNet competition results were released (http://www.image-net.org/challenges/LSVRC/2012/ ). This is mainly due to the huge recognition accuracy gain over competing approaches when large amounts of labeled data are available to efficiently train large CNNs using GPU-like high-performance computing platforms.
Just like DNN-based deep learning methods have outperformed previous state-of-the-art approaches in speech recognition in a series of benchmark tasks including phone recognition, large-vocabulary speech recognition, noise-robust speech recognition, and multi-lingual speech recognition, CNN-based deep learning methods have demonstrated the same in a set of computervision benchmark tasks including category-level object recognition, object detection, and semantic segmentation.
The basic architecture of the CNN described in (LeCun et al., 1998) is shown in Figure 10.1. To incorporate the relative invariance of the spatial relationship in typical image pixels with respect to the location, the CNN uses a convolutional layer with local receptive fields and with tied filter weights, much like 2-dimensional FIR filters in image processing. The output of the FIR filters is then passed through a nonlinear activation function to create activation maps, followed by another nonlinear pooling (labeled as "subsampling" in Figure 10.2) layer that reduces the data rate while providing invariance to slightly different input images. The output of the pooling layer are subject to a few fully-connected layers as in the DNN discussed in earlier chapters. The whole architecture above is also called the deep CNN in the literature.
Figure 10.2. The original convolutional neural network that is composed of multiple alternating convolution and pooling layers followed by fully connected layers. [after (LeCun, et al., 1998), @IEEE].
Deep models with convolution structure such as CNNs have been found effective and have been in use in computer vision and image recognition since 90's (Bengio and LeCun, 1995; LeCun et al., 1998; Jarrett et al., 2009; Kavukcuoglu et al., 2010; Ciresan et al., 2012; Krizhevsky et al., 2012). The most notable advance was achieved in the 2012 ImageNet LSVRC competition, in which the task is to train a model with 1.2 million high-resolution images to classify unseen images to one of the 1000 different image classes. On the test set consisting of 150k images, the deep
CNN approach described in (Krizhevsky et al., 2012) achieved the error rates considerably lower than the previous state-of-the-art. Very large deep-CNNs are used, consisting of 60 million weights, and 650,000 neurons, and five convolutional layers together with max-pooling layers. Additional two fully-connected layers as in the DNN described previously are used on top of the CNN layers.
Although all the above structures were developed separately in earlier work, their best combination accounted for major part of the success. See the overall architecture of the deep CNN system in Figure 10.3. Two additional factors contribute to the final success. The first is a powerful regularization technique called "dropout"; see details in (Hinton et al., 2012a) and a series of further analysis and improvement in (Baldi and Sadowski, 2013; McAllester, 2013; Frey and Ba, 2013; Wager et al., 2013). Applications of the same "dropout" techniques are also successful for some speech recognition tasks (Deng et al., 2013; Dahl et al., 2013). The second factor is the use of non-saturating neurons or rectified linear units (ReLU) that compute 𝑓(𝑥) = max(𝑥, 0), whichsignificantly speeds up the overall training process especially with efficient GPU implementation.
This deep-CNN system achieved a winning top-5 test error rate of 15.3% using extra training data from ImageNet Fall 2011 release, or 16.4% using only supplied training data in ImageNet-2012, significantly lower than 26.2% achieved by the second-best system which combines scores from many classifiers using a set of hand-crafted features such as SIFT and Fisher vectors. See details in http://www.image-net.org/challenges/LSVRC/2012/oxford_vgg.pdf about the best competing method. It is noted, however, that the Fisher-vector-encoding approach has recently been extended by Simonyan et al. (2013) via stacking in multiple layers to form deep Fisher networks, which achieve competitive results with deep CNNs at a smaller computational learning cost.
Figure 10.3. The architecture of the deep-CNN system which won the 2012 ImageNet competition by a large margin over the second-best system and the state of the art by 2012. [after (Krizhevsky et al., 2012), @NIPS].
The state of the art performance demonstrated in (Krizhevsky et al., 2012) using the deep-CNN approach is further improved by another significant margin during 2013, using a similar approach but with bigger models and larger amounts of training data. A summary of top-5 test error rates from 11 top-performing teams participating in the 2013 ImageNet ILSVRC competition is shown in Figure 10.4, with the best result of the 2012 competition shown to the right most as the baseline.
Here we see rapid error reduction on the same task from the lowest pre-2012 error rate of 26.2%(non-neural networks) to 15.3% in 2012 and further to 11.2% in 2013, both achieved with deepCNN technology. It is also interesting to observe that all major entries in the 2013 ImageNet
ILSVRC competition is based on deep learning approaches. For example, the Adobe system shown in Figure 10.4 is based on the deep-CNN reported in (Krizhevsky et al., 2012) including the use of dropout. The network architecture is modified to include more filters and connections. At test time, image saliency is used to obtain 9 crops from original images, which are combined with the standard five multiview crops. The NUS system uses a non-parametric, adaptive method to combine the outputs from multiple shallow and deep experts, including deep-CNN, kernel, and GMM methods. The VGG system is described in (Simonyan et al., 2013) and uses a combination of the deep Fisher vector network and the deep-CNN. The ZF system is based on a combination of a large CNN with a range of different architectures. The choice of architectures was assisted by visualization of model features using a deconvolutional network as described by Zeiler et al. (2011), Zeiler and Fergus (2013), and Zeiler (2014). The CognitiveVision system uses an image classification scheme based on a DNN architecture. The method is inspired by cognitive psychophysics about how the human vision system first learns to classify the basic-level categories and then learns to classify categories at the subordinate level for fine-grained object recognition.
Finally, the best-performing system called Clarifai in Figure 10.4 is based on a large and deep
CNN with dropout regularization. It augments the amount of training data by down-sampling images to 256 pixels. The system contains a total of 65M parameters. Multiple such models were averaged together to further boost performance. The main novelty is to use the visualization technique based on the deconvolutional networks as described in (Zeiler et. al, 2011; Zeiler, 2014) to identify what makes the deep model perform well, based on which a powerful deep architecture was chosen. See more details of these systems in http://www.imagenet.org/challenges/LSVRC/2013/results.php.
Figure 10.4. Summary results of ImageNet Large Scale Visual Recognition Challenge 2013(ILSVRC2013), representing the state-of-the-are performance of object recognition systems. Data source: http://www.image-net.org/challenges/LSVRC/2013/results.php
While the deep CNN has demonstrated remarkable classification performance on object recognition tasks, there has been no clear understanding of why they perform so well until recently.
Zeiler and Fergus (2013) conducted research to address just this issue, and then used the gained understanding to further improve the CNN systems, which yielded excellent performance as shown in Figure 10.4 with labels "ZF" and "Clarifai". A novel visualization technique is developed that gives insight into the function of intermediate feature layers of the deep CNN. The technique also sheds light onto the operation of the full network acting as a classifier. The visualization technique is based on a deconvolutional network, which maps the neural activities in intermediate layers ofthe original convolutional network back to the input pixel space. This allows the researchers to example what input pattern originally caused a given activation in the feature maps. Figure 10.5(the top portion) illustrates how a deconvolutional network is attached to each of its layers, thereby providing a closed loop back to image pixels as the input to the original CNN. The information flow in this closed loop is as follows. First, an input image is presented to the deep CNN in a feedforward manner so that the features at all layers are computed. To examine a given CNN activation, all other activations in the layer are set to zero and the feature maps are passed as input to the attached deconvolutional network's layer. Then, successive operations, opposite to the feedforward computation in the CNN, are carried out including unpooling, rectifying, and filtering.
This allows the reconstruction of the activity in the layer beneath that gave rise to the chosen activation. These operations are repeated until input layer is reached. During unpooling, noninvertibility of the max pooling operation in the CNN is resolved by an approximate inverse, where the locations of the maxima within each pooling region are recorded in a set of "switch" variables.
These switches are used to place the reconstructions from the layer above into appropriate locations, preserving the structure of the stimulus. This procedure is shown at the bottom portion of Figure
In addition to the deep-CNN architecture described above, the DNN architecture has also been shown to be highly successful in a number of computer vision tasks (Ciresan, et al., 2010, 2011, 2012, 2012a). We have not found in the literature on direct comparisons among the CNN, DNN, and other related architectures on the identical tasks.
Figure 10.5. The top portion shows how a deconvolutional network's layer (left) is attached to a corresponding CNN's layer (right). The deconvolutional network reconstructs an approximate version of the CNN features from the layer below. The bottom portion is an illustration of the unpooling operation in the deconvolutional network, where "Switches" are used to record the location of the local max in each pooling region during pooling in the CNN. [after (Zeiler and Fergus, 2013), @arXiv].
Finally, the most recent study on supervised learning for computer vision shows that the deep CNN architecture is not only successful for object/image classification discussed earlier in this section but also successful for objection detection in the whole images (Girshick et al., 2013). The detection task is substantially more complex than the classification task.
As a brief summary of this chapter, deep learning has made huge inroads into computer vision, soon after its success in speech recognition discussed in Chapter 7. So far, it is the supervised learning paradigm based on the deep CNN architecture and the related classification techniques that are making the greatest impact, showcased by the ImageNet competition results from 2012and 2013. These methods can be used for not only objection recognition but also many other computer vision tasks. There has been some debate as to the reasons for the success of these CNNbased deep learning methods, and about their limitations. Many questions are still open as to how these methods can be tailored to certain computer vision applications and how to scale up the models and training data. Finally, we discussed a number of studies on unsupervised and generative approaches of deep learning to computer vision and image modeling problems in the earlier part of this chapter. Their performance has not been competitive with the supervised learning approach on object recognition tasks with ample training data. To achieve long term and ultimate success in computer vision, it is likely that unsupervised learning will be needed. To this end, many open problems in unsupervised feature learning and deep learning need to be addressed and much more research need to be carried out.
CHAPTER 11
SELECTED APPLICATIONS IN MULTIMODAL AND MULTI-TASK LEARNING
Multi-task learning is a machine learning approach that learns to solve several related problems at the same time, using a shared representation. It can be regarded as one of the two major classes of transfer learning or learning with knowledge transfer, which focuses on generalizations across distributions, domains, or tasks. The other major class of transfer learning is adaptive learning, where knowledge transfer is carried out in a sequential manner, typically from a source task to a target task (Deng and Li, 2013). Multi-modal learning is a closely related concept to multi-task learning, where the learning domains or "tasks" cut across several modalities for human-computer interactions or other applications embracing a mixture of textual, audio/speech, touch, and visual information sources.
The essence of deep learning is to automate the process of discovering effective features or representations for any machine learning task, including automatically transferring knowledge from one task to another concurrently. Multi-task learning is often applied to conditions where no or very little training data are available for the target task domain, and hence is sometimes called zero-shot or one-shot learning. It is evident that difficult multi-task leaning naturally fits the paradigm of deep learning or representation learning where the shared representations and statistical strengths across tasks (e.g., those involving separate modalities of audio, image, touch, and text) is expected to greatly facilitate many machine learning scenarios under low- or zeroresource conditions. Before deep learning methods were adopted, there had been numerous efforts in multi-modal and multi-task learning. For example, a prototype called MiPad for multi-modal interactions involving capturing, leaning, coordinating, and rendering a mix of speech, touch, and visual information was developed and reported in (Huang et al., 2001; Deng et al., 2002). And in(Zheng et al., 2004; Subramanya et al., 2005), mixed sources of information from multiple-sensory microphones with separate bone-conductive and air-born paths were exploited to de-noise speech.
These early studies all used shallow models and learning methods and achieved worse than desired performance. With the advent of deep learning, it is hopeful that the difficult multi-modal learning problems can be solved with eventual success to enable a wide range of practical applications. In this chapter, we will review selected applications in this area, organized according to different combinations of more than one modalities or learning tasks. Much of the work reviewed here is on-going research, and readers should expect follow-up publications in the future.
11.1 Multi-Modalities: Text and Image
The underlying mechanism for potential effectiveness of multi-modal learning involving text and image is the common semantics associated with the text and image. The relationship between thetext and image may come, for example, from the text annotations of an image (as the training data for a multi-modal learning system). If the related text and image share the same representation in a common semantic space, the system can generalize to the unseen situation where either text or image is unavailable. It can thus be naturally used for zero-shot learning for image or text. In other words, multi-modality learning can use text information to help image/visual recognition, and vice versa. Exploiting text information for image/visual recognition constitutes most of the work done in this space, which we review in this section below.
The deep architecture, called DeViSE (Deep Visual-Semantic Embedding) and developed by
Frome et al. (2013), is a typical example of the multi-modal learning where text information is used to improve the image recognition system, especially for performing zero-shot learning. Image recognition systems are often limited in their ability to scale to large number of object categories, due in part to the increasing difficulty of acquiring sufficient training data with text labels as the number of image categories grows. The multi-modal DeViSE system is aimed to leverage text data to train the image models. The joint model is trained to identify image classes using both labeled image data and the semantic information learned from unannotated text. An illustration of the DeViSE architecture is shown in the center portion of Figure 10.1. It is initialized with the parameters pre-trained at the lower layers of two models: the deep-CNN for image classification in the left portion of the figure and the text embedding model in the right portion of the figure. The part of the deep CNN, labeled "core visual model" in Figure 10.1, is further learned to predict the target word-embedding vector using a projection layer labeled "transformation" and using a similarity metric. The loss function used in training adopts a combination of dot-product similarity and max-margin, hinge rank loss. The former is the un-normalized version of the cosine loss function used for training the DSSM model in (Huang et al., 2013) as described in Chapter 9.3.
The latter is similar to the earlier joint image-text model called WSABIE (Web Scale Annotation by Image Embedding developed by Weston, et al. (2010, 2011)). The results show that the information provided by text improves zero-shot image predictions, achieving good hit rates (close to 15%) across thousands of the labels never seen by the image model.
Figure 11.1. Illustration of the multi-modal DeViSE architecture. The left portion is an image recognition neural network with a softmax output layer. The right portion is a skip-gram text modelproviding word embedding vectors; see Chapter 8.2 and Figure 8.3 for details. The center is the joint deep image-text model of DeViSE, with the two Siamese branches initialized by the image and word embedding models below the softmax layers. The layer labeled "transformation" is responsible for mapping the outputs of the image (left) and text (right) branches into the same semantic space. [after (Frome, et al., 2013), @NIPS].
The earlier WSABIE system as described in (Weston, et al. 2010, 2011) adopted a shallow architecture and trained a joint embedding model of both images and labels. Rather than using deep architectures to derive the highly nonlinear image (as well as text-embedding) feature vectors as in DeViSE, the WSABIE uses simple image features and a linear mapping to arrive at the joint embedding space. Further, it uses an embedding vector for each possible label. Thus, unlike
DeViSE, WSABIE could not generalize to new classes.
It is also interesting to compare the DeViSE architecture of Figure 11.1 with the DSSM architecture of Figure 9.2 in Chapter 9. The branches of "Query" and "Documents" in DSSM are analogous to the branches of "image" and "text-label" in DeViSE. Both DeViSE and DSSM use the objective function related to cosine distance between two vectors for training the network weights in an end-to-end fashion. One key difference, however, is that the two sets of inputs to the DSSM are both text (i.e., "Query" and "Documents" designed for IR), and thus mapping "Query" and "Documents" to the same semantic space is conceptually more straightforward compared with the need in DeViSE for mapping from one modality (image) to another (text). Another key difference is that the generalization ability of DeViSE to unseen image classes comes from computing text embedding vectors for many unsupervised text sources (i.e., with no image counterparts) that would cover the text labels corresponding to the unseen classes. The generalization ability of the DSSM over unseen words, however, is derived from a special coding scheme for words in terms of their constituent letters.
The DeViSE architecture has inspired a more recent method, which maps images into the semantic embedding space via convex combination of embedding vectors for the text label and the image classes (Norouzi et al., 2013). Here is the main difference. DeViSE replaces the last, softmax layer of a CNN image classifier with a linear transformation layer. The new transformation layer is then trained together with the lower layers of the CNN. The method in (Norouzi et al., 2013) is much simpler --- keeping the softmax layer of the CNN while not training the CNN. For a test image, the CNN first produces top N-best candidates. Then, the convex combination of the corresponding
N embedding vectors in the semantic space is computed. This gives a deterministic transformation from the outputs of the softmax classifier into the embedding space. This simple multi-modal learning method is shown to work very well on the ImageNet zero-shot learning task.
Another thread of studies separate from but related to the above work on multi-modal learning involving text and image have centered on the use of multi-modal embeddings, where data from multiple sources with separate modalities of text and image are projected into the same vector space. For example, Socher and Fei-Fei (2010) project words and images into the same space using kernelized canonical correlation analysis. Socher et al. (2013b) map images to single-word vectors so that the constructed multi-modal system can classify images without seeing any examples of the class, i.e., zero-shot learning similar to the capability of DeViSE. The most recent work by
Socher et al. (2013c) extends their earlier work from single-word embeddings to those of phrasesand full-length sentences. The mechanism for mapping sentences instead of the earlier single words into the multi-modal embedding space is derived from the power of the recursive neural network described in Socher et al. (2013a) as summarized in Chapter 8.2, and its extension with dependency tree.
In addition to mapping text to image (or vice versa) into the same vector space or to creating the joint image/text embedding space, multi-modal learning for text and image can also be cast in the framework of language models. In (Kiros, et al., 2013), a model of natural language is made conditioned on other modalities such as image as the focus of the study. This type of multi-modal language model is used to 1) retrieve images given complex description queries, 2) retrieve phrase descriptions given image queries, and 3) generate text conditioned on images. Word representations and image features are jointly learned by training the multi-modal language model together with a convolutional network. An illustration of the multi-modal language model is shown in Figure 11.2.
Figure 11.2. A multi-modal language model (of the type of log-bilinear) which predicts a word conditioned not only on the previous words in the sentence but also on images. The model operates on word embedding vectors. [after (Kiros et al., 2013), @NIPS].
11.2 Multi-Modalities: Speech and Image
Ngiam et al. (2011) propose and evaluate an application of deep networks to learn features over audio/speech and image/video modalities. They demonstrate cross-modality feature learning, where better features for one modality (e.g., image) is learned when multiple modalities (e.g., speech and image) are present at feature learning time. A bi-modal deep autoencoder architecture for separate audio/speech and video/image input channels are shown in Figure 11.3. The essence of this architecture is to use a shared, middle layer to represent both types of modalities. This is a straightforward generalization from the single-modal deep autoencoder for speech shown in Figure
4.1 of Chapter 4 to bi-modal counterpart. The authors further show how to learn a shared audio and video representation, and evaluate it on a fixed task, where the classifier is trained with audioonly data but tested with video-only data and vice versa. The work concludes that deep learningarchitectures are generally effective in learning multimodal features from unlabeled data and in improving single modality features through cross modality information transfer. One exception is the cross-modality setting using the CUAVE dataset. The results presented in (Ngiam et al., 2011) show that learning video features with both video and audio outperforms that with only video data.
However, the same paper also shows that a model of (Papandreou, 2009) in which a sophisticated signal processing technique for extracting visual features, together with the uncertaintycompensation method developed originally from robust speech recognition (Deng et al., 2005), gives the best classification accuracy in the cross-modal learning task, beating the features derived from the generative deep architecture designed for this task.
Figure 11.3. The architecture of a deep denoising autoencoder for multi-modal audio/speech and visual features. [after (Ngiam et al., 2011), @ICML].
While the deep generative architecture for multimodal learning described in (Ngiam et al., 2011) is based on non-probabilistic autoencoder neural nets, a probabilistic version based on deep
Boltzmann machine (DBM) has appeared more recently for the same multimodal application. In(Srivastava and Salakhutdinov, 2012), a DBM is used to extract a unified representation integrating separate modalities, useful for both classification and information retrieval tasks. Rather than using the "bottleneck" layers in the deep autoencoder to represent multimodal inputs, here a probability density is defined on the joint space of multimodal inputs, and states of suitably defined latent variables are used for the representation. The advantage of this probabilistic formulation, possibly lacking in the traditional deep autoencoder, is that the missing modality's information can be filled in naturally by sampling from its conditional distribution. More recent work on autoencoders(Bengio et al., 2013, 2013b) shows the capability of generalized denoising autoencoders in carrying out sampling, thus they may overcome the earlier problem of filling-in the missing modality's information. For the bi-modal data consisting of image and text, the multimodal DBMwas shown to slightly outperform the traditional version of the deep multimodal autoencoder as well as multimodal DBN in classification and information retrieval tasks. No results on the comparisons with the generalized version of deep autoencoders has been reported but may appear soon.
The several architectures discussed so far in this chapter for multi-modal processing and learning can be regarded as special cases of more general multi-task learning and transfer learning (Caruana, 1997; Bengio et al., 2013). Transfer learning, encompassing both adaptive and multi-task learning, refers to the ability of a learning architecture and technique to exploit common hidden explanatory factors among different learning tasks. Such exploitation permits sharing of aspects of diverse types of input data sets, thus allowing the possibility of transferring knowledge across seemingly different learning tasks. As argued in (Bengio et al., 2013), the learning architecture shown in Figure 11.4 and the associated learning algorithms have an advantage for such tasks because they learn representations that capture underlying factors, a subset of which may be relevant for each particular task. We will discuss a number of such multi-task learning applications in the remainder of this chapter that are confined with a single modality of speech, natural language processing, or image domain.
Figure 11.4. A DNN architecture for multitask learning that is aimed to discover hidden explanatory factors shared among three tasks A, B, and C. [after (Bengio, 2013), @IEEE].
11.3 Multi-Task Learning within the Speech, NLP or Image Domain
Within the speech domain, one most interesting application of multi-task learning is multi-lingual or cross-lingual speech recognition, where speech recognition for different languages is considered as different tasks. Various approaches have been taken to attack this rather challenging acoustic modeling problem for speech recognition, where the difficulty lies in the lack of transcribed speech data due to economic considerations in developing speech recognition systems for all languages in the world. Cross-language data sharing and data weighing are common and useful approaches for the GMM-HMM system (Lin et al., 2009). Another successful approach for the GMM-HMMis to map pronunciation units across languages either via knowledge-based or data-driven methods(Yu et al., 2009b). But they are much inferior to the DNN-HMM approach which we now summarize.
In recent papers of (Huang et al., 2013; Deng et al., 2013a) and (Heigold et al., 2013), two research groups independently developed closely related DNN architectures with multi-task learning capabilities for multilingual speech recognition. See Figure 11.5 for an illustration of this type of architecture. The idea behind these architectures is that the hidden layers in the DNN, when learned appropriately, serve as increasingly complex feature transformations sharing common hidden factors across the acoustic data in different languages. The final softmax layer representing a loglinear classifier makes use of the most abstract feature vectors represented in the top-most hidden layer. While the log-linear classifier is necessarily separate for different languages, the feature transformations can be shared across languages. Excellent multilingual speech recognition results are reported, far exceeding the earlier results using the GMM-HMM based approaches (e.g., Lin et al., 2009; Yu et al., 2009b). The implication of this set of work is significant and far reaching.
It points to the possibility of quickly building a high-performance DNN-based system for a new language from an existing multilingual DNN. This huge benefit would require only a small amount of training data from the target language, although having more data would further improve the performance. This multitask learning approach can reduce the need for the unsupervised pretraining stage, and can train the DNN with much fewer epochs. Extension of this set of work would be to efficiently build a language-universal speech recognition system. Such a system cannot only recognize many languages and improve the accuracy for each individual language, but also expand the languages supported by simply stacking softmax layers on the DNN for new languages.
Figure 11.5. A DNN architecture for multilingual speech recognition. [after (Huang et al., 2013), @IEEE].
A closely related DNN architecture, as shown in Figure 11.6, with multitask learning capabilities was also recently applied to another acoustic modeling problem --- learning joint representations for two separate sets of acoustic data (Li et al., 2012; Deng et al., 2013a). The set that consists of the speech data with 16kHz sampling rate is of wideband and high quality, which is often collected from increasingly popular smart phones under the voice search scenario. Another, narrowband data set has a lower sampling rate of 8kHz, often collected using the telephony speech recognition systems.
Figure 11.6. A DNN architecture for speech recognition trained with mixed-bandwidth acoustic data with 16-kHz and 8-kHz sampling rates; [after (Li et al., 2012), @IEEE].
As a final example of multi-task learning within the speech domain, let us consider phone recognition and word recognition as separate "tasks". That is, phone recognition results are used not for producing text outputs but for language-type identification or for spoken document retrieval.
Then, the use of pronunciation dictionary in almost all speech systems can be considered as multitask learning that share the tasks of phone recognition and word recognition. More advanced frameworks in speech recognition have pushed this direction further by advocating the use of even finer units of speech than phones to bridge the raw acoustic information of speech to semantic content of speech via a hierarchy of linguistic structure. These atomic speech units include "speech attributes" in the detection-based and knowledge-rich modeling framework for speech recognition, whose accuracy has been significantly boosted recently by the use of deep learning methods (Yu et al., 2012a; Siniscalchi et al., 2013, 2013a).
Within the natural language processing domain, the best known example of multi-task learning is the comprehensive studies reported in (Collobert and Weston, 2008; Collobert et al., 2011), where a range of separate "tasks" of part-of-speech tagging, chunking, named entity tagging, semantic role identification, and similar-word identification in natural language processing are attackedusing a common representation of words and a unified deep learning approach. A summary of these studies can be found in Chapter 8.2.
Finally, within the domain of image/vision as a single modality, deep learning has also been found effective in multi-task learning. Srivastava and Salakhutdinov (2013) present a multi-task learning approach based on hierarchical Bayesian priors in a DNN system applied to various image classification data sets. The priors are combined with a DNN, which improves discriminative learning by encouraging information sharing among tasks and by discovering similar classes among which knowledge is transferred. More specifically, methods are developed to jointly learn to classify images and a hierarchy of classes, such that "poor classes", for which there are relatively few training examples, can benefit from similar "rich classes", for which more training examples are available. This work can be considered as an excellent instance of learning output representations, in addition to learning input representation of the DNN as the focus of nearly all deep learning work reported in the literature.
As another example of multi-task learning within the single-modality domain of image, Ciresan et al. (2012b) applied the architecture of deep CNNs to character recognition tasks for Latin and for
Chinese. The deep CNNs trained on Chinese characters are shown to be easily capable of recognizing uppercase Latin letters. Further, learning Chinese characters is accelerated by first pretraining a CNN on a small subset of all classes and then continuing to train on all classes.
CHAPTER 12
EPILOGUES
This book first presented a brief history of deep learning (focusing on speech recognition) and developed a categorization scheme to analyze the existing deep networks in the literature into unsupervised (many of which are generative), supervised, and hybrid classes. The deep autoencoder, the DSN (as well as many of its variants), and the DBN-DNN or pre-trained DNN architectures, one in each of the three classes, are discussed and analyzed in detail, as they appear to be popular and promising approaches based on the authors' personal research experiences.
Applications of deep learning in five broad areas of information processing are also reviewed, including speech and audio (Chapter 7), natural language modeling and processing (Chapter 8), information retrieval (Chapter 9), object recognition and computer vision (Chapter 10), and multimodal and multi-task learning (Chapter 11). There are other interesting yet non-mainstream applications of deep learning, which are not covered in this book. For interested readers, please consult recent papers on the applications of deep learning to optimal control in (Levine, 2013), to reinforcement learning in (Mnih, et al, 2013), to malware classification in (Dahl et al., 2013a), to compressed sensing in (Palangi et al., 2013), to recognition confidence prediction in (Huang et al., 2013a), to acoustic-articulatory inversion mapping in (Uria et al., 2011), to emotion recognition from video in (Kahou et al., 2013), to emotion recognition from speech in (Li et al., 2013; Le and Mower, 2013), to spoken language understanding in (Mesnil et al., 2013; Yao et al., 2013; Tur et al., 2012), to speaker recognition in (Vasilakakis et al., 2013; Stafylakis et al., 2012), to languagetype recognition in (Diez, 2013), to dialogue state tracking for spoken dialogue systems in(Henderson et al., 2013; Deng et al., 2013a), to automatic voice activity detection in (Zhang and Wu, 2013), to speech enhancement in (Xu et al., 2014), to voice conversion in (Nakashika et al., 2013), and to single-channel source separation in (Grais et al., 2013; Weng et al.. 2014).
The literature on deep learning is vast, mostly coming from the machine learning community. The signal processing community embraced deep learning only within the past four years or so (starting around end of 2009) and the momentum is growing fast ever since. This book is written mainly from the signal processing perspective. Beyond just surveying the existing deep learning work, a classificatory scheme based on the architectures and on the nature of the learning algorithms is developed, and an analysis and discussion with concrete examples are presented. This will hopefully provide insight for readers to better understand the capability of the various deep learning systems discussed in the book, the connection among different but similar deep learning methods, and how to design proper deep learning algorithms under different circumstances.
Throughout this review, the important message is conveyed that building and learning deep hierarchies of features are highly desirable. We have discussed the difficulty of learning parameters in all layers of deep networks in one shot due to optimization difficulties that need to be better understood. The unsupervised pre-training method in the hybrid architecture of the DBNDNN, which we reviewed in detail in Chapter 5, appears to have offered a useful, albeit empirical, solution to poor local optima in optimization and to regularization for the deep model containingmassive parameters even though a solid theoretical foundation is still lacking. The effectiveness of the pre-training method, which was one factor stimulating the interest in deep learning by the signal processing community in 2009 via collaborations between academic and industrial researchers, is most prominent when the supervised training data are limited.
Deep learning is an emerging technology. Despite the empirical promising results reported so far, much more work needs to be carried out. Importantly, it has not been the experience of deep learning researchers that a single deep learning technique can be successful for all classification tasks. For example, while the popular learning strategy of generative pre-training followed by discriminative fine-tuning seems to work well empirically for many tasks, it failed to work for some other tasks that have been explored (e.g., language identification or speaker recognition; unpublished by the authors of this book). For these tasks, the features extracted at the generative pre-training phase seem to describe the underlying speech variations well but do not contain sufficient information to distinguish between different languages. A learning strategy that can extract discriminative yet also invariant features is expected to provide better solutions. This idea has also been called "disentangling" and is developed further in (Bengio et al., 2013a). Further, extracting discriminative features may greatly reduce the model size needed in many of the current deep learning systems. Domain knowledge such as what kind of invariance is useful for a specific task in hand (e.g., vision, speech, or natural language) and what kind of regularization in terms of parameter constraints is key to the success of applying deep learning methods. Moreover, new types of DNN architectures and learning beyond the several popular ones discussed in this book are currently under active development by the deep learning research community (e.g., Bengio et al., 2013a; Deng et al., 2013b), holding the promise to improve the performance of deep learning models in more challenging applications in signal processing and in artificial intelligence.
Recent published work showed that there is vast room to improve the current optimization techniques for learning deep architectures (Martens, 2010; Le et al., 2011; Martens and Sutskever, 2011; Dean et al., 2012; Sutskever, 2013; Sainath et al., 2013; Wright et al., 2013). To what extent pre-training is essential to learning the full set of parameters in deep architectures is currently under investigation, especially when very large amounts of labeled training data are available, reducing or even obliterating the need for model regularization. Some preliminary results have been discussed in this book and in (Ciresan et al., 2010; Yu et al. 2010; Seide et al. 2011; Hinton et al., 2012).
In recent years, machine learning is becoming increasingly dependent on large-scale data sets. For instance, many of the recent successes of deep learning as discussed in this book have relied on the access to massive data sets and massive computing power. It would become increasingly difficult to explore the new algorithmic space without the access to large, real-world data sets and without the related engineering expertise. How well deep learning algorithms behave would depend heavily on the amount of data and computing power available. As we showed with speech recognition examples, a deep learning algorithm that appears to be performing not so remarkably on small data sets can begin to perform considerably better when these limitations are removed, one of main reasons for the recent resurgence in neural network research. As an example, the DBN pre-training that ignited a new era of (deep) machine learning research appears unnecessary if enough data and computing power are used.
As a consequence, effective and scalable parallel algorithms are critical for training deep models with large data sets, as in many common information processing applications such as speech recognition and machine translation. The popular mini-batch stochastic gradient technique is known to be difficult to parallelize over computers. The common practice nowadays is to use
GPGPUs to speed up the learning process, although recent advance in developing asynchronous stochastic gradient descent learning has shown promises by using large-scale CPU clusters (e.g.
Le et al., 2012; Dean et al., 2012) and GPU clusters (Coates et al., 2013). In this interesting computing architecture, many different replicas of the DNN compute gradients on different subsets of the training data in parallel. These gradients are communicated to a central parameter server that updates the shared weights. Even though each replica typically computes gradients using parameter values not immediately updated, stochastic gradient descent is robust to the slight errors this has introduced. To make deep learning techniques scalable to very large training data, theoretically sound parallel learning and optimization algorithms together with novel architectures need to be further developed (e.g., Bottou and LeCun, 2004; Chen et al., 2012; Seide et al., 2014;
Dean et al., 2012; Hutchinson et al., 2013; Sutskever, 2013; Bengio et al., 2013). Optimization methods specific to speech recognition problems may need to be taken into account in order to push speech recognition advances to the next level (e.g., Wright et al., 2013; Cardinal et al., 2013;
Heigold et al., 2013a).
One major barrier to the application of DNNs and related deep models is that it currently requires considerable skill and experience to choose sensible values for hyper-parameters such as the learning rate schedule, the strength of the regularizer, the number of layers and the number of units per layer, etc. Sensible values for one hyper-parameter may depend on the values chosen for other hyper-parameters and hyper-parameter tuning in DNNs is especially expensive. Some interesting methods for solving the problem have been developed recently, including random sampling(Bergstra et al., 2012) and Bayesian optimization procedure (Snoek et al., 2012). Further research is needed in this important area.
This book, mainly in Chapters 8 and 11 on natural language and multi-modal applications, has touched on some recent work on using deep learning methods to do reasoning, moving beyond the topic of more straightforward pattern recognition using supervised, unsupervised or hybrid learning methods to which much of this book has been devoted to. In principle, since deep networks are naturally equipped with distributed representations (rf. Table 3.1) using their layerwise collections of units for coding relations and coding entities, concepts, events, topics, etc., they can potentially perform powerful reasoning over structures, as argued in various historical publications as well as recent essays (e.g., Hinton, 1990; Smolensky, 1990; Pollack, 1990; Plate, 1995; Prince and Smolensky, 1997; Bottou, 2013). While initial explorations on this capability of deep networks have recently appeared in the literature, as reviewed in Chapters 8 and 11, much research is needed. If successful, this new type of deep learning "machine" will open up many novel and exciting applications in applied artificial intelligence as a "thinking brain". We expect growing work of deep learning in this area, full of new challenges, in the future.
Further, solid theoretical foundations of deep learning need to be established in a myriad of aspects.
As an example, the success of deep learning in unsupervised learning has not been demonstrated as much as for supervised learning; yet the essence and major motivation of deep learning lie right in unsupervised learning for automatically discovering data representation. The issues involveappropriate objectives for learning effective feature representations and the right deep learning architectures/algorithms for distributed representations to effectively disentangle the hidden explanatory factors of variation in the data. Unfortunately, a majority of the successful deep learning techniques have so far dealt with unstructured or "flat" classification problems. For example, although speech recognition is a sequential classification problem by nature, in the most successful and large-scale systems, a separate HMM is used to handle the sequence structure and the DNN is only used to produce the frame-level, unstructured posterior distributions. Recent proposals have called for and investigated moving beyond the "flat" representations and incorporating structures in both the deep learning architectures and input and output representations (Socher, 2012; Deng, 2013; Srivastava and Salakhutdinov, 2013; Graves et al., Finally, deep learning researchers have been advised by neuroscientists to seriously consider a broader set of issues and learning architectures so as to gain insight into biologically plausible representations in the brain that may be useful for practical applications (e.g., Olshausen, 2012).
How can computational neuroscience models about hierarchical brain structure help improve engineering deep learning architectures? How may the biologically feasible learning styles in the brain (e.g., Hinton, 2003; Xie and Seung, 2003) help design more effective and more robust deep learning algorithms? All these issues and those discussed in this chapter will need intensive research in order to further push the frontier of deep learning.
BIBLIOGRAPHY
Abdel-Hamid, O., Mohamed, A., Jiang, H., and G. Penn, "Applying convolutional neural networks concepts to hybrid NN-HMM model for speech recognition," Proc. ICASSP, 2012.
Abdel-Hamid, O., Deng, L., and Yu. D. "Exploring convolutional neural network structures and optimization for speech recognition," Interspeech, 2013.
Abdel-Hamid, O., Deng, L., Yu. D., Jiang, H. "Deep segmental neural networks for speech recognition," Proc. Interspeech, 2013a.
Acero, A., Deng, L., Kristjansson, T., and Zhang, J. "HMM adaptation using vector Taylor series for noisy speech recognition," Proc. Interspeech, 2000.
Alain, G. and Bengio, Y. "What Regularized Autoencoders Learn from the Data Generating
Distribution," Proc. International Conference on Learning Representations (ICLR), 2013.
Anthes, G. "Deep learning comes of age," Communications of the ACM, Vol. 56 No. 6, pp. 1315, June 2013.
Arel, I., Rose, C., and Karnowski, T. "Deep Machine Learning - A New Frontier in Artificial
Intelligence," IEEE Computational Intelligence Mag., vol. 5, pp. 13-18, November, 2010.
Aslan, O., Cheng, H., Schuurmans, D., and Zhang, X. "Convex two-layer modeling," Proc. NIPS, Ba, J. and Frey, B. "Adaptive dropout for training deep neural networks," Proc. NIPS, 2013.
Baker, J., Deng, L., Glass, J., Khudanpur, S., Lee, C.-H., Morgan, N., and O'Shaughnessy, D.
"Research developments and directions in speech recognition and understanding," IEEE Sig.
Proc. Mag., vol. 26, no. 3, May 2009, pp. 75-80.
Baker, J., Deng, L., Glass, J., Khudanpur, S., Lee, C.-H., Morgan, N., and O'Shaughnessy, D.
"Updated MINS report on speech recognition and understanding," IEEE Sig. Proc. Mag., vol.
26, no. 4, July 2009a.
Baldi, P. and Sadowski, P. "Understanding Dropout," Proc. NIPS, 2013.
Battenberg, E., Schmidt, E., and Bello, J. Deep learning for music, special session at ICASSP(http://www.icassp2014.org/special_sections.html#SS8), 2014.
Batternberg, E. and Wessel, D. "Analyzing drum patterns using conditional deep belief networks,"
Proc. ISMIR, 2012.
Bell, P., Swietojanski, P., and Renals, S. "Multi-level adaptive networks in tandem and hybrid
ASR systems", Proc. ICASSP, 2013.
Bengio, Y., Yao, L., Alain, G., and Vincent, P. "Generalized denoising autoencoders as generative models," Proc. NIPS, 2013.
Bengio, Y. "Deep learning of representations: Looking forward," in: Statistical Language and Speech Processing, pp. 1--37, Springer, 2013.
Bengio, Y., Boulanger, N., and Pascanu, R. "Advances in optimizing recurrent networks," Proc.
ICASSP, 2013.
Bengio, Y., Courville, A., and Vincent, P. "Representation learning: A review and new perspectives," IEEE Trans. PAMI, vol. 38, pp. 1798-1828, 2013a.
Bengio, Y., Thibodeau-Laufer, E., and Yosinski, J. "Deep generative stochastic networks trainable by backprop," arXiv 1306:1091, 2013b.
Bengio, Y. "Deep Learning of Representations for Unsupervised and Transfer Learning," JMLR
Workshop and Conference Proceedings, vol. 27, pp. 17-37, 2012.
Bengio, Y. "Learning deep architectures for AI," in Foundations and Trends in Machine Learning, Vol. 2, No. 1, 2009, pp. 1-127.
Bengio, Y., Lamblin, P., Popovici, D., and Larochelle, H. "Greedy Layer-Wise Training of Deep
Networks," Proc. NIPS, 2006.
Bengio, Y., Ducharme, R., Vincent, P., and Jauvin, C. "A Neural Probabilistic Language Model,"
Journal of Machine Learning Research, vol. 3, pp. 1137-1155, 2003.
Bengio, Y., Ducharme, R., Vincent, P. and Jauvin, C. "A neural probabilistic language model,"
Proc. NIPS, 2000.
Bengio, Y., De Mori, R., Flammia, G., and Kompe, R. "Global Optimization of a Neural NetworkHidden Markov Model Hybrid," IEEE Transactions on Neural Networks, vol. 3, pp. 252-259, Bengio, Y. Artificial Neural Networks and Their Application to Sequence Recognition, Ph.D.
Thesis, McGill University, Montreal, Canada, 1991.
Bergstra, J. and Bengio, Y. "Random search for hyper-parameter optimization," J. Machine
Learning Research," Vol. 3, pp. 281-305, 2012.
Bottou, L. and LeCun. Y. "Large scale online learning," Proc. NIPS, 2004.
Bilmes, J. "Dynamic graphical models," IEEE Signal Processing Mag., vol. 33, pp. 29–42, 2010.
Bilmes, J. and Bartels, C. "Graphical model architectures for speech recognition," IEEE Signal
Processing Mag., vol. 22, pp. 89–100, 2005.
Bordes, A., Weston, J., Collobert, R., and Bengio, Y. "Learning Structured Embeddings of Knowledge Bases," Proc. AAAI, 2011.
Bordes, A., Weston, J., Collobert, R., and Bengio, Y. "Learning Structured Embeddings of Knowledge Bases," Proc. AAAI, 2011.
Bordes, A., Glorot, X., Weston, J., and Bengio, Y. "A semantic matching energy function for learning with multi-relational data --- Application to word-sense disambiguation," Machine
Learning, May, 2013.
Boulanger-Lewandowski, N., Bengio, Y., and Vincent, P. "Audio Chord Recognition with
Recurrent Neural Networks," Proc. ISMIR, 2013.
Bourlard, H. and Morgan, N., Connectionist Speech Recognition: A Hybrid Approach, Norwell, MA: Kluwer, 1993.
Bottou, L. "From machine learning to machine reasoning: an essay," Journal of Machine Learning
Research, Vol. 14, pp. 3207-3260, 2013.
Bouvrie, J. "Hierarchical Learning: Theory with Applications in Speech and Vision," Ph.D. thesis, MIT, 2009.
Bridle, J., Deng, L., Picone, J., Richards, H., Ma, J., Kamm, T., Schuster, M., Pike, S., and Reagan, R. "An investigation of segmental hidden dynamic models of speech coarticulation for automatic speech recognition," Final Report for 1998 Workshop on Language Engineering, CLSP, Johns Hopkins, 1998.
Cardinal, P.; Dumouchel, P.; Boulianne, G., "Large Vocabulary Speech Recognition on Parallel
Architectures," IEEE Transactions on Audio, Speech, and Language Processing, vol.21, no.11, pp.2290,2300, Nov. 2013.
Caruana, R. "Multitask Learning," Machine Learning, Vol. 28, pp. 41-75, 1997.
Chen, J. and Deng, L. "A Primal-Dual Method for Training Recurrent Neural Networks
Constrained by the Echo-State Property", arXiv:1311.6091, pp. 1-16, 2013.
Chen, X., Eversole, A., Li, G., Yu, D. and Seide, F., "Pipelined Back-Propagation for ContextDependent Deep Neural Networks", Proc. Interspeech 2012.
Chengalvarayan, R. and Deng, L. "Speech Trajectory Discrimination using the Minimum
Classification Error Learning," IEEE Transactions on Speech and Audio Processing, vol. 6, no. 6, pp. 505-515, 1998.
Chengalvarayan R. and Deng, L. "HMM-based speech recognition using state-dependent, discriminatively derived transforms on Mel-warped DFT features," IEEE Transactions on
Speech and Audio Processing, pp. 243-256, 1997.
Chengalvarayan R. and Deng, L. "Use of generalized dynamic feature parameters for speech recognition," IEEE Transactions on Speech and Audio Processing, pp. 232-242, 1997a.
Cho, Y. and Saul, L. "Kernel methods for deep learning," Proc. NIPS, pp. 342–350, 2009.
Ciresan, D., Meier, U., and Schmidhuber, J. "Multi-column deep neural networks for image classification." Proc. CVPR, 2012.
Ciresan, D., Giusti, A., Gambardella, L., and Schmidhuber, J. "Deep neural networks segment neuronal membranes in electron microscopy images," Proc. NIPS, 2012a.
Ciresan, D. C., Meier, U., & Schmidhuber, J. "Transfer learning for Latin and Chinese characters with deep neural networks," Proc. IJCNN, 2012b.
Ciresan, D., Meier, U., Masci, J., and Schmidhuber, J. "A committee of neural networks for traffic sign classification," Proc. IJCNN, 2011.
Ciresan, D., Meier, U., Gambardella, L., and Schmidhuber, J. "Deep, Big, Simple Neural Nets for
Handwritten Digit Recognition," Neural Computation, December 2010.
Coates, A., Huval, B., Wang, T., Wu, D., Ng, A., and Catanzaro, B. "Deep Learning with COTS
HPC," Proc. ICML, 2013.
Cohen, W. and R. V. de Carvalho. "Stacked sequential learning," Proc. IJCAI, pp. 671–676, 2005.
Collobert, R. "Deep learning for efficient discriminative parsing," Proc. AISTATS, 2011.
Collobert, R. and Weston J. "A unified architecture for natural language processing: Deep neural networks with multitask learning," Proc. ICML, 2008.
Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., and Kuksa, P. "Natural language processing (almost) from scratch," J. Machine Learning Research, Vo. 12, pp. 2493Dahl, G, Sainath, T., and Hinton, G. "Improving deep neural networks for LVCSR using rectified linear units and dropout," Proc. ICASSP, 2013.
Dahl, G., Stokes, J., Deng, L., and Yu, D. "Large-Scale Malware Classification Using Random
Projections and Neural Networks," Proc. ICASSP, 2013a.
Dahl, G., Yu, D., Deng, L., and Acero, A. "Context-dependent, pre-trained deep neural networks for large vocabulary speech recognition," IEEE Trans. Audio, Speech, & Language Proc., Vol.
20 (1), pp. 30-42, January 2012.
Dahl, G., Yu, D., Deng, L., and Acero, A. "Context-dependent DBN-HMMs in large vocabulary continuous speech recognition," Proc. ICASSP, 2011.
Dahl, G., Ranzato, M., Mohamed, A. and Hinton, G. "Phone recognition with the mean-covariance restricted Boltzmann machine," Proc. NIPS, vol. 23, 2010, 469-477.
Dean, J., Corrado, G., R. Monga, K. Chen, M. Devin, Q. Le, M. Mao, M. Ranzato, A. Senior, P.
Tucker, Yang, K., and Ng, A. "Large Scale Distributed Deep Networks," Proc. NIPS, 2012.
Demuynck, K. and Triefenbach, F. "Porting concepts from DNNs back to GMMs," Proc. ASRU
Deng, L. and Chen, J. "Sequence Classification Using the High-Level Features Extracted from
Deep Neural Networks," Proc. ICASSP, 2014.
Deng, L. "Design and Learning of Output Representations for Speech Recognition," NIPS
Workshop on Learning Output Representations, December 2013.
Deng, L. "A tutorial survey of architectures, algorithms, and applications for deep learning,"
APSIPA Transactions on Signal and Information Processing, 2013.
Deng, L. and Li, X. "Machine learning paradigms in speech recognition: An overview," IEEE
Trans. Audio, Speech, & Language, vol. 21, pp. 1060 – 1089, May 2013.
Deng, L., Abdel-Hamid, O., and Yu, D. "A deep convolutional neural network using heterogeneous pooling for trading acoustic invariance with phonetic confusion," Proc.
ICASSP, 2013.
Deng, L., Li, J., Huang, K., Yao, D. Yu, F. Seide, M. Seltzer, G. Zweig, X. He, J. Williams, Y.
Gong, and A. Acero. "Recent advances in deep learning for speech research at Microsoft,"
Proc. ICASSP, 2013a.
Deng, L., Hinton, G., and Kingsbury, B. "New types of deep neural network learning for speech recognition and related applications: An overview," Proc. ICASSP, 2013b.
Deng, L., He, X., and Gao, J. "Deep stacking networks for information retrieval," Proc. ICASSP, 2013c.
Deng, L. "The MNIST database of handwritten digit images for machine learning research," IEEE
Signal Processing Magazine, no. 141-142, November 2012.
Deng, L., Tur, G, He, X, and Hakkani-Tur, D. "Use of kernel deep convex networks and end-toend learning for spoken language understanding," Proc. IEEE Workshop on Spoken Language
Technologies, December 2012.
Deng, L., Yu, D., and Platt, J. "Scalable stacking and learning for building deep architectures,"
Proc. ICASSP, 2012a.
Deng, L., Hutchinson, B., and Yu, D. "Parallel training of deep stacking networks," Proc.
Interspeech, 2012b.
Deng, L. "An Overview of Deep-Structured Learning for Information Processing," Proceedings of Asian-Pacific Signal & Information Processing Annual Summit and Conference (APSIPAASC), October 2011.
Deng, L. and Yu, D. "Deep Convex Network: A scalable architecture for speech pattern classification," Proc. Interspeech, 2011.
Deng, L., Seltzer, M., Yu, D., Acero, A., Mohamed, A., and Hinton, G. "Binary coding of speech spectrograms using a deep autoencoder," Proc. Interspeech, 2010.
Deng, L., Yu, D., and Hinton, G. "Deep Learning for Speech Recognition and Related
Applications" NIPS Workshop, 2009.
Deng, L. and Yu, D. "Use of differential cepstra as acoustic features in hidden trajectory modeling for phonetic recognition," Proc. ICASSP, 2007.
Deng, L. Dynamic Speech Models – Theory, Algorithm, and Application, Morgan & Claypool, December 2006.
Deng, L., Yu, D. and Acero, A. "Structured speech modeling," IEEE Trans. on Audio, Speech and Language Processing, vol. 14, no. 5, pp. 1492-1504, September 2006
Deng, L., Yu, D. and Acero, A. "A bidirectional target filtering model of speech coarticulation:
Two-stage implementation for phonetic recognition," IEEE Transactions on Audio and Speech
Processing, vol. 14, no. 1, pp. 256-265, January 2006a.
Deng, L., Wu, J., Droppo, J., and Acero, A. "Dynamic Compensation of HMM Variances Using the Feature Enhancement Uncertainty Computed From a Parametric Model of Speech
Distortion," IEEE Transactions on Speech and Audio Processing, vol. 13, no. 3, pp. 412–421, Deng, L. and Huang, X.D. "Challenges in Adopting Speech Recognition, Communications of the ACM, vol. 47, no. 1, pp. 11-13, January 2004.
Deng, L. and O'Shaughnessy, D. SPEECH PROCESSING – A Dynamic and OptimizationOriented Approach, Marcel Dekker, 2003.
Deng, L. "Switching dynamic system models for speech articulation and acoustics," in Mathematical Foundations of Speech and Language Processing, pp. 115–134. SpringerVerlag, New York, 2003.
Deng, L., Wang, K., Acero, A., Hon, Droppo, J., Boulis, C., Wang, Y., Jacoby, D., Mahajan, M., Chelba, C., and Huang, X. "Distributed speech processing in MiPad's multimodal user interface," IEEE Transactions on Speech and Audio Processing, vol. 10, no. 8, pp. 605–619, Deng, L., Acero, A., Jiang, L., Droppo, J., and Huang, X. "High performance robust speech recognition using stereo training data," Proc. ICASSP, 2001.
Deng, L. and Ma, J. "Spontaneous speech recognition using a statistical coarticulatory model for the vocal tract resonance dynamics," J. Acoust. Soc. Am., vol. 108, pp. 3036-3048, 2000.
Deng, L. "Computational Models for Speech Production," in Computational Models of Speech
Pattern Processing, pp. 199-213, Springer Verlag, 1999.
Deng, L. "A dynamic, feature-based approach to the interface between phonology and phonetics for speech modeling and recognition,' Speech Communication, vol. 24, no. 4, pp. 299-323, Deng L. and Aksmanovic, M. "Speaker-independent phonetic classification using hidden Markov models with state-conditioned mixtures of trend functions," IEEE Trans. Speech and Audio
Processing, vol. 5, pp. 319-324, 1997.
Deng, L., Ramsay, G., and Sun, D. "Production models as a structural basis for automatic speech recognition," Speech Communication, vol. 33, no. 2-3, pp. 93–111, Aug 1997.
Deng, L. and Sameti, H. "Transitional speech units and their representation by regressive Markov states: Applications to speech recognition," IEEE Transactions on speech and audio processing, vol. 4, no. 4, pp. 301–306, July 1996.
Deng, L., Aksmanovic, M., Sun, D., and Wu, J. "Speech recognition using hidden Markov models with polynomial regression functions as nonstationary states," IEEE Transactions on Speech and Audio Processing, vol. 2, no. 4, pp. 507-520, 1994.
Deng L. and Sun, D. "A statistical approach to automatic speech recognition using the atomic speech units constructed from overlapping articulatory features," Journal of the Acoustical
Society of America, vol. 85, no. 5, pp. 2702-2719, 1994.
Deng, L., Hassanein, K., and Elmasry, M. "Analysis of correlation structure for a neural predictive model with application to speech recognition," Neural Networks, vol. 7, no. 2, pp. 331-339, 1994a.
Deng, L. "A stochastic model of speech incorporating hierarchical nonstationarity," IEEE
Transactions on Speech and Audio Processing, vol. 1, no. 4, pp. 471-475, 1993.
Deng, L. "A generalized hidden Markov model with state-conditioned trend functions of time for the speech signal," Signal Processing, vol. 27, no. 1, pp. 65–78, 1992.
Deng. L. and Erler, K. "Structural design of a hidden Markov model based speech recognizer using multi-valued phonetic features: Comparison with segmental speech units," Journal of the Acoustical Society of America, vol. 92, no. 6, pp. 3058-3067, 1992.
Deng, L. Lennig, M. Gupta, V., Seitz, F., and Mermelstein, P., and Kenny, P. "Phonemic hidden
Markov models with continuous mixture output densities for large vocabulary word recognition," IEEE Transactions on Signal Processing, vol. 39, no. 7, pp. 1677-1681, 1991.
Deng, L. Lennig, M., Seitz, F., and Mermelstein, P. "Large vocabulary word recognition using context-dependent allophonic hidden Markov models," Computer Speech and Language, vol.
4, no. 4, pp. 345-357, 1990.
Deselaers, T., Hasan, S., Bender, O. and Ney, H. "A deep learning approach to machine transliteration," Proc. 4th Workshop on Statistical Machine Translation, pp. 233–241, Athens, Greece, March 2009.
Diez, A. "Automatic language recognition using deep neural networks," Thesis, Universidad
Autonoma de Madrid, SPAIN, September 2013.
Dognin, P. and Goel, V. "Combining stochastic average gradient and Hessian-free optimiztion for sequence training of deep neural networks," Proc. ASRU, 2013.
Erhan, D., Bengio, Y., Courvelle, A., Manzagol, P., Vencent, P., and Bengio, S. "Why does unsupervised pre-training help deep learning?" J. Machine Learning Research, pp. 201-208, Fernandez, R., Rendel, A., Ramabhadran, B., and Hoory, R. "F0 contour prediction with a deep belief network-Gaussian process hybrid Model," Proc. ICASSP, pp. 6885-6889, 2013.
Fine, S., Singer, Y. and Tishby, N. "The hierarchical hidden Markov model: Analysis and applications," Machine Learning, vol. 32, p. 41-62, 1998.
Frome, A., Corrado, G., Shlens, J., Bengio, S., Dean, J., Ranzato, M., and Mikolov, T. "DeViSE:
A Deep Visual-Semantic Embedding Model," Proc. NIPS, 2013.
Fu, Q., He, X., and Deng, L. "Phone-Discriminating Minimum Classification Error (P-MCE)
Training for Phonetic Recognition," Proc. Interspeech, 2007.
Gales, M. "Model-based approaches to handling uncertainty," in Robust Speech Recognition of Uncertain or Missing Data: Theory and Application, pp. 101–125. Springer, 2011.
Gao, J., He, X., Yih, W. and Deng, L. "Learning semantic representations for the phrase translation model," Proc. NIPS Workshop on Deep Learning, December, 2013.
Gao, J., He, X., Yih, W. and Deng, L. "Learning Semantic Representations for the Phrase
Translation Model," MSR-TR-2013-88, September 2013.
Gao, J., Toutanova, K., Yih., W-T. "Clickthrough-based latent semantic models for web search,"
Proc. SIGIR, 2011.
Gao, J., He, X., and Nie, J-Y. "Clickthrough-based translation models for web search: From word models to phrase models," Proc. CIKM, 2010.
Gens, R. and Domingo, P. "Discriminative learning of sum-product networks," NIPS, 2012.
George, D. "How the Brain Might Work: A Hierarchical and Temporal Model for Learning and Recognition," Ph.D. thesis, Stanford University, 2008.
Gibson, M. and Hain, T. "Error approximation and minimum phone error acoustic model estimation," IEEE Trans. Audio, Speech, and Language Proc., vol. 18, no. 6, August 2010, pp.
1269-1279.
Girshick, R., Donahue, J., Darrell, T., and Malik, J. "Rich feature hierarchies for accurate object detection and semantic segmentation," arXiv:1311.2524v1, 2013.
Glorot, X., Bordes, A., and Bengio, Y. "Deep sparse rectifier neural networks," Proc. AISTAT, April 2011.
Glorot, X. and Bengio, Y. "Understanding the difficulty of training deep feed-forward neural networks" Proc. AISTAT, 2010.
Goodfellow, I., Mirza, M., Courville, A., and Bengio, Y. "Multi-Prediction Deep Boltzmann
Machines," Proc. NIPS, 2013.
Grais, E., Sen, M., and Erdogan, H. "Deep neural networks for single channel source separation," arXiv:1311.2746v1, 2013.
Graves, A., Fernandez, S., Gomez, F., and Schmidhuber, J. "Connectionist temporal classification:
Labeling unsegmented sequence data with recurrent neural networks," Proc. ICML, 2006.
Graves, A. "Sequence Transduction with Recurrent Neural Networks," Representation Learning
Workshop, ICML 2012.
Graves, A., Mohamed, A., and Hinton, G. "Speech recognition with deep recurrent neural networks," Proc. ICASSP, 2013.
Graves, A., Jaitly, N., and Mahamed, A. "Hybrid speech recognition with deep bidirectional
LSTM," Proc. ASRU, 2013a.
Gutmann, M. and Hyvarinen, A. "Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics," Journal of Machine Learning Research, vol. 13, pp. 307–361, 2012.
Hain, T., Burget, L., Dines, J., Garner, P., Grezl, F., Hannani, A., Huijbregts, M., Karafiat, M., Lincoln, M., and Wan, V. "Transcribing meetings with the AMIDA systems," IEEE
Transactions on Audio, Speech, and Language Processing, vol. 20, pp. 486-498, 2012.
Hamel, P. and Eck, D. "Learning Features from Music Audio with Deep Belief Networks," Proc.
ISMIR, 2010.
Hawkins, J. and Blakeslee, S. On Intelligence: How a New Understanding of the Brain will lead to the Creation of Truly Intelligent Machines, Times Books, New York, 2004.
Hawkins, G., Ahmad, S. and Dubinsky, D. "Hierarchical Temporal Memory Including HTM
Cortical Learning Algorithms," Numenta Tech. Report, December 10, 2010.
He, X., Deng, L., Chou, W. "Discriminative learning in sequential pattern recognition – A unifying review for optimization-oriented speech recognition," IEEE Sig. Proc. Mag., vol. 25, 2008, pp.
14-36.
He, X. and Deng, L. "Speech recognition, machine translation, and speech translation – A unifying discriminative framework," IEEE Sig. Proc. Magazine, Vol. 28, November, 2011.
He, X. and Deng, L. "Optimization in speech-centric information processing: Criteria and techniques," Proc. ICASSP, 2012.
He, X. and Deng, L. "Speech-centric information processing: An optimization-oriented approach,"
Proc. of the IEEE, 2013.
Heigold, G., Vanhoucke, V., Senior, A. Nguyen, P., Ranzato, M., Devin, M., and Dean, J.
"Multilingual acoustic models using distributed deep neural networks," Proc. ICASSP, 2013.
Heigold, G., Ney, H., and Schluter, R., "Investigations on an EM-Style Optimization Algorithm for Discriminative Training of HMMs," IEEE Transactions on Audio, Speech, and Language
Processing, vol.21, no.12, pp. 2616-2626, Dec. 2013a.
Heigold, G., Ney, H., Lehnen, P., Gass, T., Schluter, R. "Equivalence of generative and log-liner models," IEEE Trans. Audio, Speech, and Language Proc., vol. 19, no. 5, February 2011, pp.
1138-1148.
Heintz, I., Fosler-Lussier, E., and Brew, C. "Discriminative input stream combination for conditional random field phone recognition," IEEE Trans. Audio, Speech, and Language Proc., vol. 17, no. 8, Nov. 2009, pp. 1533-1546.
Henderson, M., Thomson, B., and Young, S. "Deep Neural Network Approach for the Dialog State
Tracking Challenge," Proc. SIGDIAL, 2013.
Hermans, M. and Schrauwen, B. "Training and Analysing Deep Recurrent Neural Networks,"
Proc. NIPS, 2013.
Hermansky, H., Ellis, D., and Sharma, S. "Tandem connectionist feature extraction for conventional HMM systems", Proc. ICASSP, 2000.
Hifny, Y. and Renals, S. "Speech recognition using augmented conditional random fields," IEEE
Trans. Audio, Speech, and Language Proc., vol. 17, no. 2, February 2009, pp. 354-365.
Hinton, G. and Salakhutdinov, R. "Discovering binary codes for documents by learning deep generative models," Topics in Cognitive Science, pp. 1-18, 2010.
Hinton, G., Deng, L., Yu, D., Dahl, G., Mohamed, A., Jaitly, N., Senior, A., Vanhoucke, V., Nguyen, P., Sainath, T., and Kingsbury, B., "Deep Neural Networks for Acoustic Modeling in Speech Recognition," IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82-97, November
Hinton, G., Srivastava, N., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. "Improving neural networks by preventing co-adaptation of feature detectors," arXiv: 1207.0580v1, 2012a.
Hinton, G. "A better way to learn features," Communications of the ACM," Vol. 54, No. 10, October, 2011.
Hinton, G., Krizhevsky, A., and Wang, S. "Transforming autoencoders," Proc. Intern. Conf.
Artificial Neural Networks, 2011.
Hinton, G. "A practical guide to training restricted Boltzmann machines," UTML Tech Report
2010-003, Univ. Toronto, August 2010.
Hinton, G., Osindero, S., and Teh, Y. "A fast learning algorithm for deep belief nets," Neural
Computation, vol. 18, pp. 1527-1554, 2006.
Hinton, G. and Salakhutdinov, R. "Reducing the dimensionality of data with neural networks,"
Science, vol. 313. no. 5786, pp. 504 - 507, July 2006.
Hinton, G. "The ups and downs of Hebb synapses," Canadian Psychology, vol. 44, pp 10-13, Hinton, G. "Mapping part-whole hierarchies into connectionist networks," Artificial Intelligence, Vol. 46, pp. 47-75, 1990.
Hinton, G. "Preface to the special issue on connectionist symbol processing," Artificial
Intelligence, Vol. 46, pp. 1-4, 1990a.
Hochreiter, S., and Schmidhuber, J. "Long Short-Term Memory." Neural Computation, vol. 9, pp.
1735-1780, 1997.
Huang, J., Li, J., Deng, L., and Yu, D. "Cross-language knowledge transfer using multilingual deep neural networks with shared hidden layers," Proc. ICASSP, 2013.
Huang, P., Kumar, K., Liu, C., Gong, Y., and Deng, L. "Predicting speech recognition confidence using deep learning with word identity and score features," Proc. ICASSP, 2013a.
Huang, P., Deng, L., Hasegawa-Johnson, M., and He, X. "Random Features for Kernel Deep
Convex Network," Proc. ICASSP, 2013.
Huang, S. and Renals, S. "Hierarchical Bayesian language models for conversational speech recognition," IEEE Trans. Audio, Speech, and Language Proc., vol. 18, no. 8, November 2010, pp. 1941-1954.
Huang, E., Socher, R., Manning, C, and Ng, A. "Improving word representations via global context and multiple word prototypes," Proc. ACL, 2012.
Huang, P., He, X., Gao, J., Deng, L., Acero, A., and Heck, L. "Learning Deep Structured Semantic
Models for Web Search using Clickthrough Data," ACM Intern. Conf. Information and Knowledge Management (CIKM), 2013.
Huang, X., Acero, A., Chelba, C., Deng, L., Droppo, J., Duchene, D., Goodman, J., and Hon, H.
"MiPad: A multimodal interaction prototype," Proc. ICASSP, 2001.
Huang, Y., Yu, D., Gong, Y., and Liu, C. "Semi-Supervised GMM and DNN Acoustic Model
Training with Multi-system Combination and Confidence Re-calibration", Proc. Interspeech
2013, pp. 2360-2364.
Humphrey, E., Bello, J., and LeCun, Y. "Feature learning and deep architectures: New directions for music informatics," Journal of Intelligent Information Systems, 2013.
Humphrey, E., Bello, J., and LeCun, Y. "Moving beyond feature design: Deep architectures and automatic feature learning in music informatics," Proc. ISMIR, 2012.
Humphrey, E. and Bello, J. "Rethinking automatic chord recognition with convolutional neural networks," Proc. ICMLA, 2012a.
Hutchinson, B., Deng, L., and Yu, D. "A deep architecture with bilinear modeling of hidden representations: Applications to phonetic recognition," Proc. ICASSP, 2012.
Hutchinson, B., Deng, L., and Yu, D. "Tensor deep stacking networks," IEEE Trans. Pattern
Analysis and Machine Intelligence, vol. 35, pp. 1944 – 1957, 2013.
Imseng, D., Motlicek, P., Garner, P., and Bourlard, H. "Impact of deep MLP architecture on different modeling techniques for under-resourced speech recognition," Proc. ASRU, 2013.
Jaitly, N. and Hinton, G. "Learning a better representation of speech sound waves using restricted
Boltzmann machines," Proc. ICASSP, 2011.
Jaitly, N., Nguyen, P., and Vanhoucke, V. "Application of pre-trained deep neural networks to large vocabulary speech recognition," Proc. Interspeech, 2012.
Jarrett, K., Kavukcuoglu, K. and LeCun, Y. "What is the best multistage architecture for object recognition?" Proc. Intl. Conf. Computer Vision, pp. 2146–2153, 2009.
Jiang, H. and Li, X. "Parameter estimation of statistical models using convex optimization: An advanced method of discriminative training for speech and language processing," IEEE Signal
Processing Magazine, vol. 27, no. 3, pp. 115–127, 2010.
Juang, B.-H., Chou, W., and Lee, C.-H. "Minimum classification error rate methods for speech recognition," IEEE Trans. On Speech and Audio Processing, vol. 5, pp. 257–265, 1997.
Juang, B., Levinson, S. and Sondhi, M. "Maximum likelihood estimation for multivariate mixture observations of Markov chains," IEEE Trans. Inform. Theory, vol. 32, pp. 307–309, 1986.
Kahou, S. et al. "Combining modality specific deep neural networks for emotion recognition in video," Proc. ICMI, 2013.
Kang, S., Qian, X., and Meng, H. "Multi-distribution deep belief network for speech synthesis,"
Proc. ICASSP, 2013, pp. 8012-8016.
Kashiwagi, Y., Saito, D., Minematsu, N., and Hirose, K. "Discriminative piecewise linear transformation based on deep learning for noise robust automatic speech recognition," Proc.
ASRU, 2013.
Kavukcuoglu, K., Sermanet, P., Boureau, Y., Gregor, K., Mathieu M., and LeCun, Y. "Learning
Convolutional Feature Hierarchies for Visual Recognition," Proc. NIPS, 2010.
Ketabdar, H. and Bourlard, H. "Enhanced phone posteriors for improving speech recognition systems," IEEE Trans. Audio, Speech, and Language Proc., vol. 18, no. 6, August 2010, pp.
1094-1106.
Kingsbury, B. "Lattice-based optimization of sequence classification criteria for neural-network acoustic modeling," Proc. ICASSP, 2009.
Kingsbury, B., Sainath, T., and Soltau, H. "Scalable minimum Bayes risk training of deep neural network acoustic models using distributed Hessian-free optimization," Proc. Interspeech, Kiros, R., Zemel, R., and Salakhutdinov, R. "Multimodal Neural Language Models," Proc. NIPS
Deep Learning Workshop, 2013.
Ko, T. and Mak, B. "Eigentriphones for Context-Dependent Acoustic Modeling," IEEE
Transactions on Audio, Speech, and Language Processing, vol.21, no. 6, pp. 1285-1294, 2013.
Krizhevsky, A., Sutskever, I. and Hinton, G. "ImageNet classification with deep convolutional neural Networks," Proc. NIPS 2012.
Kubo, Y., Hori, T., and Nakamura, A. "Integrating deep neural networks into structural classification approach based on weighted finite-state transducers," Proc. Interspeech, 2012.
Kurzweil R. How to Create a Mind. Viking Books, Dec., 2012.
Lal, P.; King, S. "Cross-Lingual Automatic Speech Recognition Using Tandem Features," IEEE
Transactions on Audio, Speech, and Language Processing, vol.21, no.12, pp. 2506-2515, Dec.
Lang, K., Waibel, A., and Hinton, G. "A time-delay neural network architecture for isolated word recognition," Neural Networks, Vol. 3(1), pp. 23-43, 1990.
Larochelle, H. and Bengio, Y. "Classification using discriminative restricted Boltzmann machines," Proc. ICML, 2008.
Le, H., Oparin, I., Allauzen, A., Gauvain, J.-L., and Yvon, F. "Structured Output Layer Neural
Network Language Models for Speech Recognition," IEEE Transactions on Audio, Speech, and Language Processing, vol.21, no.1, pp.197-206, Jan. 2013.
Le, H., Oparin, I., Allauzen, A., Gauvain, J., and Yvon, F. "Structured output layer neural network language model," Proc. ICASSP, 2011.
Le, H., Allauzen, A., Wisniewski, G., and Yvon, F. "Training continuous space language models:
Some practical issues," Proc. EMNLP, 2010, pp. 778–788.
Le, D. and Mower P. "Emotion recognition from spontaneous speech using Hidden Markov models with deep belief networks," Proc. ASRU, 2013.
Le, Q., Ranzato, M., Monga, R., Devin, M., Corrado, G., Chen, K., Dean, J., Ng, A. "Building
High-Level Features Using Large Scale Unsupervised Learning," Proc. ICML 2012.
Le, Q., Ngiam, J., Coates, A., Lahiri, A., Prochnow, B., and Ng, A. "On optimization methods for deep learning," Proc. ICML, 2011.
LeCun, Y. "Learning invariant feature hierarchies," Proc. ECCV, 2012.
LeCun, Y., Chopra S., Ranzato, M., and Huang, F. "Energy-based models in document recognition and computer vision," Proc. Intern. Conf. Document Analysis and Recognition (ICDAR), LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. "Gradient-based learning applied to document recognition," Proceedings of the IEEE, Vol. 86, pp. 2278-2324, 1998.
LeCun, Y. and Bengio, Y. "Convolutional networks for images, speech, and time series," in The Handbook of Brain Theory and Neural Networks (M. Arbib, ed.), pp. 255- 258, Cambridge, Massachusetts: MIT Press, 1995.
Lee, C.-H. "From knowledge-ignorant to knowledge-rich modeling: A new speech research paradigm for next-generation automatic speech recognition," Proc. ICSLP, 2004, p. 109-111.
Lee, H., Grosse, R., Ranganath, R., and Ng, A. "Unsupervised learning of hierarchical representations with convolutional deep belief networks," Communications of the ACM," Vol.
54, No. 10, October, 2011, pp. 95-103.
Lee, H., Grosse, R., Ranganath, R., and Ng, A. "Convolutional Deep Belief Networks for Scalable
Unsupervised Learning of Hierarchical Representations," Proc. ICML, 2009.
Lee, H., Largman, Y., Pham, P., Ng, A. "Unsupervised feature learning for audio classification using convolutional deep belief networks," Proc. NIPS, 2010.
Lena, P., Nagata, K., and Baldi, P. "Deep spatiotemporal architectures and learning for protein structure prediction," Proc. NIPS, 2012.
Levine, S. "Exploring deep and recurrent architectures for optimal control", arXiv:1311.1761v1.
Li, L., Zhao, Y., Jiang, D., Zhang, Y., etc. "Hybrid Deep Neural Network--Hidden Markov Model(DNN-HMM) Based Speech Emotion Recognition," Proc. Conf. Affective Computing and Intelligent Interaction (ACII), pp.312-317, Sept. 2013.
Li, J., Deng, L., Gong, Y., and Haeb-Umbach, R. "An Overview of Noise-Robust Automatic
Speech Recognition," IEEE/ACM Transactions on Audio, Speech, and Language Processing, pp. 1-33, 2014.
Li, J., Yu, D., Huang, J., and Gong, Y. "Improving wideband speech recognition using mixedbandwidth training data in CD-DNN-HMM," Proc. IEEE SLT 2012.
Liao, H., McDermott, E., and Senior, A. "Large scale deep neural network acoustic modeling with semi-supervised training data for YouTube video transcription," Proc. ASRU, 2013.
Lin, H., Deng, L., Yu, D., Gong, Y., Acero, A., and C-H Lee, "A study on multilingual acoustic modeling for large vocabulary ASR." Proc. ICASSP, 2009.
Lin, Y., Lv, F., Zhu, S., Yang, M., Cour, T, Yu, K., Cao, L., and Huang, T. "Large-scale Image
Classification: Fast Feature Extraction and SVM Training," Proc. CVPR, 2011.
Ling, Z., Richmond, K., and Yamagishi, J. "Articulatory control of HMM-based parametric speech synthesis using feature-space-switched multiple regression," IEEE Trans. Audio, Speech, and Language Proc., Vol. 21, Jan, 2013.
Ling, Z., Deng, L. and Yu, D. "Modeling spectral envelopes using restricted Boltzmann machines for statistical parametric speech synthesis," in ICASSP, 2013, pp. 7825–7829.
Ling, Z., Deng, L. and Yu, D. "Modeling spectral envelopes using restricted Boltzmann machines and deep belief networks for statistical parametric speech synthesis," IEEE Trans. Audio
Speech Lang. Process., vol. 21, no. 10. pp. 2129-2139, 2013a.
Lu, L., Chin, K., Ghoshal, A., and Renals, S. "Joint uncertainty decoding for noise robust subspace
Gaussian mixture models," IEEE Trans. Audio, Speech, and Language Processing, vol. 21, no.
9, pp. 1791–1804, 2013.
Ma, J. and Deng, L. "Target-Directed Mixture Dynamic Models for Spontaneous Speech
Recognition," IEEE Trans. Speech and Audio Processing, vol. 12, no. 1, pp. 47-58, 2004.
Ma, J. and Deng, L. "Efficient Decoding Strategies for Conversational Speech Recognition Using a Constrained Nonlinear State-Space Model," IEEE Trans. Speech and Audio Processing, vol.
11, no. 6, pp. 590-602, 2003.
Ma, J. and Deng, L. "A Path-Stack Algorithm for Optimizing Dynamic Regimes in a Statistical
Hidden Dynamical Model of Speech," Computer, Speech and Language, 2000.
Manning, C., Raghavan, P., and Schütze, H. Introduction to Information Retrieval, Cambridge
University Press. 2009.
Maas, A., Hannun, A., and Ng, A. "Rectifier nonlinearities improve neural network acoustic models," ICML Workshop on Deep Learning for Audio, Speech, and Language Processing, Maas, A., Le, Q., O'Neil, T., Vinyals, O., Nguyen, P., and Ng, P. "Recurrent Neural Networks for
Noise Reduction in Robust ASR," Proc. Interspeech, 2012.
Markoff, J. "Scientists See Promise in Deep-Learning Programs," New York Times, Nov 24, 2012.
Martens, J. "Deep learning with Hessian-free optimization," Proc. ICML, 2010.
Martens, J. and Sutskever, I. "Learning recurrent neural networks with Hessian-free optimization,"
Proc. ICML, 2011.
McAllester, D. "A PAC-Bayesian Tutorial with a Dropout Bound," ArXive1307.2118, July, 2013.
McGraw, I.; Badr, I.; Glass, J.R., "Learning Lexicons From Speech Using a Pronunciation Mixture
Model," IEEE Transactions on Audio, Speech, and Language Processing, vol.21, no.2, pp.357,366, Feb. 2013.
Mesnil, G., He, X., Deng, L. and Bengio, Y. "Investigation of Recurrent-Neural-Network
Architectures and Learning Methods for Spoken Language Understanding," Proc. Interspeech
Miao, Y. and Metze, F. "Improving Low-Resource CD-DNN-HMM using Dropout and Multilingual DNN Training," Proc. Interspeech, 2013.
Miao, Y., Rawat, S., and Metze, F. "Deep maxout networks for low resource speech recognition,"
Proc. ASRU 2013.
Mikolov, T., Sutskever, I., Chen, K., Corrado, G., and Dean, J. "Distributed Representations ofWords and Phrases and their Compositionality," Proc. NIPS, 2013.
Mikolov, T., Chen, K., Corrado, G., and Dean, J. "Efficient estimation of word representations in vector space," Proc. ICLR, 2013a.
Mikolov, T., Le, Q., and Sutskever, I. "Exploiting Similarities among Languages for Machine
Translation," arXiv:1309.4168v1, 2013b.
Mikolov, T. "Statistical Language Models based on Neural Networks," PhD thesis, Brno
University of Technology, 2012.
Mikolov, T., Deoras, A., Povey, D., Burget, L., and Cernocky, J. "Strategies for training large scale neural network language models," Proc. IEEE ASRU, 2011.
Mikolov, T., Karafiat, M., Burget, L., Cernocky, J., and Khudanpur, S. "Recurrent neural network based language model," Proc. ICASSP, 2010, 1045–1048.
Minami, Y., McDermott, E. Nakamura, A. and Katagiri, S. "A recognition method with parametric trajectory synthesized using direct relations between static and dynamic feature vector time series," Proc. ICASSP, pp. 957-960, 2002.
Mnih, V., Kavukcuoglu, K. Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M. "Playing Arari with deep reinforcement learning," NIPS Deep Learning Workshop, 2013, also arXiv:1312.5602v1.
Mnih, A. and Kavukcuoglu, K. "Learning word embeddings efficiently with noise-contrastive estimation," Proc. NIPS, 2013.
Mnih, A. and Teh, W.-T. "A fast and simple algorithm for training neural probabilistic language
Models," Proc. ICML, pp. 1751–1758, 2012.
Mnih, A. and Hinton G. "A scalable hierarchical distributed language model" Proc. NIPS, 2008, pp. 1081-1088.
Mnih, A. and Hinton G. "Three new graphical models for statistical language modeling," Proc.
ICML, 2007, pp. 641-648.
Mohamed, A., Dahl, G. and Hinton, G. "Acoustic modeling using deep belief networks", IEEE
Trans. Audio, Speech, & Language Proc. Vol. 20 (1), January 2012.
Mohamed, A., Hinton, G., and Penn, G., "Understanding how deep belief networks perform acoustic modelling," Proc. ICASSP, 2012a.
Mohamed, A., Yu, D., and Deng, L. "Investigation of full-sequence training of deep belief networks for speech recognition," Proc. Interspeech, 2010.
Mohamed, A., Dahl, G., and Hinton, G. "Deep belief networks for phone recognition," in Proc.
NIPS Workshop Deep Learning for Speech Recognition and Related Applications, 2009.
Morgan, N. "Deep and Wide: Multiple Layers in Automatic Speech Recognition," IEEE Trans.
Audio, Speech, & Language Proc. Vol. 20 (1), January 2012.
Morgan, N., Q. Zhu, A. Stolcke, K. Sonmez, S. Sivadas, T. Shinozaki, M. Ostendorf, P. Jain, H.
Hermansky, D. Ellis, G. Doddington, B. Chen, O. Cretin, H. Bourlard,, and M. Athineos, "Pushing the envelope - aside [speech recognition]," IEEE Signal Processing Magazine, vol.
22, no. 5, pp. 81–88, Sep 2005.
Murphy, K. Machine Learning – A Probabilistic Perspective, the MIT Press, 2012.
Nair, V. and Hinton, G. "3-d object recognition with deep belief nets," Proc. NIPS, 2009.
Nakashika, T., Takashima, R., Takiguchi, T., and Ariki, Y. "Voice conversion in high-order eigen space using deep belief nets," Proc. Interspeech, 2013.
Ney, H. "Speech translation: Coupling of recognition and translation," Proc. ICASSP, 1999.
Ngiam, J., Khosla, A., Kim, M., Nam, J., Lee, H., and Ng, A. "Multimodal deep learning," Proc.
ICML, 2011.
Ngiam, J., Chen, Z., Koh, P., and Ng, A. "Learning deep energy models," Proc. ICML, 2011.
Norouzi, M., Mikolov, T., Bengio, S., Shlens, J., Frome, A., Corrado, G. and Dean, J. "Zero-shot learning by convex combination of semantic embeddings," arXiv:1312.5650v2, 2013.
Oliver, N., Garg, A., and Horvitz, E. "Layered Representations for Learning and Inferring Office
Activity from Multiple Sensory Channels," Computer Vision and Image Understanding," vol.
96, pp. 163-180, 2004.
Olshausen, B. "Can 'Deep Learning' offer deep insights about Visual Representation?" NIPS
Workshop on Deep Learning and Unsupervised Feature Learning, 2012.
Ostendorf, M. "Moving beyond the 'beads-on-a-string'model of speech," Proc. ASRU, 1999.
Ostendorf, M., Digalakis, V., and O. Kimball, "From HMMs to segment models: A unified view of stochastic modeling for speech recognition," IEEE Trans. Speech and Audio Proc., vol. 4, no. 5, September 1996.
Oudre, L., Fevotte, C., and Grenier, Y. "Probabilistic Template-Based Chord Recognition," IEEE
Transactions on Audio, Speech, and Language Processing,, vol.19, no.8, pp.2249-2259, Nov.
Palangi, H., Ward, R., Deng, L. "Using deep stacking network to improve structured compressive sensing with multiple measurement vectors," Proc. ICASSP, 2013.
Palangi, H., Deng, L. and Ward, R. "Learning Input and Recurrent Weight Matrices in Echo State
Networks," NIPS Deep Learning Workshop, December 2013a.
Papandreou, G., Katsamanis, A., Pitsikalis, V., and Maragos, P. "Adaptive multimodal fusion by uncertainty compensation with application to audiovisual speech recognition," IEEE Trans.
Audio, Speech, and Lang. Processing, Vol.17, pp. 423-435, 2009.
Pascanu, R., Mikolov, T., and Bengio, Y. "On the difficulty of training recurrent neural networks,"
Proc. ICML, 2013.
Peng, J., Bo, L., and Xu, J. "Conditional neural fields," Proc. NIPS, 2009.
Picone, P., S. Pike, R. Regan, T. Kamm, J. bridle, L. Deng, Z. Ma, H. Richards, and M. Schuster, "Initial evaluation of hidden dynamic models on conversational speech," Proc. ICASSP, 1999.
Pinto, J., Garimella, S., Magimai-Doss, M., Hermansky, H., and Bourlard, H. "Analysis of MLPbased hierarchical phone posterior probability estimators," IEEE Trans. Audio, Speech, and Language Proc., vol. 19, no. 2, Feb. 2011.
Plahl, C., Schlüter, R., and Ney, H. "Hierarchical Bottleneck Features for LVCSR," Proc.
Interspeech, 2010.
Plate, T. "Holographic reduced representations," IEEE Transactions on Neural Networks, Vol. 6, No. 3, pp. 623-641, May 1995.
Poggio. T. "How the Brain Might Work: The Role of Information and Learning in Understanding and Replicating Intelligence," In: Information: Science and Technology for the New Century, Editors: G. Jacovitt, A. Pettorossi, R. Consolo and V. Senni, Lateran University Press, pp. 45Pollack, J. "Recursive Distributed Representations," Artificial Intelligence, vol. 46, pp. 77-105, Poon, H. and Domingos, P. "Sum-product networks: A new deep architecture," Proc. UAI, 2011.
Povey, D. and Woodland, P. "Minimum phone error and I-smoothing for improved discriminative training," Proc. ICASSP, 2002.
Prabhavalkar, R. and Fosler-Lussier, E. "Backpropagation training for multilayer conditional random field based phone recognition", Proc. ICASSP, 2010.
Prince, A. and Smolensky, P. "Optimality: From neural networks to universal grammar," Science, vol. 275, pp. 1604-1610, 1997.
Rabiner, L. "A tutorial on hidden Markov models and selected applications in speech recognition,"
Proceedings of the IEEE, pp. 257-286, 1989.
Ranzato, M., Susskind, J., Mnih, V., and Hinton, G. "On deep generative models with applications to recognition," Proc. CVPR, 2011.
Ranzato, M., Chopra, S. and LeCun, Y., and Huang, F.-J. "Energy-based models in document recognition and computer vision," Proc. International Conference on Document Analysis and Recognition (ICDAR), 2007.
Ranzato, M., Boureau, Y., and LeCun, Y. "Sparse Feature Learning for Deep Belief Networks,"
Proc. NIPS, 2007.
Ranzato, M., Poultney, C., Chopra, S., and LeCun, Y. "Efficient Learning of Sparse
Representations with an Energy-Based Model," Proc. NIPS, 2006.
Rathinavalu C. and Deng, L. "Construction of state-dependent dynamic parameters by maximum likelihood: Applications to speech recognition," Signal Processing, vol. 55, no. 2, pp. 149-165, Rennie, S., Hershey, H., and Olsen, P. "Single-channel multi-talker speech recognition —
Graphical modeling approaches," IEEE Signal Processing Mag., vol. 33, pp. 66–80, 2010.
Riedmiller, M. and Braun, H. "A Direct Adaptive Method for Faster Backpropagation Learning:
The RPROP Algorithm," Proc. IEEE International Conf. Neural Networks, 1993.
Rifai, S., Vincent, P., X. Muller, X. Glorot, and Y. Bengio, "Contractive autoencoders: Explicit invariance during feature extraction," Proc. ICML, 2011, pp. 833-840.
Robinson, A. "An application of recurrent nets to phone probability estimation," IEEE Trans.
Neural Networks, Vol. 5, pp. 298-305, 1994.
Sainath, T., Kingsbury, B., Soltau, H., and Ramabhadran, B. "Optimization Techniques to Improve
Training Speed of Deep Neural Networks for Large Speech Tasks," IEEE Transactions on
Audio, Speech, and Language Processing, vol.21, no.11, pp.2267-2276, Nov. 2013.
Sainath, T., Mohamed, A., Kingsbury, B., and Ramabhadran, B. "Convolutional neural networks for LVCSR," Proc. ICASSP, 2013a.
Sainath, T., Kingsbury, B., Mohamed, A., and Ramabhadran, B. "Learning filter banks within a deep neural network framework," Proc. ASRU, 2013b.
Sainath, T., Kingsbury, B., Sindhwani, Arisoy, E., and Ramabhadran, B. "Low-rank matrix factorization for deep neural network training with high-dimensional output targets," Proc.
ICASSP, 2013c.
Sainath, T., Horesh, L., Kingsbury, B., Aravkin, A., and B. Ramabhadran. "Accelerating Hessianfree optimization for deep neural networks by implicit pre-conditioning and sampling," arXiv:
1309.1508v3, 2013d.
Sainath, T., Kingsbury, Mohamed, A., Dahl, G., Saon, G., Soltau, H., Beran, T., Aravkin, A., and B. Ramabhadran. "Improvements to deep convolutional neural networks for LVCSR," Proc.
ASRU, 2013e.
Sainath, T., Kingsbury, B., and Ramabhadran, B. "Autoencoder Bottleneck Features Using Deep
Belief Networks," Proc. ICASSP, 2012.
Sainath, T., Kingsbury, B., Ramabhadran, B., Novak, P., and Mohamed, A. "Making deep belief networks effective for large vocabulary continuous speech recognition," Proc. ASRU, 2011.
Sainath, T., Ramabhadran, B., Picheny, M., Nahamoo, D., and Kanevsky, D., "Exemplar-Based
Sparse Representation Features: From TIMIT to LVCSR," IEEE Transactions on Speech and Audio Processing, November 2011a.
Salakhutdinov R. and Hinton, G. "Semantic hashing," Proc. SIGIR Workshop on Information
Retrieval and Applications of Graphical Models, 2007.
Salakhutdinov R. and Hinton, G. "Deep Boltzmann machines," Proc. AISTATS, 2009.
Salakhutdinov R. and Hinton, G. "A better way to pretrain deep Boltzmann machines," Proc.
NIPS, 2012.
Saon, G., Soltau, H., Nahamoo, D., and Picheny, M. "Speaker adaptation of neural network acoustic models using i-vecors," Proc. ASRU, 2013.
Sarikaya, R., Hinton, G., Ramabhadran, B. "Deep belief nets for natural language call-routing,"
Proc. ICASSP, pp. 5680-5683, 2011.
Schmidt, E. and Kim, Y. "Learning emotion-based acoustic features with deep belief networks,"
Proc. IEEE Applications of Signal Processing to Audio and Acoustics, 2011.
Schwenk, H. "Continuous space translation models for phrase-based statistical machine translation," Proc.
COLING, 2012.
Schwenk, H., Rousseau, A., and Mohammed A. "Large, pruned or continuous space language models on a GPU for statistical machine translation," NAACL-HLT 2012 Workshop on the future of language modeling for HLT, pp. 11-19.
Seide, F., Li, G., Chen, X., and Yu, D. "Feature engineering in context-dependent deep neural networks for conversational speech transcription," Proc. ASRU 2011, pp. 24-29.
Seide, F., Li, G., and Yu, D. "Conversational Speech Transcription Using Context-Dependent
Deep Neural Networks," Proc. Interspeech, 2011, pp. 437-440.
Seide, F., Fu, H., Droppo, J., Li, G., Yu, D. "On Parallelizability of Stochastic Gradient Descent for Speech DNNs," Proc. ICASSP, 2014.
Seltzer, M., Yu, D. and Wang, E. "An Investigation of Deep Neural Networks for Noise Robust
Speech Recognition," Proc. ICASSP, 2013.
Shannon, M., Zen, H., and Byrne W. "Autoregressive models for statistical parametric speech synthesis," IEEE Trans. Audio, Speech, Language Proc., Vol. 21, No. 3, 2013, pp. 587-597.
Sheikhzadeh, H. and Deng, L. "Waveform-based speech recognition using hidden filter models:
Parameter selection and sensitivity to power normalization," IEEE Trans. on Speech and Audio
Processing, Vol. 2, pp. 80-91, 1994.
Shen, Y., He, X., Gao, J., Deng, L., and Mesnel, G. "Learning semantic representations using convolutional neural networks for Web search," Proc. WWW, 2014.
Simonyan, K., Vedaldi, A., and Zisserman, A. "Deep Fisher networks for large-scale image classification," Proc. NIPS, 2013.
Siniscalchi, M., Yu, D., Deng, L., and Lee, C.-H. "Exploiting deep neural networks for detectionbased speech recognition," Neurocomputing, Vol 106, 148-157, 2013.
Siniscalchi, M., Li, J., and Lee, C. "Hermitian Polynomial for Speaker Adaptation of Connectionist
Speech Recognition Systems," IEEE Transactions on Audio, Speech, and Language
Processing, Vol. 21, No. 10, pp. 2152-2161, 2013a.
Siniscalchi, M., Yu, D., Deng, L., and Lee, C.-H. "Speech Recognition Using Long-Span
Temporal Patterns in a Deep Network Model," IEEE Signal Processing Letters, vol. 20, no. 3, pp. 201-204, March 2013a.
Siniscalchi, M., Svendsen, T., and Lee, C.-H. "A bottom-up modular search approach to large vocabulary continuous speech recognition," IEEE Trans. Audio, Speech, Language Proc., Vol.
21, 2013a.
Sivaram G. and Hermansky, H. "Sparse multilayer perceptron for phoneme recognition," IEEE
Trans. Audio, Speech, & Language Proc. Vol. 20 (1), January 2012.
Smolensky, P. "Tensor Product Variable Binding and the Representation of Symbolic Structures in Connectionist Systems," Artificial Intelligence," Vol. 46, pp. 159-216, 1990.
Snoek, J., Larochelle, H., and Adams, R. "Practical Bayesian Optimization of Machine Learning
Algorithms," Proc. NIPS, 2012.
Socher, R., Chen, D., Manning, C., and Ng, A. "Reasoning With Neural Tensor Networks for
Knowledge Base Completion," Proc. NIPS, 2013.
Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C., Ng A., and Potts. C. "Recursive Deep
Models for Semantic Compositionality Over a Sentiment Treebank," Proc. EMNLP, 2013a.
Socher, R., Ganjoo, M, Sridhar, H., Bastani, O., Manning, C. and Ng, A. "Zero-shot learning through cross-modal transfer," Proc. NIPS, 2013b.
Socher, R., Le, Q., Manning, C. and Ng, A. "Grounded Compositional Semantics for Finding and Describing Images with Sentences," NIPS Deep Learning Workshop, 2013c.
Socher, R., Bengio, Y., and Manning, C. "Deep learning for NLP," Tutorial at ACL, 2012, and NAACL, 2013: http://www.socher.org/index.php/DeepLearningTutorial
Socher, R. "New Directions in Deep Learning: Structured Models, Tasks, and Datasets," NIPS
Workshop on Deep Learning and Unsupervised Feature Learning, 2012.
Socher, R., Lin, C., Ng, A., and Manning, C. "Learning continuous phrase representations and syntactic parsing with recursive neural networks," Proc. ICML, 2011.
Socher, R., Pennington, J., Huang, E., Ng, A., and Manning, C. "Semi-Supervised Recursive
Autoencoders for Predicting Sentiment Distributions," Proc. EMNLP, 2011a.
Socher, R., Pennington, J., Huang, E., Ng, A., and Manning, C. "Dynamic Pooling and Unfolding
Recursive Autoencoders for Paraphrase Detection, Proc. NIPS 2011b.
Socher, R. and Fei-Fei, L. "Connecting modalities: Semi-supervised segmentation and annotation of images using unaligned text corpora," Proc. CVPR, 2010.
Stoyanov, V., Ropson, A. and Eisner, J. "Empirical Risk Minimization of Graphical Model
Parameters Given Approximate Inference, Decoding, and Model Structure," Proc. AISTAT, Srivastava, N. and Salakhutdinov, R. "Discriminative Transfer Learning with Tree-based Priors,"
Proc. NIPS, 2013.
Srivastava, N. and Salakhutdinov R. "Multimodal learning with deep Boltzmann machines," Proc.
NIPS, 2012.
Stafylakis, T., Kenny, P., Senoussaoui, M., and Dumouchel, P. "Preliminary investigation of Boltzmann Machine classifiers for speaker recognition," Proc. Odyssey 2012, pp. 109–116, Su, H., Li, G., Yu, D., and Seide, F. "Error Back Propagation For Sequence Training Of ContextDependent Deep Networks For Conversational Speech Transcription," Proc. ICASSP, 2013.
Subramanya, A., Deng, L., Liu, Z. and Zhang, Z. "Multi-sensory speech processing: Incorporating automatically extracted hidden dynamic information," Proc. IEEE Intern. Conf. Multimedia &
Expo (ICME), Amsterdam, July 2005.
Sun, J. and Deng, L. "An overlapping-feature based phonological model incorporating linguistic constraints: Applications to speech recognition," J. Acoust. Society of America, vol. 111, no.
2, pp. 1086-1101, 2002.
Sutskever. I. "Training Recurrent Neural Networks," Ph.D. Thesis, University of Toronto, 2013.
Sutskever, I., Martens J., and Hinton, G. "Generating text with recurrent neural networks," Proc.
ICML, 2011.
Taylor, G., Hinton, G. E., and Roweis, S. "Modeling human motion using binary latent variables."
Proc. NIPS, 2007.
Tang, Y. and Eliasmith, C. "Deep networks for robust visual recognition," Proc. ICML, 2010.
Tarralba, A, Fergus R, and Weiss, Y. "Small codes and large image databases for recognition,"
Proc. CVPR, 2008.
Thomas, S., Seltzer, M., Church, K., and Hermansky, H. "Deep neural network features and seisupervised training for low resource speech recognition," Proc. Interspeech, 2013.
Tieleman, T. "Training Restricted Boltzmann Machines using Approximations to the Likelihood
Gradient," Proc. ICML, 2008.
Tokuda, K., Nankaku, Y., Toda, T. Zen, H., Yamagishi, H., and Oura, K. "Speech synthesis based on hidden Markov models," Prooceedings of the IEEE, vol. 101, no. 5, pp. 1234–1252, 2013.
Triefenbach, F.; Jalalvand, A.; Demuynck, K.; Martens, J.-P., "Acoustic Modeling With
Hierarchical Reservoirs," IEEE Transactions on Audio, Speech, and Language Processing, vol.21, no.11, pp.2439,2450, Nov. 2013.
Tur, G., Deng, L., Hakkani-Tür, D., and X. He. "Towards deep understanding: Deep convex networks for semantic utterance classification," Proc. ICASSP, 2012.
Turian, J., Ratinov, L., and Bengio, Y. "Word representations: A simple and general method for semi-supervised learning," Proc. ACL, 2010.
Tüske, Z., Sundermeyer, M., Schlüter, R., and Ney. H. "Context-Dependent MLPs for LVCSR:
TANDEM, Hybrid or Both?" Proc. Interspeech, 2012.
Uria, B., Renals, S., and Richmond, K. "A deep neural network for acoustic-articulatory speech inversion," NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2011. van Dalen, R. and Gales, M. "Extended VTS for noise-robust speech recognition," IEEE Trans.
Audio, Speech, and Language Processing, vol. 19, no. 4, pp. 733–743, 2011.van den Oord, A., Dieleman, S., Schrauwen, B. "Deep content-based music recommendation,"
Proc. NIPS, 2013.
Vasilakakis,V., Cumani, S., and Laface, P. "Speaker recognition by means of Deep Belief
Networks," Proc. Biometric Technologies in Forensic Science, 2013.
Vesely, K., Hannemann, M., and Burget, L. "Semi-supervised training of deep neural networks,"
Proc. ASRU, 2013.
Vesely, K., Ghoshal, A., Burget, L., and Povey, D. "Sequence-discriminative training of deep neural networks", Proc. Interspeech, 2013a.
Vincent, P. "A connection between score matching and denoising autoencoder", Neural
Computation, Vol. 23, No. 7, pp. 1661-1674, 2011.
Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., and Manzagol, P. "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion," J. Machine Learning Research, Vol. 11, 2010, pp. 3371-3408.
Vinyals, O., & Povey, D. "Krylov Subspace Descent for Deep Learning," Proc. AISTAT, 2012.
Vinyals, O., Jia, Y., Deng, L., and Darrell, T. "Learning with recursive perceptual representations,"
Proc. NIPS, 2012.
Vinyals O., and Ravuri, S. "Comparing multilayer perceptron to deep belief network tandem features for robust ASR," Proc. ICASSP, 2011.
Wager, S., Wang, S., and Liang, P. "Dropout Training as Adaptive Regularization," Proc. NIPS, Waibel, A., Hanazawa, T., Hinton, G., Shikano, K., and Lang, K. "Phoneme recognition using time-delay neural networks," IEEE Trans. ASSP, vol. 37, pp. 328-339, 1989.
Welling, M., Rosen-Zvi, M., and Hinton, G. "Exponential family harmoniums with an application to information retrieval," Proc. NIPS, Vol. 20, 2005.
Weng, C., Yu, D., Seltzer, M., and Droppo, J. "Single-channel Mixed Speech Recognition Using
Deep Neural Networks", Proc. ICASSP 2014.
Wang, G. and Sim, K. "Context-dependent modelling of deep neural network using logistic regression," Proc. ASRU, 2013.
Wang, G. and Sim, K. "Regression-based context-dependent modeling of deep neural networks for speech recognition," IEEE/ACM Trans. Audio, Speech, and Language Processing, 2014.
Weston, J., Bengio, S., and Usunier, N. "Wsabie: Scaling up to large vocabulary image annotation," Proc. IJCAI, 2011.
Weston, J., Bengio, S., and Usunier, N. "Large scale image annotation: learning to rank with joint word-image embeddings," Machine Learning, vol. 81(1), pp. 21–35, 2010.
Wiesler, S., Li, J., and Xue, J. "Investigations on Hessian-Free Optimization for Cross-Entropy
Training of Deep Neural Networks," Proc. Interspeech, 2013.
Wohlmayr, M., Stark, M., Pernkopf, F. "A probabilistic interaction model for multi-pitch tracking with factorial hidden Markov model," IEEE Trans. Audio, Speech, and Language Proc., vol.
19, no. 4, May. 2011.
Wolpert, D. "Stacked generalization," Neural Networks, vol. 5, no. 2, pp. 241-259, 1992.
Wright, S.J.; Kanevsky, D.; Deng, L.; He, X.; Heigold, G.; Li, H., "Optimization Algorithms and Applications for Speech and Language Processing," IEEE Transactions on Audio, Speech, and Language Processing, vol.21, no.11, pp.2231-2243, Nov. 2013.
Xiao, L. and Deng, L. "A geometric perspective of large-margin training of Gaussian models,"
IEEE Signal Processing Magazine, vol. 27, no. 6, pp. 118-123, IEEE, November 2010.
Xie, X. and Seung, S. "Equivalence of backpropagation and contrastive Hebbian learning in a layered network," Neural computation, Vol. 15, pp. 441-454, 2003.
Xu, Y., Du, J., Dai, L., and Lee, C. "An experimental study on speech enhancement based on deep neural networks," IEEE Signal Processing Letters, vol. 21, no. 1, pp. 65–68, 2014.
Xue, J., Li, J., and Gong, Y. "Restructuring of Deep Neural Network Acoustic Models with
Singular Value Decomposition," Proc. Interspeech, 2013.
Yamin, S., Deng, L., Wang, Y., and Acero, A. "An integrative and discriminative technique for spoken utterance classification," IEEE Trans. Audio, Speech, and Language Proc., Vol 16, 1207-1214, 2008.
Yan, Z., Huo Q., Xu, J. "A Scalable Approach to Using DNN-Derived Features in GMM-HMM
Based Acoustic Modeling For LVCSR," Proc. Interspeech, 2013.
Yang, D., Furui, S. "Combining a two-step CRF model and a joint source channel model for machine transliteration," Proc. ACL, 2010, pp. 275-280.
Yao, K., Zweig, G., Hwang, M., Shi, Y., and Yu, D. "Recurrent Neural Networks for Language
Understanding," Proc. Interspeech, 2013.
Yao, K., Yu, D., Deng, L., and Gong, Y. "A Fast Maximum Likelihood Nonlinear Feature
Transformation Method for GMM-HMM Speaker Adaptation," Neurocomputing, 2013a.
Yao, K., Yu, D., Seide, F., Su, H., Deng, L., and Gong, Y. "Adaptation of context-dependent deep neural networks for automatic speech recognition," Proc. ICASSP, 2012.
Yoshioka, T. and Nakatani, T. "Noise model transfer: Novel approach to robustness against nonstationary noise," IEEE Trans. Audio, Speech, and Language Processing, vol. 21, no. 10, pp. 2182–2192, 2013.
Younes, L. "On the convergence of Markovian stochastic algorithms with rapidly decreasing ergodicity rates," Stochastics and Stochastic Reports, vol. 65(3), pp. 177-228, 1999.
Yu, K., Gales, M., and Woodland, P. "Unsupervised adaptation with discriminative mapping transforms," IEEE Trans. Audio, Speech, and Language Processing, vol. 17, no.4, pp. 714–
Yu, D., Deng, L., and Seide, F. "The Deep Tensor Neural Network with Applications to Large
Vocabulary Speech Recognition," IEEE Transactions on Audio, Speech, and Language
Processing, vol. 21, no. 2, pp. 388-396, 2013.
Yu, D., Seltzer, M., Li, J., Huang, J.-T., and Seide, F. "Feature Learning in Deep Neural Networks
- Studies on Speech Recognition," Proc. ICLR, 2013a.
Yu, D., Yao, K., Su, H., Li, G., and Seide, F. "KL-Divergence Regularized Deep Neural Network
Adaptation For Improved Large Vocabulary Speech Recognition," Proc. ICASSP 2013b.
Yu, D. and Deng, L. "Efficient and effective algorithms for training single-hidden-layer neural networks," Pattern Recognition Letters, Vol. 33, 554-558, 2012.
Yu, D., Seide, F., Li, G., Deng, L. "Exploiting sparseness in deep neural networks for large vocabulary speech recognition," Proc. ICASSP 2012.
Yu, D., Siniscalchi, S., Deng, L., and Lee, C. "Boosting attribute and phone estimation accuracies with deep neural networks for detection-based speech recognition", Proc. ICASSP 2012a.
Yu, D., Chen, X., and Deng, L., "Factorized deep neural networks for adaptive speech recognition," International Workshop on Statistical Machine Learning for Speech Processing, March 2012b.
Yu, D., Deng, L. and Seide, F. "Large Vocabulary Speech Recognition Using Deep Tensor Neural
Networks", Proc. Interspeech 2012c.
Yu, D. and Seltzer, M. "Improved bottleneck features using pre-trained deep neural networks,"
Proc. Interspeech 2011.
Yu, D. and Deng, L. "Deep learning and its applications to signal and information processing,"
IEEE Signal Processing Magazine, January 2011, pp. 145-154.
Yu, D. and Deng, L. "Accelerated parallelizable neural networks learning algorithms for speech recognition," Proc. Interspeech 2011.
Yu, D., Deng, L., Li, G., and F. Seide. "Discriminative pretraining of deep neural networks," U.S.
Patent Filing, Nov. 2011.
Yu, D. and Deng, L. "Deep-structured hidden conditional random fields for phonetic recognition,"
Proc. Interspeech, Sept. 2010.
Yu, D., Wang, S., Karam, Z., Deng, L. "Language recognition using deep-structured conditional random fields," Proc. ICASSP, 2010, pp. 5030-5033.
Yu, D., Wang, S., Deng, L., "Sequential labeling using deep-structured conditional random fields", J. of Selected Topics in Signal Processing, vol.4, pp. 965 – 973, 2010a.
Yu, D., Li, J.-Y., and Deng, L. "Calibration of confidence measures in speech recognition," IEEE
Trans. Audio, Speech and Language, vol 19, 2461–2473, 2010b.
Yu, D., Deng, L., and Dahl, G.E., "Roles of Pre-Training and Fine-Tuning in Context-Dependent
DBN-HMMs for Real-World Speech Recognition," NIPS 2010 Workshop on Deep Learning and Unsupervised Feature Learning, Dec. 2010c.
Yu, D., Deng, D., Wang, S., "Learning in the Deep-Structured Conditional Random Fields," NIPS
2009 Workshop on Deep Learning for Speech Recognition and Related Applications, 2009.
Yu, D, Deng, L., Gong, Y. and Acero, A. "A novel framework and training algorithm for variableparameter hidden Markov models," IEEE Transactions on Audio, Speech and Language
Processing, vol. 17, no. 7, pp. 1348-1360, 2009a.
Yu, D., Deng, L., Liu, P., Wu, J., Gong, Y., and Acero, A. "Cross-lingual speech recognition under runtime resource constraints," Proc. ICASSP, 2009b.
Yu, D. and Deng, L. "Solving nonlinear estimation problems using Splines," IEEE Signal
Processing Magazine, vol. 26, no. 4, pp. 86-90, July 2009.
Yu, D., Deng, L., Droppo, J., Wu, J., Gong, Y., Acero, A. "Robust speech recognition using cepstral minimum-mean-square-error noise suppressor," IEEE Trans. Audio, Speech, and Language Processing, vol. 16, no. 5, July 2008.
Yu, D., Deng, L., He, X., and Acero, X. "Large-Margin Minimum Classification Error Training for Large-Scale Speech Recognition Tasks," Proc. ICASSP, 2007.
Yu, D., Deng, L., He, X., and Acero, A. "Large-Margin Minimum Classification Error Training:
A Theoretical Risk Minimization Perspective," Computer Speech and Language, vol. 22, no.
4, pp. 415-429, October 2008.
Yu, D. and Deng, L. "Large-Margin Discriminative Training of Hidden Markov Models for
Speech Recognition," Proc. ICASSP, 2007.
Yu, K., Lin, Y., and Lafferty, H. "Learning Image Representations from the Pixel Level via
Hierarchical Sparse Coding," Proc. CVPR, 2011.
Zamora-Martínez, F., Castro-Bleda, M., España-Boquera, S. "Fast evaluation of connectionist language models," Intern. Conf. Artificial Neural Networks, 2009, pp. 144-151.
Zeiler, M. Hierarchical Convolutional Deep Learning in Computer Vision, Ph.D. Thesis, New
York University, January 2014.
Zeiler, M. and Fergus, R. "Visualizing and understanding convolutional networks," arXiv:1311.2901, pp. 1-11, 2013.
Zeiler M. and Fergus. R. "Stochastic pooling for regularization of deep convolutional neural networks," Proc. ICLR, 2013.
Zeiler, M., Taylor, G., and Fergus, R. "Adaptive deconvolutional networks for mid and high level feature learning," Proc. ICCV, 2011.
Zen, H., Senior, A., and Schuster, M. "Statistical parametric speech synthesis using deep neural networks," Proc. ICASSP, pp. 7962-7966, 2013.
Zen, H. Gales, M. J. F. Nankaku, Y. Tokuda, K. "Product of experts for statistical parametric speech synthesis," IEEE Trans. Audio, Speech, and Language Proc., vol. 20, no. 3, March, 2012, pp. 794-805.
Zen, H., Nankaku, Y., and Tokuda, K. "Continuous stochastic feature mapping based on trajectory
HMMs," IEEE Trans. Audio, Speech, and Language Proc., vol. 19, no. 2, Feb. 2011, pp. 417Zhang, X. and Wu, J. "Deep belief networks based voice activity detection," IEEE Transactions on Audio, Speech, and Language Processing, vol. 21, no. 4, pp. 697–710, 2013.
Zhang, X., Trmal, J., Povey, D., and Khudanpur, S. "Improving deep neural network acoustic models using generalized maxout networks," Proc. ICASSP, 2014.
Zhang, Z., Liu, Z., Sinclair, M., Acero, A., Deng, L., Droppo, J., Huang, X., and Zheng, Y. "Multisensory microphones for robust speech detection, enhancement and recognition." Proc.
ICASSP, 2004.
Zhao, Y. and Juang, B. "Nonlinear compensation using the Gauss-Newton method for noise-robust speech recognition," IEEE Trans. Audio, Speech, and Language Processing, vol. 20, no. 8, pp.
2191–2206, 2012.
Zou, W., Socher, R., Cer, D., and Manning, C. "Bilingual Word Embeddings for Phrase-Based
Machine Translation," Proc. EMNLP, 2013.
Zweig, G. and Nguyen, P. "A segmental CRF approach to large vocabulary continuous speech recognition," Proc. ASRU, 2009.Large Scale Distributed Deep Networks
Jeffrey Dean, Greg S. Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc V. Le, Mark Z. Mao, Marc'Aurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang, Andrew Y. Ng
{jeff, gcorrado}@google.com
Google Inc., Mountain View, CA
Abstract
Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance. In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores. We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models. Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii)
Sandblaster, a framework that supports a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS. Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training. We have successfully used our system to train a deep network 30x larger than previously reported in the literature, and achieves state-of-the-art performance on
ImageNet, a visual object recognition task with 16 million images and 21k categories. We show that these same techniques dramatically accelerate the training of a more modestly- sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm.
Introduction
Deep learning and unsupervised feature learning have shown great promise in many practical applications. State-of-the-art performance has been reported in several domains, ranging from speech recognition, visual object recognition, to text processing.
It has also been observed that increasing the scale of deep learning, with respect to the number of training examples, the number of model parameters, or both, can drastically improve ultimate classification accuracy. These results have led to a surge of interest in scaling up the training and inference algorithms used for these models and in improving applicable optimization procedures. The use of GPUs is a significant advance in recent years that makes the training of modestly sized deep networks practical. A known limitation of the GPU approach is that the training speed-up is small when the model does not fit in GPU memory (typically less than
6 gigabytes). To use a GPU effectively, researchers often reduce the size of the data or parameters so that CPU-to-GPU transfers are not a significant bottleneck. While data and parameter reduction work well for small problems (e.g. acoustic modeling for speech recognition), they are less attractive for problems with a large number of examples and dimensions (e.g., high-resolution images).
In this paper, we describe an alternative approach: using large-scale clusters of machines to distribute training and inference in deep networks. We have developed a software framework called DistBelief that enables model parallelism within a machine (via multithreading) and across machines (via
1 message passing), with the details of parallelism, synchronization and communication managed by the framework. In addition to supporting model parallelism, the DistBelief framework also supports data parallelism, where multiple replicas of a model are used to optimize a single objective. Within this framework, we have designed and implemented two novel methods for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure which leverages adaptive learning rates and supports a large number of model replicas, and (ii) Sandblaster
L-BFGS, a distributed implementation of L-BFGS that uses both data and model parallelism.1 Both
Downpour SGD and Sandblaster L-BFGS enjoy significant speed gains compared to more conventional implementations of SGD and L-BFGS.
Our experiments reveal several surprising results about large-scale nonconvex optimization. Firstly, asynchronous SGD, rarely applied to nonconvex problems, works very well for training deep networks, particularly when combined with Adagrad adaptive learning rates. Secondly, we show that given sufficient resources, L-BFGS is competitive with or faster than many variants of SGD.
With regard to specific applications in deep learning, we report two main findings: that our distributed optimization approach can both greatly accelerate the training of modestly sized models, and that it can also train models that are larger than could be contemplated otherwise. To illustrate the first point, we show that we can use a cluster of machines to train a modestly sized speech model to the same classification accuracy in less than 1/10th the time required on a GPU. To illustrate the second point, we trained a large neural network of more than 1 billion parameters and used this network to drastically improve on state-of-the-art performance on the ImageNet dataset, one of the largest datasets in computer vision.
Previous work
In recent years commercial and academic machine learning data sets have grown at an unprecedented pace. In response, a great many authors have explored scaling up machine learning algorithms through parallelization and distribution. Much of this research has focused on linear, convex models, where distributed gradient computation is the natural first step.
Within this area, some groups have relaxed synchronization requirements, exploring delayed gradient updates for convex problems. In parallel, other groups working on problems with sparse gradients (problems where only a tiny fraction of the coordinates of the gradient vector are non-zero for any given training example) have explored lock-less asynchronous stochastic gradient descent on shared-memory architectures (i.e. single machines). We are interested in an approach that captures the best of both worlds, allowing the use of a cluster of machines asynchronously computing gradients, but without requiring that the problem be either convex or sparse.
In the context of deep learning, most work has focused on training relatively small models on a single machine (e.g., Theano ). Suggestions for scaling up deep learning include the use of a farm of GPUs to train a collection of many small models and subsequently averaging their predictions, or modifying standard deep networks to make them inherently more parallelizable. Our focus is scaling deep learning techniques in the direction of training very large models, those with a few billion parameters, but without introducing restrictions on the form of the model. In special cases where one layer dominates computation, some authors have considered distributing computation in that one layer and replicating computation in the remaining layers. But in the general case where many layers of the model are computationally intensive, full model parallelism in a spirit similar to is required. To be successful, however, we believe that model parallelism must be combined with clever distributed optimization techniques that leverage data parallelism.
We considered a number of existing large-scale computational tools for application to our problem, MapReduce and GraphLab being notable examples. We concluded that MapReduce, designed for parallel data processing, was ill-suited for the iterative computations inherent in deep network training; whereas GraphLab, designed for general (unstructured) graph computations, would not exploit computing efficiencies available in the structured graphs typically found in deep networks.
1We implemented L-BFGS within the Sandblaster framework, but the general approach is also suitable for a variety of other batch optimization methods.
Machine 1
Machine 2
Machine 3
Machine 4
Figure 1: An example of model parallelism in DistBelief. A five layer deep neural network with local connectivity is shown here, partitioned across four machines (blue rectangles). Only those nodes with edges that cross partition boundaries (thick lines) will need to have their state transmitted between machines. Even in cases where a node has multiple edges crossing a partition boundary, its state is only sent to the machine on the other side of that boundary once. Within each partition, computation for individual nodes will the parallelized across all available CPU cores.
Model parallelism
To facilitate the training of very large deep networks, we have developed a software framework, DistBelief, that supports distributed computation in neural networks and layered graphical models.
The user defines the computation that takes place at each node in each layer of the model, and the messages that should be passed during the upward and downward phases of computation.2 For large models, the user may partition the model across several machines (Figure 1), so that responsibility for the computation for different nodes is assigned to different machines. The framework automatically parallelizes computation in each machine using all available cores, and manages communication, synchronization and data transfer between machines during both training and inference.
The performance benefits of distributing a deep network across multiple machines depends on the connectivity structure and computational needs of the model. Models with a large number of parameters or high computational demands typically benefit from access to more CPUs and memory, up to the point where communication costs dominate. We have successfully run large models with up to 144 partitions in the DistBelief framework with significant speedups, while more modestly sized models show decent speedups for up to 8 or 16 partitions. (See Section 5, under the heading Model
Parallelism Benchmarks, for experimental results.) Obviously, models with local connectivity structures tend to be more amenable to extensive distribution than fully-connected structures, given their lower communication requirements. The typical cause of less-than-ideal speedups is variance in processing times across the different machines, leading to many machines waiting for the single slowest machine to finish a given phase of computation. Nonetheless, for our largest models, we can efficiently use 32 machines where each machine achieves an average CPU utilization of 16 cores, for a total of 512 CPU cores training a single large neural network. When combined with the distributed optimization algorithms described in the next section, which utilize multiple replicas of the entire neural network, it is possible to use tens of thousands of CPU cores for training a single model, leading to significant reductions in overall training times.
Distributed optimization algorithms
Parallelizing computation within the DistBelief framework allows us to instantiate and run neural networks considerably larger than have been previously reported. But in order to train such large models in a reasonable amount of time, we need to parallelize computation not only within a single
2In the case of a neural network 'upward' and 'downward' might equally well be called 'feedforward' and 'backprop', while for a Hidden Markov Model, they might be more familiar as 'forward' and 'backward'.
Parameter Server
Model
Replicas
Data
Shards w' = w - ηΔw w
Δw
Parameter Server
Model
Replicas
Data
Coordinator(small messages)
Figure 2: Left: Downpour SGD. Model replicas asynchronously fetch parameters w and push gradients ∆w to the parameter server. Right: Sandblaster L-BFGS. A single 'coordinator' sends small messages to replicas and the parameter server to orchestrate batch optimization. instance of the model, but to distribute training across multiple model instances. In this section we describe this second level of parallelism, where we employ a set of DistBelief model instances, or replicas, to simultaneously solve a single optimization problem.
We present a comparison of two large-scale distributed optimization procedures: Downpour SGD, an online method, and Sandblaster L-BFGS, a batch method. Both methods leverage the concept of a centralized sharded parameter server, which model replicas use to share their parameters. Both methods take advantage of the distributed computation DistBelief allows within each individual replica. But most importantly, both methods are designed to tolerate variance in the processing speed of different model replicas, and even the wholesale failure of model replicas which may be taken offline or restarted at random.
In a sense, these two optimization algorithms implement an intelligent version of data parallelism.
Both approaches allow us to simultaneously process distinct training examples in each of the many model replicas, and periodically combine their results to optimize our objective function.
Downpour SGD
Stochastic gradient descent (SGD) is perhaps the most commonly used optimization procedure for training deep neural networks. Unfortunately, the traditional formulation of SGD is inherently sequential, making it impractical to apply to very large data sets where the time required to move through the data in an entirely serial fashion is prohibitive.
To apply SGD to large data sets, we introduce Downpour SGD, a variant of asynchronous stochastic gradient descent that uses multiple replicas of a single DistBelief model. The basic approach is as follows: We divide the training data into a number of subsets and run a copy of the model on each of these subsets. The models communicate updates through a centralized parameter server, which keeps the current state of all parameters for the model, sharded across many machines (e.g., if we have 10 parameter server shards, each shard is responsible for storing and applying updates to 1/10th of the model parameters) (Figure 2). This approach is asynchronous in two distinct aspects: the model replicas run independently of each other, and the parameter server shards also run independently of one another.
In the simplest implementation, before processing each mini-batch, a model replica asks the parameter server service for an updated copy of its model parameters. Because DistBelief models are themselves partitioned across multiple machines, each machine needs to communicate with just the subset of parameter server shards that hold the model parameters relevant to its partition. After receiving an updated copy of its parameters, the DistBelief model replica processes a mini-batch of data to compute a parameter gradient, and sends the gradient to the parameter server, which then applies the gradient to the current value of the model parameters.
It is possible to reduce the communication overhead of Downpour SGD by limiting each model replica to request updated parameters only every nfetch steps and send updated gradient values only every npush steps (where nfetch might not be equal to npush). In fact, the process of fetching
4 parameters, pushing gradients, and processing training data can be carried out in three only weakly synchronized threads (see the Appendix for pseudocode). In the experiments reported below we fixed nfetch = npush = 1 for simplicity and ease of comparison to traditional SGD.
Downpour SGD is more robust to machines failures than standard (synchronous) SGD. For synchronous SGD, if one machine fails, the entire training process is delayed; whereas for asynchronous
SGD, if one machine in a model replica fails, the other model replicas continue processing their training data and updating the model parameters via the parameter servers. On the other hand, the multiple forms of asynchronous processing in Downpour SGD introduce a great deal of additional stochasticity in the optimization procedure. Most obviously, a model replica is almost certainly computing its gradients based on a set of parameters that are slightly out of date, in that some other model replica will likely have updated the parameters on the parameter server in the meantime. But there are several other sources of stochasticity beyond this: Because the parameter server shards act independently, there is no guarantee that at any given moment the parameters on each shard of the parameter server have undergone the same number of updates, or that the updates were applied in the same order. Moreover, because the model replicas are permitted to fetch parameters and push gradients in separate threads, there may be additional subtle inconsistencies in the timestamps of parameters. There is little theoretical grounding for the safety of these operations for nonconvex problems, but in practice we found relaxing consistency requirements to be remarkably effective.
One technique that we have found to greatly increase the robustness of Downpour SGD is the use of the Adagrad adaptive learning rate procedure. Rather than using a single fixed learning rate on the parameter sever (η in Figure 2), Adagrad uses a separate adaptive learning rate for each parameter. Let ηi,K be the learning rate of the i-th parameter at iteration K and ∆wi,K its gradient, then we set: ηi,K = γ/
��K j=1 ∆wi,j2. Because these learning rates are computed only from the summed squared gradients of each parameter, Adagrad is easily implemented locally within each parameter server shard. The value of γ, the constant scaling factor for all learning rates, is generally larger (perhaps by an order of magnitude) than the best fixed learning rate used without Adagrad.
The use of Adagrad extends the maximum number of model replicas that can productively work simultaneously, and combined with a practice of "warmstarting" model training with only a single model replica before unleashing the other replicas, it has virtually eliminated stability concerns in training deep networks using Downpour SGD (see results in Section 5).
Sandblaster L-BFGS
Batch methods have been shown to work well in training small deep networks. To apply these methods to large models and large datasets, we introduce the Sandblaster batch optimization framework and discuss an implementation of L-BFGS using this framework.
A key idea in Sandblaster is distributed parameter storage and manipulation. The core of the optimization algorithm (e.g L-BFGS) resides in a coordinator process (Figure 2), which does not have direct access to the model parameters. Instead, the coordinator issues commands drawn from a small set of operations (e.g., dot product, scaling, coefficient-wise addition, multiplication) that can be performed by each parameter server shard independently, with the results being stored locally on the same shard. Additional information, e.g the history cache for L-BFGS, is also stored on the parameter server shard on which it was computed. This allows running large models (billions of parameters) without incurring the overhead of sending all the parameters and gradients to a single central server. (See the Appendix for pseudocode.)
In typical parallelized implementations of L-BFGS, data is distributed to many machines and each machine is responsible for computing the gradient on a specific subset of data examples. The gradients are sent back to a central server (or aggregated via a tree ). Many such methods wait for the slowest machine, and therefore do not scale well to large shared clusters. To account for this problem, we employ the following load balancing scheme: The coordinator assigns each of the N model replicas a small portion of work, much smaller than 1/Nth of the total size of a batch, and assigns replicas new portions whenever they are free. With this approach, faster model replicas do more work than slower replicas. To further manage slow model replicas at the end of a batch, the coordinator schedules multiple copies of the outstanding portions and uses the result from whichever model replica finishes first. This scheme is similar to the use of "backup tasks" in the MapReduce framework. Prefetching of data, along with supporting data affinity by assigning sequential
5 portions of data to the same worker makes data access a non-issue. In contrast with Downpour
SGD, which requires relatively high frequency, high bandwidth parameter synchronization with the parameter server, Sandblaster workers only fetch parameters at the beginning of each batch (when they have been updated by the coordinator), and only send the gradients every few completed portions (to protect against replica failures and restarts).
Experiments
We evaluated our optimization algorithms by applying them to training models for two different deep learning problems: object recognition in still images and acoustic processing for speech recognition.
The speech recognition task was to classify the central region (or frame) in a short snippet of audio as one of several thousand acoustic states. We used a deep network with five layers: four hidden layer with sigmoidal activations and 2560 nodes each, and a softmax output layer with 8192 nodes. The input representation was 11 consecutive overlapping 25 ms frames of speech, each represented by
40 log-energy values. The network was fully-connected layer-to-layer, for a total of approximately
42 million model parameters. We trained on a data set of 1.1 billion weakly labeled examples, and evaluated on a hold out test set. See for similar deep network configurations and training procedures.
For visual object recognition we trained a larger neural network with locally-connected receptive fields on the ImageNet data set of 16 million images, each of which we scaled to 100x100 pixels.
The network had three stages, each composed of filtering, pooling and local contrast normalization, where each node in the filtering layer was connected to a 10x10 patch in the layer below. Our infrastructure allows many nodes to connect to the same input patch, and we ran experiments varying the number of identically connected nodes from 8 to 36. The output layer consisted of 21 thousand one-vs-all logistic classifier nodes, one for each of the ImageNet object categories. See for similar deep network configurations and training procedures.
Model parallelism benchmarks:
To explore the scaling behavior of DistBelief model parallelism(Section 3), we measured the mean time to process a single mini-batch for simple SGD training as a function of the number of partitions (machines) used in a single model instance. In Figure 3 we quantify the impact of parallelizing across N machines by reporting the average training speed-up: the ratio of the time taken using only a single machine to the time taken using N. Speedups for inference steps in these models are similar and are not shown here.
The moderately sized speech model runs fastest on 8 machines, computing 2.2× faster than using a single machine. (Models were configured to use no more than 20 cores per machine.) Partitioning
Machines per model instance
Training speed�up
Speech: 42M parameters
Images: 80M parameters
Images: 330M parameters
Images: 1.7B parameters
Figure 3: Training speed-up for four different deep networks as a function of machines allocated to a single DistBelief model instance. Models with more parameters benefit more from the use of additional machines than do models with fewer parameters.
Time (hours)
Average Frame Accuracy (%)
Accuracy on Training Set
SGD 
DownpourSGD 
DownpourSGD w/Adagrad
Sandblaster L−BFGS 
Time (hours)
Average Frame Accuracy (%)
Accuracy on Test Set
SGD 
GPU 
DownpourSGD 
DownpourSGD w/Adagrad
DownpourSGD w/Adagrad
Sandblaster L−BFGS 
Figure 4: Left: Training accuracy (on a portion of the training set) for different optimization methods. Right: Classification accuracy on the hold out test set as a function of training time. Downpour and Sandblaster experiments initialized using the same ∼10 hour warmstart of simple SGD. the model on more than 8 machines actually slows training, as network overhead starts to dominate in the fully-connected network structure and there is less work for each machine to perform with more partitions.
In contrast, the much larger, locally-connected image models can benefit from using many more machines per model replica. The largest model, with 1.7 billion parameters benefits the most, giving a speedup of more than 12× using 81 machines. For these large models using more machines continues to increase speed, but with diminishing returns.
Optimization method comparisons:
To evaluate the proposed distributed optimization procedures, we ran the speech model described above in a variety of configurations. We consider two baseline optimization procedures: training a DistBelief model (on 8 partitions) using conventional(single replica) SGD, and training the identical model on a GPU using CUDA. The three distributed optimization methods we compare to these baseline methods are: Downpour SGD with a fixed learning rate, Downpour SGD with Adagrad learning rates, and Sandblaster L-BFGS.
Figure 4 shows classification performance as a function of training time for each of these methods on both the training and test sets. Our goal is to obtain the maximum test set accuracy in the minimum amount of training time, regardless of resource requirements. Conventional single replica
SGD (black curves) is the slowest to train. Downpour SGD with 20 model replicas (blue curves) shows a significant improvement. Downpour SGD with 20 replicas plus Adagrad (orange curve) is modestly faster. Sandblaster L-BFGS using 2000 model replicas (green curves) is considerably faster yet again. The fastest, however, is Downpour SGD plus Adagrad with 200 model replicas (red curves). Given access to sufficient CPU resourses, both Sandblaster L-BFGS and Downpour SGD with Adagrad can train models substantially faster than a high performance GPU.
Though we did not confine the above experiments to a fixed resource budget, it is interesting to consider how the various methods trade off resource consumption for performance. We analyze this by arbitrarily choosing a fixed test set accuracy (16%), and measuring the time each method took to reach that accuracy as a function of machines and utilized CPU cores, Figure 5. One of the four points on each traces corresponds to a training configuration shown in Figure 4, the other three points are alternative configurations.
In this plot, points closer to the origin are preferable in that they take less time while using fewer resources. In this regard Downpour SGD using Adagrad appears to be the best trade-off: For any fixed budget of machines or cores, Downpour SGD with Adagrad takes less time to reach the accuracy target than either Downpour SGD with a fixed learning rate or Sandblaster L-BFGS. For any allotted training time to reach the accuracy target, Downpour SGD with Adagrad used few resources than
Sandblaster L-BFGS, and in many cases Downpour SGD with a fixed learning rate could not even reach the target within the deadline. The Sandblaster L-BFGS system does show promise in terms
Machines
Time (hours)
Time to 16% accuracy
Downpour SGD
Downpour SGD w/Adagrad
Sandblaster L−BFGS
GPU
Cores
Time (hours)
Time to 16% accuracy
Downpour SGD
Downpour SGD w/Adagrad
Sandblaster L−BFGS
GPU (CUDA cores)
Figure 5: Time to reach a fixed accuracy (16%) for different optimization strategies as a function of number of the machines (left) and cores (right). of its scaling with additional cores, suggesting that it may ultimately produce the fastest training times if used with an extremely large resource budget (e.g., 30k cores).
Application to ImageNet:
The previous experiments demonstrate that our techniques can accelerate the training of neural networks with tens of millions of parameters. However, the more significant advantage of our cluster-based approach to distributed optimization is its ability to scale to models that are much larger than can be comfortably fit on single machine, let alone a single GPU.
As a first step toward exploring the capabilities of very large neural networks, we used Downpour
SGD to train the 1.7 billion parameter image model described above on the ImageNet object classification task. As detailed in, this network achieved a cross-validated classification accuracy of over 15%, a relative improvement over 60% from the best performance we are aware of on the 21k category ImageNet classification task.
Conclusions
In this paper we introduced DistBelief, a framework for parallel distributed training of deep networks. Within this framework, we discovered several effective distributed optimization strategies.
We found that Downpour SGD, a highly asynchronous variant of SGD works surprisingly well for training nonconvex deep learning models. Sandblaster L-BFGS, a distributed implementation of L-BFGS, can be competitive with SGD, and its more efficient use of network bandwidth enables it to scale to a larger number of concurrent cores for training a single model. That said, the combination of Downpour SGD with the Adagrad adaptive learning rate procedure emerges as the clearly dominant method when working with a computational budget of 2000 CPU cores or less.
Adagrad was not originally designed to be used with asynchronous SGD, and neither method is typically applied to nonconvex problems. It is surprising, therefore, that they work so well together, and on highly nonlinear deep networks. We conjecture that Adagrad automatically stabilizes volatile parameters in the face of the flurry of asynchronous updates, and naturally adjusts learning rates to the demands of different layers in the deep network.
Our experiments show that our new large-scale training methods can use a cluster of machines to train even modestly sized deep networks significantly faster than a GPU, and without the GPU's limitation on the maximum size of the model. To demonstrate the value of being able to train larger models, we have trained a model with over 1 billion parameters to achieve better than state-of-the-art performance on the ImageNet object recognition challenge.
Acknowledgments
The authors would like to thank Samy Bengio, Tom Dean, John Duchi, Yuval Netzer, Patrick Nguyen, Yoram
Singer, Sebastian Thrun, and Vincent Vanhoucke for their indispensable advice, support, and comments.
References
 G. Dahl, D. Yu, L. Deng, and A. Acero. Context-dependent pre-trained deep neural networks for large vocabulary speech recognition. IEEE Transactions on Audio, Speech, and Language Processing, 2012.
 G. Hinton, L. Deng, D. Yu, G. Dahl, A. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. Sainath, and B. Kingsbury. Deep neural networks for acoustic modeling in speech recognition. IEEE
Signal Processing Magazine, 2012.
 D. C. Ciresan, U. Meier, L. M. Gambardella, and J. Schmidhuber. Deep big simple neural nets excel on handwritten digit recognition. CoRR, 2010.
 A. Coates, H. Lee, and A. Y. Ng. An analysis of single-layer networks in unsupervised feature learning.
In AISTATS 14, 2011.
 Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137–1155, 2003.
 R. Collobert and J. Weston. A unified architecture for natural language processing: Deep neural networks with multitask learning. In ICML, 2008.
 Q.V. Le, J. Ngiam, A. Coates, A. Lahiri, B. Prochnow, and A.Y. Ng. On optimization methods for deep learning. In ICML, 2011.
 R. Raina, A. Madhavan, and A. Y. Ng. Large-scale deep unsupervised learning using graphics processors.
In ICML, 2009.
 J. Martens. Deep learning via hessian-free optimization. In ICML, 2010.
 J. C. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12:2121–2159, 2011.
 Q. Shi, J. Petterson, G. Dror, J. Langford, A. Smola, A. Strehl, and V. Vishwanathan. Hash kernels. In
AISTATS, 2009.
 J. Langford, A. Smola, and M. Zinkevich. Slow learners are fast. In NIPS, 2009.
 G. Mann, R. McDonald, M. Mohri, N. Silberman, and D. Walker. Efficient large-scale distributed training of conditional maximum entropy models. In NIPS, 2009.
 R. McDonald, K. Hall, and G. Mann. Distributed training strategies for the structured perceptron. In
NAACL, 2010.
 M. Zinkevich, M. Weimer, A. Smola, and L. Li. Parallelized stochastic gradient descent. In NIPS, 2010.
 A. Agarwal, O. Chapelle, M. Dudik, and J. Langford. A reliable effective terascale linear learning system.
In AISTATS, 2011.
 A. Agarwal and J. Duchi. Distributed delayed stochastic optimization. In NIPS, 2011.
 F. Niu, B. Retcht, C. Re, and S. J. Wright. Hogwild! A lock-free approach to parallelizing stochastic gradient descent. In NIPS, 2011.
 J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu, G. Desjardins, J. Turian, D. Warde-Farley, and Y. Bengio. Theano: a CPU and GPU math expression compiler. In SciPy, 2010.
 D. Ciresan, U. Meier, and J. Schmidhuber. Multi-column deep neural networks for image classification.
Technical report, IDSIA, 2012.
 L. Deng, D. Yu, and J. Platt. Scalable stacking and learning for building deep architectures. In ICASSP, A. Krizhevsky. Learning multiple layers of features from tiny images. Technical report, U. Toronto, 2009.
 J. Dean and S. Ghemawat. Map-Reduce: simplified data processing on large clusters. CACM, 2008.
 Y. Low, J. Gonzalez, A. Kyrola, D. Bickson, C. Guestrin, and J. Hellerstein. Distributed GraphLab: A framework for machine learning in the cloud. In VLDB, 2012.
 L. Bottou. Stochastic gradient learning in neural networks. In Proceedings of Neuro-Nˆımes 91, 1991.
 Y. LeCun, L. Bottou, G. Orr, and K. Muller. Efficient backprop. In Neural Networks: Tricks of the trade.
Springer, 1998.
 V. Vanhoucke, A. Senior, and M. Z. Mao. Improving the speed of neural networks on cpus. In Deep
Learning and Unsupervised Feature Learning Workshop, NIPS 2011, 2011.
 J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical
Image Database. In CVPR, 2009.
 Q.V. Le, M.A. Ranzato, R. Monga, M. Devin, K. Chen, G.S. Corrado, J. Dean, and A.Y. Ng. Building high-level features using large scale unsupervised learning. In ICML, 2012.
Large Scale Distributed Deep Networks: Appendix
Jeffrey Dean, Greg S. Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc V. Le, Mark Z. Mao, Marc'Aurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang, Andrew Y. Ng
{jeff, gcorrado}@google.com
Google Inc., Mountain View, CA
Appendix
For completeness, here we provide pseudocode for the model replica (client) side of Downpour SGD(Algorithm 0.1), and Sandblaster L-BFGS (Algorithm 0.2).
Algorithm 1.1: DOWNPOURSGDCLIENT(α, nfetch, npush) procedure STARTASYNCHRONOUSLYFETCHINGPARAMETERS(parameters) parameters ← GETPARAMETERSFROMPARAMSERVER() procedure STARTASYNCHRONOUSLYPUSHINGGRADIENTS(accruedgradients)
SENDGRADIENTSTOPARAMSERVER(accruedgradients) accruedgradients ← 0 main global parameters, accruedgradients step ← 0 accruedgradients ← 0 while true do
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
� if (step mod nfetch) == 0 then STARTASYNCHRONOUSLYFETCHINGPARAMETERS(parameters) data ← GETNEXTMINIBATCH() gradient ← COMPUTEGRADIENT(parameters, data) accruedgradients ← accruedgradients + gradient parameters ← parameters − α ∗ gradient if (step mod npush) == 0 then STARTASYNCHRONOUSLYPUSHINGGRADIENTS(accruedgradients) step ← step + 1
Sandblaster is a framework for distributed batch optimization procedures. An essential concept in Sandblaster is decomposing operations into local computation on the DistBelief parameter server.
By way of example, suppose we have 1 billion parameters and 10 parameter server shards, so that each shard has 1/10 of the parameters. It is possible to decompose L-BFGS into a sequence of scalar-vector products (α ×x) and vector-vector inner products (xT y), where each vector is 1 billion dimensional. If one shard is always responsible for the first 1/10 of every vector used internally in L-BFGS, and a second shard is always responsible for the second 1/10 of every vector, and so on up to the final shard always being responsible for the final 1/10 of every vector, it is possible to show that these scalar-vector and vector-vector operations can all be done in a distributed fashion with very little communication, so that any intermediate vector-valued results are automatically stored in the same distributed fashion, and any intermediate scalar-valued result is communicated to all the shards.
Algorithm 1.2: SANDBLASTERLBFGS() procedure REPLICA.PROCESSPORTION(portion) if (!hasParametersForStep) then parameters ← GETPARAMETERSFROMPARAMSERVER() data ← GETDATAPORTION(portion) gradient ← COMPUTEGRADIENT(parameters, data) localAccruedGradients ← localAccruedGradients + gradient procedure PARAMETERSERVER.PERFORMOPERATION(operation)
PerformOperation main step ← 0 while true do
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
� comment: PS: ParameterServer
PS.accruedgradients ← 0 while (batchProcessed < batchSize) do
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
� for all (modelReplicas)comment: Loop is parallel and asynchronous
�
�
�
�
�
�
�
�
�
�
�
�
� if (modelReplicaAvailable) then
�
REPLICA.PROCESSPORTION(modelReplica) batchProcessed ← batchProcessed + portion if (modelReplicaWorkDone and timeToSendGradients) then
�
SENDGRADIENTS(modelReplica)
PS.accruedGradients ← PS.accruedGradients + gradient
COMPUTELBFGSDIRECTION(PS.Gradients, PS.History, PS.Direction)
LINESEARCH(PS.Parameters, PS.Direction)
PS.UPDATEPARAMETERS(PS.parameters, PS.accruedGradients) step ← step + 1Journal of Machine Learning Research 15 (2014) 1929-1958
Submitted 11/13; Published 6/14
Dropout: A Simple Way to Prevent Neural Networks from
Overfitting
Nitish Srivastava nitish@cs.toronto.edu
Geoffrey Hinton hinton@cs.toronto.edu
Alex Krizhevsky kriz@cs.toronto.edu
Ilya Sutskever ilya@cs.toronto.edu
Ruslan Salakhutdinov rsalakhu@cs.toronto.edu
Department of Computer Science
University of Toronto
10 Kings College Road, Rm 3302
Toronto, Ontario, M5S 3G4, Canada.
Editor: Yoshua Bengio
Abstract
Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem.
The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.
Keywords: neural networks, regularization, model combination, deep learning
1. Introduction
Deep neural networks contain multiple non-linear hidden layers and this makes them very expressive models that can learn very complicated relationships between their inputs and outputs.
With limited training data, however, many of these complicated relationships will be the result of sampling noise, so they will exist in the training set but not in real test data even if it is drawn from the same distribution. This leads to overfitting and many methods have been developed for reducing it. These include stopping the training as soon as performance on a validation set starts to get worse, introducing weight penalties of various kinds such as L1 and L2 regularization and soft weight sharing (Nowlan and Hinton, 1992).
With unlimited computation, the best way to "regularize" a fixed-sized model is to average the predictions of all possible settings of the parameters, weighting each setting by c⃝2014 Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever and Ruslan Salakhutdinov.
Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov(a) Standard Neural Net(b) After applying dropout.
Figure 1: Dropout Neural Net Model. Left: A standard neural net with 2 hidden layers. Right:
An example of a thinned net produced by applying dropout to the network on the left.
Crossed units have been dropped. its posterior probability given the training data. This can sometimes be approximated quite well for simple or small models (Xiong et al., 2011; Salakhutdinov and Mnih, 2008), but we would like to approach the performance of the Bayesian gold standard using considerably less computation. We propose to do this by approximating an equally weighted geometric mean of the predictions of an exponential number of learned models that share parameters.
Model combination nearly always improves the performance of machine learning methods. With large neural networks, however, the obvious idea of averaging the outputs of many separately trained nets is prohibitively expensive. Combining several models is most helpful when the individual models are different from each other and in order to make neural net models different, they should either have different architectures or be trained on different data. Training many different architectures is hard because finding optimal hyperparameters for each architecture is a daunting task and training each large network requires a lot of computation. Moreover, large networks normally require large amounts of training data and there may not be enough data available to train different networks on different subsets of the data. Even if one was able to train many different large networks, using them all at test time is infeasible in applications where it is important to respond quickly.
Dropout is a technique that addresses both these issues. It prevents overfitting and provides a way of approximately combining exponentially many different neural network architectures efficiently.
The term "dropout" refers to dropping out units (hidden and visible) in a neural network. By dropping a unit out, we mean temporarily removing it from the network, along with all its incoming and outgoing connections, as shown in Figure 1.
The choice of which units to drop is random. In the simplest case, each unit is retained with a fixed probability p independent of other units, where p can be chosen using a validation set or can simply be set at 0.5, which seems to be close to optimal for a wide range of networks and tasks. For the input units, however, the optimal probability of retention is usually closer to 1 than to 0.5.
Dropout
Present with probability p w(a) At training time
Always present pw(b) At test time
Figure 2: Left: A unit at training time that is present with probability p and is connected to units in the next layer with weights w. Right: At test time, the unit is always present and the weights are multiplied by p. The output at test time is same as the expected output at training time.
Applying dropout to a neural network amounts to sampling a "thinned" network from it. The thinned network consists of all the units that survived dropout (Figure 1b). A neural net with n units, can be seen as a collection of 2n possible thinned neural networks.
These networks all share weights so that the total number of parameters is still O(n2), or less. For each presentation of each training case, a new thinned network is sampled and trained. So training a neural network with dropout can be seen as training a collection of 2n thinned networks with extensive weight sharing, where each thinned network gets trained very rarely, if at all.
At test time, it is not feasible to explicitly average the predictions from exponentially many thinned models. However, a very simple approximate averaging method works well in practice. The idea is to use a single neural net at test time without dropout. The weights of this network are scaled-down versions of the trained weights. If a unit is retained with probability p during training, the outgoing weights of that unit are multiplied by p at test time as shown in Figure 2. This ensures that for any hidden unit the expected output (under the distribution used to drop units at training time) is the same as the actual output at test time. By doing this scaling, 2n networks with shared weights can be combined into a single neural network to be used at test time. We found that training a network with dropout and using this approximate averaging method at test time leads to significantly lower generalization error on a wide variety of classification problems compared to training with other regularization methods.
The idea of dropout is not limited to feed-forward neural nets. It can be more generally applied to graphical models such as Boltzmann Machines.
In this paper, we introduce the dropout Restricted Boltzmann Machine model and compare it to standard Restricted
Boltzmann Machines (RBM). Our experiments show that dropout RBMs are better than standard RBMs in certain respects.
This paper is structured as follows. Section 2 describes the motivation for this idea.
Section 3 describes relevant previous work. Section 4 formally describes the dropout model.
Section 5 gives an algorithm for training dropout networks. In Section 6, we present our experimental results where we apply dropout to problems in different domains and compare it with other forms of regularization and model combination. Section 7 analyzes the effect of dropout on different properties of a neural network and describes how dropout interacts with the network's hyperparameters. Section 8 describes the Dropout RBM model. In Section 9 we explore the idea of marginalizing dropout. In Appendix A we present a practical guide
Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov for training dropout nets. This includes a detailed analysis of the practical considerations involved in choosing hyperparameters when training dropout networks.
2. Motivation
A motivation for dropout comes from a theory of the role of sex in evolution (Livnat et al., 2010). Sexual reproduction involves taking half the genes of one parent and half of the other, adding a very small amount of random mutation, and combining them to produce an offspring. The asexual alternative is to create an offspring with a slightly mutated copy of the parent's genes. It seems plausible that asexual reproduction should be a better way to optimize individual fitness because a good set of genes that have come to work well together can be passed on directly to the offspring. On the other hand, sexual reproduction is likely to break up these co-adapted sets of genes, especially if these sets are large and, intuitively, this should decrease the fitness of organisms that have already evolved complicated coadaptations. However, sexual reproduction is the way most advanced organisms evolved.
One possible explanation for the superiority of sexual reproduction is that, over the long term, the criterion for natural selection may not be individual fitness but rather mix-ability of genes. The ability of a set of genes to be able to work well with another random set of genes makes them more robust. Since a gene cannot rely on a large set of partners to be present at all times, it must learn to do something useful on its own or in collaboration with a small number of other genes. According to this theory, the role of sexual reproduction is not just to allow useful new genes to spread throughout the population, but also to facilitate this process by reducing complex co-adaptations that would reduce the chance of a new gene improving the fitness of an individual. Similarly, each hidden unit in a neural network trained with dropout must learn to work with a randomly chosen sample of other units. This should make each hidden unit more robust and drive it towards creating useful features on its own without relying on other hidden units to correct its mistakes. However, the hidden units within a layer will still learn to do different things from each other. One might imagine that the net would become robust against dropout by making many copies of each hidden unit, but this is a poor solution for exactly the same reason as replica codes are a poor way to deal with a noisy channel.
A closely related, but slightly different motivation for dropout comes from thinking about successful conspiracies.
Ten conspiracies each involving five people is probably a better way to create havoc than one big conspiracy that requires fifty people to all play their parts correctly. If conditions do not change and there is plenty of time for rehearsal, a big conspiracy can work well, but with non-stationary conditions, the smaller the conspiracy the greater its chance of still working. Complex co-adaptations can be trained to work well on a training set, but on novel test data they are far more likely to fail than multiple simpler co-adaptations that achieve the same thing.
3. Related Work
Dropout can be interpreted as a way of regularizing a neural network by adding noise to its hidden units. The idea of adding noise to the states of units has previously been used in the context of Denoising Autoencoders (DAEs) by Vincent et al. (2008, 2010) where noise
Dropout is added to the input units of an autoencoder and the network is trained to reconstruct the noise-free input. Our work extends this idea by showing that dropout can be effectively applied in the hidden layers as well and that it can be interpreted as a form of model averaging.
We also show that adding noise is not only useful for unsupervised feature learning but can also be extended to supervised learning problems. In fact, our method can be applied to other neuron-based architectures, for example, Boltzmann Machines. While
5% noise typically works best for DAEs, we found that our weight scaling procedure applied at test time enables us to use much higher noise levels. Dropping out 20% of the input units and 50% of the hidden units was often found to be optimal.
Since dropout can be seen as a stochastic regularization technique, it is natural to consider its deterministic counterpart which is obtained by marginalizing out the noise. In this paper, we show that, in simple cases, dropout can be analytically marginalized out to obtain deterministic regularization methods.
Recently, van der Maaten et al. (2013) also explored deterministic regularizers corresponding to different exponential-family noise distributions, including dropout (which they refer to as "blankout noise"). However, they apply noise to the inputs and only explore models with no hidden layers. Wang and Manning(2013) proposed a method for speeding up dropout by marginalizing dropout noise. Chen et al. (2012) explored marginalization in the context of denoising autoencoders.
In dropout, we minimize the loss function stochastically under a noise distribution.
This can be seen as minimizing an expected loss function. Previous work of Globerson and Roweis (2006); Dekel et al. (2010) explored an alternate setting where the loss is minimized when an adversary gets to pick which units to drop. Here, instead of a noise distribution, the maximum number of units that can be dropped is fixed. However, this work also does not explore models with hidden units.
4. Model Description
This section describes the dropout neural network model. Consider a neural network with
L hidden layers. Let l ∈ {1,..., L} index the hidden layers of the network. Let z(l) denote the vector of inputs into layer l, y(l) denote the vector of outputs from layer l (y(0) = x is the input). W (l) and b(l) are the weights and biases at layer l. The feed-forward operation of a standard neural network (Figure 3a) can be described as (for l ∈ {0,..., L − 1} and any hidden unit i) z(l+1) i
= w(l+1) i yl + b(l+1) i, y(l+1) i
= f(z(l+1) i
), where f is any activation function, for example, f(x) = 1/ (1 + exp(−x)).
With dropout, the feed-forward operation becomes (Figure 3b) r(l) j
∼
Bernoulli(p), �y(l)
= r(l) ∗ y(l), z(l+1) i
= w(l+1) i
�yl + b(l+1) i, y(l+1) i
= f(z(l+1) i
Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov
+1 y(l)
1 y(l)
2 y(l)
3 z(l+1) i y(l+1) i w(l+1) i b(l+1) i f(a) Standard network
�y(l)
�y(l)
�y(l)
3 z(l+1) i y(l+1) i y(l)
1 y(l)
2 y(l)
3 r(l)
1 r(l)
2 r(l)
3 w(l+1) i b(l+1) i f(b) Dropout network
Figure 3: Comparison of the basic operations of a standard and dropout network.
Here ∗ denotes an element-wise product. For any layer l, r(l) is a vector of independent
Bernoulli random variables each of which has probability p of being 1.
This vector is sampled and multiplied element-wise with the outputs of that layer, y(l), to create the thinned outputs �y(l). The thinned outputs are then used as input to the next layer. This process is applied at each layer. This amounts to sampling a sub-network from a larger network. For learning, the derivatives of the loss function are backpropagated through the sub-network. At test time, the weights are scaled as W (l) test = pW (l) as shown in Figure 2.
The resulting neural network is used without dropout.
5. Learning Dropout Nets
This section describes a procedure for training dropout neural nets.
5.1 Backpropagation
Dropout neural networks can be trained using stochastic gradient descent in a manner similar to standard neural nets. The only difference is that for each training case in a mini-batch, we sample a thinned network by dropping out units. Forward and backpropagation for that training case are done only on this thinned network. The gradients for each parameter are averaged over the training cases in each mini-batch. Any training case which does not use a parameter contributes a gradient of zero for that parameter. Many methods have been used to improve stochastic gradient descent such as momentum, annealed learning rates and L2 weight decay. Those were found to be useful for dropout neural networks as well.
One particular form of regularization was found to be especially useful for dropout— constraining the norm of the incoming weight vector at each hidden unit to be upper bounded by a fixed constant c. In other words, if w represents the vector of weights incident on any hidden unit, the neural network was optimized under the constraint ||w||2 ≤ c. This constraint was imposed during optimization by projecting w onto the surface of a ball of radius c, whenever w went out of it. This is also called max-norm regularization since it implies that the maximum value that the norm of any weight can take is c. The constant
Dropout c is a tunable hyperparameter, which is determined using a validation set.
Max-norm regularization has been previously used in the context of collaborative filtering (Srebro and Shraibman, 2005).
It typically improves the performance of stochastic gradient descent training of deep neural nets, even when no dropout is used.
Although dropout alone gives significant improvements, using dropout along with maxnorm regularization, large decaying learning rates and high momentum provides a significant boost over just using dropout. A possible justification is that constraining weight vectors to lie inside a ball of fixed radius makes it possible to use a huge learning rate without the possibility of weights blowing up. The noise provided by dropout then allows the optimization process to explore different regions of the weight space that would have otherwise been difficult to reach. As the learning rate decays, the optimization takes shorter steps, thereby doing less exploration and eventually settles into a minimum.
5.2 Unsupervised Pretraining
Neural networks can be pretrained using stacks of RBMs (Hinton and Salakhutdinov, 2006), autoencoders (Vincent et al., 2010) or Deep Boltzmann Machines (Salakhutdinov and Hinton, 2009). Pretraining is an effective way of making use of unlabeled data. Pretraining followed by finetuning with backpropagation has been shown to give significant performance boosts over finetuning from random initializations in certain cases.
Dropout can be applied to finetune nets that have been pretrained using these techniques. The pretraining procedure stays the same. The weights obtained from pretraining should be scaled up by a factor of 1/p. This makes sure that for each unit, the expected output from it under random dropout will be the same as the output during pretraining.
We were initially concerned that the stochastic nature of dropout might wipe out the information in the pretrained weights. This did happen when the learning rates used during finetuning were comparable to the best learning rates for randomly initialized nets. However, when the learning rates were chosen to be smaller, the information in the pretrained weights seemed to be retained and we were able to get improvements in terms of the final generalization error compared to not using dropout when finetuning.
6. Experimental Results
We trained dropout neural networks for classification problems on data sets in different domains. We found that dropout improved generalization performance on all data sets compared to neural networks that did not use dropout. Table 1 gives a brief description of the data sets. The data sets are
• MNIST : A standard toy data set of handwritten digits.
• TIMIT : A standard speech benchmark for clean speech recognition.
• CIFAR-10 and CIFAR-100 : Tiny natural images (Krizhevsky, 2009).
• Street View House Numbers data set (SVHN) : Images of house numbers collected by
Google Street View (Netzer et al., 2011).
• ImageNet : A large collection of natural images.
• Reuters-RCV1 : A collection of Reuters newswire articles.
Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov
• Alternative Splicing data set: RNA features for predicting alternative gene splicing(Xiong et al., 2011).
We chose a diverse set of data sets to demonstrate that dropout is a general technique for improving neural nets and is not specific to any particular application domain. In this section, we present some key results that show the effectiveness of dropout. A more detailed description of all the experiments and data sets is provided in Appendix B.
Data Set
Domain
Dimensionality
Training Set
Test Set
MNIST
Vision
784 (28 × 28 grayscale)
60K
10K
SVHN
Vision
3072 (32 × 32 color)
600K
26K
CIFAR-10/100
Vision
3072 (32 × 32 color)
60K
10K
ImageNet (ILSVRC-2012)
Vision
65536 (256 × 256 color)
1.2M
150K
TIMIT
Speech
2520 (120-dim, 21 frames)
1.1M frames
58K frames
Reuters-RCV1
Text
200K
200K
Alternative Splicing
Genetics
Table 1: Overview of the data sets used in this paper.
6.1 Results on Image Data Sets
We used five image data sets to evaluate dropout—MNIST, SVHN, CIFAR-10, CIFAR-100 and ImageNet. These data sets include different image types and training set sizes. Models which achieve state-of-the-art results on all of these data sets use dropout.
6.1.1 MNIST
Method
Unit
Type
Architecture
Error
Standard Neural Net (Simard et al., 2003)
Logistic
2 layers, 800 units
SVM Gaussian kernel
NA
NA
Dropout NN
Logistic
3 layers, 1024 units
Dropout NN
ReLU
3 layers, 1024 units
Dropout NN + max-norm constraint
ReLU
3 layers, 1024 units
Dropout NN + max-norm constraint
ReLU
3 layers, 2048 units
Dropout NN + max-norm constraint
ReLU
2 layers, 4096 units
Dropout NN + max-norm constraint
ReLU
2 layers, 8192 units
Dropout NN + max-norm constraint (Goodfellow et al., 2013)
Maxout
2 layers, (5 × 240) units
DBN + finetuning (Hinton and Salakhutdinov, 2006)
Logistic
500-500-2000
DBM + finetuning (Salakhutdinov and Hinton, 2009)
Logistic
500-500-2000
DBN + dropout finetuning
Logistic
500-500-2000
DBM + dropout finetuning
Logistic
500-500-2000
Table 2: Comparison of different models on MNIST.
The MNIST data set consists of 28 × 28 pixel handwritten digit images. The task is to classify the images into 10 digit classes. Table 2 compares the performance of dropout with other techniques. The best performing neural networks for the permutation invariant
Dropout setting that do not use dropout or unsupervised pretraining achieve an error of about
1.60% (Simard et al., 2003). With dropout the error reduces to 1.35%. Replacing logistic units with rectified linear units (ReLUs) (Jarrett et al., 2009) further reduces the error to
1.25%. Adding max-norm regularization again reduces it to 1.06%. Increasing the size of the network leads to better results. A neural net with 2 layers and 8192 units per layer gets down to 0.95% error. Note that this network has more than 65 million parameters and is being trained on a data set of size 60,000. Training a network of this size to give good generalization error is very hard with standard regularization methods and early stopping.
Dropout, on the other hand, prevents overfitting, even in this case. It does not even need early stopping. Goodfellow et al. (2013) showed that results can be further improved to
0.94% by replacing ReLU units with maxout units. All dropout nets use p = 0.5 for hidden units and p = 0.8 for input units. More experimental details can be found in Appendix B.1.
Dropout nets pretrained with stacks of RBMs and Deep Boltzmann Machines also give improvements as shown in Table 2. DBM—pretrained dropout nets achieve a test error of 0.79% which is the best performance ever reported for the permutation invariant setting.
We note that it possible to obtain better results by using 2-D spatial information and augmenting the training set with distorted versions of images from the standard training set. We demonstrate the effectiveness of dropout in that setting on more interesting data sets.
Number of weight updates
Classification Error %
With dropout
Without dropout
@
R
@
@
R
Figure 4: Test error for different architectures with and without dropout.
The networks have 2 to 4 hidden layers each with 1024 to 2048 units.
In order to test the robustness of dropout, classification experiments were done with networks of many different architectures keeping all hyperparameters, including p, fixed.
Figure 4 shows the test error rates obtained for these different architectures as training progresses.
The same architectures trained with and without dropout have drastically different test errors as seen as by the two separate clusters of trajectories. Dropout gives a huge improvement across all architectures, without using hyperparameters that were tuned specifically for each architecture.
6.1.2 Street View House Numbers
The Street View House Numbers (SVHN)
Data Set (Netzer et al., 2011) consists of color images of house numbers collected by
Google Street View. Figure 5a shows some examples of images from this data set. The part of the data set that we use in our experiments consists of 32 × 32 color images roughly centered on a digit in a house number. The task is to identify that digit.
For this data set, we applied dropout to convolutional neural networks (LeCun et al., 1989). The best architecture that we found has three convolutional layers followed by 2 fully connected hidden layers. All hidden units were ReLUs. Each convolutional layer was
Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov
Method
Error %
Binary Features (WDCH) (Netzer et al., 2011)
HOG (Netzer et al., 2011)
Stacked Sparse Autoencoders (Netzer et al., 2011)
KMeans (Netzer et al., 2011)
Multi-stage Conv Net with average pooling (Sermanet et al., 2012)
Multi-stage Conv Net + L2 pooling (Sermanet et al., 2012)
Multi-stage Conv Net + L4 pooling + padding (Sermanet et al., 2012)
Conv Net + max-pooling
Conv Net + max pooling + dropout in fully connected layers
Conv Net + stochastic pooling (Zeiler and Fergus, 2013)
Conv Net + max pooling + dropout in all layers
Conv Net + maxout (Goodfellow et al., 2013)
Human Performance
Table 3: Results on the Street View House Numbers data set. followed by a max-pooling layer. Appendix B.2 describes the architecture in more detail.
Dropout was applied to all the layers of the network with the probability of retaining a hidden unit being p = (0.9, 0.75, 0.75, 0.5, 0.5, 0.5) for the different layers of the network (going from input to convolutional layers to fully connected layers). Max-norm regularization was used for weights in both convolutional and fully connected layers. Table 3 compares the results obtained by different methods. We find that convolutional nets outperform other methods. The best performing convolutional nets that do not use dropout achieve an error rate of 3.95%. Adding dropout only to the fully connected layers reduces the error to 3.02%.
Adding dropout to the convolutional layers as well further reduces the error to 2.55%. Even more gains can be obtained by using maxout units.
The additional gain in performance obtained by adding dropout in the convolutional layers (3.02% to 2.55%) is worth noting. One may have presumed that since the convolutional layers don't have a lot of parameters, overfitting is not a problem and therefore dropout would not have much effect. However, dropout in the lower layers still helps because it provides noisy inputs for the higher fully connected layers which prevents them from overfitting.
6.1.3 CIFAR-10 and CIFAR-100
The CIFAR-10 and CIFAR-100 data sets consist of 32 × 32 color images drawn from 10 and 100 categories respectively. Figure 5b shows some examples of images from this data set. A detailed description of the data sets, input preprocessing, network architectures and other experimental details is given in Appendix B.3. Table 4 shows the error rate obtained by different methods on these data sets. Without any data augmentation, Snoek et al.(2012) used Bayesian hyperparameter optimization to obtained an error rate of 14.98% on
CIFAR-10. Using dropout in the fully connected layers reduces that to 14.32% and adding dropout in every layer further reduces the error to 12.61%. Goodfellow et al. (2013) showed that the error is further reduced to 11.68% by replacing ReLU units with maxout units. On
CIFAR-100, dropout reduces the error from 43.48% to 37.20% which is a huge improvement.
No data augmentation was used for either data set (apart from the input dropout).
Dropout(a) Street View House Numbers (SVHN)(b) CIFAR-10
Figure 5: Samples from image data sets. Each row corresponds to a different category.
Method
CIFAR-10
CIFAR-100
Conv Net + max pooling (hand tuned)
Conv Net + stochastic pooling (Zeiler and Fergus, 2013)
Conv Net + max pooling (Snoek et al., 2012)Conv Net + max pooling + dropout fully connected layers
Conv Net + max pooling + dropout in all layers
Conv Net + maxout (Goodfellow et al., 2013)
Table 4: Error rates on CIFAR-10 and CIFAR-100.
6.1.4 ImageNet
ImageNet is a data set of over 15 million labeled high-resolution images belonging to roughly
22,000 categories. Starting in 2010, as part of the Pascal Visual Object Challenge, an annual competition called the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) has been held. A subset of ImageNet with roughly 1000 images in each of 1000 categories is used in this challenge. Since the number of categories is rather large, it is conventional to report two error rates: top-1 and top-5, where the top-5 error rate is the fraction of test images for which the correct label is not among the five labels considered most probable by the model. Figure 6 shows some predictions made by our model on a few test images.
ILSVRC-2010 is the only version of ILSVRC for which the test set labels are available, so most of our experiments were performed on this data set. Table 5 compares the performance of different methods. Convolutional nets with dropout outperform other methods by a large margin. The architecture and implementation details are described in detail in Krizhevsky et al. (2012).
Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov
Figure 6: Some ImageNet test cases with the 4 most probable labels as predicted by our model.
The length of the horizontal bars is proportional to the probability assigned to the labels by the model. Pink indicates ground truth.
Model
Top-1
Top-5
Sparse Coding (Lin et al., 2010)
SIFT + Fisher Vectors (Sanchez and Perronnin, 2011)
Conv Net + dropout (Krizhevsky et al., 2012)
Table 5: Results on the ILSVRC-2010 test set.
Model
Top-1(val)
Top-5(val)
Top-5(test)
SVM on Fisher Vectors of Dense SIFT and Color StatisticsAvg of classifiers over FVs of SIFT, LBP, GIST and CSIFTConv Net + dropout (Krizhevsky et al., 2012)Avg of 5 Conv Nets + dropout (Krizhevsky et al., 2012)
Table 6: Results on the ILSVRC-2012 validation/test set.
Our model based on convolutional nets and dropout won the ILSVRC-2012 competition.
Since the labels for the test set are not available, we report our results on the test set for the final submission and include the validation set results for different variations of our model. Table 6 shows the results from the competition. While the best methods based on standard vision features achieve a top-5 error rate of about 26%, convolutional nets with dropout achieve a test error of about 16% which is a staggering difference. Figure 6 shows some examples of predictions made by our model. We can see that the model makes very reasonable predictions, even when its best guess is not correct.
6.2 Results on TIMIT
Next, we applied dropout to a speech recognition task. We use the TIMIT data set which consists of recordings from 680 speakers covering 8 major dialects of American English reading ten phonetically-rich sentences in a controlled noise-free environment.
Dropout neural networks were trained on windows of 21 log-filter bank frames to predict the label of the central frame.
No speaker dependent operations were performed.
Appendix B.4 describes the data preprocessing and training details. Table 7 compares dropout neural
Dropout nets with other models. A 6-layer net gives a phone error rate of 23.4%. Dropout further improves it to 21.8%. We also trained dropout nets starting from pretrained weights. A
4-layer net pretrained with a stack of RBMs get a phone error rate of 22.7%. With dropout, this reduces to 19.7%. Similarly, for an 8-layer net the error reduces from 20.5% to 19.7%.
Method
Phone Error Rate%
NN (6 layers) (Mohamed et al., 2010)
Dropout NN (6 layers)
DBN-pretrained NN (4 layers)
DBN-pretrained NN (6 layers) (Mohamed et al., 2010)
DBN-pretrained NN (8 layers) (Mohamed et al., 2010)
20.7 mcRBM-DBN-pretrained NN (5 layers) (Dahl et al., 2010)
DBN-pretrained NN (4 layers) + dropout
DBN-pretrained NN (8 layers) + dropout
Table 7: Phone error rate on the TIMIT core test set.
6.3 Results on a Text Data Set
To test the usefulness of dropout in the text domain, we used dropout networks to train a document classifier. We used a subset of the Reuters-RCV1 data set which is a collection of over 800,000 newswire articles from Reuters. These articles cover a variety of topics. The task is to take a bag of words representation of a document and classify it into 50 disjoint topics. Appendix B.5 describes the setup in more detail. Our best neural net which did not use dropout obtained an error rate of 31.05%. Adding dropout reduced the error to
29.62%. We found that the improvement was much smaller compared to that for the vision and speech data sets.
6.4 Comparison with Bayesian Neural Networks
Dropout can be seen as a way of doing an equally-weighted averaging of exponentially many models with shared weights. On the other hand, Bayesian neural networks (Neal, 1996) are the proper way of doing model averaging over the space of neural network structures and parameters.
In dropout, each model is weighted equally, whereas in a Bayesian neural network each model is weighted taking into account the prior and how well the model fits the data, which is the more correct approach. Bayesian neural nets are extremely useful for solving problems in domains where data is scarce such as medical diagnosis, genetics, drug discovery and other computational biology applications. However, Bayesian neural nets are slow to train and difficult to scale to very large network sizes. Besides, it is expensive to get predictions from many large nets at test time. On the other hand, dropout neural nets are much faster to train and use at test time. In this section, we report experiments that compare Bayesian neural nets with dropout neural nets on a small data set where Bayesian neural networks are known to perform well and obtain state-of-the-art results. The aim is to analyze how much does dropout lose compared to Bayesian neural nets.
The data set that we use (Xiong et al., 2011) comes from the domain of genetics. The task is to predict the occurrence of alternative splicing based on RNA features. Alternative splicing is a significant cause of cellular diversity in mammalian tissues. Predicting the Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov
Method
Code Quality (bits)
Neural Network (early stopping) (Xiong et al., 2011)
Regression, PCA (Xiong et al., 2011)
SVM, PCA (Xiong et al., 2011)
Neural Network with dropout
Bayesian Neural Network (Xiong et al., 2011)
Table 8: Results on the Alternative Splicing Data Set. occurrence of alternate splicing in certain tissues under different conditions is important for understanding many human diseases. Given the RNA features, the task is to predict the probability of three splicing related events that biologists care about. The evaluation metric is Code Quality which is a measure of the negative KL divergence between the target and the predicted probability distributions (higher is better). Appendix B.6 includes a detailed description of the data set and this performance metric.
Table 8 summarizes the performance of different models on this data set. Xiong et al.(2011) used Bayesian neural nets for this task. As expected, we found that Bayesian neural nets perform better than dropout. However, we see that dropout improves significantly upon the performance of standard neural nets and outperforms all other methods. The challenge in this data set is to prevent overfitting since the size of the training set is small.
One way to prevent overfitting is to reduce the input dimensionality using PCA. Thereafter, standard techniques such as SVMs or logistic regression can be used. However, with dropout we were able to prevent overfitting without the need to do dimensionality reduction. The dropout nets are very large (1000s of hidden units) compared to a few tens of units in the Bayesian network. This shows that dropout has a strong regularizing effect.
6.5 Comparison with Standard Regularizers
Several regularization methods have been proposed for preventing overfitting in neural networks. These include L2 weight decay (more generally Tikhonov regularization (Tikhonov, 1943)), lasso (Tibshirani, 1996), KL-sparsity and max-norm regularization. Dropout can be seen as another way of regularizing neural networks. In this section we compare dropout with some of these regularization methods using the MNIST data set.
The same network architecture (784-1024-1024-2048-10) with ReLUs was trained using stochastic gradient descent with different regularizations. Table 9 shows the results.
The values of different hyperparameters associated with each kind of regularization (decay constants, target sparsity, dropout rate, max-norm upper bound) were obtained using a validation set. We found that dropout combined with max-norm regularization gives the lowest generalization error.
7. Salient Features
The experiments described in the previous section provide strong evidence that dropout is a useful technique for improving neural networks. In this section, we closely examine how dropout affects a neural network. We analyze the effect of dropout on the quality of features produced. We see how dropout affects the sparsity of hidden unit activations. We
Dropout
Method
Test Classification error %
L2
L2 + L1 applied towards the end of training
L2 + KL-sparsity
Max-norm
Dropout + L2
Dropout + Max-norm
Table 9: Comparison of different regularization methods on MNIST. also see how the advantages obtained from dropout vary with the probability of retaining units, size of the network and the size of the training set. These observations give some insight into why dropout works so well.
7.1 Effect on Features(a) Without dropout(b) Dropout with p = 0.5.
Figure 7: Features learned on MNIST with one hidden layer autoencoders having 256 rectified linear units.
In a standard neural network, the derivative received by each parameter tells it how it should change so the final loss function is reduced, given what all other units are doing.
Therefore, units may change in a way that they fix up the mistakes of the other units.
This may lead to complex co-adaptations. This in turn leads to overfitting because these co-adaptations do not generalize to unseen data. We hypothesize that for each hidden unit, dropout prevents co-adaptation by making the presence of other hidden units unreliable.
Therefore, a hidden unit cannot rely on other specific units to correct its mistakes. It must perform well in a wide variety of different contexts provided by the other hidden units. To observe this effect directly, we look at the first level features learned by neural networks trained on visual tasks with and without dropout.
Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov
Figure 7a shows features learned by an autoencoder on MNIST with a single hidden layer of 256 rectified linear units without dropout. Figure 7b shows the features learned by an identical autoencoder which used dropout in the hidden layer with p = 0.5. Both autoencoders had similar test reconstruction errors. However, it is apparent that the features shown in Figure 7a have co-adapted in order to produce good reconstructions. Each hidden unit on its own does not seem to be detecting a meaningful feature. On the other hand, in Figure 7b, the hidden units seem to detect edges, strokes and spots in different parts of the image. This shows that dropout does break up co-adaptations, which is probably the main reason why it leads to lower generalization errors.
7.2 Effect on Sparsity(a) Without dropout(b) Dropout with p = 0.5.
Figure 8: Effect of dropout on sparsity. ReLUs were used for both models. Left: The histogram of mean activations shows that most units have a mean activation of about 2.0. The histogram of activations shows a huge mode away from zero. Clearly, a large fraction of units have high activation. Right: The histogram of mean activations shows that most units have a smaller mean mean activation of about 0.7. The histogram of activations shows a sharp peak at zero. Very few units have high activation.
We found that as a side-effect of doing dropout, the activations of the hidden units become sparse, even when no sparsity inducing regularizers are present. Thus, dropout automatically leads to sparse representations. To observe this effect, we take the autoencoders trained in the previous section and look at the sparsity of hidden unit activations on a random mini-batch taken from the test set. Figure 8a and Figure 8b compare the sparsity for the two models. In a good sparse model, there should only be a few highly activated units for any data case. Moreover, the average activation of any unit across data cases should be low. To assess both of these qualities, we plot two histograms for each model. For each model, the histogram on the left shows the distribution of mean activations of hidden units across the minibatch. The histogram on the right shows the distribution of activations of the hidden units.
Comparing the histograms of activations we can see that fewer hidden units have high activations in Figure 8b compared to Figure 8a, as seen by the significant mass away from
Dropout zero for the net that does not use dropout. The mean activations are also smaller for the dropout net. The overall mean activation of hidden units is close to 2.0 for the autoencoder without dropout but drops to around 0.7 when dropout is used.
7.3 Effect of Dropout Rate
Dropout has a tunable hyperparameter p (the probability of retaining a unit in the network).
In this section, we explore the effect of varying this hyperparameter. The comparison is done in two situations.
1. The number of hidden units is held constant.
2. The number of hidden units is changed so that the expected number of hidden units that will be retained after dropout is held constant.
In the first case, we train the same network architecture with different amounts of dropout. We use a 784-2048-2048-2048-10 architecture. No input dropout was used. Figure 9a shows the test error obtained as a function of p. If the architecture is held constant, having a small p means very few units will turn on during training. It can be seen that this has led to underfitting since the training error is also high. We see that as p increases, the error goes down. It becomes flat when 0.4 ≤ p ≤ 0.8 and then increases as p becomes close to 1.
Probability of retaining a unit (p)
Classification Error %
Test Error
Training Error(a) Keeping n fixed.
Probability of retaining a unit (p)
Classification Error %
Test Error
Training Error(b) Keeping pn fixed.
Figure 9: Effect of changing dropout rates on MNIST.
Another interesting setting is the second case in which the quantity pn is held constant where n is the number of hidden units in any particular layer. This means that networks that have small p will have a large number of hidden units.
Therefore, after applying dropout, the expected number of units that are present will be the same across different architectures. However, the test networks will be of different sizes. In our experiments, we set pn = 256 for the first two hidden layers and pn = 512 for the last hidden layer.
Figure 9b shows the test error obtained as a function of p. We notice that the magnitude of errors for small values of p has reduced by a lot compared to Figure 9a (for p = 0.1 it fell from 2.7% to 1.7%). Values of p that are close to 0.6 seem to perform best for this choice of pn but our usual default value of 0.5 is close to optimal.
Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov
7.4 Effect of Data Set Size
One test of a good regularizer is that it should make it possible to get good generalization error from models with a large number of parameters trained on small data sets. This section explores the effect of changing the data set size when dropout is used with feedforward networks. Huge neural networks trained in the standard way overfit massively on small data sets. To see if dropout can help, we run classification experiments on MNIST and vary the amount of data given to the network.
Dataset size
Classification Error %
With dropout
Without dropout
Figure 10: Effect of varying data set size.
The results of these experiments are shown in Figure 10. The network was given data sets of size 100, 500, 1K, 5K, 10K and 50K chosen randomly from the MNIST training set.
The same network architecture (784-1024-1024-2048-10) was used for all data sets. Dropout with p = 0.5 was performed at all the hidden layers and p = 0.8 at the input layer. It can be observed that for extremely small data sets (100, 500) dropout does not give any improvements.
The model has enough parameters that it can overfit on the training data, even with all the noise coming from dropout. As the size of the data set is increased, the gain from doing dropout increases up to a point and then declines. This suggests that for any given architecture and dropout rate, there is a "sweet spot" corresponding to some amount of data that is large enough to not be memorized in spite of the noise but not so large that overfitting is not a problem anyways.
7.5 Monte-Carlo Model Averaging vs. Weight Scaling
Number of samples used for Monte-Carlo averaging (k)
Test Classification error %
Monte-Carlo Model Averaging
Approximate averaging by weight scaling
Figure 11: Monte-Carlo model averaging vs. weight scaling.
The efficient test time procedure that we propose is to do an approximate model combination by scaling down the weights of the trained neural network. An expensive but more correct way of averaging the models is to sample k neural nets using dropout for each test case and average their predictions.
As k → ∞, this Monte-Carlo model average gets close to the true model average. It is interesting to see empirically how many samples k are needed to match the performance of the approximate averaging method. By computing the error for different values of k we can see how quickly the error rate of the finite-sample average approaches the error rate of the true model average.
Dropout
We again use the MNIST data set and do classification by averaging the predictions of k randomly sampled neural networks. Figure 11 shows the test error rate obtained for different values of k. This is compared with the error obtained using the weight scaling method (shown as a horizontal line). It can be seen that around k = 50, the Monte-Carlo method becomes as good as the approximate method. Thereafter, the Monte-Carlo method is slightly better than the approximate method but well within one standard deviation of it. This suggests that the weight scaling method is a fairly good approximation of the true model average.
8. Dropout Restricted Boltzmann Machines
Besides feed-forward neural networks, dropout can also be applied to Restricted Boltzmann
Machines (RBM). In this section, we formally describe this model and show some results to illustrate its key properties.
8.1 Model Description
Consider an RBM with visible units v ∈ {0, 1}D and hidden units h ∈ {0, 1}F. It defines the following probability distribution
P(h, v; θ) =
Z(θ) exp(v⊤Wh + a⊤h + b⊤v).
Where θ = {W, a, b} represents the model parameters and Z is the partition function.
Dropout RBMs are RBMs augmented with a vector of binary random variables r ∈
{0, 1}F.
Each random variable rj takes the value 1 with probability p, independent of others. If rj takes the value 1, the hidden unit hj is retained, otherwise it is dropped from the model. The joint distribution defined by a Dropout RBM can be expressed as
P(r, h, v; p, θ)
=
P(r; p)P(h, v|r; θ), P(r; p)
=
F
� j=1 prj(1 − p)1−rj, P(h, v|r; θ)
=
Z′(θ, r) exp(v⊤Wh + a⊤h + b⊤v)
F
� j=1 g(hj, rj), g(hj, rj)
=
1(rj = 1) + 1(rj = 0)1(hj = 0).
Z′(θ, r) is the normalization constant. g(hj, rj) imposes the constraint that if rj = 0, hj must be 0. The distribution over h, conditioned on v and r is factorial
P(h|r, v)
=
F
� j=1
P(hj|rj, v), P(hj = 1|rj, v)
=
1(rj = 1)σ
� bj +
� i
Wijvi
�
Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov(a) Without dropout(b) Dropout with p = 0.5.
Figure 12: Features learned on MNIST by 256 hidden unit RBMs. The features are ordered by L2 norm.
The distribution over v conditioned on h is same as that of an RBM
P(v|h)
=
D
� i=1
P(vi|h), P(vi = 1|h)
= σ
�
�ai +
� j
Wijhj
�
�.
Conditioned on r, the distribution over {v, h} is same as the distribution that an RBM would impose, except that the units for which rj = 0 are dropped from h. Therefore, the Dropout RBM model can be seen as a mixture of exponentially many RBMs with shared weights each using a different subset of h.
8.2 Learning Dropout RBMs
Learning algorithms developed for RBMs such as Contrastive Divergence (Hinton et al., 2006) can be directly applied for learning Dropout RBMs. The only difference is that r is first sampled and only the hidden units that are retained are used for training. Similar to dropout neural networks, a different r is sampled for each training case in every minibatch.
In our experiments, we use CD-1 for training dropout RBMs.
8.3 Effect on Features
Dropout in feed-forward networks improved the quality of features by reducing co-adaptations.
This section explores whether this effect transfers to Dropout RBMs as well.
Figure 12a shows features learned by a binary RBM with 256 hidden units. Figure 12b shows features learned by a dropout RBM with the same number of hidden units. Features
Dropout(a) Without dropout(b) Dropout with p = 0.5.
Figure 13: Effect of dropout on sparsity. Left: The activation histogram shows that a large number of units have activations away from zero. Right: A large number of units have activations close to zero and very few units have high activation. learned by the dropout RBM appear qualitatively different in the sense that they seem to capture features that are coarser compared to the sharply defined stroke-like features in the standard RBM. There seem to be very few dead units in the dropout RBM relative to the standard RBM.
8.4 Effect on Sparsity
Next, we investigate the effect of dropout RBM training on sparsity of the hidden unit activations. Figure 13a shows the histograms of hidden unit activations and their means on a test mini-batch after training an RBM. Figure 13b shows the same for dropout RBMs.
The histograms clearly indicate that the dropout RBMs learn much sparser representations than standard RBMs even when no additional sparsity inducing regularizer is present.
9. Marginalizing Dropout
Dropout can be seen as a way of adding noise to the states of hidden units in a neural network. In this section, we explore the class of models that arise as a result of marginalizing this noise. These models can be seen as deterministic versions of dropout. In contrast to standard ("Monte-Carlo") dropout, these models do not need random bits and it is possible to get gradients for the marginalized loss functions. In this section, we briefly explore these models.
Deterministic algorithms have been proposed that try to learn models that are robust to feature deletion at test time (Globerson and Roweis, 2006). Marginalization in the context of denoising autoencoders has been explored previously (Chen et al., 2012). The marginalization of dropout noise in the context of linear regression was discussed in Srivastava (2013).
Wang and Manning (2013) further explored the idea of marginalizing dropout to speed-up training. van der Maaten et al. (2013) investigated different input noise distributions and Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov the regularizers obtained by marginalizing this noise. Wager et al. (2013) describes how dropout can be seen as an adaptive regularizer.
9.1 Linear Regression
First we explore a very simple case of applying dropout to the classical problem of linear regression. Let X ∈ RN×D be a data matrix of N data points. y ∈ RN be a vector of targets. Linear regression tries to find a w ∈ RD that minimizes
||y − Xw||2.
When the input X is dropped out such that any input dimension is retained with probability p, the input can be expressed as R∗X where R ∈ {0, 1}N×D is a random matrix with Rij ∼ Bernoulli(p) and ∗ denotes an element-wise product. Marginalizing the noise, the objective function becomes minimize w
ER∼Bernoulli(p)
�
||y − (R ∗ X)w||2�
This reduces to minimize w
||y − pXw||2 + p(1 − p)||Γw||2, where Γ = (diag(X⊤X))1/2.
Therefore, dropout with linear regression is equivalent, in expectation, to ridge regression with a particular form for Γ. This form of Γ essentially scales the weight cost for weight wi by the standard deviation of the ith dimension of the data. If a particular data dimension varies a lot, the regularizer tries to squeeze its weight more.
Another interesting way to look at this objective is to absorb the factor of p into w.
This leads to the following form minimize w
||y − X �w||2 + 1 − p p
||Γ�w||2, where �w = pw. This makes the dependence of the regularization constant on p explicit.
For p close to 1, all the inputs are retained and the regularization constant is small. As more dropout is done (by decreasing p), the regularization constant grows larger.
9.2 Logistic Regression and Deep Networks
For logistic regression and deep neural nets, it is hard to obtain a closed form marginalized model. However, Wang and Manning (2013) showed that in the context of dropout applied to logistic regression, the corresponding marginalized model can be trained approximately.
Under reasonable assumptions, the distributions over the inputs to the logistic unit and over the gradients of the marginalized model are Gaussian. Their means and variances can be computed efficiently. This approximate marginalization outperforms Monte-Carlo dropout in terms of training time and generalization performance.
However, the assumptions involved in this technique become successively weaker as more layers are added. Therefore, the results are not directly applicable to deep networks.
Dropout
Data Set
Architecture
Bernoulli dropout
Gaussian dropout
MNIST
2 layers, 1024 units each
1.08 ± 0.04
0.95 ± 0.04
CIFAR-10
3 conv + 2 fully connected layers
12.6 ± 0.1
12.5 ± 0.1
Table 10: Comparison of classification error % with Bernoulli and Gaussian dropout. For MNIST, the Bernoulli model uses p = 0.5 for the hidden units and p = 0.8 for the input units.
For CIFAR-10, we use p = (0.9, 0.75, 0.75, 0.5, 0.5, 0.5) going from the input layer to the top. The value of σ for the Gaussian dropout models was set to be
�
1−p p. Results were averaged over 10 different random seeds.
10. Multiplicative Gaussian Noise
Dropout involves multiplying hidden activations by Bernoulli distributed random variables which take the value 1 with probability p and 0 otherwise. This idea can be generalized by multiplying the activations with random variables drawn from other distributions. We recently discovered that multiplying by a random variable drawn from N(1, 1) works just as well, or perhaps better than using Bernoulli noise. This new form of dropout amounts to adding a Gaussian distributed random variable with zero mean and standard deviation equal to the activation of the unit.
That is, each hidden activation hi is perturbed to hi + hir where r ∼ N(0, 1), or equivalently hir′ where r′ ∼ N(1, 1). We can generalize this to r′ ∼ N(1, σ2) where σ becomes an additional hyperparameter to tune, just like p was in the standard (Bernoulli) dropout. The expected value of the activations remains unchanged, therefore no weight scaling is required at test time.
In this paper, we described dropout as a method where we retain units with probability p at training time and scale down the weights by multiplying them by a factor of p at test time.
Another way to achieve the same effect is to scale up the retained activations by multiplying by 1/p at training time and not modifying the weights at test time. These methods are equivalent with appropriate scaling of the learning rate and weight initializations at each layer.
Therefore, dropout can be seen as multiplying hi by a Bernoulli random variable rb that takes the value 1/p with probability p and 0 otherwise. E[rb] = 1 and V ar[rb] = (1 − p)/p.
For the Gaussian multiplicative noise, if we set σ2 = (1 − p)/p, we end up multiplying hi by a random variable rg, where E[rg] = 1 and V ar[rg] = (1 − p)/p. Therefore, both forms of dropout can be set up so that the random variable being multiplied by has the same mean and variance. However, given these first and second order moments, rg has the highest entropy and rb has the lowest. Both these extremes work well, although preliminary experimental results shown in Table 10 suggest that the high entropy case might work slightly better. For each layer, the value of σ in the Gaussian model was set to be
�
1−p p using the p from the corresponding layer in the Bernoulli model.
11. Conclusion
Dropout is a technique for improving neural networks by reducing overfitting. Standard backpropagation learning builds up brittle co-adaptations that work for the training data but do not generalize to unseen data. Random dropout breaks up these co-adaptations by
Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov making the presence of any particular hidden unit unreliable. This technique was found to improve the performance of neural nets in a wide variety of application domains including object classification, digit recognition, speech recognition, document classification and analysis of computational biology data. This suggests that dropout is a general technique and is not specific to any domain. Methods that use dropout achieve state-of-the-art results on SVHN, ImageNet, CIFAR-100 and MNIST. Dropout considerably improved the performance of standard neural nets on other data sets as well.
This idea can be extended to Restricted Boltzmann Machines and other graphical models. The central idea of dropout is to take a large model that overfits easily and repeatedly sample and train smaller sub-models from it. RBMs easily fit into this framework. We developed Dropout RBMs and empirically showed that they have certain desirable properties.
One of the drawbacks of dropout is that it increases training time. A dropout network typically takes 2-3 times longer to train than a standard neural network of the same architecture. A major cause of this increase is that the parameter updates are very noisy.
Each training case effectively tries to train a different random architecture. Therefore, the gradients that are being computed are not gradients of the final architecture that will be used at test time. Therefore, it is not surprising that training takes a long time. However, it is likely that this stochasticity prevents overfitting. This creates a trade-off between overfitting and training time. With more training time, one can use high dropout and suffer less overfitting. However, one way to obtain some of the benefits of dropout without stochasticity is to marginalize the noise to obtain a regularizer that does the same thing as the dropout procedure, in expectation. We showed that for linear regression this regularizer is a modified form of L2 regularization. For more complicated models, it is not obvious how to obtain an equivalent regularizer. Speeding up dropout is an interesting direction for future work.
Acknowledgments
This research was supported by OGS, NSERC and an Early Researcher Award.
Appendix A. A Practical Guide for Training Dropout Networks
Neural networks are infamous for requiring extensive hyperparameter tuning.
Dropout networks are no exception. In this section, we describe heuristics that might be useful for applying dropout.
A.1 Network Size
It is to be expected that dropping units will reduce the capacity of a neural network. If n is the number of hidden units in any layer and p is the probability of retaining a unit, then instead of n hidden units, only pn units will be present after dropout, in expectation.
Moreover, this set of pn units will be different each time and the units are not allowed to build co-adaptations freely. Therefore, if an n-sized layer is optimal for a standard neural net on any given task, a good dropout net should have at least n/p units. We found this to be a useful heuristic for setting the number of hidden units in both convolutional and fully connected networks.
Dropout
A.2 Learning Rate and Momentum
Dropout introduces a significant amount of noise in the gradients compared to standard stochastic gradient descent. Therefore, a lot of gradients tend to cancel each other. In order to make up for this, a dropout net should typically use 10-100 times the learning rate that was optimal for a standard neural net. Another way to reduce the effect the noise is to use a high momentum. While momentum values of 0.9 are common for standard nets, with dropout we found that values around 0.95 to 0.99 work quite a lot better. Using high learning rate and/or momentum significantly speed up learning.
A.3 Max-norm Regularization
Though large momentum and learning rate speed up learning, they sometimes cause the network weights to grow very large. To prevent this, we can use max-norm regularization.
This constrains the norm of the vector of incoming weights at each hidden unit to be bound by a constant c. Typical values of c range from 3 to 4.
A.4 Dropout Rate
Dropout introduces an extra hyperparameter—the probability of retaining a unit p. This hyperparameter controls the intensity of dropout. p = 1, implies no dropout and low values of p mean more dropout. Typical values of p for hidden units are in the range 0.5 to 0.8.
For input layers, the choice depends on the kind of input. For real-valued inputs (image patches or speech frames), a typical value is 0.8. For hidden layers, the choice of p is coupled with the choice of number of hidden units n. Smaller p requires big n which slows down the training and leads to underfitting. Large p may not produce enough dropout to prevent overfitting.
Appendix B. Detailed Description of Experiments and Data Sets
This section describes the network architectures and training details for the experimental results reported in this paper. The code for reproducing these results can be obtained from http://www.cs.toronto.edu/~nitish/dropout. The implementation is GPU-based. We used the excellent CUDA libraries—cudamat (Mnih, 2009) and cuda-convnet (Krizhevsky et al., 2012) to implement our networks.
B.1 MNIST
The MNIST data set consists of 60,000 training and 10,000 test examples each representing a 28×28 digit image. We held out 10,000 random training images for validation. Hyperparameters were tuned on the validation set such that the best validation error was produced after 1 million weight updates. The validation set was then combined with the training set and training was done for 1 million weight updates. This net was used to evaluate the performance on the test set. This way of using the validation set was chosen because we found that it was easy to set up hyperparameters so that early stopping was not required at all.
Therefore, once the hyperparameters were fixed, it made sense to combine the validation and training sets and train for a very long time.
Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov
The architectures shown in Figure 4 include all combinations of 2, 3, and 4 layer networks with 1024 and 2048 units in each layer. Thus, there are six architectures in all. For all the architectures (including the ones reported in Table 2), we used p = 0.5 in all hidden layers and p = 0.8 in the input layer. A final momentum of 0.95 and weight constraints with c = 2 was used in all the layers.
To test the limits of dropout's regularization power, we also experimented with 2 and 3 layer nets having 4096 and 8192 units. 2 layer nets gave improvements as shown in Table 2.
However, the three layer nets performed slightly worse than 2 layer ones with the same level of dropout. When we increased dropout, performance improved but not enough to outperform the 2 layer nets.
B.2 SVHN
The SVHN data set consists of approximately 600,000 training images and 26,000 test images. The training set consists of two parts—A standard labeled training set and another set of labeled examples that are easy. A validation set was constructed by taking examples from both the parts. Two-thirds of it were taken from the standard set (400 per class) and one-third from the extra set (200 per class), a total of 6000 samples. This same process is used by Sermanet et al. (2012). The inputs were RGB pixels normalized to have zero mean and unit variance. Other preprocessing techniques such as global or local contrast normalization or ZCA whitening did not give any noticeable improvements.
The best architecture that we found uses three convolutional layers each followed by a max-pooling layer. The convolutional layers have 96, 128 and 256 filters respectively.
Each convolutional layer has a 5 × 5 receptive field applied with a stride of 1 pixel. Each max pooling layer pools 3 × 3 regions at strides of 2 pixels. The convolutional layers are followed by two fully connected hidden layers having 2048 units each. All units use the rectified linear activation function. Dropout was applied to all the layers of the network with the probability of retaining the unit being p = (0.9, 0.75, 0.75, 0.5, 0.5, 0.5) for the different layers of the network (going from input to convolutional layers to fully connected layers). In addition, the max-norm constraint with c = 4 was used for all the weights. A momentum of 0.95 was used in all the layers. These hyperparameters were tuned using a validation set. Since the training set was quite large, we did not combine the validation set with the training set for final training. We reported test error of the model that had smallest validation error.
B.3 CIFAR-10 and CIFAR-100
The CIFAR-10 and CIFAR-100 data sets consists of 50,000 training and 10,000 test images each. They have 10 and 100 image categories respectively. These are 32 × 32 color images.
We used 5,000 of the training images for validation. We followed the procedure similar to MNIST, where we found the best hyperparameters using the validation set and then combined it with the training set. The images were preprocessed by doing global contrast normalization in each color channel followed by ZCA whitening. Global contrast normalization means that for image and each color channel in that image, we compute the mean of the pixel intensities and subtract it from the channel. ZCA whitening means that we mean center the data, rotate it onto its principle components, normalize each component
Dropout and then rotate it back. The network architecture and dropout rates are same as that for
SVHN, except the learning rates for the input layer which had to be set to smaller values.
B.4 TIMIT
The open source Kaldi toolkit (Povey et al., 2011) was used to preprocess the data into logfilter banks. A monophone system was trained to do a forced alignment and to get labels for speech frames. Dropout neural networks were trained on windows of 21 consecutive frames to predict the label of the central frame. No speaker dependent operations were performed.
The inputs were mean centered and normalized to have unit variance.
We used probability of retention p = 0.8 in the input layers and 0.5 in the hidden layers.
Max-norm constraint with c = 4 was used in all the layers. A momentum of 0.95 with a high learning rate of 0.1 was used. The learning rate was decayed as ϵ0(1 + t/T)−1. For
DBN pretraining, we trained RBMs using CD-1. The variance of each input unit for the Gaussian RBM was fixed to 1. For finetuning the DBN with dropout, we found that in order to get the best results it was important to use a smaller learning rate (about 0.01).
Adding max-norm constraints did not give any improvements.
B.5 Reuters
The Reuters RCV1 corpus contains more than 800,000 documents categorized into 103 classes. These classes are arranged in a tree hierarchy. We created a subset of this data set consisting of 402,738 articles and a vocabulary of 2000 words comprising of 50 categories in which each document belongs to exactly one class. The data was split into equal sized training and test sets. We tried many network architectures and found that dropout gave improvements in classification accuracy over all of them. However, the improvement was not as significant as that for the image and speech data sets. This might be explained by the fact that this data set is quite big (more than 200,000 training examples) and overfitting is not a very serious problem.
B.6 Alternative Splicing
The alternative splicing data set consists of data for 3665 cassette exons, 1014 RNA features and 4 tissue types derived from 27 mouse tissues. For each input, the target consists of 4 softmax units (one for tissue type). Each softmax unit has 3 states (inc, exc, nc) which are of the biological importance. For each softmax unit, the aim is to predict a distribution over these 3 states that matches the observed distribution from wet lab experiments as closely as possible. The evaluation metric is Code Quality which is defined as
|data points|
� i=1
� t∈tissue types
� s∈{inc, exc, nc} ps i,t log(qs t (ri)
¯ps
), where, ps i,t is the target probability for state s and tissue type t in input i; qs t (ri) is the predicted probability for state s in tissue type t for input ri and ¯ps is the average of ps i,t over i and t.
A two layer dropout network with 1024 units in each layer was trained on this data set.
A value of p = 0.5 was used for the hidden layer and p = 0.7 for the input layer. Max-norm regularization with high decaying learning rates was used. Results were averaged across the same 5 folds used by Xiong et al. (2011).
Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov
References
M. Chen, Z. Xu, K. Weinberger, and F. Sha.
Marginalized denoising autoencoders for domain adaptation.
In Proceedings of the 29th International Conference on Machine
Learning, pages 767–774. ACM, 2012.
G. E. Dahl, M. Ranzato, A. Mohamed, and G. E. Hinton. Phone recognition with the meancovariance restricted Boltzmann machine. In Advances in Neural Information Processing
Systems 23, pages 469–477, 2010.
O. Dekel, O. Shamir, and L. Xiao. Learning to classify with missing and corrupted features.
Machine Learning, 81(2):149–178, 2010.
A. Globerson and S. Roweis. Nightmare at test time: robust learning by feature deletion. In
Proceedings of the 23rd International Conference on Machine Learning, pages 353–360.
ACM, 2006.
I. J. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, and Y. Bengio. Maxout networks.
In Proceedings of the 30th International Conference on Machine Learning, pages 1319–
1327. ACM, 2013.
G. Hinton and R. Salakhutdinov. Reducing the dimensionality of data with neural networks.
Science, 313(5786):504 – 507, 2006.
G. E. Hinton, S. Osindero, and Y. Teh. A fast learning algorithm for deep belief nets.
Neural Computation, 18:1527–1554, 2006.
K. Jarrett, K. Kavukcuoglu, M. Ranzato, and Y. LeCun. What is the best multi-stage architecture for object recognition?
In Proceedings of the International Conference on
Computer Vision (ICCV'09). IEEE, 2009.
A. Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009.
A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems 25, pages
1106–1114, 2012.
Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D.
Jackel. Backpropagation applied to handwritten zip code recognition. Neural Computation, 1(4):541–551, 1989.
Y. Lin, F. Lv, S. Zhu, M. Yang, T. Cour, K. Yu, L. Cao, Z. Li, M.-H. Tsai, X. Zhou, T. Huang, and T. Zhang. Imagenet classification: fast descriptor coding and large-scale svm training. Large scale visual recognition challenge, 2010.
A. Livnat, C. Papadimitriou, N. Pippenger, and M. W. Feldman.
Sex, mixability, and modularity. Proceedings of the National Academy of Sciences, 107(4):1452–1457, 2010.
V. Mnih. CUDAMat: a CUDA-based matrix class for Python. Technical Report UTML
TR 2009-004, Department of Computer Science, University of Toronto, November 2009.
Dropout
A. Mohamed, G. E. Dahl, and G. E. Hinton. Acoustic modeling using deep belief networks.
IEEE Transactions on Audio, Speech, and Language Processing, 2010.
R. M. Neal. Bayesian Learning for Neural Networks. Springer-Verlag New York, Inc., 1996.
Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading digits in natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning 2011, 2011.
S. J. Nowlan and G. E. Hinton. Simplifying neural networks by soft weight-sharing. Neural
Computation, 4(4), 1992.
D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. Hannemann, P. Motlicek, Y. Qian, P. Schwarz, J. Silovsky, G. Stemmer, and K. Vesely. The Kaldi
Speech Recognition Toolkit. In IEEE 2011 Workshop on Automatic Speech Recognition and Understanding. IEEE Signal Processing Society, 2011.
R. Salakhutdinov and G. Hinton. Deep Boltzmann machines. In Proceedings of the International Conference on Artificial Intelligence and Statistics, volume 5, pages 448–455, R. Salakhutdinov and A. Mnih. Bayesian probabilistic matrix factorization using Markov chain Monte Carlo.
In Proceedings of the 25th International Conference on Machine
Learning. ACM, 2008.
J. Sanchez and F. Perronnin. High-dimensional signature compression for large-scale image classification.
In Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition, pages 1665–1672, 2011.
P. Sermanet, S. Chintala, and Y. LeCun. Convolutional neural networks applied to house numbers digit classification. In International Conference on Pattern Recognition (ICPR
P. Simard, D. Steinkraus, and J. Platt. Best practices for convolutional neural networks applied to visual document analysis. In Proceedings of the Seventh International Conference on Document Analysis and Recognition, volume 2, pages 958–962, 2003.
J. Snoek, H. Larochelle, and R. Adams. Practical Bayesian optimization of machine learning algorithms. In Advances in Neural Information Processing Systems 25, pages 2960–2968, N. Srebro and A. Shraibman. Rank, trace-norm and max-norm. In Proceedings of the 18th annual conference on Learning Theory, COLT'05, pages 545–560. Springer-Verlag, 2005.
N. Srivastava. Improving Neural Networks with Dropout. Master's thesis, University of Toronto, January 2013.
R. Tibshirani.
Regression shrinkage and selection via the lasso.
Journal of the Royal
Statistical Society. Series B. Methodological, 58(1):267–288, 1996.
Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov
A. N. Tikhonov. On the stability of inverse problems. Doklady Akademii Nauk SSSR, 39(5):
195–198, 1943.
L. van der Maaten, M. Chen, S. Tyree, and K. Q. Weinberger. Learning with marginalized corrupted features.
In Proceedings of the 30th International Conference on Machine
Learning, pages 410–418. ACM, 2013.
P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol. Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th International Conference on Machine Learning, pages 1096–1103. ACM, 2008.
P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. In Proceedings of the 27th International Conference on Machine Learning, pages
3371–3408. ACM, 2010.
S. Wager, S. Wang, and P. Liang. Dropout training as adaptive regularization. In Advances in Neural Information Processing Systems 26, pages 351–359, 2013.
S. Wang and C. D. Manning. Fast dropout training. In Proceedings of the 30th International
Conference on Machine Learning, pages 118–126. ACM, 2013.
H. Y. Xiong, Y. Barash, and B. J. Frey. Bayesian prediction of tissue-regulated splicing using RNA sequence and cellular context. Bioinformatics, 27(18):2554–2562, 2011.
M. D. Zeiler and R. Fergus. Stochastic pooling for regularization of deep convolutional neural networks. CoRR, abs/1301.3557, 2013.Fast Inference in Sparse Coding Algorithms with Applications to Object Recognition
Koray Kavukcuoglu
Marc'Aurelio Ranzato
Yann LeCun
Department of Computer Science
Courant Institute of Mathematical Sciences
New York University, New York, NY 10003
{koray,ranzato,yann}@cs.nyu.edu
December 4, 2008
Computational and Biological Learning Laboratory
Technical Report
CBLL-TR-2008-12-01†
Abstract
Adaptive sparse coding methods learn a possibly overcomplete set of basis functions, such that natural image patches can be reconstructed by linearly combining a small subset of these bases.
The applicability of these methods to visual object recognition tasks has been limited because of the prohibitive cost of the optimization algorithms required to compute the sparse representation. In this work we propose a simple and efficient algorithm to learn basis functions. After training, this model also provides a fast and smooth approximator to the optimal representation, achieving even better accuracy than exact sparse coding algorithms on visual object recognition tasks.
Introduction
Object recognition is one of the most challenging tasks in computer vision. Most methods for visual recognition rely on handcrafted features to represent images.
It has been shown that making these representations adaptive to image data can improve performance on vision tasks as demonstrated in in a supervised
†Presented at OPT 2008 Optimization for Machine Learning Workshop, Neural Information Processing Systems, 2008
INTRODUCTION
2 learning framework and in using unsupervised learning. In particular, learning sparse representations can be advantageous since features are more likely to be linearly separable in a high-dimensional space and they are more robust to noise. Many sparse coding algorithms have been shown to learn good local feature extractors for natural images. However, application of these methods to vision problems has been limited due to prohibitive cost of calculating sparse representations for a given image.
In this work, we propose an algorithm named Predictive Sparse Decomposition (PSD) that can simultaneously learn an overcomplete linear basis set, and produce a smooth and easy-to-compute approximator that predicts the optimal sparse representation. Experiments demonstrate that the predictor is over 100 times faster than the fastest sparse optimization algorithm, and yet produces features that yield better recognition accuracy on visual object recognition tasks than the optimal representations produced through optimization.
1.1 Sparse Coding Algorithms
Finding a representation Z ∈ Rm for a given signal Y ∈ Rn by linear combination of an overcomplete set of basis vectors, columns of matrix B ∈ Rn×m with m > n, has infinitely many solutions. In optimal sparse coding, the problem is formulated as: min ||Z||0 s.t. Y = BZ(1) where the ℓ0 "norm" is defined as the number of non-zero elements in a given vector. Unfortunately, the solution to this problem requires a combinatorial search, intractable in high-dimensional spaces. Matching Pursuit methods offer a greedy approximation to this problem.
Another way to approximate this problem is to make a convex relaxation by turning the ℓ0 norm into an ℓ1 norm.
This problem, dubbed Basis Pursuit in the signal processing community, has been shown to give the same solution to eq. (1), provided that the solution is sparse enough. Furthermore, the problem can be written as an unconstrained optimization problem:
L(Y, Z; B) = 1
2||Y − BZ||2
2 + λ||Z||1
This particular formulation, called Basis Pursuit Denoising, can be seen as minimizing an objective that penalizes the reconstruction error using a linear basis set and the sparsity of the corresponding representation. Many recent works have focused on efficiently solving the problem in eq. (2).
Yet, inference requires running some sort of iterative minimization algorithm that is always computationally expensive.
Additionally, some algorithms are also able to learn the set of basis functions.
The learning procedure finds the B matrix that minimizes the same loss of eq. (2). The columns of B are constrained to have unit norm in order to prevent trivial solutions where the loss is minimized by scaling down the coefficients
THE ALGORITHM
3 while scaling up the bases. Learning proceeds by alternating the optimization over Z to infer the representation for a given set of bases B, and the minimization over B for the given set of optimal Z found at the previous step.
Loosely speaking, basis functions learned on natural images under sparsity constraints are localized oriented edge detectors reminiscent of Gabor wavelets.
The Algorithm
In order to make inference efficient, we train a non-linear regressor that maps input patches Y to sparse representations Z. We consider the following nonlinear mapping:
F(Y ; G, W, D) = G tanh(WY + D)(3) where W ∈ Rm×n is a filter matrix, D ∈ Rm is a vector of biases, tanh is the hyperbolic tangent non-linearity, and G ∈ Rm×m is a diagonal matrix of gain coefficients allowing the outputs of F to compensate for the scaling of the input, given that the reconstruction performed by B uses bases with unit norm.
Let Pf collectively denote the parameters that are learned in this predictor, Pf = {G, W, D}. The goal of the algorithm is to make the prediction of the regressor, F(Y ; Pf) as close as possible to the optimal set of coefficients: Z∗ = arg minZ L(Y, Z; B) in eq. (2). This optimization can be carried out separately after the problem in eq. (2) has been solved. However, training becomes much faster by jointly optimizing the Pf and the set of bases B all together. This is achieved by adding another term to the loss function in eq. (2), enforcing the representation Z to be as close as possible to the feed-forward prediction
F(Y ; Pf):
L(Y, Z; B, Pf) = ∥Y − BZ∥2
2 + λ∥Z∥1 + α∥Z − F(Y ; Pf)∥2
Minimizing this loss with respect to Z produces a representation that simultaneously reconstructs the patch, is sparse, and is not too different from the predicted representation. If multiple solutions to the original loss (without the prediction term) exist, minimizing this compound loss will drive the system towards producing basis functions and optimal representations that are easily predictable. After training, the function F(Y ; Pf) will provide good and smooth approximations to the optimal sparse representations. Note that, a linear mapping would not be able to produce sparse representations using an overcomplete set because of the non-orthogonality of the filters, therefore a non-linear mapping is required.
2.1 Learning
The goal of learning is to find the optimal value of the basis functions B, as well as the value of the parameters in the regressor Pf. Learning proceeds by an on-line block coordinate gradient descent algorithm, alternating the following two steps for each training sample Y :
THE ALGORITHM
1. keeping the parameters Pf and B constant, minimize L(Y, Z; B, Pf) of eq. (4) with respect to Z, starting from the initial value provided by the regressor F(Y ; Pf). In our experiments we use gradient descent, but any other optimization method can be used;
2. using the optimal value of the coefficients Z provided by the previous step, update the parameters Pf and B by one step of stochastic gradient descent; The update is: U ← U − η ∂L
∂U, where U collectively denotes
{Pf, B} and η is the step size. The columns of B are then re-scaled to unit norm.
Interestingly, we recover different algorithms depending on the value of the parameter α:
• α = 0. The loss of eq. (4) reduces to the one in eq. (2). The learning algorithm becomes similar to Olshausen and Field's sparse coding algorithm. The regressor is trained separately from the set of basis functions
B.
• α ∈ (0, +∞). The parameters are updated taking into account also the constraint on the representation, using the same principle employed by
SESM training, for instance.
• α → +∞. The additional constraint on the representation (the third term in eq. (4)) becomes an equality, i.e. Z = F(Y ; Pf), and the model becomes similar to an auto-encoder neural network with a sparsity regularization term acting on the internal representation Z instead of a regularization acting on the parameters Pf and B.
In this paper, we always set α = 1. However, sec. 3 shows that training the regressor after training the set of bases B yields similar performance in terms of recognition accuracy. When the regressor is trained afterwards, the approximate representation is usually less sparse and the overall training time increases considerably. Finally, additional experiments not reported here show that training the system as an auto-encoder (α → +∞) provides a very fast and efficient algorithm that can produce good representations when the dimensionality of the representation is not much greater than the input dimensionality, i.e. m ≃ n.
When the sparse representation is highly overcomplete the block-coordinate descent algorithm with α ∈ (0, +∞) provides better features.
2.2 Inference
Once the parameters are learned, inferring the representation Z can be done in two ways.
Optimal inference consists of setting the representation to Z∗ = arg minz L, where L is defined in eq. (4), by running an iterative gradient descent algorithm involving two possibly large matrix-vector multiplications at each iteration (one for computing the value of the objective, and one for computing the derivatives
EXPERIMENTS
5 through B).
Approximate inference, on the other hand sets the representation to the value produced by F(Y ; Pf) as given in eq. (3), involving only a forward propagation through the regressor, i.e. a single matrix-vector multiplication.
Experiments
First, we demonstrate that the proposed algorithm (PSD) is able to produce good features for recognition by comparing to other unsupervised feature extraction algorithms, Principal Components Analysis (PCA), Restricted Boltzman
Machine (RBM), and Sparse Encoding Symmetric Machine (SESM).
Then, we compare the recognition accuracy and inference time of PSD feedforward approximation to feature sign algorithm, on the Caltech 101 dataset.
Finally we investigate the stability of representations under naturally changing inputs.
3.1 Comparison against PCA, RBM and SESM on the MNIST
The MNIST dataset has a training set with 60,000 handwritten digits of size
28x28 pixels, and a test set with 10,000 digits.
Each image is preprocessed by normalizing the pixel values so that their standard deviation is equal to
1. In this experiment the sparse representation has 256 units. This internal representation is used as a global feature vector and fed to a linear regularized logistic regression classifier. Fig. 1 shows the comparison between PSD (using feed-forward approximate codes) and, PCA, SESM, and RBM. Even though PSD provides the worst reconstruction error, it can achieve the best recognition accuracy on the test set under different number of training samples per class.
−10
RMSE
ERROR RATE %
10 samples
RMSE
ERROR RATE %
100 samples
RMSE
ERROR RATE %
1000 samples
RAW: train
RAW: test
PCA: train
PCA: test
SESM: train
SESM: test
RBM: train
RBM: test
PSD train
PSD test
Figure 1: Classification error on MNIST as a function of reconstruction error using raw pixel values and, PCA, RBM, SESM and PSD features. Left-to-Right
: 10-100-1000 samples per class are used for training a linear classifier on the features. The unsupervised algorithms were trained on the first 20,000 training samples of the MNIST dataset.
EXPERIMENTS
Table 1: Comparison between representations produced by FS and PSD. In order to compute the SNR, the noise is defined as (Signal − Approximation).
Comparison (Signal / Approximation)
Signal to Noise Ratio (SNR)
1. PSD Optimal / PSD Predictor
2. FS / PSD Optimal
3. FS / PSD Predictor
4. FS / Regressor
3.2 Comparison with Exact Algorithms
In order to quantify how well our jointly trained predictor given in eq. (3) approximates the optimal representations obtained by minimizing the loss in eq. (4) and the optimal representations that are produced by an exact algorithm minimizing eq. (2) such as feature sign (FS), we measure the average signal to noise ratio1 (SNR) over a test dataset of 20,000 natural image patches of size
9x9. The data set of images was constructed by randomly picking 9x9 patches from the images of the Berkeley dataset converted to gray-scale values, and these patches were normalized to have zero mean and unit standard deviation.
The algorithms were trained to learn sparse codes with 64 units2.
We compare representations obtained by "PSD Predictor" using the approximate inference, "PSD Optimal" using the optimal inference, "FS" minimizing eq. (2) with, and "Regressor" that is separately trained to approximate the exact optimal codes produced by FS. The results given in table 1 show that
PSD direct predictor achieves about the same SNR on the true optimal sparse representations produced by FS, as the Regressor that was trained to predict these representations.
Despite the lack of absolute precision in predicting the exact optimal sparse codes, PSD predictor achieves even better performance in recognition.
The
Caltech 101 dataset is pre-processed in the following way: 1) each image is converted to gray-scale, 2) it is down-sampled so that the longest side is 151 pixels, 3) the mean is subtracted and each pixel is divided by the image standard deviation, 4) the image is locally normalized by subtracting the weighted local mean from each pixel and dividing it by the weighted norm if this is larger than 1 with weights forming a 9x9 Gaussian window centered on each pixel, and 5) the image is 0-padded to 143x143 pixels. 64 feature detectors (either produced by FS or PSD predictor) were plugged into an image classification system that A) used the sparse coding algorithms convolutionally to produce
64 feature maps of size 128x128 for each pre-processed image, B) applied an absolute value rectification, C) computed an average down-sampling to a spatial resolution of 30x30 and D) used a linear SVM classifier to recognize the object
1SNR = 10log10(σ2 signal/σ2 noise)
2Principal Component Analysis shows that the effective dimensionality of 9x9 natural image patches is about 47 since the first 47 principal components capture the 95% of the variance in the data.
Hence, a 64-dimensional feature vector is actually an overcomplete representation for these 9x9 image patches.
EXPERIMENTS
Figure 2: a) 256 basis functions of size 12x12 learned by PSD, trained on the Berkeley dataset. Each 12x12 block is a column of matrix B in eq. (4), i.e. a basis function. b) Object recognition architecture: linear adaptive filter bank, followed by abs rectification, average down-sampling and linear SVM classifier.
Figure 3: a) Speed up for inferring the sparse representation achieved by PSD predictor over FS for a code with 64 units.
The feed-forward extraction is more than 100 times faster. b) Recognition accuracy versus measured sparsity(average ℓ1 norm of the representation) of PSD predictor compared to the to the representation of FS algorithm. A difference within 1% is not statistically significant. c) Recognition accuracy as a function of number of basis functions. in the image (see fig. 2(b)). Using this system with 30 training images per class we can achieve 53% accuracy on Caltech 101 dataset.
Since FS finds exact sparse codes, its representations are generally sparser than those found by PSD predictor trained with the same value of sparsity penalty λ. Hence, we compare the recognition accuracy against the measured sparsity level of the representation as shown in fig. 3(b). PSD is not only able to achieve better accuracy than exact sparse coding algorithms, but also, it does it much more efficiently. Fig. 3(a) demonstrates that our feed-forward predictor extracts features more than 100 times faster than feature sign.
In fact, the speed up is over 800 when the sparsity is set to the value that gives the highest accuracy shown in fig. 3(b).
Finally, we observe that these sparse coding algorithms are somewhat inefficient when applied convolutionally. Many feature detectors are the translated versions of each other as shown in fig. 2(a). Hence, the resulting feature maps are highly redundant. This might explain why the recognition accuracy tends to saturate when the number of filters is increased as shown in fig. 3(c).
SUMMARY AND FUTURE WORK
3.3 Stability
In order to quantify the stability of PSD and FS, we investigate their behavior under naturally changing input signals. For this purpose, we train a basis set with 128 elements, each of size 9x9, using the PSD algorithm on the Berkeley dataset. This basis set is then used with FS on the standard "foreman" test video together with the PSD Predictor. We extract 784 uniformly distributed patches from each frame with a total of 400 frames.
P(−|+) 0.00
P(+|−) 0.00
P(−|0) 0.01
P(+|0) 0.01
P(0|−) 0.40
P(0|+) 0.41
P(+|+) 0.59
P(−|−) 0.60
P(0|0) 0.99
Feature Sign
P(−|+) 0.00
P(+|−) 0.00
P(−|0) 0.00
P(+|0) 0.00
P(0|−) 0.06
P(0|+) 0.05
P(+|+) 0.95
P(−|−) 0.94
P(0|0) 1.00
PSD
P(−|+) 0.00
P(+|−) 0.01
P(−|0) 0.00
P(+|0) 0.01
P(0|−) 0.45
P(0|+) 0.41
P(+|+) 0.59
P(−|−) 0.54
P(0|0) 0.98
PSD Random
Figure 4: Conditional probabilities for sign transitions between two consecutive frames. For instance, P(−|+) shows the conditional probability of a unit being negative given that it was positive in the previous frame. The figure on the right is used as baseline, showing the conditional probabilities computed on pairs of random frames.
For each patch, a 128 dimensional representation is calculated using both FS and the PSD predictor. The stability is measured by the number of times a unit of the representation changes its sign, either negative, zero or positive, between two consecutive frames. Since the PSD predictor does not generate exact zero values, we threhsold its output units in such a way that the average number of zero units equals the one produced by FS (roughly, only the 4% of the units are non-zero). The transition probabilities are given in Figure 4. It can be seen from this figure that the PSD predictor generates a more stable representation of slowly varying natural frames compared to the representation produced by the exact optimization algorithm.
Summary and Future Work
Sparse coding algorithms can be used as pre-processor in many vision applications and, in particular, to extract features in object recognition systems. To the best of our knowledge, no sparse coding algorithm is computationally efficient because inference involves some sort of iterative optimization. We showed that sparse codes can actually be approximated by a feed-forward regressor without compromising the recognition accuracy, but making the recognition process very fast and suitable for use in real-time systems. We proposed a very simple algorithm to train such a regressor.
In the future, we plan to train the model convolutionally in order to make the sparse representation more efficient, and to build hierarchical deep models by sequentially replicating the model on the representation produced by the previous stage as successfully proposed in.
REFERENCES
References
 Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, November
 M. Elad and M. Aharon. Image denoising via learned dictionaries and sparse representation. In CVPR, 2006.
 M. Ranzato, F.J. Huang, Y. Boureau, and Y. LeCun. Unsupervised learning of invariant feature hierarchies with applications to object recognition. In CVPR, B. A. Olshausen and D. J. Field. Sparse coding with an overcomplete basis set: a strategy employed by v1? Vision Research, 37:3311–3325, 1997.
 M. Aharon, M. Elad, and A.M. Bruckstein. K-svd and its non-negative variant for dictionary design. volume 5914, 2005.
 J. Mairal, F. Bach, J. Ponce, G. Sapiro, and A. Zisserman. Discriminative learned dictionaries for local image analysis. In CVPR, 2008.
 H. Lee, A. Battle, R. Raina, and A.Y. Ng. Efficient sparse coding algorithms. In
NIPS, 2006.
 M. Ranzato, C. Poultney, S. Chopra, and Y. LeCun. Efficient learning of sparse representations with an energy-based model. In NIPS 2006. MIT Press, 2006.
 S Mallat and Z Zhang. Matching pursuits with time-frequency dictionaries. IEEE
Transactions on Signal Processing, 41(12):3397:3415, 1993.
 S.S. Chen, D.L. Donoho, and M.A. Saunders. Atomic decomposition by basis pursuit. SIAM Journal on Scientific Computing, 20(1):33–61, 1999.
 DL Donoho and M Elad. Optimally sparse representation in general (nonorthogonal) dictionaries via ℓ1 minimization. Proc Natl Acad Sci U S A, 100(5):2197–
2202, 2003 Mar 4.
 B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani.
Least angle regression, J.F. Murray and K. Kreutz-Delgado.
Learning sparse overcomplete codes for images. The Journal of VLSI Signal Processing, 45:97–110, 2008.
 C.J. Rozell, D.H. Johnson, Baraniuk R.G., and B.A. Olshausen. Sparse coding via thresholding and local competition in neural circuits. Neural Computation, M. Ranzato, Y. Boureau, and Y. LeCun. Sparse feature learning for deep belief networks. In NIPS, 2007.
 G.E. Hinton. Training products of experts by minimizing contrastive divergence.
Neural Computation, 14:1771–1800, 2002.
 L. Fei-Fei, R. Fergus, and P. Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In CVPR Workshop, 2004.
 G.E. Hinton and R. R Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504–507, 2006.
 http://yann.lecun.com/exdb/mnist/.
 http://www.cs.berkeley.edu/projects/vision/grouping/segbench/.Going Deeper with Convolutions
Christian Szegedy1, Wei Liu2, Yangqing Jia1, Pierre Sermanet1, Scott Reed3, Dragomir Anguelov1, Dumitru Erhan1, Vincent Vanhoucke1, Andrew Rabinovich4
1Google Inc. 2University of North Carolina, Chapel Hill
3University of Michigan, Ann Arbor 4Magic Leap Inc.
1{szegedy,jiayq,sermanet,dragomir,dumitru,vanhoucke}@google.com
2wliu@cs.unc.edu, 3reedscott@umich.edu, 4arabinovich@magicleap.com
Abstract
We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014(ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called
GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.
1. Introduction
In the last three years, our object classification and detection capabilities have dramatically improved due to advances in deep learning and convolutional networks.
One encouraging news is that most of this progress is not just the result of more powerful hardware, larger datasets and bigger models, but mainly a consequence of new ideas, algorithms and improved network architectures. No new data sources were used, for example, by the top entries in the ILSVRC 2014 competition besides the classification dataset of the same competition for detection purposes. Our
GoogLeNet submission to ILSVRC 2014 actually uses 12 times fewer parameters than the winning architecture of Krizhevsky et al from two years ago, while being significantly more accurate. On the object detection front, the biggest gains have not come from naive application of bigger and bigger deep networks, but from the synergy of deep architectures and classical computer vision, like the R-CNN algorithm by Girshick et al.
Another notable factor is that with the ongoing traction of mobile and embedded computing, the efficiency of our algorithms – especially their power and memory use – gains importance. It is noteworthy that the considerations leading to the design of the deep architecture presented in this paper included this factor rather than having a sheer fixation on accuracy numbers. For most of the experiments, the models were designed to keep a computational budget of 1.5 billion multiply-adds at inference time, so that the they do not end up to be a purely academic curiosity, but could be put to real world use, even on large datasets, at a reasonable cost.
In this paper, we will focus on an efficient deep neural network architecture for computer vision, codenamed Inception, which derives its name from the Network in network paper by Lin et al in conjunction with the famous
"we need to go deeper" internet meme. In our case, the word "deep" is used in two different meanings: first of all, in the sense that we introduce a new level of organization in the form of the "Inception module" and also in the more direct sense of increased network depth. In general, one can view the Inception model as a logical culmination of while taking inspiration and guidance from the theoretical work by Arora et al. The benefits of the architecture are experimentally verified on the ILSVRC 2014 classification and detection challenges, where it significantly outperforms the current state of the art.
2. Related Work
Starting with LeNet-5, convolutional neural networks (CNN) have typically had a standard structure – stacked convolutional layers (optionally followed by con1 trast normalization and max-pooling) are followed by one or more fully-connected layers. Variants of this basic design are prevalent in the image classification literature and have yielded the best results to-date on MNIST, CIFAR and most notably on the ImageNet classification challenge.
For larger datasets such as Imagenet, the recent trend has been to increase the number of layers
 and layer size, while using dropout to address the problem of overfitting.
Despite concerns that max-pooling layers result in loss of accurate spatial information, the same convolutional network architecture as
 has also been successfully employed for localization, object detection and human pose estimation.
Inspired by a neuroscience model of the primate visual cortex, Serre et al. used a series of fixed Gabor filters of different sizes to handle multiple scales. We use a similar strategy here. However, contrary to the fixed 2-layer deep model of, all filters in the Inception architecture are learned. Furthermore, Inception layers are repeated many times, leading to a 22-layer deep model in the case of the GoogLeNet model.
Network-in-Network is an approach proposed by Lin et al. in order to increase the representational power of neural networks. In their model, additional 1 × 1 convolutional layers are added to the network, increasing its depth.
We use this approach heavily in our architecture. However, in our setting, 1 × 1 convolutions have dual purpose: most critically, they are used mainly as dimension reduction modules to remove computational bottlenecks, that would otherwise limit the size of our networks. This allows for not just increasing the depth, but also the width of our networks without a significant performance penalty.
Finally, the current state of the art for object detection is the Regions with Convolutional Neural Networks (R-CNN) method by Girshick et al.. R-CNN decomposes the overall detection problem into two subproblems: utilizing lowlevel cues such as color and texture in order to generate object location proposals in a category-agnostic fashion and using CNN classifiers to identify object categories at those locations. Such a two stage approach leverages the accuracy of bounding box segmentation with low-level cues, as well as the highly powerful classification power of state-ofthe-art CNNs. We adopted a similar pipeline in our detection submissions, but have explored enhancements in both stages, such as multi-box prediction for higher object bounding box recall, and ensemble approaches for better categorization of bounding box proposals.
3. Motivation and High Level Considerations
The most straightforward way of improving the performance of deep neural networks is by increasing their size.
This includes both increasing the depth – the number of netFigure 1:
Two distinct classes from the 1000 classes of the ILSVRC 2014 classification challenge. Domain knowledge is required to distinguish between these classes. work levels – as well as its width: the number of units at each level. This is an easy and safe way of training higher quality models, especially given the availability of a large amount of labeled training data. However, this simple solution comes with two major drawbacks.
Bigger size typically means a larger number of parameters, which makes the enlarged network more prone to overfitting, especially if the number of labeled examples in the training set is limited. This is a major bottleneck as strongly labeled datasets are laborious and expensive to obtain, often requiring expert human raters to distinguish between various fine-grained visual categories such as those in ImageNet(even in the 1000-class ILSVRC subset) as shown in Figure 1.
The other drawback of uniformly increased network size is the dramatically increased use of computational resources.
For example, in a deep vision network, if two convolutional layers are chained, any uniform increase in the number of their filters results in a quadratic increase of computation. If the added capacity is used inefficiently (for example, if most weights end up to be close to zero), then much of the computation is wasted. As the computational budget is always finite, an efficient distribution of computing resources is preferred to an indiscriminate increase of size, even when the main objective is to increase the quality of performance.
A fundamental way of solving both of these issues would be to introduce sparsity and replace the fully connected layers by the sparse ones, even inside the convolutions. Besides mimicking biological systems, this would also have the advantage of firmer theoretical underpinnings due to the groundbreaking work of Arora et al.. Their main result states that if the probability distribution of the dataset is representable by a large, very sparse deep neural network, then the optimal network topology can be constructed layer after layer by analyzing the correlation statistics of the preceding layer activations and clustering neurons with highly correlated outputs. Although the strict mathematical proof requires very strong conditions, the fact that this statement resonates with the well known Hebbian principle – neurons that fire together, wire together – suggests that the underlying idea is applicable even under less strict conditions, in practice.
Unfortunately, today's computing infrastructures are very inefficient when it comes to numerical calculation on non-uniform sparse data structures. Even if the number of arithmetic operations is reduced by 100×, the overhead of lookups and cache misses would dominate: switching to sparse matrices might not pay off. The gap is widened yet further by the use of steadily improving and highly tuned numerical libraries that allow for extremely fast dense matrix multiplication, exploiting the minute details of the underlying CPU or GPU hardware. Also, non-uniform sparse models require more sophisticated engineering and computing infrastructure. Most current vision oriented machine learning systems utilize sparsity in the spatial domain just by the virtue of employing convolutions. However, convolutions are implemented as collections of dense connections to the patches in the earlier layer. ConvNets have traditionally used random and sparse connection tables in the feature dimensions since in order to break the symmetry and improve learning, yet the trend changed back to full connections with in order to further optimize parallel computation. Current state-of-the-art architectures for computer vision have uniform structure. The large number of filters and greater batch size allows for the efficient use of dense computation.
This raises the question of whether there is any hope for a next, intermediate step: an architecture that makes use of filter-level sparsity, as suggested by the theory, but exploits our current hardware by utilizing computations on dense matrices. The vast literature on sparse matrix computations (e.g. ) suggests that clustering sparse matrices into relatively dense submatrices tends to give competitive performance for sparse matrix multiplication. It does not seem far-fetched to think that similar methods would be utilized for the automated construction of non-uniform deeplearning architectures in the near future.
The Inception architecture started out as a case study for assessing the hypothetical output of a sophisticated network topology construction algorithm that tries to approximate a sparse structure implied by for vision networks and covering the hypothesized outcome by dense, readily available components. Despite being a highly speculative undertaking, modest gains were observed early on when compared with reference networks based on. With a bit of tuning the gap widened and Inception proved to be especially useful in the context of localization and object detection as the base network for and. Interestingly, while most of the original architectural choices have been questioned and tested thoroughly in separation, they turned out to be close to optimal locally. One must be cautious though: although the Inception architecture has become a success for computer vision, it is still questionable whether this can be attributed to the guiding principles that have lead to its construction. Making sure of this would require a much more thorough analysis and verification.
4. Architectural Details
The main idea of the Inception architecture is to consider how an optimal local sparse structure of a convolutional vision network can be approximated and covered by readily available dense components. Note that assuming translation invariance means that our network will be built from convolutional building blocks. All we need is to find the optimal local construction and to repeat it spatially. Arora et al. suggests a layer-by layer construction where one should analyze the correlation statistics of the last layer and cluster them into groups of units with high correlation. These clusters form the units of the next layer and are connected to the units in the previous layer. We assume that each unit from an earlier layer corresponds to some region of the input image and these units are grouped into filter banks. In the lower layers (the ones close to the input) correlated units would concentrate in local regions. Thus, we would end up with a lot of clusters concentrated in a single region and they can be covered by a layer of 1×1 convolutions in the next layer, as suggested in. However, one can also expect that there will be a smaller number of more spatially spread out clusters that can be covered by convolutions over larger patches, and there will be a decreasing number of patches over larger and larger regions. In order to avoid patch-alignment issues, current incarnations of the Inception architecture are restricted to filter sizes 1×1, 3×3 and 5×5; this decision was based more on convenience rather than necessity. It also means that the suggested architecture is a combination of all those layers with their output filter banks concatenated into a single output vector forming the input of the next stage. Additionally, since pooling operations have been essential for the success of current convolutional networks, it suggests that adding an alternative parallel pooling path in each such stage should have additional beneficial effect, too (see Figure 2(a)).
As these "Inception modules" are stacked on top of each other, their output correlation statistics are bound to vary: as features of higher abstraction are captured by higher layers, their spatial concentration is expected to decrease. This suggests that the ratio of 3×3 and 5×5 convolutions should increase as we move to higher layers.
One big problem with the above modules, at least in this na¨ıve form, is that even a modest number of 5×5 convolutions can be prohibitively expensive on top of a convolutional layer with a large number of filters. This problem becomes even more pronounced once pooling units are added to the mix: the number of output filters equals to the num1x1 convolutions
3x3 convolutions
5x5 convolutions
Filter concatenation
Previous layer
3x3 max pooling(a) Inception module, na¨ıve version
1x1 convolutions
3x3 convolutions
5x5 convolutions
Filter concatenation
Previous layer
3x3 max pooling
1x1 convolutions
1x1 convolutions
1x1 convolutions(b) Inception module with dimensionality reduction
Figure 2: Inception module ber of filters in the previous stage. The merging of output of the pooling layer with outputs of the convolutional layers would lead to an inevitable increase in the number of outputs from stage to stage. While this architecture might cover the optimal sparse structure, it would do it very inefficiently, leading to a computational blow up within a few stages.
This leads to the second idea of the Inception architecture: judiciously reducing dimension wherever the computational requirements would increase too much otherwise.
This is based on the success of embeddings: even low dimensional embeddings might contain a lot of information about a relatively large image patch.
However, embeddings represent information in a dense, compressed form and compressed information is harder to process. The representation should be kept sparse at most places (as required by the conditions of ) and compress the signals only whenever they have to be aggregated en masse. That is, 1×1 convolutions are used to compute reductions before the expensive 3×3 and 5×5 convolutions. Besides being used as reductions, they also include the use of rectified linear activation making them dual-purpose. The final result is depicted in Figure 2(b).
In general, an Inception network is a network consisting of modules of the above type stacked upon each other, with occasional max-pooling layers with stride 2 to halve the resolution of the grid. For technical reasons (memory efficiency during training), it seemed beneficial to start using Inception modules only at higher layers while keeping the lower layers in traditional convolutional fashion. This is not strictly necessary, simply reflecting some infrastructural inefficiencies in our current implementation.
A useful aspect of this architecture is that it allows for increasing the number of units at each stage significantly without an uncontrolled blow-up in computational complexity at later stages. This is achieved by the ubiquitous use of dimensionality reduction prior to expensive convolutions with larger patch sizes. Furthermore, the design follows the practical intuition that visual information should be processed at various scales and then aggregated so that the next stage can abstract features from the different scales simultaneously.
The improved use of computational resources allows for increasing both the width of each stage as well as the number of stages without getting into computational difficulties.
One can utilize the Inception architecture to create slightly inferior, but computationally cheaper versions of it.
We have found that all the available knobs and levers allow for a controlled balancing of computational resources resulting in networks that are 3 − 10× faster than similarly performing networks with non-Inception architecture, however this requires careful manual design at this point.
5. GoogLeNet
By the"GoogLeNet" name we refer to the particular incarnation of the Inception architecture used in our submission for the ILSVRC 2014 competition. We also used one deeper and wider Inception network with slightly superior quality, but adding it to the ensemble seemed to improve the results only marginally. We omit the details of that network, as empirical evidence suggests that the influence of the exact architectural parameters is relatively minor. Table 1 illustrates the most common instance of Inception used in the competition. This network (trained with different imagepatch sampling methods) was used for 6 out of the 7 models in our ensemble.
All the convolutions, including those inside the Inception modules, use rectified linear activation. The size of the receptive field in our network is 224×224 in the RGB color space with zero mean. "#3×3 reduce" and "#5×5 reduce" stands for the number of 1×1 filters in the reduction layer used before the 3×3 and 5×5 convolutions. One can see the number of 1×1 filters in the projection layer after the built-in max-pooling in the pool proj column. All these reduction/projection layers use rectified linear activation as well.
The network was designed with computational efficiency and practicality in mind, so that inference can be run on individual devices including even those with limited computational resources, especially with low-memory footprint. type patch size/ stride output size depth
#1×1
#3×3 reduce
#3×3
#5×5 reduce
#5×5 pool proj params ops convolution
7×7/2
112×112×64
2.7K
34M max pool
3×3/2
56×56×64
0 convolution
3×3/1
56×56×192
112K
360M max pool
3×3/2
28×28×192
0 inception (3a)
28×28×256
159K
128M inception (3b)
28×28×480
380K
304M max pool
3×3/2
14×14×480
0 inception (4a)
14×14×512
364K
73M inception (4b)
14×14×512
437K
88M inception (4c)
14×14×512
463K
100M inception (4d)
14×14×528
580K
119M inception (4e)
14×14×832
840K
170M max pool
3×3/2
7×7×832
0 inception (5a)
7×7×832
1072K
54M inception (5b)
7×7×1024
1388K
71M avg pool
7×7/1
1×1×1024
0 dropout (40%)
1×1×1024
0 linear
1×1×1000
1000K
1M softmax
1×1×1000
Table 1: GoogLeNet incarnation of the Inception architecture.
The network is 22 layers deep when counting only layers with parameters (or 27 layers if we also count pooling). The overall number of layers (independent building blocks) used for the construction of the network is about 100. The exact number depends on how layers are counted by the machine learning infrastructure. The use of average pooling before the classifier is based on, although our implementation has an additional linear layer. The linear layer enables us to easily adapt our networks to other label sets, however it is used mostly for convenience and we do not expect it to have a major effect. We found that a move from fully connected layers to average pooling improved the top-1 accuracy by about 0.6%, however the use of dropout remained essential even after removing the fully connected layers.
Given relatively large depth of the network, the ability to propagate gradients back through all the layers in an effective manner was a concern. The strong performance of shallower networks on this task suggests that the features produced by the layers in the middle of the network should be very discriminative. By adding auxiliary classifiers connected to these intermediate layers, discrimination in the lower stages in the classifier was expected. This was thought to combat the vanishing gradient problem while providing regularization.
These classifiers take the form of smaller convolutional networks put on top of the output of the Inception (4a) and (4d) modules. During training, their loss gets added to the total loss of the network with a discount weight (the losses of the auxiliary classifiers were weighted by 0.3). At inference time, these auxiliary networks are discarded. Later control experiments have shown that the effect of the auxiliary networks is relatively minor (around 0.5%) and that it required only one of them to achieve the same effect.
The exact structure of the extra network on the side, including the auxiliary classifier, is as follows:
• An average pooling layer with 5×5 filter size and stride 3, resulting in an 4×4×512 output for the (4a), and 4×4×528 for the (4d) stage.
• A 1×1 convolution with 128 filters for dimension reduction and rectified linear activation.
• A fully connected layer with 1024 units and rectified linear activation.
• A dropout layer with 70% ratio of dropped outputs.
• A linear layer with softmax loss as the classifier (predicting the same 1000 classes as the main classifier, but removed at inference time).
A schematic view of the resulting network is depicted in Figure 3.
6. Training Methodology
GoogLeNet networks were trained using the DistBelief distributed machine learning system using modest amount of model and data-parallelism. Although we used a CPU based implementation only, a rough estimate suggests that the GoogLeNet network could be trained to convergence using few high-end GPUs within a week, the main limitation being the memory usage. Our training used asynchronous stochastic gradient descent with 0.9 momentum, fixed learning rate schedule (decreasing the learning rate by 4% every 8 epochs). Polyak averaging was used to create the final model used at inference time.
Image sampling methods have changed substantially over the months leading to the competition, and already converged models were trained on with other options, sometimes in conjunction with changed hyperparameters, such as dropout and the learning rate. Therefore, it is hard to give a definitive guidance to the most effective single way to train these networks. To complicate matters further, some of the models were mainly trained on smaller relative crops, others on larger ones, inspired by. Still, one prescription that was verified to work very well after the competition, includes sampling of various sized patches of the image whose size is distributed evenly between 8% and 100% of the image area with aspect ratio constrained to the interval [ 3. Also, we found that the photometric distortions of Andrew Howard were useful to combat overfitting to the imaging conditions of training data.
ILSVRC
Classification
Challenge
Setup and Results
The ILSVRC 2014 classification challenge involves the task of classifying the image into one of 1000 leaf-node categories in the Imagenet hierarchy. There are about 1.2 million images for training, 50,000 for validation and 100,000 images for testing.
Each image is associated with one ground truth category, and performance is measured based on the highest scoring classifier predictions.
Two numbers are usually reported: the top-1 accuracy rate, which compares the ground truth against the first predicted class, and the top-5 error rate, which compares the ground truth against the first 5 predicted classes: an image is deemed correctly classified if the ground truth is among the top-5, regardless of its rank in them. The challenge uses the top-5 error rate for ranking purposes. input
Conv
7x7+2(S)
MaxPool
3x3+2(S)
LocalRespNorm
Conv
1x1+1(V)
Conv
3x3+1(S)
LocalRespNorm
MaxPool
3x3+2(S)
Conv
1x1+1(S)
Conv
1x1+1(S)
Conv
1x1+1(S)
MaxPool
3x3+1(S)
DepthConcat
Conv
3x3+1(S)
Conv
5x5+1(S)
Conv
1x1+1(S)
Conv
1x1+1(S)
Conv
1x1+1(S)
Conv
1x1+1(S)
MaxPool
3x3+1(S)
DepthConcat
Conv
3x3+1(S)
Conv
5x5+1(S)
Conv
1x1+1(S)
MaxPool
3x3+2(S)
Conv
1x1+1(S)
Conv
1x1+1(S)
Conv
1x1+1(S)
MaxPool
3x3+1(S)
DepthConcat
Conv
3x3+1(S)
Conv
5x5+1(S)
Conv
1x1+1(S)
Conv
1x1+1(S)
Conv
1x1+1(S)
Conv
1x1+1(S)
MaxPool
3x3+1(S)
AveragePool
5x5+3(V)
DepthConcat
Conv
3x3+1(S)
Conv
5x5+1(S)
Conv
1x1+1(S)
Conv
1x1+1(S)
Conv
1x1+1(S)
Conv
1x1+1(S)
MaxPool
3x3+1(S)
DepthConcat
Conv
3x3+1(S)
Conv
5x5+1(S)
Conv
1x1+1(S)
Conv
1x1+1(S)
Conv
1x1+1(S)
Conv
1x1+1(S)
MaxPool
3x3+1(S)
DepthConcat
Conv
3x3+1(S)
Conv
5x5+1(S)
Conv
1x1+1(S)
Conv
1x1+1(S)
Conv
1x1+1(S)
Conv
1x1+1(S)
MaxPool
3x3+1(S)
AveragePool
5x5+3(V)
DepthConcat
Conv
3x3+1(S)
Conv
5x5+1(S)
Conv
1x1+1(S)
MaxPool
3x3+2(S)
Conv
1x1+1(S)
Conv
1x1+1(S)
Conv
1x1+1(S)
MaxPool
3x3+1(S)
DepthConcat
Conv
3x3+1(S)
Conv
5x5+1(S)
Conv
1x1+1(S)
Conv
1x1+1(S)
Conv
1x1+1(S)
Conv
1x1+1(S)
MaxPool
3x3+1(S)
DepthConcat
Conv
3x3+1(S)
Conv
5x5+1(S)
Conv
1x1+1(S)
AveragePool
7x7+1(V)
FC
Conv
1x1+1(S)
FC
FC
SoftmaxActivation softmax0
Conv
1x1+1(S)
FC
FC
SoftmaxActivation softmax1
SoftmaxActivation softmax2
Figure 3: GoogLeNet network with all the bells and whistles.
We participated in the challenge with no external data used for training.
In addition to the training techniques aforementioned in this paper, we adopted a set of techniques during testing to obtain a higher performance, which we describe next.
1. We independently trained 7 versions of the same
GoogLeNet model (including one wider version), and performed ensemble prediction with them.
These models were trained with the same initialization (even with the same initial weights, due to an oversight) and learning rate policies. They differed only in sampling methodologies and the randomized input image order.
2. During testing, we adopted a more aggressive cropping approach than that of Krizhevsky et al.. Specifically, we resized the image to 4 scales where the shorter dimension (height or width) is 256, 288, 320 and 352 respectively, take the left, center and right square of these resized images (in the case of portrait images, we take the top, center and bottom squares).
For each square, we then take the 4 corners and the center 224×224 crop as well as the square resized to
224×224, and their mirrored versions. This leads to
4×3×6×2 = 144 crops per image.
A similar approach was used by Andrew Howard in the previous year's entry, which we empirically verified to perform slightly worse than the proposed scheme. We note that such aggressive cropping may not be necessary in real applications, as the benefit of more crops becomes marginal after a reasonable number of crops are present (as we will show later on).
3. The softmax probabilities are averaged over multiple crops and over all the individual classifiers to obtain the final prediction. In our experiments we analyzed alternative approaches on the validation data, such as max pooling over crops and averaging over classifiers, but they lead to inferior performance than the simple averaging.
In the remainder of this paper, we analyze the multiple factors that contribute to the overall performance of the final submission.
Our final submission to the challenge obtains a top-5 error of 6.67% on both the validation and testing data, ranking the first among other participants. This is a 56.5% relative reduction compared to the SuperVision approach in 2012, and about 40% relative reduction compared to the previous year's best approach (Clarifai), both of which used external data for training the classifiers. Table 2 shows the statistics of some of the top-performing approaches over the past 3 years.
We also analyze and report the performance of multiple testing choices, by varying the number of models and the Team
Year
Place
Error(top-5)
Uses external data
SuperVision
1st
16.4% no
SuperVision
1st
Imagenet 22k
Clarifai
1st
11.7% no
Clarifai
1st
Imagenet 22k
MSRA
3rd
7.35% no
VGG
2nd
7.32% no
GoogLeNet
1st
6.67% no
Table 2: Classification performance.
Number of models
Number of Crops
Cost
Top-5 error compared to base
10.07% base
-0.92%
-2.18%
-1.98%
-2.45%
-3.45%
Table 3: GoogLeNet classification performance break down. number of crops used when predicting an image in Table 3.
When we use one model, we chose the one with the lowest top-1 error rate on the validation data. All numbers are reported on the validation dataset in order to not overfit to the testing data statistics.
8. ILSVRC 2014 Detection Challenge Setup and Results
The ILSVRC detection task is to produce bounding boxes around objects in images among 200 possible classes.
Detected objects count as correct if they match the class of the groundtruth and their bounding boxes overlap by at least 50% (using the Jaccard index). Extraneous detections count as false positives and are penalized. Contrary to the classification task, each image may contain many objects or none, and their scale may vary. Results are reported using the mean average precision (mAP). The approach taken by
GoogLeNet for detection is similar to the R-CNN by, but is augmented with the Inception model as the region classifier. Additionally, the region proposal step is improved by combining the selective search approach with multibox predictions for higher object bounding box recall.
In order to reduce the number of false positives, the superTeam
Year
Place mAP external data ensemble approach
UvA-Euvision
1st
22.6% none
?
Fisher vectors
Deep Insight
3rd
ImageNet 1k
CNN
CUHK DeepID-Net
2nd
ImageNet 1k
?
CNN
GoogLeNet
1st
ImageNet 1k
CNN
Table 4: Comparison of detection performances. Unreported values are noted with question marks. pixel size was increased by 2×. This halves the proposals coming from the selective search algorithm. We added back
200 region proposals coming from multi-box resulting, in total, in about 60% of the proposals used by, while increasing the coverage from 92% to 93%. The overall effect of cutting the number of proposals with increased coverage is a 1% improvement of the mean average precision for the single model case. Finally, we use an ensemble of 6 GoogLeNets when classifying each region. This leads to an increase in accuracy from 40% to 43.9%. Note that contrary to R-CNN, we did not use bounding box regression due to lack of time.
We first report the top detection results and show the progress since the first edition of the detection task. Compared to the 2013 result, the accuracy has almost doubled.
The top performing teams all use convolutional networks.
We report the official scores in Table 4 and common strategies for each team: the use of external data, ensemble models or contextual models. The external data is typically the ILSVRC12 classification data for pre-training a model that is later refined on the detection data. Some teams also mention the use of the localization data. Since a good portion of the localization task bounding boxes are not included in the detection dataset, one can pre-train a general bounding box regressor with this data the same way classification is used for pre-training. The GoogLeNet entry did not use the localization data for pretraining.
In Table 5, we compare results using a single model only.
The top performing model is by Deep Insight and surprisingly only improves by 0.3 points with an ensemble of 3 models while the GoogLeNet obtains significantly stronger results with the ensemble.
9. Conclusions
Our results yield a solid evidence that approximating the expected optimal sparse structure by readily available dense building blocks is a viable method for improving neural networks for computer vision.
The main advantage of this method is a significant quality gain at a modest increase of computational requirements compared to shallower and narrower architectures.
Our object detection work was competitive despite not
Team mAP
Contextual model
Bounding box regression
TrimpsSoushen
31.6% no
?
Berkeley
Vision
34.5% no yes
UvAEuvision
?
?
CUHK
DeepIDNet2
37.7% no
?
GoogLeNet
38.02% no no
Deep
Insight
40.2% yes yes
Table 5: Single model performance for detection. utilizing context nor performing bounding box regression, suggesting yet further evidence of the strengths of the Inception architecture.
For both classification and detection, it is expected that similar quality of result can be achieved by much more expensive non-Inception-type networks of similar depth and width. Still, our approach yields solid evidence that moving to sparser architectures is feasible and useful idea in general. This suggest future work towards creating sparser and more refined structures in automated ways on the basis of, as well as on applying the insights of the Inception architecture to other domains.
References
 Know your meme:
We need to go deeper. http://knowyourmeme.com/memes/we-need-to-go-deeper.
Accessed: 2014-09-15.
 S. Arora, A. Bhaskara, R. Ge, and T. Ma. Provable bounds for learning some deep representations. CoRR, abs/1310.6343, 2013.
 U. V. C¸ ataly¨urek, C. Aykanat, and B. Uc¸ar.
On two-dimensional sparse matrix partitioning:
Models, methods, and a recipe.
SIAM J. Sci. Comput., 32(2):656–683, Feb. 2010.
 J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, M. Mao, M. Ranzato, A. Senior, P. Tucker, K. Yang, Q. V. Le, and A. Y. Ng. Large scale distributed deep networks. In P. Bartlett, F. Pereira, C. Burges, L. Bottou, and K. Weinberger, editors, NIPS, pages 1232–
 D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov.
Scalable object detection using deep neural networks.
In CVPR, 2014.
 R. B. Girshick, J. Donahue, T. Darrell, and J. Malik.
Rich feature hierarchies for accurate object detection and semantic segmentation. In Computer Vision and Pattern Recognition, 2014. CVPR 2014. IEEE Conference on, 2014.
 G.
E.
Hinton, N.
Srivastava, A.
Krizhevsky, I. Sutskever, and R. Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors. CoRR, abs/1207.0580, 2012.
 A. G. Howard.
Some improvements on deep convolutional neural network based image classification.
CoRR, abs/1312.5402, 2013.
 A. Krizhevsky, I. Sutskever, and G. Hinton.
Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems 25, pages 1106–1114, 2012.
 Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E.
Howard, W. Hubbard, and L. D. Jackel. Backpropagation applied to handwritten zip code recognition.
Neural Comput., 1(4):541–551, Dec. 1989.
 Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner.
Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, M. Lin, Q. Chen, and S. Yan. Network in network.
CoRR, abs/1312.4400, 2013.
 B. T. Polyak and A. B. Juditsky.
Acceleration of stochastic approximation by averaging. SIAM J. Control Optim., 30(4):838–855, July 1992.
 P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun.
Overfeat: Integrated recognition, localization and detection using convolutional networks. CoRR, abs/1312.6229, 2013.
 T. Serre, L. Wolf, S. M. Bileschi, M. Riesenhuber, and T. Poggio. Robust object recognition with cortex-like mechanisms. IEEE Trans. Pattern Anal. Mach. Intell., 29(3):411–426, 2007.
 F. Song and J. Dongarra. Scaling up matrix computations on shared-memory manycore systems with 1000 cpu cores. In Proceedings of the 28th ACM International Conference on Supercomputing, ICS '14, pages
333–342, New York, NY, USA, 2014. ACM.
 I. Sutskever, J. Martens, G. E. Dahl, and G. E. Hinton.
On the importance of initialization and momentum in deep learning. In ICML, volume 28 of JMLR Proceedings, pages 1139–1147. JMLR.org, 2013.
 C. Szegedy, A. Toshev, and D. Erhan. Deep neural networks for object detection. In C. J. C. Burges, L. Bottou, Z. Ghahramani, and K. Q. Weinberger, editors, NIPS, pages 2553–2561, 2013.
 A. Toshev and C. Szegedy.
Deeppose:
Human pose estimation via deep neural networks.
CoRR, abs/1312.4659, 2013.
 K. E. A. van de Sande, J. R. R. Uijlings, T. Gevers, and A. W. M. Smeulders. Segmentation as selective search for object recognition. In Proceedings of the 2011 International Conference on Computer Vision, ICCV '11, pages 1879–1886, Washington, DC, USA, 2011. IEEE Computer Society.
 M. D. Zeiler and R. Fergus. Visualizing and understanding convolutional networks. In D. J. Fleet, T. Pajdla, B. Schiele, and T. Tuytelaars, editors, ECCV, volume 8689 of Lecture Notes in Computer Science, pages 818–833. Springer, 2014.Transitive Transfer Learning
Ben Tan
Department of Computer
Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, China btan@cse.ust.hk
Yangqiu Song
Department of Computer
Science, University of Illinois at Urbana-Champaign, USA yqsong@illinois.edu
Erheng Zhong
Personalization Sciences, Yahoo Labs, Sunnyvale, USA erheng@yahoo-inc.com
Qiang Yang
Department of Computer
Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, China qyang@cse.ust.hk
ABSTRACT
Transfer learning, which leverages knowledge from source domains to enhance learning ability in a target domain, has been proven effective in various applications. A major limitation of transfer learning is that the source and target domains should be directly related. If there is little overlap between the two domains, performing knowledge transfer between these domains will not be effective. Inspired by human transitive inference and learning ability, whereby two seemingly unrelated concepts can be connected by series of intermediate bridges using auxiliary concepts, in this paper we study a novel learning problem: Transitive Transfer Learning (abbreviated to TTL). TTL is aimed at breaking the large domain distances and transferring knowledge even when the source and target domains share few factors directly. For example, when the source and target domains are text and images respectively, TTL can use some annotated images as the intermediate domain to bridge them. To solve the TTL problem, we propose a framework wherein we first select one or more domains to act as a bridge between the source and target domains to enable transfer learning, and then perform the transferring of knowledge via this bridge. Extensive empirical evidence shows that the framework yields state-of-the-art classification accuracies on several classification data sets.
Categories and Subject Descriptors
H.2.8 [Database Management]: Database Applications - Data Mining
General Terms
Machine Learning
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org.
KDD '15, August 11 - 14, 2015, NSW, Australia
Copyright 2015 ACM. ISBN 978-1-4503-3664-2/15/08...$15.00
DOI: http://dx.doi.org/10.1145/2783258.2783295.
Figure 1: An pictorial illustration of the transitive transfer learning problem. In TTL, the source and target domains have few common factors, but they can be connected by intermediate domains through some underlying factors.
Keywords
Transfer Learning, Transitive Transfer Learning, Nonnegative Matrix Tri-factorizations
INTRODUCTION
Transfer learning, which aims to borrow knowledge from source domains to help the learning in a target domain, has been established as one of the most important machine learning paradigms.
Various algorithms have been widely used and proven effective in many applications, for example, classification, reinforcement learning and recommendation systems, and so on. A critical requirement for successful transfer learning is that the source and target domains should be related. This relation can be in the form of related instances, features or models. If no direct relation can be found, forcibly transferring knowledge will not work. In the worst case, it could lead to having no improvement, or even worse performance, in the target domain. This is one of the major limitations of traditional transfer learning. However, as human beings, we naturally have the ability to carry out inference and learning via transitivity. This ability helps humans connect many concepts and transfer knowledge between two seemingly unrelated concepts by introducing a few intermediate concepts as a bridge.
For example, after taking a class in elementary computer science, 1155 we may find it easier to transfer the knowledge to theoretical computer science if we have taken an applied algorithm design course in between, since the algorithm course may involve both concepts in programming and theory. Likewise, having learned some basic math, we may find it impossible to directly take a course in convex optimization. However, this becomes feasible when we take an intermediate course in linear algebra and probability. The linear algebra and probability course serves as the intermediate domain for knowledge transfer.
Human ability to conduct transitive inference and learning inspires us to study a novel learning problem known as Transitive
Transfer Learning (TTL). As illustrated in Figure 1, in TTL, the source and target domains have few common factors, but they can be connected by intermediate domains through some underlining factors. We expect TTL to have wide practical applications. For example, when the source domain is composed of text documents and the target domain contains image data, they share no overlap feature spaces, knowledge learned in text documents can hardly be transferred to images. However, TTL can introduce some annotated images to learn a feature mapping between these two different feature spaces and have a smooth knowledge transfer. In other applications, such as text sentiment classification, all the data have the same feature space, but two group of data may have large distribution gap, TTL can introduce some auxiliary intermediate data to form a transitive knowledge transfer structure with which we can obtain a more versatile sentiment classification system.
In this paper, we propose a learning framework for the TTL problem. The framework is composed of two steps. The first step is to find an appropriate domain to bridge the given source and target domains. The second step is to do effective knowledge transfer among all domains. In the first step, we propose a probability model to select appropriate domains that is able to draw the source and target domains closer, based on domain characteristics such as domain difficulty and pairwise closeness. As data from different domains are collected from different data sources, each pair of domains may have distribution shift. In the second step, considering both of the domain relationship and distribution shift, we propose a transfer learning algorithm that allows to learn overlap features among domains and propagate label information through them. A high-level description of the TTL framework is summarized in Table 1. We give a formal definition of the TTL problem in Section 2, and describe the technical details of these two steps in Sections 3 and 4 respectively.
PROBLEM DEFINITION
In the problem, we have labeled source domain data S={(xs i, yi)}ns i=1, unlabeled target domain data T ={xt i}nt i=1, and k unlabeled intermediate domains Dj = {x dj i } nj i=1, j = 1,..., k, x∗ ∈
Rm∗ is a m∗ dimensional feature vector. The data from different domains could have different dimensions. S and T have a large distribution gap, thus directly transferring knowledge between them may cause a substantial performance loss in the target domain.
The TTL framework is aimed at finding intermediate domain(s) to bridge S and T, and minimizing the performance loss in T.
Formally, given a domain distribution gap measure g(·, ·), the first step is to find an intermediate domain that satisfies g(S, T |Di)
< g(S, T ). The second step performs transfer learning from the source domain S to target domain T via intermediate domain Di; this is implemented via learning two feature clustering functions psd(S, Di) and pdt(Di, T ), such that the distribution gap of data on common feature clusters selected by psd(S, Di) and pdt(Di, T ) are further reduced. The label information in the source domain is Table 1: The TTL Framework
Input: The source,target and candidate intermediate domains
Step 1: Intermediate domain selection (see Section 3)
Step 2: Transitive knowledge transfer (see Section 4)
Output: Prediction results in the target domain propagated to the intermediate and target data on the selected common feature clusters.
INTERMEDIATE DOMAIN SELECTION
Intermediate domain selection is problem specific, different problems may have different strategies. For example, when the source domain is composed of text data and the target domain is image data, one can crawl some annotated images from Flickr as the intermediate domain data. In other problems when there are multiple candidate intermediate domains, one should propose some selection algorithms according to domain properties. In this paper, we propose an algorithm for text sentiment classification problem as an example. As studied by previous research, domain difficulty and domain distance are two major factors that affect the transfer learning performance between two domains. On one hand, intuitively, if the source domain is less difficult than the intermediate and target domains, the model learned from the source data is highly predictive and is very likely to achieve high performance on the intermediate and target domains as well. On the other hand, if the intermediate domain is able to draw closer the source and target domains than their original distance, then the knowledge transfer process between the source and target domains will have less information loss. Hence, in this paper, we introduce domain complexity and A-distance to estimate domain difficulty and pairwise domain distance respectively. We summarize these measures as follows:
• Domain complexity: domain difficulty measure is problem specific, as different problem may have different feature types.
In this paper, we choose domain complexity to measure the difficulty. The domain complexity is calculated as the percentage of long tail features that have low frequency.
These long tail features bring in long tail feature distribution and significant feature diversity, thus make automatic machine learning difficult. We calculate the domain complexity as follows: cplx(D) = |{x|c(x) < t × n}| m
For non-negative features, c(x) is the number of instances whose feature x is larger than zero. |{x|c(x) < t × n}| is the number of features that appear in less than t×n instances.
In this paper, we measure the domain complexity as the percentage of long tail features that appear in less than 10% instances. For continuous features, we can measure their relative entropy as domain difficulty.
• A-distance: The A-distance estimates the distribution difference of two sets of data samples that are drawn from two probability distributions. Practically, given two sets of domain data Di and Dj, we can calculate the A-distance as follows: disA(Di, Dj) = 2(1 − 2 min h∈H error(h|Di, Dj)), Table 2: Domain characteristic features feature description cplx_src (c1) source domain complexity cplx_inter (c2) intermediate domain complexity cplx_tar (c3) target domain complexity dissi
A (c4) a_distance between source and intermediate disst
A(c5) a_distance between source and target disit
A (c6) a_distance between intermediate and target
H is a hypothesis space, h is the optimal proxy classifier that discriminates data points from different domains. In this paper, we first assign the source data positive labels, and target data negative ones, then use logistic regression as the proxy classifier to estimate the error(h|Di, Dj)) in A-distance.
In, the authors have been proven that the prediction error of the target domain is bounded by the error of the source domain, the A-distance and some other constant factor.
Given a triple tr = {S, D, T }, we can extract six features as described in Table 2. The first three features summarize individual in-domain characteristics, the last three features capture the pairwise cross domain distances. These features together affect the success probability of a transfer learning algorithm. However, it is impossible to design a universal domain selection criteria, as different problems may have different preferences (weights) on these features. To model the success probability of the introduced intermediate domain, we propose the following logistic function: f(tr) = δ(β0 +
� i=1 βici), (3) where δ(x) =
1+e−x. We estimate the parameters β = {β0, · · ·, β6} to maximize the log likelihood defined as:
L(β) = t
� i=1 l(i) log f(tri) + (1 − l(i)) log(1 − f(tri)), (4) l(i) is a binary label, indicating whether the intermediate domain in the ith triple is able to bridge the source and target domains.
We get the label by the following strategy. We perform a semisupervised label propagation algorithm with input S and T, and obtain a prediction accuracy accst on the target domain. We also perform the same algorithm with input {S, D, T }, and obtain another accuracy accsit on the target domain. If accsit > accst, we set l(i) = 1, otherwise, l(i) = 0. The label is determined by both the domain characteristics and the propagation model. A sophisticated model may accept more intermediate domains than a simple model. In this paper, we prefer to use a simple model such as KNN that are able to provide us strictly fitted candidates.
We transform the intermediate domain selection problem to a probability estimation problem. A candidate intermediate domain with high f(tr) is more likely to be selected.
TRANSITIVE KNOWLEDGE TRANSFER
In the first step, an intermediate domain that can bridge the source and target domains has been selected, however, there is still distribution shift among these domains. Thus, in the second step of the TTL framework, we propose a novel transfer learning algorithm that considers both of the transitive relationship and distribution shift among all the domains. The algorithm is based on nonnegative matrix tri-factorization that can perform feature clustering and label propagation simultaneously, so we first give some background knowledge.
Non-negative Matrix Tri-factorization
Non-negative Matrix Tri-factorization (NMTF) is a popular and effective technique for data clustering and classification. In
NMTF, the feature-instance matrix is decomposed into three submatrices. In general, given a feature-instance matrix X ∈ Rm×n, m is the number of dimensions, n is the number of instances. One can obtain the factorized sub-matrices by solving the optimization problem as follows: arg minF,A,GT
L = ||X − FAGT ||, (5) where || · || denotes the Frobenius norm of matrix.
The matrix F ∈ Rm×p indicates the information of feature clusters and p is the number of hidden feature clusters. The element
Fi,j indicates the probability that the ith feature belongs to the jth feature cluster.
The matrix G ∈ Rc×n is the instance cluster assignment matrix and c is the number of instance clusters. If the largest element of the ith row is located in the jth column, it means that the ith instance belongs to the jth instance cluster. In the classification problem, each instance cluster can be regarded as a label class.
A ∈ Rp×c is the association matrix. c is the number of instance clusters or label classes, for the binary classification problem c = 2.
The element Ai,j is the probability that the ith feature cluster is associated with the jth instance cluster.
NMTF for Transfer Learning
NMTF is also used as a basic technique for transfer learning algorithms. Given the source and target domains S and T, Xs and Xt are their feature-instance matrices respectively, one can decompose these two matrices simultaneously, and allow the decomposed matrices share some cross-domain information (sub-matrices). Formally, given two related domains S and T, their feature-instance matrices can be decomposed simultaneously as follows:
LST = ||Xs − FsAsGs|| + ||Xt − FtAtGt||
=
����Xs − [F 1, F 2 s ]
�
A1
A2 s
�
GT s
���� +
����Xt − [F 1, F 2 t ]
�
A1
A2 t
�
GT t
����, (6) where F 1 ∈ Rm×p1
+ and A1 ∈ Rp1×c
+ contain the common factors shared by the source and target domains. F 2 s, F 2 t ∈ Rm×p2
+ and A2 s, A2 t ∈ Rp2×n
+ contain domain-specific information. They are not shared by domains. p1, p2 are two parameters that indicate the number of hidden feature clusters. Gs ∈ Rn×c is the label class matrix and generated from the instance labels {yi|i = 1, · · ·, n} of the source domain S. If the ith instance belongs to jth class, then the (i, j) element in Gs equals to 1, otherwise, it equals to 0.
Gs is a constant matrix and keeps unchanged during the factorization process. Gt is the label class matrix of the target domain. Its elements are variables that we want to learn by the matrix decomposition.
From Eq. (6), we can notice that the label information of the source domain is propagated to the target domain through the shared common factors F1 and A1.
The TTL Transfer Learning Algorithm
As shown in Figure 1, the source, intermediate and target domains have a transitive relationship. In other words, the intermediate domain bridges the source and target domains, but has different common factors to them respectively. Hence, to capture these properties, we propose a coupled NMTF algorithm. The proposed
Figure 2: An illustration of the proposed transfer learning algorithm in the TTL framework.
The algorithm learns two coupled feature representations by feature clustering, and then propagates the label information from the source to the target domain through the intermediate domain on the coupled feature representation. transfer learning algorithm is illustrated in Figure 2, and written in Eq. (7)
L = ||Xs − FsAsGT s || + ||XI − FIAIGT
I ||+
||XI − F
′
IA
′
IGT
I || + ||Xt − FtAtGT t ||
=
����Xs − [ ˆF 1, ˆF 2 s ]
� ˆA1
ˆA2 s
�
GT s
���� +
����XI − [ ˆF 1, ˆF 2
I ]
� ˆA1
ˆA2
I
�
GT
I
���� +
����XI − [ ˜F 1, ˜F 2
I ]
� ˜A1
˜A2
I
�
GT
I
���� +
����Xt − [ ˜F 1, ˜F 2 t ]
� ˜A1
˜A2 t
�
GT t
����.
From the above equation, we can see that the first two terms (
||Xs − FsAsGT s || + ||XI − FIAIGT
I ||) refer to the first feature clustering and label propagation between the source and intermediate domains in Figure 2, the last two terms refer to the second feature clustering and label propagation between the intermediate and target domains. In Eq. (7), it is worth noting that we decompose
XI twice with different decomposition matrices, since XI shares different knowledge with Xs and Xt respectively. At the same time, we couple these two decomposition processes together by the label matrix GI. It is reasonable that the instances in the intermediate domain should have the same labels in different decomposition processes. Moreover, if we solve the matrix decomposition by iterative algorithms, in every iteration, each decomposition process is able to consider the feedbacks from the other decomposition.
If these two processes are separately solved, the first decomposition process will not consider the results from the second one, and may suffer from the bias problem. In the experiment, we find that the coupled strategy achieves better performance than separated decomposition.
Overall, the proposed learning algorithm fits the transitive relationship among domains. The label information in the source domain is transferred through ˆF1 and ˆA1 to the intermediate domain, and affects the learning results of GI. The knowledge on class labels incorporated with GI from the intermediate domain is further transferred to the target domain through ˜F1 and ˜A1.
As we discussed in Section 4.1, the decomposed matrix F contains the information on hidden feature clusters, indicating the distribution of features on each hidden cluster. Therefore, the summation of each column of F has to be equal to one. The label matrix
G indicates the label distribution of each instance. Thus, the summation of each row of G has to be equal to one. Considering these matrix constrains, we obtain the final optimization objective function for the proposed learning algorithm: arg minFs,As,FI,AI,GI,F ′
I,A′
I,Ft,At,Gt
L s.t.
�m i=1 ˆF 1(i, j) = 1, �m i=1 ˆF 2 s (i, j) = 1, �m i=1 ˆF 2
I (i, j) = 1, �m i=1 ˜F 1(i, j) = 1, �m i=1 ˜F 2
I (i, j) = 1, �m i=1 ˜F 2 t (i, j) = 1, �c j=1 GI(i, j) = 1
�c j=1 Gt(i, j) = 1.
Since the objective function in Eq. (8) is non-convex, it is intractable to obtain the global optimal solution. Therefore, we develop an alternating optimization algorithm to achieve the local optimal solution. We first show the updating rules of matrices ˜F 1, ˜F 2
I, ˜F 2 t, and Gt. We summarize the notations of matrix multiplications in Table 3, and show the updating rules as follows:
˜F 1(i, j) = ˜F 1(i, j) ×
�
[ ˜
M1
I+ ˜
M1 t ](i,j)
[ ˜
T 1
I + ˜
T 1 t ](i,j), ˜F 2
I (i, j) = ˜F 2
I (i, j) ×
�
˜
M2
I(i,j)
˜
T 2
I (s,t), ˜F 2 t (i, j) = ˜F 2 t (i, j) ×
�
˜
M2 t (i,j)
˜
T 2 t (s,t), Gt(i, j) = Gt(i, j) ×
�
[XT t FtAt](i,j)
[GtAT t F T t FtAt](i,j).
From Eq. (8), after the matrices are updated, the constrained matrices have to be normalized as:
˜F 1(i, j) =
˜
F 1(i,j)
�m i=1 ˜
F 1(i,j), ˜F 2
I (i, j) =
˜
F 2
I (i,j)
�m i=1 ˜
F 2
I (i,j), ˜F 2 t (i, j) =
˜
F 2 t (i,j)
�m i=1 ˜
F 2 t (i,j), Gt(i, j) =
Gt(i,j)
�c j=1 Gt(i,j).
The updating rules and normalization methods for other submatrices are similar and are shown in the Appendix.
We need not update Gs, which contains the ground-truth label information.
We give the procedure of the proposed learning algorithm in Algorithm 1. As shown in Eq. (7) and the Appendix section, the updating rule for GI is constrained by FI, F ′
I, AI and A′
I. In addition, the sub-matrices ˆF 1, ˆA1 and, ˜F 1, ˜A1 are constrained by Xs, Gs and Xt, Gt respectively. Therefore, the updating rule of Gt is transitively constrained by Xs, Gs and, the discriminative information in the source domain is transitively transferred to the target domain.
The updating processes of Fs, FI, F
′
I and Ft refer to the feature clusterings in Figure 2. The updating processes of GI and Gt refer to the label propagations in Figure 2.
We analyze the convergence property of Eq. (9) with normalization rules in Eq. (10). We first analyze the convergence of ˜F 1 with the rest of the parameters fixed. By using the properties of trace operation and frobenius norm ||X||2 = tr(XT X) = tr(XXT ), we re-formulate the objective function Eq. (8) as a Lagrangian function and keep the terms related to ˜F 1:
Table 3: Notations of matrix multiplications
ˆ
M1
I = XIGI ˆA1T
ˆ
M2
I = XIGI ˆA2T
I
ˆ
M1 t = XtGt ˆA1T
ˆ
M2 t = XtGt ˆA2T t
ˆ
NI = ˆF 1 ˆA1GT
I + ˆF 2
I ˆA2
IGT
I
ˆ
Nt = ˆF 1 ˆA1GT t + ˆF 2 t ˆA2 tGT t
ˆT 1
I = ˆ
NIGI ˆA1T
ˆT 1 t = ˆ
NtGt ˆA1T
ˆT 2
I = ˆ
NIGI ˆA2T
I
ˆT 2 t = ˆ
NtGt ˆA2T t
Ft = [ ˆF1 ˆF 2 t ]
At = [ ˆA1 ˆA2 t]
Algorithm 1 The TTL Transfer Learning Algorithm
1: Input: Source, target, intermediate domains S, T and D, the parameters p, and the number of iterations Itermax.
2: Initialize the matrices Fs, As, FI, AI, GI, Ft, At, Gt.
3: while iter < Itermax do
Update the sub-matrices of Fs, As, FI, AI, Ft, At and label matrices GI, Gt according to the updating rules given in Eq. (9) and Eq. (12) of the Appendix section.
Normalize the sub-matrices of Fs, FI, Ft, and label matrices GI, Gt according to the normalization rules given in Eq. (10) and Eq. (13) of the Appendix.
6: end while
7: Output: the predicted results of Gt.
L( ˜F 1) = tr(−2XT
I ˜F 1 ˜A1GT
I + 2GI ˜A1T ˜F 1T ˜
NI)
+tr(−2XT t ˜F 1 ˜A1GT t + 2GT ˜A1T ˜F 1T ˜
Nt)
+tr[λ( ˜F 1T 1m1T m ˜F 1 − 21p1T m ˜F 1)], (11) where λ ∈ Rp×p is a diagonal matrix. 1m and 1p are all-ones vectors with dimension m and p respectively.
LEMMA 1. Using the update rule in Eq. (9) and normalization rules in Eq. (10), the loss function in Eq. (11) will monotonously decrease.
The proof of Lemma 1 is shown in the Appendix. The convergence of other terms can be proven in the same way. According to the convergence analysis on the update rules and the multiplicative update rules, each update step in Algorithm 1 will not increase
Eq. (8). The objective has a lower bounded by zero. The convergence of the proposed transfer learning algorithm is proven.
EXPERIMENTS
In this section, we perform three tests. The first test is designed to analyze how the intermediate domain and model parameters affect the performance of the TTL framework, and to evaluate the convergence rate empirically. This is done by conducting experiments on six synthetic text classification tasks generated from the 20Newsgroups data set 1.
The second test is designed to evaluate the TTL framework when the source and target domain data have completely different structures. The experiments are conducted on the text-to-image data set.
The intermediate domains for all tasks in the data set are crawled from Flicker.
1http://qwone.com/~jason/20Newsgroups/
Finally, the third test is designed to test the efficiency of the intermediate domain selection algorithm and the transfer learning algorithm in the framework. The experiments are conducted on some text sentiment classification tasks 2. The data from different domains have the same feature space but different distribution. Moreover, there are many candidate intermediate domains for each pair of source and target domains.
Baseline methods
In the synthetic text classification and sentiment classification tasks, all the data have the same feature space. We compare the proposed framework with three baseline methods to verify the effectiveness.
The first baseline is SVM, which is a classical supervised learning algorithm. We use the linear kernel of SVM with the implementation in LibLinear3. The second one is the triplex transfer learning(TriplexTL) algorithm, which is a state-of-the-art transfer learning method implemented with NMTF. The other transfer learning algorithm is LatentMap, which is also a state-of-the-art transfer learning algorithm. It draws the joint distribution of two domains closer by mapping the data to a low dimensional latent space. The three baseline methods are tested under two different settings. The first one is direct-transfer. We train the learners based on the labeled data in the source domain and test them directly on the data in the target domain. We use subscript ST to indicate the methods under this setting in the following experiments, for example, TriplexTLST and LMST. The second setting is a 2-stage transfer learning process. We first apply TriplexTL/LM between the source and the intermediate domain to predict the intermediate domain labels, and then again apply TriplexTL/LM between the intermediate domain and the target domain. The major difference between this naive transitive transfer learning strategy and the proposed transfer learning algorithm is that no iterative feature clustering and label propagation is performed. We use subscript SIT to represent methods under this setting, for instance, TriplexTLSIT and LMSIT.
In the text-to-image data set, the data have different feature spaces.
The above mentioned baselines cannot handle these data. Hence, we compare TTL with two heterogeneous transfer learning (HTL) algorithms.
The first baseline is co-transfer. It models the problem as a coupled Markov chain with restart. The transition probabilities of the Markov chain is construdture by using the intra-relationship based on affinity metric among data in the source and target domains, and the inter-relationship between the source and target domains based on co-occurrence information of the intermediate do2http://www.cs.jhu.edu/~mdredze/datasets/ sentiment/
3http://www.csie.ntu.edu.tw/~cjlin/ liblinear/
Figure 3: The problem setting of the 20Newsgroup data set. main. The second one is HTLIC 4. It learns a new target feature representation by using data from the source, intermediate and target domain data via the collective matrix factorization technique.
A SVM classifier is then learned on the new target feature representation.
All methods in the experiments are performed ten times, and we report their average performances and variances.
Synthetic text classification tasks
20Newsgroups Data Set
The 20Newsgroups is a hierarchical text collection, containing some top categories like 'comp', 'sci', 'rec' and 'talk'. Each category has some sub-categories, such as 'sci.crypt' and 'sci.med'.
We use four main categories to generate six tasks, in each of which two top categories are chosen for generating binary categorization.
With a hierarchical structure, for each category, all of the subcategories are then organized into three parts, where each part has different subcategories and is of a different distribution. Therefore, they can be treated as the source, intermediate and target domains, respectively. To generate the transitive transfer learning setting, we divide the vocabularies into two separated subsets Set A and Set B.
Then, we set the term frequencies of words in Set A of the source domain to zero. Similarly, we set the term frequencies of words in Set B of the target domain to zero. Therefore, the source and target domains have no overlapping words. The problem setting on this data set is illustrated in Figure 3, where the blocks with texture indicate that the features have values. We can see that the source and target domains have no shared features, but they have shared features with the intermediate domain, respectively. Apparently, the intermediate domains here can bridge the generated source and target domains. We give a detailed description of the six tasks in Table 4. The feature dimensions in these tasks range from 2405 to
5984. The number of instances in these tasks are around 7000.
Performance on synthetic tasks
In experiments, we compare the proposed framework with the baseline methods on six text classification tasks.
The text classification tasks are very challenging. The source and target domains have no overlapping features. The SVM classifiers trained with labeled source data have almost no discriminative ability on the target data. From the results in Table 5, we can see that the SVMST classifiers obtain a very bad performance.
Likewise, the source classifiers can barely be adapted for the target domain data. Hence, TriplexTLST and LMST obtain bad performance also, but better than SVMST. The naive transfer learn4http://www.cse.ust.hk/~yinz/htl4ic.zip
# of labeled data
TTL
TriplexTLIT(a) vary # of labeled data
# of removed feature : d
Accuracy
TTL
TriplexTLSIT(b) remove d features
Figure 4: Performance with different intermediate domains. ing algorithms, TriplexTLSIT and LMSIT, achieve relative good performance, because they use the intermediate domain data as a bridge to perform a 2-stage knowledge transfer. The proposed TTL framework achieves the best performance. This can be ascribed to the reason that TTL not only bridges the source and target domains by using the intermediate domain data, but also has iterative feature clustering and label propagation loops where the knowledge provided by the source domain can be deeply reshaped and reorganized to be exploited for the target domain.
Performance with different intermediate domains
The intermediate domain plays an important role in bridging the source and target domains. Hence, we also conduct some experiments on the "comp-vs-talk" task to test the proposed TTL framework when 1) the amount of labeled intermediate data increases;
2) the connection between the source/target and the intermediate domains becomes weaker.
In the first setting, we compare TTL with TriplexTLIT that transfers knowledge from labeled intermediate domain data to the target data. We vary the amount of labeled intermediate data from 50 to
400. We randomly sample the labeled intermediate domain data ten times, and show the average performance and variance in Figure 4(a). From the results, we can see that the performance of TTL is better than TriplexTLIT when the amount of labeled intermediate domain data is small. However, when there is a large amount of labeled intermediate data, the performance of TriplexTLIT is better. The results are reasonable, because when we have large amount of data that are near and adaptable to the target data, we need not seek help from domains that are far away.
In the second setting, some overlap features in the intermediate domain are removed. We compare the TTL framework with
TriplexTLSIT. In each comparison experiment, we randomly remove d features ten times, and show the average performance and its variance in Figure 4(b). From the results we can see that the performance decreases as features are removed. The reason is that the connection between the intermediate and source/target domain becomes weaker when more features are removed.
Model Analysis
In the Appendix, we have theoretically proven the convergence of the transfer learning algorithm in the TTL framework. Here we test the convergence rate. We conduct an experiment on "compvs-talk" task, and set the number of iterations to 100. We show the objective value of Eq. (8) as the dashed line in Figure 5(a), and see that after around five to ten iterations, the objective value experiences almost no change. Similarly, we show the classification accuracy of the target domain of each iteration as the solid line in Table 4: Dataset Description
Task
Source
Intermediate
Target rec-vs-comp autos : misc baseball : mac hockey : windows rec-vs-talk autos : guns motorcycles : mideast hockey : misc rec-vs-sci autos : electronics motorcycles : med hockey : space sci-vs-comp electronics : graphics med : misc space : windows sci-vs-talk crypt : guns electronics : mideast med : misc comp-vs-talk graphics : guns misc : mideast windows : politic
Table 5: Accuracy (%) on the synthetic text classification tasks
SVMST
TriplexTLST
TriplexTLSIT
LMST
LMSIT
TTL rec-vs-comp
53.04 ± 1.87
56.74 ± 4.95
52.23 ± 2.97
55.34 ± 3.75
57.91 ± 3.27 rec-vs-talk
59.41 ± 9.74
61.67 ± 7.93
60.11 ± 7.22
60.97 ± 6.53
68.77 ± 1.61 rec-vs-sci
51.64 ± 1.44
51.95 ± 1.70
50.89 ± 2.13
51.23 ± 1.56
51.95 ± 0.98 sci-vs-comp
52.14 ± 2.65
55.93 ± 2.39
53.26 ± 2.95
55.29 ± 2.76
56.26 ± 2.14 sci-vs-talk
51.57 ± 2.07
52.80 ± 1.66
50.98 ± 2.14
52.69 ± 1.73
53.15 ± 1.53 comp-vs-talk
60.90 ± 9.35
64.08 ± 10.19
61.34 ± 9.73
64.58 ± 9.67
72.22 ± 3.20
Iteration
Objective Value
Accuracy(a) Convergence
The parameter p in TTL
Accuracy(b) Varying the value of p
Figure 5: Convergence analysis and model parameter analysis.
Figure 5(a). The results show that there is no change in the performance after 60-80 iterations. The convergence trends on other tasks are similar.
We also analyze the model parameter p. We vary p from 5 to
100 to test how it affects the classification performance. The experiments are also conducted on "comp-vs-talk" task. The results are shown in Figure 5(b), from which we can see that the algorithm achieves better performance when p is between 20 and 40. For different tasks, we can use ten-fold cross validation to choose the value. In this paper, we simply set p to be 30 in the experiments.
Text-to-image classification tasks
NUS-WISE data set
The NUS-WISE data set for heterogeneous transfer learning problem is generated by. It contains 45 text-to-image tasks. Each task is composed of 1200 text documents, 600 images, and 1600 co-occurred text-image pairs. The data in each task are about two different categories, such as "boat" and "flower". Therefore, we can do binary classification for each task. There are 10 categories in the data set, including "bird", "boat", "flower", "food", "rock", "sun", "tower", "toy", "tree" and "car". The text vocabulary size is 500. Each text data is represented by a 500 dimensional bagof-word vector. For image data, we extract SIFT features and represent each image in a 512 dimensional feature vector. In this
No. of labeled target data
Avg. Accuracy
TTL co−transfer
HTLIC
SVM(a) Average performance
Task
Classification Accuracy
TTL co−transfer
HTLIC
SVM(b) Detailed performance
Figure 6: The classification accuracy on the tasks of the textto-image data set. data set, our task is to transfer knowledge from source text documents to images through co-occurred text-image pairs.
Performance on text-to-image tasks
As HTLIC needs some labeled target domain data to train the SVM classifier, in the text-to-image tasks, we assume all the source domain data and a few target domain data are labeled. We vary the amount of labeled data in the target domain from 5 to 25, and show the average classification accuracies of all the tasks in Fig. 6(a), from which we can see that the performance of each algorithm increases when more labeled target data are used. We can also find that SVM achieves the worst performance, since it considers no auxiliary information. HTLIC and co-transfer achieve better performance than SVM, since they successfully leverage some knowledge from the source domain by using the intermediate domain data. The proposed TTL framework obtains the best performance.
The reason is that TTL takes the distribution shift between three domains into account and explicitly exploits the transitively shared knowledge for label propagation from the source to the target domain.
We also report the detailed results on each individual task with
25 labeled target domain data. The classification accuracies and variances on each task are shown in Fig. 6(b). The x-axis indicates
1161 the task and the y-axis represents the classification accuracy. We sort the tasks by the performance of the proposed TTL framework in ascend order. From the results, we can find that TTL is superior to other algorithms on most tasks and is always at the top. In addition, TTL is more stable than other algorithms.
Sentiment classification tasks
Sentiment Classification Data set
The sentiment classification data set used in our experiment consist of Amazon product reviews on 12 different categories, including "Apparel", "Books", "Camera_&_photo", "DVD", "Electronics", "Health_&_personal_care", "Kitchen_&_housewares", "Music", "Sports_&_outdoors", "Toys_&_games" and "Video". Each product review consists of review text and a sentiment label. The data from different domains have different distributions. For example, reviews in "Kitchen_&_housewares" may have adjectives such as "malfunctioning", "reliable" and "sturdy". However, reviews in the "DVD" domain may have "thrilling", "horrific" and "hilarious". In this data set, the data within each domain are balanced. One half of the data are positive reviews and the other half are negative. The data size in each domain ranges from 2,000 to
20,000. The vocabulary size for each domain is around 20,000. We randomly sample around 2,000 instances for each domain. From the 12 domains, we can generate P 3
12=1,320 triples, such as <"Apparel", "Books", "Camera_&_photo"> where "Apparel", "Books" and "Camera_&_photo" are the source, intermediate and target domains respectively. We conduct experiments on all the 1320 triple to evaluate the performance of the proposed intermediate domain selection algorithm. We also conduct experiments on triples that are selected by the intermediate domain selection algorithm to test the proposed transfer learning algorithm in the TTL framework.
Intermediate Domain Selection
In order to evaluate the proposed intermediate domain selection algorithm, we propagate labels from the labeled source domain data to the unlabeled target domain data, and evaluate the prediction accuracy accst on the target domain data. We also propagate labels from the labeled source domain data to the unlabeled intermediate and target domain data by the same algorithm, and evaluate the prediction accuracy accsit on the target domain data. In the experiment, we use semi-supervised learning with RBF kernel to do label propagation. If accsit > t × accst, (t > 1.0), it means that the intermediate domain data are able to bridge the source and target domain, and we assign a positive label to the triple. Otherwise, we assign a negative label. In the experiment, we set t = 1.03, and get 102 positive labels among 1,320 triples.
We then randomly split all the triples into two parts, each part contains the same number of positive and negative triples. The first part is used to train the intermediate domain selection algorithm, the second part is for testing. Since the data are unbalanced, we randomly sampled some negative triples to form a balanced data set. We do the random sampling ten times. Each time, we use 10fold cross validation to assess the performance of the intermediate domain selection algorithm on the first part. The average accuracy is 0.845 ± 0.034.
Performance on Sentiment Classification Tasks
We also test the proposed transfer learning algorithm in the TTL framework on some triples selected by the intermediate domain selection algorithm with high confidence from the second part. We learn the selection model on the training triples and select 10 triples with highest confidence from the testing triple set. The selected triples are listed in Table 6. Some results are interesting and explainable. For example, "video" domain is able to bridge the "music" and "apparel" domains. Intuitively, most music review words are about sound such as rhythm and melody. Most apparel reviews may talk about the appearance like the color. The video reviews contain both the vocal and visual aspects, and are able to draw the music and apparel domains close.
From the results in Table 6, we can see that TriplexST has almost the same results as SVMST. The direct transfer learning algorithm here achieves no performance improvement. This is because the source and target domains have large distribution gap. TTL and TriplexSIT are better than TriplexST. We can also see that TTL always achieves the best performance.
RELATED WORKS
We discuss two categories of research related to transitive transfer learning: transfer learning and multi-task learning.
Transfer Learning solves the lack of class label problem in the target domain by "borrowing" supervised knowledge from related source domains. There are mainly two typical types of algorithms. The first one is instance based knowledge transfer, which selects or adapts the weights of the relevant data from source domains for the target domain. The second one is feature based knowledge transfer, that transforms both source and target data into a common feature space where data follow similar distributions. More recently, multi-source transfer learning performs transfer learning with multiple source domains. For instance, the work in extends TrAdaboost by adding a wrapper boosting framework on weighting each source domain. Different from previous transfer learning, transitive transfer learning does not assume that the source domain and the target domain should be related.
That means, transitive learning can be more general and more useful when the existing labeled and related source domains are not adequate enough to improve the target domain.
Multi-task Learning algorithms simultaneously learn several tasks together and mutually enhance the classification results of each task. It assumes that different tasks share some natural "compact" representations, such as the information reflected by shared data clusters or subspaces. In practice, for example, classifiers for different tasks can be designed to share some global parameters or even a global classifier. More recently, approaches that learn the relationships between pairwise tasks are also being developed. However, these methods require reasonably large amounts of labeled data for each task to learn the relationship.
In contrast, transitive transfer learning works even when both intermediate and target domains are unlabeled. It only assumes that the source domain should have sufficient labeling information to transfer. The intermediate domain serves as a bridge between source and target domains. Even if the intermediate domain is not labeled, the classification information passed from the source domain still contributes to the final classification task through the latent factors learnt in the learning process.
CONCLUSIONS AND FUTURE WORK
In this paper, we study a new problem, transitive transfer learning (TTL), which transfers knowledge from a source domain to an indirectly related target domain with the help of some intermediate domains. We propose a TTL framework to solve the problem. The framework first selects one or more intermediate domains to bridge the given source and target domains, and then performs knowledge transfer along this bridge by capturing overlap hidden features among them. The experiments are conducted on three data
Table 6: Accuracy (%) on the Sentiment classification tasks
Source
Intermediate
Target
SV MST
TriplexST
TriplexSIT
TTL music video apperal
78.57 ± 1.84
78.51 ± 1.24
79.21 ± 1.47 health_&_personal_care baby books
74.15 ± 1.20
74.26 ± 1.21
75.38 ± 1.51 dvd toys_&_games apparel
80.10 ± 1.46
81.11 ± 1.46
83.57 ± 1.34 music toys_&_games baby
76.64 ± 1.52
77.64 ± 1.46
81.22 ± 1.37 books camera_&_photo apparel
80.38 ± 1.34
80.98 ± 1.21
82.74 ± 1.04 sports_&_outdoors video books
72.25 ± 1.58
73.00 ± 1.67
76.01 ± 1.05 video baby camera_&_photo
78.43 ± 1.06
79.42 ± 1.03
81.07 ± 1.06 dvd kitchen_&_housewares baby
78.01 ± 1.13
81.11 ± 1.05
81.42 ± 1.03 electronics baby toys_&_games
81.60 ± 1.63
81.95 ± 1.49
82.12 ± 0.09 electronics baby kitchen_&_housewares
83.52 ± 1.02
84.50 ± 1.06
85.63 ± 1.04 sets, showing that the proposed framework achieves state-of-theart performance. The convergence of the proposed TTL framework has also been theoretically and experimentally proven.
Future Work As a new learning problem, it raises several issues for further exploration in the future. For example, when the source and target need a string of domains to build a connection, how to find the string of intermediate domains to enable max transfer is a valuable research problem. In addition, extending the algorithm to multiple source domains may be an interesting way to generalize transitive transfer learning to be more powerful.
ACKNOWLEDGMENTS
We thank the support of China National 973 project 2014CB340304 and Hong Kong RGC Projects 621013, 620 812, and 621211. We also thank Yin Zhu, Lili Zhao, Zhongqi Lu, Kaixiang Mo and Ying
Wei for discussion.
REFERENCES
 R. K. Ando and T. Zhang. A framework for learning predictive structures from multiple tasks and unlabeled data.
JMLR, 6:1817–1853, Dec. 2005.
 J. Baxter. A bayesian/information theoretic model of learning to learn viamultiple task sampling. Machine
Learning, 28(1):7–39, 1997.
 S. Ben-David, J. Blitzer, K. Crammer, F. Pereira, et al.
Analysis of representations for domain adaptation. NIPS, S. Ben-David, J. Gehrke, and R. Schuller. A theoretical framework for learning from a pool of disparate data sources.
In KDD, pages 443–449, 2002.
 S. Ben-David and R. Schuller. Exploiting task relatedness for mulitple task learning. In COLT, pages 567–580, 2003.
 J. Blitzer, M. Dredze, and F. Pereira. Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. In ACL, Prague, Czech Republic, P. E. Bryant and T. Trabasso. Transitive inferences and memory in young children. Nature, 1971.
 O. Chapelle, P. Shivaswamy, S. Vadrevu, K. Weinberger, Y. Zhang, and B. Tseng. Multi-task learning for boosting with application to web search ranking. In KDD, pages
1189–1198, 2010.
 W. Dai, Q. Yang, G.-R. Xue, and Y. Yu. Boosting for transfer learning. In ICML, pages 193–200, 2007.
 C. Ding, T. Li, W. Peng, and H. Park. Orthogonal nonnegative matrix t-factorizations for clustering. In KDD, pages 126–135. ACM, 2006.
 T. Evgeniou and M. Pontil. Regularized multi–task learning.
In KDD, pages 109–117, 2004.
 Z. Kang, K. Grauman, and F. Sha. Learning with whom to share in multi-task feature learning. In ICML, pages
521–528, 2011.
 D. D. Lee and H. S. Seung. Algorithms for non-negative matrix factorization. In NIPS, pages 556–562. MIT Press, C.-K. Lin, Y.-Y. Lee, C.-H. Yu, and H.-H. Chen. Exploring ensemble of models in taxonomy-based cross-domain sentiment classification. CIKM '14, pages 1279–1288, New
York, NY, USA, 2014. ACM.
 M. Long, J. Wang, G. Ding, W. Cheng, X. Zhang, and W. Wang. Dual transfer learning. In SDM, pages 540–551.
SIAM, 2012.
 D. G. Lowe. Distinctive image features from scale-invariant keypoints. IJCV, 60:91–110, 2004.
 M. Ng, Q. Wu, and Y. Ye. Co-transfer learning using coupled markov chains with restart. 2013.
 S. J. Pan and Q. Yang. A survey on transfer learning. TKDE, 22(10):1345–1359, October 2010.
 W. Pan, N. N. Liu, E. W. Xiang, and Q. Yang. Transfer learning to predict missing ratings via heterogeneous user feedbacks. In IJCAI, pages 2318–2323, 2011.
 N. Ponomareva and M. Thelwall. Biographies or blenders:
Which resource is best for cross-domain sentiment analysis?
In CICLing, pages 488–499. Springer, 2012.
 M. T. Rosenstein, Z. Marx, L. P. Kaelbling, and T. G.
Dietterich. To transfer or not to transfer. In NIPS 2005
Workshop on Transfer Learning, volume 898, 2005.
 B. Tan, E. Zhong, M. Ng, and Q. Yang. Mixed-transfer: transfer learning over mixed graphs. In SDM, 2014.
 B. Tan, E. Zhong, W. Xiang, and Q. Yang. Multi-transfer:
Transfer learning with multiple views and multiple sources.
In SDM, 2013.
 M. E. Taylor and P. Stone. Transfer learning for reinforcement learning domains: A survey. JMLR, 10:1633–1685, Dec. 2009.
 S. Xie, W. Fan, J. Peng, O. Verscheure, and J. Ren. Latent space domain transfer between high dimensional overlapping distributions. In WWW, pages 91–100, 2009.
 Y. Yao and G. Doretto. Boosting for transfer learning with multiple sources. In CVPR, pages 1855–1862, 2010.
 Y. Zhang and D.-Y. Yeung. A convex formulation for learning task relationships in multi-task learning. In UAI, Y. Zhang and D.-Y. Yeung. Multi-task boosting by exploiting task relationships. In ECML/PKDD, pages 697–710, 2012.
 E. Zhong, W. Fan, Q. Yang, O. Verscheure, and J. Ren. Cross validation framework to choose amongst models and datasets for transfer learning. In ECML/PKDD, pages 547–562, 2010.
 X. Zhu. Semi-supervised learning literature survey. 2005.
 Y. Zhu, Y. Chen, Z. Lu, S. J. Pan, G.-R. Xue, Y. Yu, and Q. Yang. Heterogeneous transfer learning for image classification. In AAAI, 2011.
 F. Zhuang, P. Luo, C. Du, Q. He, and Z. Shi. Triplex transfer learning: exploiting both shared and distinct concepts for text classification. In WSDM, pages 425–434, 2013.
Appendix
Table 7: Notations of matrix multiplications
ˆ
M1 s = XsGs ˆA1T
ˆ
M2 s = XsGs ˆA2T s
ˆ
M1
I = XIGI ˆA1T
ˆ
M2
I = XIGI ˆA2T
I
ˆ
Ns = ˆF 1 ˆA1GT s + ˆF 2 s ˆA2 sGT s
ˆT 1 s = ˆ
NsGs ˆA1T
ˆ
NI = ˆF 1 ˆA1GT
I + ˆF 2
I ˆA2
IGT
I
ˆT 2 s = ˆ
NsGs ˆA2T s
ˆT 1
I = ˆ
NT GT ˆA1T
ˆT 2
I = ˆ
NIGI ˆA2T
I
FI = [ ˆF1 ˆF 2
I ]
AI = [ ˆA1 ˆA2
I]
F ′
I = [ ˜F1 ˜F 2
I ]
A′
I = [ ˜A1 ˜A2
I]
We summarize some other matrix multiplication notations in Table 7, and give the update rules for ˆFs, ˆAs, ˆFI and ˆAI as follow:
ˆF 1(i, j) = ˆF 1(i, j) ×
�
[ ˆ
M1s+ ˆ
M1
I](i,j)
[ ˆ
T 1 s + ˆ
T 1
I ](i,j), ˆF 2 s (i, j) = ˆF 2 s (i, j) ×
�
ˆ
M2s(i,j)
ˆ
T 2 s (i,j), ˆF 2
I (i, j) = ˆF 2
I (i, j) ×
�
ˆ
M2
I(i,j)
ˆ
T 2
I (i,j), ˆA1(i, j) = ˆA1(i, j) ×
�
[ ˆ
F 1T (XsGs+XIGI)](i,j)
[ ˆ
F 1T ( ˆ
NsGs+ ˆ
NIGI)](i,j), ˆA2 s(i, j) = ˆA2 s(i, j) ×
�
[ ˆ
F 2T s
XsGs](i,j)
[ ˆ
F 2T s
ˆ
NsGs](i,j), ˆA2
I(i, j) = ˆA2
I(i, j) ×
�
[ ˆ
F 2T
I
XIGI](i,j)
[ ˆ
F 2T
I
ˆ
NIGI](i,j), GI(i, j) = GI(i, j) ×
�
[XT
I F ′
IA′
I+XT
I FIAI](i,j)
[GIA′T
I
F ′T
I
F ′
IA′
I+GIAT
I F T
I FIAI](i,j)
The normalization methods for ˆFs and ˆFI are:
ˆFs(i, j) =
ˆ
Fs(i,j)
�m i=1 ˆ
Fs(i,j), ˆFI(i, j) =
ˆ
FI(i,j)
�m i=1 ˆ
FI(i,j), Convergence Analysis
We first analyze the convergence of ˆF 1 with the rest parameters are fixed. By using the properties of trace operation and frobenius norm ||X||2 = tr(XT X) = tr(XXT ), we re-formulate the objective function Eq. (8) as a Lagrangian function and keep the terms related to ˆF 1:
L( ˆF 1) = tr(−2XT s ˆF 1 ˆA1GT s + 2Gs ˆA1T ˆF 1T ˆ
Ns)
+tr(−2XT
I ˆF 1 ˆA1GT
I + 2GI ˆA1T ˆF 1T ˆ
NI)
+tr[λ( ˆF 1T 1m1T m ˆF 1 − 21p1T m ˆF 1)], (14) where λ ∈ Rp×p is a diagonal matrix. 1m and 1p are all-ones vectors with dimension ms and p respectively. The differential of Eq. (14) is:
∂L( ˆ
F 1)
∂ ˆ
F 1
= tr(−2XsGs ˆA1T + 2 ˆ
NsGs ˆA1T )
+tr(−2XIYt ˆA1T + 2 ˆ
NtGt ˆA1T )
+21m(1T m ˆF 1 − 1T p )λ, Then, we obtain the temporary updating rule:
ˆF 1(i, j) = ˆF 1(i, j) ×
�
[XsGs ˆ
A1T +XIGI ˆ
A1T +1m1T p λ](i,j)
[ ˆ
NsGs ˆ
A1T + ˆ
NIGI ˆ
A1T +1m1T m ˆ
F 1λ](i,j), As proved in, the temporary update rule in Eq. (16) is able to monotonously decrease the Eq. (14). Therefore, there is still one variable λ that needs further calculation. Considering the constrains in Eq. (8), we find that λ is used to satisfy the conditions that the summation of each column of ˆF 1 has to be equal to one. We use the the normalization method in Eq. (13) to normalize ˆF 1. The method satisfies the condition regardless of λ. After that, 1m1T p λ is equal to 1m1T m ˆF 1λ. By getting rid of the terms that contain λ, we get the final update rule in Eq. (12) that is approximately equal to Eq. (16) in terms of convergence, since both 1m1T p λ and 1m1T m ˆF 1λ are constants. Using update rule in Eq. (12) will also monotonously decrease the value of Eq. (14).
We can use similar methodology to analyze the convergence of the update rules and normalization methods for other terms in Eq. (8).
According to the Multiplicative Update Rules in, using the update rules in Eq. (9) and Eq. (12) and using the normalization methods in Eq. (10) and Eq. (13), the value of the objective function in Eq. (8) will not increase. The objective function has a zero lower bound. The convergence of Algorithm 1 is guaranteed.Manuscript accepted by Neurocomputing 2018
Deep Visual Domain Adaptation: A Survey
Mei Wang, Weihong Deng
School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China. wm0245@126.com, whdeng@bupt.edu.cn
Abstract—Deep domain adaptation has emerged as a new learning technique to address the lack of massive amounts of labeled data. Compared to conventional methods, which learn shared feature subspaces or reuse important source instances with shallow representations, deep domain adaptation methods leverage deep networks to learn more transferable representations by embedding domain adaptation in the pipeline of deep learning. There have been comprehensive surveys for shallow domain adaptation, but few timely reviews the emerging deep learning based methods. In this paper, we provide a comprehensive survey of deep domain adaptation methods for computer vision applications with four major contributions. First, we present a taxonomy of different deep domain adaptation scenarios according to the properties of data that define how two domains are diverged. Second, we summarize deep domain adaptation approaches into several categories based on training loss, and analyze and compare briefly the state-of-the-art methods under these categories. Third, we overview the computer vision applications that go beyond image classification, such as face recognition, semantic segmentation and object detection. Fourth, some potential deficiencies of current methods and several future directions are highlighted.
I. INTRODUCTION
Over the past few years, machine learning has achieved great success and has benefited real-world applications. However, collecting and annotating datasets for every new task and domain are extremely expensive and time-consuming processes, sufficient training data may not always be available.
Fortunately, the big data era makes a large amount of data available for other domains and tasks. For instance, although large-scale labeled video databases that are publicly available only contain a small number of samples, statistically, the YouTube face dataset (YTF) consists of 3.4K videos. The number of labeled still images is more than sufficient.
Hence, skillfully using the auxiliary data for the current task with scarce data will be helpful for real-world applications.
However, due to many factors (e.g., illumination, pose, and image quality), there is always a distribution change or domain shift between two domains that can degrade the performance, as shown in Fig. 1. Mimicking the human vision system, domain adaptation (DA) is a particular case of transfer learning (TL) that utilizes labeled data in one or more relevant source domains to execute new tasks in a target domain. Over the past decades, various shallow DA methods have been proposed to solve a domain shift between the source and target domains. The common algorithms for shallow DA can mainly be categorized into two classes: instance-based DA, and feature-based DA,,,. The first class reduces the discrepancy by reweighting the source samples, and it trains on the weighted source samples. For the second class, a common shared space is generally learned in which the distributions of the two datasets are matched.
Recently, neural-network-based deep learning approaches have achieved many inspiring results in visual categorization applications, such as image classification, face recognition, and object detection. Simulating the perception of the human brain, deep networks can represent high-level abstractions by multiple layers of non-linear transformations.
Existing deep network architectures include convolutional neural networks (CNNs),,,, deep belief networks (DBNs), and stacked autoencoders (SAEs), among others. Although some studies have shown that deep networks can learn more transferable representations that disentangle the exploratory factors of variations underlying the data samples and group features hierarchically in accordance with their relatedness to invariant factors, Donahue et al.
 showed that a domain shift still affects their performance. The deep features would eventually transition from general to specific, and the transferability of the representation sharply decreases in higher layers. Therefore, recent work has addressed this problem by deep DA, which combines deep learning and DA.
There have been other surveys on TL and DA over the past few years,,,,,. Pan et al. categorized TL under three subsettings, including inductive TL, transductive TL, and unsupervised TL, but they only studied homogeneous feature spaces. Shao et al. categorized TL techniques into feature-representation-level knowledge transfer and classifier-level knowledge transfer.
The survey written by Patel only focused on DA, a subtopic of TL. discussed 38 methods for heterogeneous
TL that operate under various settings, requirements, and domains. Zhang et al. were the first to summarize several transferring criteria in detail from the concept level. These five surveys mentioned above only cover the methodologies on shallow TL or DA. The work presented by Csurka et al.
 briefly analyzed the state-of-the-art shallow DA methods and categorized the deep DA methods into three subsettings based on training loss: classification loss, discrepancy loss and adversarial loss. However, Csurka's work mainly focused on shallow methods, and it only discussed deep DA in image classification applications.
In this paper, we focus on analyzing and discussing deep
DA methods. Specifically, the key contributions of this survey are as follows: 1) we present a taxonomy of different deep
DA scenarios according to the properties of data that define how two domains are diverged. 2) extending Csurka's work, we improve and detail the three subsettings (training with arXiv:1802.03601v4 [cs.CV] 25 May 2018
Manuscript accepted by Neurocomputing 2018
Amazon
Webcam
DSLR
Caltech-256
MNIST
USPS
SVHN(a)(b)
CUFS
BCS(c)
LFW
Fig. 1. (a) Some object images from the "Bike" and "Laptop" categories in Amazon, DSLR, Webcam, and Caltech-256 databases. (b) Some digit images from MNIST, USPS, and SVHN databases. (c) Some face images from LFW, BCS and CUFS databases. Realworld computer vision applications, such as face recognition, must learn to adapt to distributions specific to each domain. classification loss, discrepancy loss and adversarial loss) and summarize different approaches used in different DA scenes.
3) Considering the distance of the source and target domains, multi-step DA methods are studied and categorized into handcrafted, feature-based and representation-based mechanisms.
4) We provide a survey of many computer vision applications, such as image classification, face recognition, style translation, object detection, semantic segmentation and person reidentification.
The remainder of this survey is structured as follows.
In Section II, we first define some notations, and then we categorize deep DA into different settings (given in Fig. 2). In the next three sections, different approaches are discussed for each setting, which are given in Table I and Table II in detail.
Then, in Section VI, we introduce some successful computer vision applications of deep DA. Finally, the conclusion of this paper and discussion of future works are presented in Section
VII.
II. OVERVIEW
A. Notations and Definitions
In this section, we introduce some notations and definitions that are used in this survey. The notations and definitions match those from the survey papers by, to maintain consistency across surveys. A domain D consists of a feature space X and a marginal probability distribution P(X), where X = {x1,..., xn} ∈ X. Given a specific domain
D = {X, P(X)}, a task T consists of a feature space Y and an objective predictive function f(·), which can also be viewed as a conditional probability distribution P(Y |X) from a probabilistic perspective. In general, we can learn P(Y |X) in a supervised manner from the labeled data {xi, yi}, where xi ∈ X and yi ∈ Y.
Assume that we have two domains: the training dataset with sufficient labeled data is the source domain Ds
=
{X s, P(X)s}, and the test dataset with a small amount of labeled data or no labeled data is the target domain Dt =
{X t, P(X)t}. We see that the partially labeled part, Dtl, and the unlabeled parts, Dtu, form the entire target domain, that is, Dt = Dtl ∪ Dtu. Each domain is together with its task: the former is T s = {Ys, P(Y s|Xs)}, and the latter is T t = {Yt, P(Y t|Xt)}. Similarly, P(Y s|Xs) can be learned from the source labeled data {xs i, ys i }, while P(Y t|Xt) can be learned from labeled target data {xtl i, ytl i } and unlabeled data {xtu i }.
B. Different Settings of Domain Adaptation
The case of traditional machine learning is Ds = Dt and T s = T t. For TL, Pan et al. summarized that the differences between different datasets can be caused by domain divergence Ds ̸= Dt (i.e., distribution shift or feature space difference) or task divergence T s ̸= T t (i.e., conditional distribution shift or label space difference), or both. Based on this summary, Pan et al. categorized TL into three main groups: inductive, transductive and unsupervised TL.
According to this classification, DA methods are transductive TL solutions with the assumption that the tasks are the same, i.e., T s = T t, and the differences are only caused by domain divergence, Ds ̸= Dt. Therefore, DA can be split into two main categories based on different domain divergences (distribution shift or feature space difference): homogeneous and heterogeneous DA. Then, we can further categorize DA into supervised, semi-supervised and unsupervised DA in consideration of labeled data of the target domain.
The classification is given in Fig. 2.
• In the homogeneous DA setting, the feature spaces between the source and target domains are identical(X s = X t) with the same dimension (ds = dt). Hence, the source and target datasets are generally different in terms of data distributions (P(X)s ̸= P(X)t).
In addition, we can further categorize the homogeneous DA setting into three cases:
1) In the supervised DA, a small amount of labeled target data, Dtl, are present. However, the labeled data are commonly not sufficient for tasks.
Manuscript accepted by Neurocomputing 2018
One-step
Domain adaptation
Homogeneous
Heterogeneous
Supervised
Semi-Supervised
Unsupervised
Supervised
Semi-Supervised
Unsupervised
Labeled data are available in target domain
Labeled+unlabeled data are available in target domain
No labeled data in target domain
Labeled data are available in target domain
Labeled+unlabeled data are available in target domain
No labeled data in target domain
Feature Space is same between source and target domain
Feature Space is different between source and target domain
Multi-step
Domain adaptation
Domain adaptation
Select Intermediate Domain
Fig. 2. An overview of different settings of domain adaptation
2) In the semi-supervised DA, both limited labeled data, Dtl, and redundant unlabeled data, Dtu, in the target domain are available in the training stage, which allows the networks to learn the structure information of the target domain.
3) In the unsupervised DA, no labeled but sufficient unlabeled target domain data, Dtu, are observable when training the network.
• In the heterogeneous DA setting, the feature spaces between the source and target domains are nonequivalent(X s ̸= X t), and the dimensions may also generally differ(ds ̸= dt).
Similar to the homogeneous setting, the heterogeneous DA setting can also be divided into supervised, semi-supervised and unsupervised DA.
All of the above DA settings assumed that the source and target domains are directly related; thus, transferring knowledge can be accomplished in one step. We call them onestep DA. In reality, however, this assumption is occasionally unavailable. There is little overlap between the two domains, and performing one-step DA will not be effective. Fortunately, there are some intermediate domains that are able to draw the source and target domains closer than their original distance.
Thus, we use a series of intermediate bridges to connect two seemingly unrelated domains and then perform onestep DA via this bridge, named multi-step (or transitive) DA,. For example, face images and vehicle images are dissimilar between each other due to different shapes or other aspects, and thus, one-step DA would fail. However, some intermediate images, such as 'football helmet', can be introduced to be an intermediate domain and have a smooth knowledge transfer. Fig. 3 shows the differences between the learning processes of one-step and multi-step DA techniques.
III. APPROACHES OF DEEP DOMAIN ADAPTATION
In a broad sense, deep DA is a method that utilizes a deep network to enhance the performance of DA. Under this definition, shallow methods with deep features,,,, can be considered as a deep DA approach. DA is adopted by shallow methods, whereas deep networks only extract vectorial features and are not helpful for transferring knowledge directly. For example, extracted the convolutional activations from a CNN as the tensor representation, and then performed tensor-aligned invariant subspace learning to realize DA. This approach reliably outperforms current state-of-the-art approaches based on traditional hand-crafted features because sufficient representational and transferable features can be extracted through deep networks, which can work better on discrimination tasks.
In a narrow sense, deep DA is based on deep learning architectures designed for DA and can obtain a firsthand effect from deep networks via back-propagation. The intuitive idea is to embed DA into the process of learning representation and to learn a deep feature representation that is both semantically meaningful and domain invariant. With the "good" feature representations, the performance of the target task would improve significantly. In this paper, we focus on the narrow definition and discuss how to utilize deep networks to learn
"good" feature representations with extra training criteria.
A. Categorization of One-Step Domain Adaptation
In one-step DA, the deep approaches can be summarized into three cases, which refers to. Table 1 shows these three cases and brief descriptions. The first case is the discrepancybased deep DA approach, which assumes that fine-tuning the deep network model with labeled or unlabeled target data can diminish the shift between the two domains. Class criterion, statistic criterion, architecture criterion and geometric criterion are four major techniques for performing fine-tuning.
• Class Criterion: uses the class label information as a guide for transferring knowledge between different domains. When the labeled samples from the target domain are available in supervised DA, soft label and metric learning are always effective,,,,. When such samples are unavailable, some other techniques can be adopted to substitute for class labeled data, such as pseudo labels,,, and attribute representation,.
• Statistic Criterion: aligns the statistical distribution shift between the source and target domains using some mechanisms. The most commonly used methods for comparing and reducing distribution shift are maximum mean discrepancy (MMD),,,,,, correlation alignment (CORAL),, KullbackLeibler (KL) divergence and H divergence, among others.
• Architecture Criterion: aims at improving the ability of learning more transferable features by adjusting the architectures of deep networks. The techniques that are
Manuscript accepted by Neurocomputing 2018
Different Domain
Learning System
Learning System
Learning System intermediate
Knowledge
Knowledge
Learning System
Source Domain
Knowledge
Learning System
Target Domain
Target Domain
Source Domain
Traditional Machine Learning
One-step Domain Adaptation
Multi-step Domain Adaptation(a)(b)(c)
One-Step
Fig. 3. Different learning processes between (a) traditional machine learning, (b) one-step domain adaptation and (c) multi-step domain adaptation.
TABLE I
DIFFERENT DEEP APPROACHES TO ONE-STEP DA
One-step DA
Approaches
Brief Description
Subsettings
Discrepancy-based fine-tuning the deep network with labeled or unlabeled target data to diminish the domain shift class criterion,,,,,,,,,, statistic criterion,,,,,,, architecture criterion,,,,, geometric criterion 
Adversarial-based using domain discriminators to encourage domain confusion through an adversarial objective generative models,, non-generative models,,,, 
 
Reconstructionbased using the data reconstruction as an auxiliary task to ensure feature invariance encoder-decoder reconstruction,,, adversarial reconstruction,, 
TABLE II
DIFFERENT DEEP APPROACHES TO MULTI-STEP DA
Multi-step Approaches
Brief Description
Hand-crafted users determine the intermediate domains based on experience 
Instance-based selecting certain parts of data from the auxiliary datasets to compose the intermediate domains, 
Representation-based freeze weights of one network and use their intermediate representations as input to the new network proven to be cost effective include adaptive batch normalization (BN),,, weak-related weight, domain-guided dropout, and so forth.
• Geometric Criterion: bridges the source and target domains according to their geometrical properties. This criterion assumes that the relationship of geometric structures can reduce the domain shift.
The second case can be referred to as an adversarial-based deep DA approach. In this case, a domain discriminator that classifies whether a data point is drawn from the source or target domain is used to encourage domain confusion through an adversarial objective to minimize the distance between the empirical source and target mapping distributions.
Furthermore, the adversarial-based deep DA approach can be categorized into two cases based on whether there are generative models.
• Generative Models: combine the discriminative model with a generative component in general based on generative adversarial networks (GANs). One of the typical cases is to use source images, noise vectors or both to generate simulated samples that are similar to the target samples and preserve the annotation information of the source domain,,.
• Non-Generative Models: rather than generating models with input image distributions, the feature extractor learns a discriminative representation using the labels in the source domain and maps the target data to the same space through a domain-confusion loss, thus resulting in the domain-invariant representations,,,,.
The third case can be referred to as a reconstruction-based
DA approach, which assumes that the data reconstruction of the source or target samples can be helpful for improving the performance of DA. The reconstructor can ensure both specificity of intra-domain representations and indistinguishability of inter-domain representations.
• Encoder-Decoder Reconstruction: by using stacked autoencoders (SAEs), encoder-decoder reconstruction methods combine the encoder network for representation learning with a decoder network for data reconstruction,,,.
• Adversarial Reconstruction: the reconstruction error is measured as the difference between the reconstructed and original images within each image domain by a cyclic mapping obtained via a GAN discriminator, such as dual
GAN, cycle GAN and disco GAN.
Manuscript accepted by Neurocomputing 2018
TABLE III
DIFFERENT APPROACHES USED IN DIFFERENT DOMAIN ADAPTATION SETTINGS
Supervised DA
Unsupervised DA
Discrepancy-based
Class Criterion
√
Statistic Criterion
√
Architecture Criterion
√
√
Geometric Criterion
√
Adversarial-based
Generative Model
√
Non-Generative Model
√
Reconstruction-based
Encoder-Decoder Model
√
Adversarial Model
√
B. Categorization of Multi-Step Domain Adaptation
In multi-step DA, we first determine the intermediate domains that are more related with the source and target domains than their direct connection. Second, the knowledge transfer process will be performed between the source, intermediate and target domains by one-step DA with less information loss. Thus, the key of multi-step DA is how to select and utilize intermediate domains; additionally, it can fall into three categories referring to : hand-crafted, feature-based and representation-based selection mechanisms.
• Hand-Crafted: users determine the intermediate domains based on experience.
• Instance-Based: selecting certain parts of data from the auxiliary datasets to compose the intermediate domains to train the deep network,.
• Representation-Based: transfer is enabled via freezing the previously trained network and using their intermediate representations as input to the new one.
IV. ONE-STEP DOMAIN ADAPTATION
As mentioned in Section II-A, the data in the target domain have three types regardless of homogeneous or heterogeneous
DA: 1) supervised DA with labeled data, 2) semi-supervised
DA with labeled and unlabeled data and 3) non-supervised
DA with unlabeled data. The second setting is able to be accomplished by combining the methods of setting 1 and setting 3; thus, we only focus on the first and third settings in this paper. The cases where the different approaches are mainly used for each DA setting are shown in Table III. As shown, more work is focused on unsupervised scenes because supervised DA has its limitations. When only few labeled data in the target domain are available, using the source and target labeled data to train parameters of models typically results in overfitting to the source distribution. In addition, the discrepancy-based approaches have been studied for years and produced more methods in many research works, whereas the adversarial-based and reconstruction-based approaches are a relatively new research topic but have recently been attracting more attention.
A. Homogeneous Domain Adaptation
1) Discrepancy-Based Approaches:
Yosinski et al. proved that transferable features learned by deep networks have limitations due to fragile co-adaptation and representation specificity and that fine-tuning can enhance generalization performance. Fine-tuning (can also be viewed as a discrepancybased deep DA approach) is to train a base network with source data and then directly reuse the first n layers to conduct a target network. The remaining layers of the target network are randomly initialized and trained with loss based on discrepancy. During training, the first n layers of the target network can be fine-tuned or frozen depending on the size of the target dataset and its similarity to the source dataset. Some common rules of thumb for navigating the 4 major scenarios are given in Table IV.
Fig. 4.
The average accuracy over the validation set for a network trained with different strategies. Baseline B: the network is trained on dataset B. 2)
BnB: the first n layers are reused from baseline B and frozen. The higher layers are trained on dataset B. 3) BnB+: the same as BnB but where all layers are fine-tuned. 4) AnB: the first n layers are reused from the network trained on dataset A and frozen. The higher layers are trained on dataset B.
5) AnB+: the same as AnB but where all layers are fine-tuned.
• Class Criterion
The class criterion is the most basic training loss in deep
DA. After pre-training the network with source data, the remaining layers of the target model use the class label information as a guide to train the network. Hence, a small number of labeled samples from the target dataset is assumed to be available.
Ideally, the class label information is given directly in supervised DA. Most work commonly uses the negative loglikelihood of the ground truth class with softmax as their training loss, L = − �N i=0 yi log ˆyi (ˆyi are the softmax predictions of the model, which represent class probabilities),,,. To extend this, Hinton et al. modified the softmax function to soft label loss: qi = exp(zi/T)
� j (exp(zj/T))
Manuscript accepted by Neurocomputing 2018
TABLE IV
SOME COMMON RULES OF THUMB FOR DECIDING FINE-TUNED OR FROZEN IN THE FIRST N LAYERS. 
The Size of Target Dataset
Low
Medium
High
The Distance
Low
Freeze
Try Freeze or Tune
Tune between Medium
Try Freeze or Tune
Tune
Tune
Source and Target
High
Try Freeze or Tune
Tune
Tune where zi is the logit output computed for each class. T is a temperature that is normally set to 1 in standard softmax, but it takes a higher value to produce a softer probability distribution over classes. By using it, much of the information about the learned function that resides in the ratios of very small probabilities can be obtained. For example, when recognizing digits, one version of 2 may obtain a probability of 106 of being a 3 and 109 of being a 7; in other words, this version of 2 looks more similar to 3 than 7. Inspired by Hinton, fine-tuned the network by simultaneously minimizing the domain confusion loss (belonging to adversarial-based approaches, which will be presented in Section IV-A2) and soft label loss. Using soft labels rather than hard labels can preserve the relationships between classes across domains. Gebru et al.
 modified existing adaptation algorithms based on and utilized soft label loss at the fine-grained class level Lcsoft and attribute level Lasoft.
Fig. 5.
Deep DA by combining domain confusion loss and soft label loss.
In addition to softmax loss, there are other methods that can be used as training loss to fine-tune the target model in supervised DA. Embedding metric learning in deep networks is another method that can make the distance of samples from different domains with the same labels be closer while those with different labels are far away. Based on this idea, constructed the semantic alignment loss and the separation loss accordingly. Deep transfer metric learning is proposed by, which applies the marginal Fisher analysis criterion and MMD criterion (described in Statistic Criterion) to minimize their distribution difference: min J = S(M) c
− αS(M) b
+ βD(M) ts
�
X s, X t�
+γ
M
� m=1
���W (m)���
F +
���b(m)���(2) where α, β and γ are regularization parameters and W (m) and b(m) are the weights and biases of the mth layer of the network. D(M) ts(X s, X t) is the MMD between representations of the source and target domains. Sc and Sb define the intraclass compactness and the interclass separability.
However, what can we do if there is no class label information in the target domain directly? As we all know, humans can identify unseen classes given only a high-level description. For instance, when provided the description "tall brown animals with long necks", we are able to recognize giraffes.
To imitate the ability of humans, introduced high-level semantic attributes per class. Assume that ac = (ac
1,..., ac m) is the attribute representation for class c, which has fixedlength binary values with m attributes in all the classes. The classifiers provide estimates of p(am|x) for each attribute am.
In the test stage, each target class y obtains its attribute vector ay in a deterministic way, i.e., p(a|y) = [[a = ay]]. By applying
Bayes rule, p(y|a) = p(y) p(ay)[[a = ay]], the posterior of a test class can be calculated as follows: p(y|x) =
� a∈{0,1}M p(y|a)p(a|x) = p(y) p(ay)
M
� m=1 p(ay m|x)
Gebru et al. drew inspiration from these works and leveraged attributes to improve performance in the DA of finegrained recognition. There are multiple independent softmax losses that simultaneously perform attribute and class level to fine-tune the target model. To prevent the independent classifiers from obtaining conflicting labels with attribute and class level, an attribute consistency loss is also implemented.
Occasionally, when fine-tuning the network in unsupervised
DA, a label of target data, which is called a pseudo label, can preliminarily be obtained based on the maximum posterior probability. Yan et al. initialized the target model using the source data and then defined the class posterior probability p(yt j = c|xt j) by the output of the target model.
With p(yt j = c|xt j), they assigned pseudo-label �yt j to xt j by
�yt j = arg max c p(yt j = c|xt j). In, two different networks assign pseudo-labels to unlabeled samples, another network is trained by the samples to obtain target discriminative representations. The deep transfer network (DTN) used some base classifiers, e.g., SVMs and MLPs, to obtain the pseudo
Manuscript accepted by Neurocomputing 2018
7 labels for the target samples to estimate the conditional distribution of the target samples and match both the marginal and the conditional distributions with the MMD criterion. When casting the classifier adaptation into the residual learning framework, used the pseudo label to build the conditional entropy E(Dt, f t), which ensures that the target classifier f t fits the target-specific structures well.
• Statistic Criterion
Although some discrepancy-based approaches search for pseudo labels, attribute labels or other substitutes to labeled target data, more work focuses on learning domain-invariant representations via minimizing the domain distribution discrepancy in unsupervised DA.
MMD is an effective metric for comparing the distributions between two datasets by a kernel two-sample test. Given two distributions s and t, the MMD is defined as follows:
MMD2(s, t) = sup
∥φ∥H≤1
��Exs∼s[φ(xs)] − Ext∼s[φ(xt)]
��2
H(4) where φ represents the kernel function that maps the original data to a reproducing kernel Hilbert space (RKHS) and ∥φ∥H ≤ 1 defines a set of functions in the unit ball of RKHS
H.
Based on the above, Ghifary et al. proposed a model that introduced the MMD metric in feedforward neural networks with a single hidden layer. The MMD metric is computed between representations of each domain to reduce the distribution mismatch in the latent space. The empirical estimate of MMD is as follows:
MMD2(Ds, Dt) =
������
M
M
� i=1 φ(xs i)− 1
N
N
� j=1 φ(xt j)
������
H
Subsequently, Tzeng et al. and Long et al. extended MMD to a deep CNN model and achieved great success. The deep domain confusion network (DDC) by Tzeng et al. used two CNNs for the source and target domains with shared weights. The network is optimized for classification loss in the source domain, while domain difference is measured by an adaptation layer with the MMD metric.
L=LC(XL, y) + λMMD2(XsXt)(6) where the hyperparameter λ is a penalty parameter.
LC(XL, y) denotes classification loss on the available labeled data, XL, and the ground-truth labels, y. MMD2(XsXt) denotes the distance between the source and target data. DDC only adapts one layer of the network, resulting in a reduction in the transferability of multiple layers. Rather than using a single layer and linear MMD, Long et al. proposed the deep adaptation network (DAN) that matches the shift in marginal distributions across domains by adding multiple adaptation layers and exploring multiple kernels, assuming that the conditional distributions remain unchanged. However, this assumption is rather strong in practical applications; in other words, the source classifier cannot be directly used in the target domain. To make it more generalized, a joint adaptation network (JAN) aligns the shift in the joint distributions of input features and output labels in multiple domain-specific layers based on a joint maximum mean discrepancy (JMMD) criterion. proposed DTN, where both the marginal and the conditional distributions are matched based on MMD. The shared feature extraction layer learns a subspace to match the marginal distributions of the source and the target samples, and the discrimination layer matches the conditional distributions by classifier transduction. In addition to adapting features using MMD, residual transfer networks (RTNs) added a gated residual layer for classifier adaptation. More recently, proposed a weighted MMD model that introduces an auxiliary weight for each class in the source domain when the class weights in the target domain are not the same as those in the source domain.
If φ is a characteristic kernel (i.e., Gaussian kernel or Laplace kernel), MMD will compare all the orders of statistic moments. In contrast to MMD, CORAL learned a linear transformation that aligns the second-order statistics between domains. Sun et al. extended CORAL to deep neural networks (deep CORAL) with a nonlinear transformation.
LCORAL= 1
4d2 ∥CS − CT ∥2
F(7) where ∥ · ∥2
F denotes the squared matrix Frobenius norm. CS and CT denote the covariance matrices of the source and target data, respectively.
By the Taylor expansion of the Gaussian kernel, MMD can be viewed as minimizing the distance between the weighted sums of all raw moments. The interpretation of MMD as moment matching procedures motivated Zellinger et al.
 to match the higher-order moments of the domain distributions, which we call central moment discrepancy (CMD).
An empirical estimate of the CMD metric for the domain discrepancy in the activation space [a, b]N is given by
CMDK(Xs, Xt) =(b − a)
��E(Xs) − E(Xt)
��
K
� k=2
|b − a|k
��Ck(Xs) − Ck(Xt)
��(8) where Ck(X) = E((x − E(X))k is the vector of all kthorder sample central moments and E(X) =
|X|
� x∈X x is the empirical expectation.
The association loss Lassoc proposed by is an alternative discrepancy measure, it enforces statistical associations between source and target data by making the two-step roundtrip probabilities P aba ij be similar to the uniform distribution over the class labels.
• Architecture Criterion
Some other methods optimize the architecture of the network to minimize the distribution discrepancy. This adaptation behavior can be achieved in most deep DA models, such as supervised and unsupervised settings.
Rozantsev et al. considered that the weights in corresponding layers are not shared but related by a weight regularizer rw(·) to account for the differences between the Manuscript accepted by Neurocomputing 2018(c) The Residual Transfer Network (RTN) architecture(a) The Deep Adaptation Network (DAN) architecture(b) The Joint Adaptation Network (JAN) architecture
Fig. 6.
Different approaches with the MMD metric. (a) The deep adaptation network (DAN) architecture, (b) the joint adaptation network (JAN) architecture and (c) the residual transfer network (RTN) architecture. two domains. The weight regularizer rw(·) can be expressed as the exponential loss function: rw(θs j, θt j) = exp
���θs j − θt j
��2�
− 1(9) where θs j and θt j denote the parameters of the jth layer of the source and target models, respectively. To further relax this restriction, they allow the weights in one stream to undergo a linear transformation: rw(θs j, θt j) = exp(
��ajθs j + bj − θt j
��2) − 1(10) where aj and bj are scalar parameters that encode the linear transformation. The work of Shu et al. is similar to
 using weakly parameter-shared layers. The penalty term
Ω controls the relatedness of parameters.
Ω=
L
� i=1
���W (l)
S
− W (l)
T
���
F +
���b(l)
S − b(l)
T
���
F )(11) where {W (l)
S, b(l)
S }L l=1 and {W (l)
T, b(l)
T }L l=1 are the parameters of the lth layer in the source and target domains, respectively.
Fig. 7. The two-stream architecture with related weight.
Li et al. hypothesized that the class-related knowledge is stored in the weight matrix, whereas domain-related knowledge is represented by the statistics of the batch normalization (BN) layer. BN normalizes the mean and standard deviation for each individual feature channel such that each layer receives data from a similar distribution, irrespective of whether it comes from the source or the target domain.
Therefore, Li et al. used BN to align the distribution for recomputing the mean and standard deviation in the target domain.
BN(Xt) = λ
�x − µ(Xt) σ(Xt)
�
+ β(12) where λ and β are parameters learned from the target data and µ(x) and σ(x) are the mean and standard deviation computed independently for each feature channel. Based on, endowed BN layers with a set of alignment parameters which can be learned automatically and can decide the degree of feature alignment required at different levels of the deep network. Furthermore, Ulyanov et al. found that when replacing BN layers with instance normalization (IN) layers, where µ(x) and σ(x) are computed independently for each channel and each sample, the performance of DA can be further improved.
Occasionally, neurons are not effective for all domains because of the presence of domain biases. For example, when recognizing people, the target domain typically contains one person centered with minimal background clutter, whereas the source dataset contains many people with more clutter. Thus, the neurons that capture the features of other people and clutter are useless. Domain-guided dropout was proposed by to solve the problem of multi-DA, and it mutes non-related neurons for each domain. Rather than assigning dropout with a specific dropout rate, it depends on the gain of the loss function of each neuron on the domain sample when the neuron is removed. si = L(g(x)\i) − L(g(x))(13) where L is the softmax loss function and g(x)\i is the feature vector after setting the response of the ith neuron to zero. In, each source domain is assigned with different parameters, Θ(i) = Θ(0) + ∆(i), where Θ(0) is a domain general model, and ∆(i) is a domain specific bias term. After the low rank parameterized CNNs are trained, Θ(0) can serve as the classifier for target domain.
• Geometric Criterion
The geometric criterion mitigates the domain shift by integrating intermediate subspaces on a geodesic path from the source to the target domains. A geodesic flow curve is constructed to connect the source and target domains on the Grassmannian. The source and target subspaces are points on a Grassmann manifold. By sampling a fixed or infinite
 number of subspaces along the geodesic, we can form the Manuscript accepted by Neurocomputing 2018
9 intermediate subspaces to help to find the correlations between domains. Then, both source and target data are projected to the obtained intermediate subspaces to align the distribution.
Inspired by the intermediate representations on the geodesic path, Chopra et al. proposed a model called deep learning for DA by interpolating between domains (DLID). DLID generates intermediate datasets, starting with all the source data samples and gradually replacing source data with target data. Each dataset is a single point on an interpolating path between the source and target domains. Once intermediate datasets are generated, a deep nonlinear feature extractor using the predictive sparse decomposition is trained in an unsupervised manner.
2) Adversarial-Based Approaches: Recently, great success has been achieved by the GAN method, which estimates generative models via an adversarial process. GAN consists of two models: a generative model G that extracts the data distribution and a discriminative model D that distinguishes whether a sample is from G or training datasets by predicting a binary label. The networks are trained on the label prediction loss in a mini-max fashion: simultaneously optimizing G to minimize the loss while also training D to maximize the probability of assigning the correct label: min
G max
D V (D, G) = Ex∼pdata(x)[log D(x)]
+Ez∼pz(z)[log(1 − D(G(z)))]
In DA, this principle has been employed to ensure that the network cannot distinguish between the source and target domains. proposed a unified framework for adversarialbased approaches and summarized the existing approaches according to whether to use a generator, which loss function to employ, or whether to share weights across domains. In this paper, we only categorize the adversarial-based approaches into two subsettings: generative models and non-generative models.
Fig. 8. Generalized architecture for adversarial domain adaptation. Existing adversarial adaptation methods can be viewed as instantiations of a framework with different choices regarding their properties. 
• Generative Models
Synthetic target data with ground-truth annotations are an appealing alternative to address the problem of a lack of training data. First, with the help of source data, generators render unlimited quantities of synthetic target data, which are paired with synthetic source data to share labels or appear as if they were sampled from the target domain while maintaining labels, or something else. Then, synthetic data with labels are used to train the target model as if no DA were required.
Adversarial-based approaches with generative models are able to learn such a transformation in an unsupervised manner based on GAN.
The core idea of CoGAN is to generate synthetic target data that are paired with synthetic source ones. It consists of a pair of GANs: GAN1 for generating source data and GAN2 for generating target data. The weights of the first few layers in the generative models and the last few layers in the discriminative models are tied. This weight-sharing constraint allows CoGAN to achieve a domain-invariant feature space without correspondence supervision. A trained CoGAN can adapt the input noise vector to paired images that are from the two distributions and share the labels. Therefore, the shared labels of synthetic target samples can be used to train the target model.
Fig. 9. The CoGAN architecture. 
More work focuses on generating synthetic data that are similar to the target data while maintaining annotations. Yoo et al. transferred knowledge from the source domain to pixel-level target images with GANs. A domain discriminator ensures the invariance of content to the source domain, and a real/fake discriminator supervises the generator to produce similar images to the target domain. Shrivastava et al. developed a method for simulated+unsupervised (S+U) learning that uses a combined objective of minimizing an adversarial loss and a self-regularization loss, where the goal is to improve the realism of synthetic images using unlabeled real data. In contrast to other works in which the generator is conditioned only on a noise vector or source images, Bousmalis et al. proposed a model that exploits GANs conditioned on both.
The classifier T is trained to predict class labels of both source and synthetic images, while the discriminator is trained to predict the domain labels of target and synthetic images. In addition, to expect synthetic images with similar foregrounds and different backgrounds from the same source images, a content similarity is used that penalizes large differences between source and synthetic images for foreground pixels only by a masked pairwise mean squared error. The goal of the network is to learn G, D and T by solving the optimization problem: min
G,T max
D V (D, G) = αLd(D, G)
+βLt(T, G) + γLc(G)(15) where α, β, and γ are parameters that control the trade-off between the losses. Ld, Lt and Lc are the adversarial loss, softmax loss and content-similarity loss, respectively.
• Non-Generative Models
Manuscript accepted by Neurocomputing 2018
Fig. 10.
The model that exploits GANs conditioned on noise vector and source images. 
The key of deep DA is learning domain-invariant representations from source and target samples. With these representations, the distribution of both domains can be similar enough such that the classifier is fooled and can be directly used in the target domain even if it is trained on source samples.
Therefore, whether the representations are domain-confused or not is crucial to transferring knowledge. Inspired by GAN, domain confusion loss, which is produced by the discriminator, is introduced to improve the performance of deep DA without generators.
Fig. 11. The domain-adversarial neural network (DANN) architecture. 
The domain-adversarial neural network (DANN) integrates a gradient reversal layer (GRL) into the standard architecture to ensure that the feature distributions over the two domains are made similar. The network consists of shared feature extraction layers and two classifiers. DANN minimizes the domain confusion loss (for all samples) and label prediction loss (for source samples) while maximizing domain confusion loss via the use of the GRL. In contrast to the above methods, the adversarial discriminative domain adaptation (ADDA) considers independent source and target mappings by untying the weights, and the parameters of the target model are initialized by the pre-trained source one. This is more flexible because of allowing more domainspecific feature extractions to be learned. ADDA minimizes the source and target representation distances through iteratively minimizing these following functions, which is most similar to the original GAN: min
M s,CLcls(Xs, Y s) =
− E(xs,ys)∼(Xs,Y s)
K
� k=1
1[k=ys] log C(M s(xs)) min
D LadvD(Xs,Xt, M s, M t) =
− E(xs)∼(Xs)[log D(M s(xs))]
− E(xt)∼(Xt)[log(1 − D(M t(xt)))] min
M s,M t LadvM(M s, M t) =
− E(xt)∼(Xt)[log D(M t(xt))](16) where the mappings M s and M t are learned from the source and target data, Xs and Xt. C represents a classifier working on the source domain. The first classification loss function Lcls is optimized by training the source model using the labeled source data. The second function LadvD is minimized to train the discriminator, while the third function LadvM is learning a representation that is domain invariant.
Fig. 12.
The Adversarial discriminative domain adaptation (ADDA) architecture. 
Tzeng et al. proposed adding an additional domain classification layer that performs binary domain classification and designed a domain confusion loss to encourage its prediction to be as close as possible to a uniform distribution over binary labels. Unlike previous methods that match the entire source and target domains, Cao et al. introduced a selective adversarial network (SAN) to address partial transfer learning from large domains to small domains, which assumes that the target label space is a subspace of the source label space. It simultaneously avoids negative transfer by filtering out outlier source classes, and it promotes positive transfer by matching the data distributions in the shared label space via splitting the domain discriminator into many class-wise domain discriminators. encoded domain labels and class labels to produce four groups of pairs, and replaced the typical binary adversarial discriminator by a four-class discriminator.
Volpi et al. trained a feature generator (S) to perform data augmentation in the source feature space and obtained a domain invariant feature through playing a minimax game against features from S.
Rather than using discriminator to classify domain label, some papers make some other explorations. Inspired by
Wasserstein GAN, Shen et al. utilized discriminator to estimate empirical Wasserstein distance between the source and target samples and optimized the feature extractor network to minimize the distance in an adversarial manner. In, two classifiers are treated as discriminators and are trained to maximize the discrepancy to detect target samples outside the support of the source, while a feature extractor is trained to minimize the discrepancy by generating target features near the support.
3) Reconstruction-Based Approaches: In DA, the data reconstruction of source or target samples is an auxiliary task that simultaneously focuses on creating a shared representation
Manuscript accepted by Neurocomputing 2018
11 between the two domains and keeping the individual characteristics of each domain.
• Encoder-Decoder Reconstruction
The basic autoencoder framework is a feedforward neural network that includes the encoding and decoding processes. The autoencoder first encodes an input to some hidden representation, and then it decodes this hidden representation back to a reconstructed version. The DA approaches based on encoder-decoder reconstruction typically learn the domaininvariant representation by a shared encoder and maintain the domain-special representation by a reconstruction loss in the source and target domains.
Xavier and Bengio proposed extracting a high-level representation based on stacked denoising autoencoders (SDA). By reconstructing the union of data from various domains with the same network, the high-level representations can represent both the source and target domain data. Thus, a linear classifier that is trained on the labeled data of the source domain can make predictions on the target domain data with these representations. Despite their remarkable results, SDAs are limited by their high computational cost and lack of scalability to high-dimensional features. To address these crucial limitations, Chen et al. proposed the marginalized
SDA (mSDA), which marginalizes noise with linear denoisers; thus, parameters can be computed in closed-form and do not require stochastic gradient descent.
The deep reconstruction classification network (DRCN) proposed in learns a shared encoding representation that provides useful information for cross-domain object recognition. DRCN is a CNN architecture that combines two pipelines with a shared encoder. After a representation is provided by the encoder, the first pipeline, which is a CNN, works for supervised classification with source labels, whereas the second pipeline, which is a deconvolutional network, optimizes for unsupervised reconstruction with target data. min λLc({θenc, θlab}) + (1 − λ)Lr({θenc, θdec})(17) where λ is a hyper-parameter that controls the trade-off between classification and reconstruction. θenc, θdec and θlab denote the parameters of the encoder, decoder and source classifier, respectively. Lc is cross-entropy loss for classification, and Lr is squared loss ∥ x − fr(x) ∥2
2 for reconstruction in which fr(x) is the reconstruction of x.
Fig. 13. The deep reconstruction classification network (DRCN) architecture.
 
Domain separation networks (DSNs) explicitly and jointly model both private and shared components of the domain representations. A shared-weight encoder learns to capture shared representations, while a private encoder is used for domain-specific components in each domain. Additionally, a shared decoder learns to reconstruct the input samples by both the private and shared representations. Then, a classifier is trained on the shared representation. By partitioning the space in such a manner, the shared representations will not be influenced by domain-specific representations such that a better transfer ability can be obtained. Finding that the separation loss is simple and that the private features are only used for reconstruction in DSNs, reinforced them by incorporating a hybrid adversarial learning in a separation network and an adaptation network.
Zhuang et al. proposed transfer learning with deep autoencoders (TLDA), which consists of two encoding layers.
The distance in distributions between domains is minimized with KL divergence in the embedding encoding layer, and label information of the source domain is encoded using a softmax loss in the label encoding layer. Ghifary et al. extended the autoencoder into a model that jointly learns two types of data-reconstruction tasks taken from related domains: one is self-domain reconstruction, and the other is betweendomain reconstruction.
• Adversarial Reconstruction
Dual learning was first proposed by Xia et al. to reduce the requirement of labeled data in natural language processing.
Dual learning trains two "opposite" language translators, e.g., A-to-B and B-to-A. The two translators represent a primaldual pair that evaluates how likely the translated sentences belong to the targeted language, and the closed loop measures the disparity between the reconstructed and the original ones.
Inspired by dual learning, adversarial reconstruction is adopted in deep DA with the help of dual GANs.
Zhu et al. proposed a cycle GAN that can translate the characteristics of one image domain into the other in the absence of any paired training examples. Compared to dual learning, cycle GAN uses two generators rather than translators, which learn a mapping G : X → Y and an inverse mapping F : Y → X. Two discriminators, DX and DY, measure how realistic the generated image is (G(X) ≈ Y or G(Y ) ≈ X) by an adversarial loss and how well the original input is reconstructed after a sequence of two generations(F(G(X)) ≈ X or G(F(Y )) ≈ Y ) by a cycle consistency loss(reconstruction loss). Thus, the distribution of images from
G(X) (or F(Y )) is indistinguishable from the distribution Y(or X).
LGAN(G, DY, X, Y ) = Ey∼pdata(y)[log DY (y)]
+Ex∼pdata(x)[log(1 − DY (G(x)))]
Lcyc(G, F) = Ex∼data(x)[∥F(G(x)) − x∥1]
+Ey∼data(y)[∥G(F(y)) − y∥1](18) where LGAN is the adversarial loss produced by discriminator
DY with mapping function G : X
→ Y. Lcyc is the reconstruction loss using L1 norm.
The dual GAN and the disco GAN were proposed at the same time, where the core idea is similar to cycle
GAN. In dual GAN, the generator is configured with skip connections between mirrored downsampling and upsampling
Manuscript accepted by Neurocomputing 2018
Fig. 14. The cycle GAN architecture. layers,, making it a U-shaped net to share low-level information (e.g., object shapes, textures, clutter, and so forth).
For discriminators, the Markovian patch-GAN architecture is employed to capture local high-frequency information.
In disco GAN, various forms of distance functions, such as mean-square error (MSE), cosine distance, and hinge loss, can be used as the reconstruction loss, and the network is applied to translate images, changing specified attributes including hair color, gender and orientation while maintaining all other components.
4) Hybrid Approaches: To obtain better performance, some of the aforementioned methods have been used simultaneously.
 combined a domain confusion loss and a soft label loss, while used both statistic (MMD) and architecture criteria (adapt classifier by residual function) for unsupervised
DA. introduced class-specific auxiliary weights assigned by the pseudo-labels into the original MMD. In DSNs, encoder-decoder reconstruction approaches separate representations into private and shared representations, while the MMD criterion or domain confusion loss is helpful to make the shared representations similar and soft subspace orthogonality constraints ensure dissimilarity between the private and shared representations. used the MMD between the learned source and target representations and also allowed the weights of the corresponding layers to differ. learned domaininvariant representations by encoder-decoder reconstruction approaches and the KL divergence.
B. Heterogeneous Domain Adaptation
In heterogeneous DA, the feature spaces of the source and target domains are not the same, Xs ̸= Xt, and the dimensions of the feature spaces may also differ. According to the divergence of feature spaces, heterogeneous DA can be further divided into two scenarios. In one scenario, the source and target domain both contain images, and the divergence of feature spaces is mainly caused by different sensory devices(e.g., visual light (VIS) vs. near-infrared (NIR) or RGB vs. depth) and different styles of images (e.g., sketches vs. photos). In the other scenario, there are different types of media in source and target domain (e.g., text vs. image and language vs. image). Obviously, the cross-domain gap of the second scenario is much larger.
Most heterogeneous DA with shallow methods fall into two categories: symmetric transformation and asymmetric transformation. The symmetric transformation learns feature transformations to project the source and target features onto a common subspace. Heterogeneous feature augmentation(HFA) first transformed the source and target data into a common subspace using projection matrices P and Q respectively, then proposed two new feature mapping functions, ϕs (xs) = [Pxs, xs, 0dt]T and ϕt (xt) = [Qxt, 0ds, xt]T, to augment the transformed data with their original features and zeros. These projection matrices are found using standard
SVM with hinge loss in both the linear and nonlinear cases and an alternating optimization algorithm is proposed to simultaneously solve the dual SVM and to find the optimal transformations. treated each input domain as a manifold which is represented by a Laplacian matrix, and used labels rather than correspondences to align the manifolds.
The asymmetric transformation transforms one of source and target features to align with the other. proposed a sparse and class-invariant feature transformation matrix to map the weight vector of classifiers learned from the source domain to the target domain. The asymmetric regularized cross-domain transfer (ARC-t) used asymmetric, nonlinear transformations learned in Gaussian RBF kernel space to map the target data to the source domain. Extended from, ARC-t performed asymmetric transformation based on metric learning, and transfer knowledge between domains with different dimensions through changes of the regularizer. Since we focus on deep DA, we refer the interested readers to, which summarizes shallow approaches of heterogeneous DA.
However, as for deep methods, there is not much work focused on heterogeneous DA so far. The special and effective methods of heterogeneous deep DA have not been proposed, and heterogeneous deep DA is still performed similar to some approaches of homogeneous DA.
1) Discrepancy-Based Approach: In discrepancy-based approaches, the network generally shares or reuses the first n layers between the source and target domains, which limits the feature spaces of the input to the same dimension. However, in heterogeneous DA, the dimensions of the feature spaces of source domain may differ from those of target domain.
In first scenario of heterogeneous DA, the images in different domains can be directly resized into the same dimensions, so the Class Criterion and Statistic Criterion are still effective and are mainly used. For example, given an RGB image and its paired depth image, used the mid-level representation learned by CNNs as a supervisory signal to re-train a CNN on depth images. To transform an RGB object detector into a RGB-D detector without needing complete RGB-D data, Hoffman et al. first trained an RGB network using labeled
RGB data from all categories and finetuned the network with labeled depth data from partial categories, then combined midlevel RGB and depth representations at fc6 to incorporate both modalities into the final object class prediction. first trained the network using large face database of photos and then finetuned it using small database of composite sketches;
 transferred the VIS deep networks to the NIR domain in the same way.
In second scenario, the features of different media can not be directly resized into the same dimensions. Therefore, discrepancy-based methods fail to work without extra process.
 proposed weakly shared DTNs to transfer labeled information across heterogeneous domains, particularly from the text domain to the image domain. DTNs take paired data, such as text and image, as input to two SAEs, followed by weakly
Manuscript accepted by Neurocomputing 2018
13 parameter-shared network layers at the top. Chen et al. proposed transfer neural trees (TNTs), which consist of two stream networks to learn a domain-invariant feature representation for each modality. Then, a transfer neural decision forest(Transfer-NDF), is used with stochastic pruning for adapting representative neurons in the prediction layer.
2) Adversarial-Based Approach: Using Generative Models can generate the heterogeneous target data while transferring some information of source domain to them. employed a compound loss function that consists of a multiclass GAN loss, a regularizing component and an f-constancy component to transfer unlabeled face photos to emoji images. To generate images for birds and flowers based on text, trained a GAN conditioned on text features encoded by a hybrid characterlevel convolutional-recurrent neural network. proposed stacked generative adversarial networks (StackGAN) with conditioning augmentation for synthesizing photo-realistic images from text. It decomposes the synthesis problem into several sketch-refinement processes. Stage-I GAN sketches the primitive shape and basic colors of the object to yield low-resolution image, and Stage-II GAN completes details of the object to produce a high-resolution photo-realistic image.
Fig. 15. The StackGAN architecture. 
3) Reconstruction-Based Approach: The Adversarial Reconstruction can be used in heterogeneous DA as well. For example, the cycle GAN, dual GAN and disco
GAN used two generators, GA and GB, to generate sketches from photos and photos from sketches, respectively.
Based on cycle GAN, proposed a multi-adversarial network to avoid artifacts of facial photo-sketch synthesis by leveraging the implicit presence of feature maps of different resolutions in the generator subnetwork.
V. MULTI-STEP DOMAIN ADAPTATION
For multi-step DA, the selection of the intermediate domain is problem specific, and different problems may have different strategies.
A. Hand-Crafted Approaches
Occasionally, the intermediate domain can be selected by experience, that is, it is decided in advance. For example, when the source domain is image data and the target domain is composed of text data, some annotated images will clearly be crawled as intermediate domain data.
With the common sense that nighttime light intensities can be used as a proxy for economic activity, Xie et al.
 transferred knowledge from daytime satellite imagery to poverty prediction with the help of some nighttime light intensity information as an intermediate domain.
B. Instance-Based Approaches
In other problems where there are many candidate intermediate domains, some automatic selection criterion should be considered. Similar to the instance-transfer approaches proposed by Pan, because the samples of the source domain cannot be used directly, the mixture of certain parts of the source and target data can be useful for constructing the intermediate domain.
Tan et al. proposed distant domain transfer learning(DDTL), where long-distance domains fail to transfer knowledge by only one intermediate domain but can be related via multiple intermediate domains. DDTL gradually selects unlabeled data from the intermediate domains by minimizing reconstruction errors on the selected instances in the source and intermediate domains and all the instances in the target domain simultaneously. With removal of the unrelated source data, the selected intermediate domains gradually become closer to the target domain from the source domain:
J1(fe, fd, vS, vT ) = 1 nS nS
� i=1 vi
S
��ˆxi
S − xi
S
��2
+ 1 nI nI
� i=1 vi
I
��ˆxi
I − xi
I
��2
+ 1 nT nT
� i=1
��ˆxi
T − xi
T
��2
2 + R(vS, vT )(19) where ˆxi
S, ˆxi
T and ˆxi
I are reconstructions of source data
Si, target data T i and intermediate data Ii based on the autoencoder, respectively, and fe and fd are the parameters of the encoder and decoder, respectively. vS = (v1
S,..., vnS
S )
⊤ and vI = (v1
I,..., vnI
I )
⊤, vi
S, vi
I ∈ 0, 1 are selection indicators for the ith source and intermediate instance, respectively.
R(vS, vT ) is a regularization term that avoids all values of vS and vI being zero.
The DLID model mentioned in Section IV-A1 (Geometric Criterion) constructs the intermediate domains with a subset of the source and target domains, where source samples are gradually replaced by target samples.
C. Representation-Based Approaches
Representation-based approaches freeze the previously trained network and use their intermediate representations as input to the new network. Rusu et al. introduced progressive networks that have the ability to accumulate and transfer knowledge to new domains over a sequence of experiences. To avoid the target model losing its ability to solve the source domain, they constructed a new neural network for each domain, while transfer is enabled via lateral connections to features of previously learned networks. In the process, the parameters in the latest network are frozen to remember knowledge of intermediate domains.
Manuscript accepted by Neurocomputing 2018
Fig. 16. The progressive network architecture. 
VI. APPLICATION OF DEEP DOMAIN ADAPTATION
Deep DA techniques have recently been successfully applied in many real-world applications, including image classification, object recognition, face recognition, object detection, style translation, and so forth. In this section, we present different application examples using various visual deep DA methods. Because the information of commonly used datasets for evaluating the performance is provided in in detail, we do not introduce it in this paper.
A. Image Classification
Because image classification is a basic task of computer vision applications, most of the algorithms mentioned above were originally proposed to solve such problems. Therefore, we do not discuss this application repeatedly, but we show how much benefit deep DA methods for image classification can bring. Because different papers often use different parameters, experimental protocols and tuning strategies in the preprocessing steps, it is quite difficult to perform a fair comparison among all the methods directly. Thus, similar to the work of Pan, we show the comparison results between the proposed deep DA methods and non-adaptation methods using only deep networks. A list of simple experiments taken from some published deep DA papers are presented in Table V.
In,, and, the authors used the Office-31 dataset1 as one of the evaluation data sets, as shown in Fig.
1(a). The Office dataset is a computer vision classification data set with images from three distinct domains: Amazon (A), DSLR (D), and Webcam (W). The largest domain, Amazon, has 2817 labeled images and its corresponding 31 classes, which consists of objects commonly encountered in office settings. By using this dataset, previous works can show the performance of methods across all six possible DA tasks. showed comparison experiments among the standard AlexNet, the DANN method, and the MMD algorithm and its variations, such as DDC, DAN, JAN and RTN. Zellinger et al. evaluated their proposed CMD algorithm in comparison to other discrepancy-based methods(DDC, deep CROAL, DLID, AdaBN ) and the adversarial-based method DANN. proposed an algorithm combining soft label loss and domain confusion loss, and they
1 https://cs.stanford.edu/∼jhoffman/domainadapt/ also compared them with DANN and DLID under a supervised
DA setting.
In, MNIST2(M), USPS3(U), and SVHN4 (S) digit datasets (shown in Fig. 1(b)) are used for a cross-domain handwritten digit recognition task, and the experiment showed the comparison results on some adversarial-based methods, such as DANN, CoGAN and ADDA, where the baseline is VGG-16.
B. Face Recognition
The performance of face recognition significantly degrades when there are variations in the test images that are not present in the training images. The dataset shift can be caused by poses, resolution, illuminations, expressions, and modality.
Kan et al. proposed a bi-shifting auto-encoder network(BAE) for face recognition across view angle, ethnicity, and imaging sensor. In BAE, source domain samples are shifted to the target domain, and sparse reconstruction is used with several local neighbors from the target domain to ensure its correction, and vice versa. Single sample per person domain adaptation network (SSPP-DAN) in generates synthetic images with varying poses to increase the number of samples in the source domain and bridges the gap between the synthetic and source domains by adversarial training with a GRL in realworld face recognition. improved the performance of video face recognition by using an adversarial-based approach with large-scale unlabeled videos, labeled still images and synthesized images. Considering that age variations are difficult problems for smile detection and that networks trained on the current benchmarks do not perform well on young children, Xia et al. applied DAN and JAN (mentioned in Section IV-A1) to two baseline deep models, i.e., AlexNet and ResNet, to transfer the knowledge from adults to infants.
Fig. 17.
The single sample per person domain adaptation network (SSPPDAN) architecture. 
C. Object Detection
Recent advances in object detection are driven by regionbased convolutional neural networks (R-CNNs, fast RCNNs and faster R-CNNs ). They are composed of a window selection mechanism and classifiers that are pre-trained labeled bounding boxes by using the features extracted from CNNs. At test time, the classifier decides whether a region obtained by sliding windows contains the object. Although the R-CNN algorithm is effective, a large
2 http://yann.lecun.com/exdb/mnist/
3http://statweb.stanford.edu/∼tibs/ElemStatLearn/data.html
4http://ufldl.stanford.edu/housenumbers/
Manuscript accepted by Neurocomputing 2018
TABLE V
COMPARISON BETWEEN TRANSFER LEARNING AND NON-ADAPTATION LEARNING METHODS
Data Set(reference)
Source vs. Target
Baselines
Deep Domain Adaptation Methods
AlexNet
DDC
DAN
RTN
JAN
DANN
A vs. W
61.6±0.5
61.8±0.4
73.3±0.3
75.2±0.4
73.0±0.5
D vs. W
95.4±0.3
95.0±0.5
96.0±0.3
96.8±0.2
96.6±0.2
96.4±0.3
Office-31 Dataset
W vs. D
99.0±0.2
98.5±0.4
99.0±0.3
99.6±0.1
99.6±0.1
99.2±0.3
ACC (unit:%) 
A vs. D
63.8±0.5
64.4±0.3
67.0±0.4
71.0±0.2
72.8±0.3
72.3±0.3
D vs. A
51.1±0.6
52.1±0.6
54.0±0.5
50.5±0.3
57.5±0.2
53.4±0.4
W vs. A
49.8±0.4
52.2±0.4
53.1±0.5
51.0±0.1
56.3±0.2
51.2±0.5
Avg
AlexNet
Deep CORAL
CMD
DLID
AdaBN
DANN
A vs. W
77.0±0.6
D vs. W
96.3±0.4
Office-31 Dataset
W vs. D
99.2±0.2
ACC (unit:%) 
A vs. D
79.6±0.6D vs. A
63.8±0.7W vs. A
63.3±0.6AvgAlexNet
DLID
DANN
Soft Labels
Domain
Confusion
Confusion
+Soft
A vs. W
56.5±0.3
53.6±0.2
82.7±0.7
82.8±0.9
82.7±0.8
D vs. W
92.4±0.3
71.2±0.0
95.9±0.6
95.6±0.4
95.7±0.5
Office-31 Dataset
W vs. D
93.6±0.2
83.5±0.0
98.3±0.3
97.5±0.2
97.6±0.2
ACC (unit:%) 
A vs. D
64.6±0.484.9±1.2
85.9±1.1
86.1±1.2
D vs. A
47.6±0.166.0±0.5
66.2±0.4
66.2±0.3
W vs. A
42.7±0.165.2±0.6
64.9±0.5
65.0±0.5
AvgMNIST, USPS, VGG-16
DANN
CoGAN
ADDA and SVHN
M vs. U
75.2±1.6
77.1±1.8
91.2±0.8
89.4±0.2 digits datasets
U vs. M
57.1±1.7
73.0±2.0
89.1±0.8
90.1±0.8
ACC (unit:%) 
S vs. M
60.1±1.176.0±1.8 amount of bounding box labeled data is required to train each detection category. To solve the problem of lacking labeled data, considering the window selection mechanism as being domain independent, deep DA methods can be used in classifiers to adapt to the target domain.
Because R-CNNs train classifiers on regions just like classification, weak labeled data (such as image-level class labels) are directly useful for the detector. Most works learn the detector with limited bounding box labeled data and massive weak labeled data. The large-scale detection through adaptation(LSDA) trains a classification layer for the target domain and then uses a pre-trained source model along with output layer adaptation techniques to update the target classification parameters directly. Rochan et al. used word vectors to establish the semantic relatedness between weak labeled source objects and target objects and then transferred the bounding box labeled information from source objects to target objects based on their relatedness. Extending and, Tang et al. transferred visual (based on the LSDA model) and semantic similarity (based on work vectors) for training an object detector on weak labeled category. incorporated both an image-level and an instance-level adaptation component into faster R-CNN and minimized the domain discrepancy based on adversarial training. By using bounding box labeled data in a source domain and weak labeled data in a target domain, progressively fine-tuned the pre-trained model with domain-transfer samples and pseudo-labeling samples.
D. Semantic Segmentation
Fully convolutional network models (FCNs) for dense prediction have proven to be successful for evaluating semantic segmentation, but their performance will also degrade under domain shifts. Therefore, some work has also explored using weak labels to improve the performance of semantic segmentation. Hong et al. used a novel encoder-decoder architecture with attention model by transferring weak class labeled knowledge in the source domain, while, transferred weak object location knowledge.
Much attention has also been paid to deep unsupervised DA in semantic segmentation. Hoffman et al. first introduced it, in which global domain alignment is performed using FCNs with adversarial-based training, while transferring spatial layout is achieved by leveraging class-aware constrained multiple instance loss. Zhang et al. enhanced the segmentation performance on real images with the help of virtual ones. It uses the global label distribution loss of the images and local label distribution loss of the landmark superpixels in the target domain to effectively regularize the fine-tuning of the semantic segmentation network. Chen et al. proposed a framework for cross-city semantic segmentation. The framework assigns pseudo labels to pixels/grids in the target domain and jointly utilizes global and class-wise alignment by domain adversarial learning to minimize domain shift. In, a target guided distillation module adapts the style from the real images by imitating the pre-trained source network, and a spatial-aware adaptation module leverages the intrinsic spatial structure to reduce the domain divergence. Rather than operating a simple adversarial objective on the feature space, used a GAN
Manuscript accepted by Neurocomputing 2018
16 to address domain shift in which a generator projects the features to the image space and a discriminator operates on this projected image space.
Fig. 18.
The architecture of pixel-level adversarial and constraint-based adaptation. 
E. Image-to-Image Translation
Image-to-image translation has recently achieved great success with deep DA, and it has been applied to various tasks, such as style transferring. Specially, when the feature spaces of source and target images are not same, image-to-image translation should be performed by heterogeneous DA.
More approaches of image-to-image translation use a dataset of paired images and incorporate a DA algorithm into generative networks. Isola et al. proposed the pix2pix framework, which uses a conditional GAN to learn a mapping from source to target images. Tzeng et al. utilized domain confusion loss and pairwise loss to adapt from simulation to real-world data in a PR2 robot. However, several other methods also address the unpaired setting, such as CoGAN, cycle GAN, dual GAN and disco GAN.
Matching the statistical distribution by fine-tuning a deep network is another way to achieve image-to-image translation.
Gatys et al. fine-tuned the CNN to achieve DA by the total loss, which is a linear combination between the content and the style loss, such that the target image is rendered in the style of the source image maintaining the content. The content loss minimizes the mean squared difference of the feature representation between the original image and generated image in higher layers, while the style loss minimizes the elementwise mean squared difference between the Gram matrix of them on each layer. demonstrated that matching the Gram matrices of feature maps is equivalent to minimizing the MMD. Rather than MMD, proposed a deep generative correlation alignment network (DGCAN) that bridges the domain discrepancy between CAD synthetic and real images by applying the content and CORAL losses to different layers.
F. Person Re-identification
In the community, person re-identification (re-ID) has become increasingly popular. When given video sequences of a person, person re-ID recognizes whether this person has been in another camera to compensate for the limitations of fixed devices. Recently, deep DA methods have been used in re-ID when models trained on one dataset are directly used on another. Xiao et al. proposed the domain-guided dropout algorithm to discard useless neurons for re-identifying persons on multiple datasets simultaneously. Inspired by cycle GAN and Siamese network, the similarity preserving generative adversarial network (SPGAN) translated the labeled source image to the target domain, preserving self similarity and domain-dissimilarity in an unsupervised manner, and then it trains re-ID models with the translated images using supervised feature learning methods.
G. Image Captioning
Recently, image captioning, which automatically describes an image with a natural sentence, has been an emerging challenge in computer vision and natural language processing.
Due to lacking of paired image-sentence training data, DA leverages different types of data in other source domains to tackle this challenge. Chen et al. proposed a novel adversarial training procedure (captioner v.s. critics) for crossdomain image captioning using paired source data and unpaired target data. One captioner adapts the sentence style from source to target domain, whereas two critics, namely domain critic and multi-modal critic, aim at distinguishing them. Zhao et al. fine-tuned the pre-trained source model on limited data in the target domain via a dual learning mechanism.
VII. CONCLUSION
In a broad sense, deep DA is utilizing deep networks to enhance the performance of DA, such as shallow DA methods with features extracted by deep networks. In a narrow sense, deep DA is based on deep learning architectures designed for
DA and optimized by back propagation. In this survey paper, we focus on this narrow definition, and we have reviewed deep
DA techniques on visual categorization tasks.
Deep DA is classified as homogeneous DA and heterogeneous DA, and it can be further divided into supervised, semisupervised and unsupervised settings. The first setting is the simplest but is generally limited due to the need for labeled data; thus, most previous works focused on unsupervised cases. Semi-supervised deep DA is a hybrid method that combines the methods of the supervised and unsupervised settings.
Furthermore, the approaches of deep DA can be classified into one-step DA and multi-step DA considering the distance of the source and target domains. When the distance is small, one-step DA can be used based on training loss. It consists of the discrepancy-based approach, the adversarialbased approach, and the reconstruction-based approach. When the source and target domains are not directly related, multistep (or transitive) DA can be used. The key of multi-step
DA is to select and utilize intermediate domains, thus falling into three categories, including hand-crafted, feature-based and representation-based selection mechanisms.
Manuscript accepted by Neurocomputing 2018
Although deep DA has achieved success recently, many issues still remain to be addressed. First, most existing algorithms focus on homogeneous deep DA, which assumes that the feature spaces between the source and target domains are the same. However, this assumption may not be true in many applications. We expect to transfer knowledge without this severe limitation and take advantage of existing datasets to help with more tasks. Heterogeneous deep DA may attract increasingly more attention in the future.
In addition, deep DA techniques have been successfully applied in many real-world applications, including image classification, and style translation. We have also found that only a few papers address adaptation beyond classification and recognition, such as object detection, face recognition, semantic segmentation and person re-identification. How to achieve these tasks with no or a very limited amount of data is probably one of the main challenges that should be addressed by deep DA in the next few years.
Finally, since existing deep DA methods aim at aligning marginal distributions, they commonly assume shared label space across the source and target domains. However, in realistic scenario, the images of the source and target domain may be from the different set of categories or only a few categories of interest are shared. Recently, some papers,, have begun to focus on this issue and we believe it is worthy of more attention.
VIII. ACKNOWLEDGEMENTS
This work was partially supported by the National Natural
Science Foundation of China under Grant Nos. 61573068, 61471048, and 61375031, and Beijing Nova Program under
Grant No. Z161100004916088.
REFERENCES
 M. Arjovsky, S. Chintala, and L. Bottou.
Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.
 Y. Bengio. Learning deep architectures for ai. Foundations and Trends in Machine Learning, 2(1):1–127, 2009.
 K. M. Borgwardt, A. Gretton, M. J. Rasch, H.-P. Kriegel, B. Sch¨olkopf, and A. J. Smola.
Integrating structured biological data by kernel maximum mean discrepancy. Bioinformatics, 22(14):e49–e57, 2006.
 K. Bousmalis, N. Silberman, D. Dohan, D. Erhan, and D. Krishnan.
Unsupervised pixel-level domain adaptation with generative adversarial networks. arXiv preprint arXiv:1612.05424, 2016.
 K. Bousmalis, G. Trigeorgis, N. Silberman, D. Krishnan, and D. Erhan.
Domain separation networks.
In Advances in Neural Information
Processing Systems, pages 343–351, 2016.
 L. Bruzzone and M. Marconcini.
Domain adaptation problems: A dasvm classification technique and a circular validation strategy. IEEE transactions on pattern analysis and machine intelligence, 32(5):770–
 P. P. Busto and J. Gall.
Open set domain adaptation.
In The IEEE International Conference on Computer Vision (ICCV), volume 1, page 3, 2017.
 Z. Cao, M. Long, J. Wang, and M. I. Jordan. Partial transfer learning with selective adversarial networks. arXiv preprint arXiv:1707.07901, F. M. Carlucci, L. Porzi, B. Caputo, E. Ricci, and S. R. Bul`o. Autodial:
Automatic domain alignment layers. In International Conference on
Computer Vision, 2017.
 M. Chen, Z. Xu, K. Weinberger, and F. Sha. Marginalized denoising autoencoders for domain adaptation. arXiv preprint arXiv:1206.4683, T.-H. Chen, Y.-H. Liao, C.-Y. Chuang, W.-T. Hsu, J. Fu, and M. Sun.
Show, adapt and tell: Adversarial training of cross-domain image captioner. In The IEEE International Conference on Computer Vision(ICCV), volume 2, 2017.
 W.-Y. Chen, T.-M. H. Hsu, Y.-H. H. Tsai, Y.-C. F. Wang, and M.-S.
Chen. Transfer neural trees for heterogeneous domain adaptation. In
European Conference on Computer Vision, pages 399–414. Springer, Y. Chen, W. Li, C. Sakaridis, D. Dai, and L. Van Gool.
Domain adaptive faster r-cnn for object detection in the wild. arXiv preprint arXiv:1803.03243, 2018.
 Y. Chen, W. Li, and L. Van Gool.
Road: Reality oriented adaptation for semantic segmentation of urban scenes. arXiv preprint arXiv:1711.11556, 2017.
 Y.-H. Chen, W.-Y. Chen, Y.-T. Chen, B.-C. Tsai, Y.-C. F. Wang, and M. Sun. No more discrimination: Cross city adaptation of road scene segmenters. arXiv preprint arXiv:1704.08509, 2017.
 S. Chopra, S. Balakrishnan, and R. Gopalan.
Dlid: Deep learning for domain adaptation by interpolating between domains.
In ICML workshop on challenges in representation learning, volume 2, 2013.
 B. Chu, V. Madhavan, O. Beijbom, J. Hoffman, and T. Darrell. Best practices for fine-tuning visual classifiers to new domains. In Computer
Vision–ECCV 2016 Workshops, pages 435–442. Springer, 2016.
 W.-S. Chu, F. De la Torre, and J. F. Cohn. Selective transfer machine for personalized facial action unit detection.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages
3515–3522, 2013.
 G. Csurka. Domain adaptation for visual applications: A comprehensive survey. arXiv preprint arXiv:1702.05374, 2017.
 O. Day and T. M. Khoshgoftaar. A survey on heterogeneous transfer learning. Journal of Big Data, 4(1):29, 2017.
 W. Deng, L. Zheng, G. Kang, Y. Yang, Q. Ye, and J. Jiao.
Image-image domain adaptation with preserved self-similarity and domain-dissimilarity for person re-identification. arXiv preprint arXiv:1711.07027, 2017.
 J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and T. Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. In International conference on machine learning, pages 647–655, 2014.
 L. Duan, D. Xu, and I. Tsang.
Learning with augmented features for heterogeneous domain adaptation. arXiv preprint arXiv:1206.4660, D. Eigen, C. Puhrsch, and R. Fergus. Depth map prediction from a single image using a multi-scale deep network. In Advances in neural information processing systems, pages 2366–2374, 2014.
 Y. Ganin and V. Lempitsky.
Unsupervised domain adaptation by backpropagation. In International Conference on Machine Learning, pages 1180–1189, 2015.
 Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette, M. Marchand, and V. Lempitsky. Domain-adversarial training of neural networks. Journal of Machine Learning Research, 17(59):1–35, L. A. Gatys, A. S. Ecker, and M. Bethge. Image style transfer using convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2414–2423, 2016.
 W. Ge and Y. Yu.
Borrowing treasures from the wealthy: Deep transfer learning through selective joint fine-tuning. arXiv preprint arXiv:1702.08690, 2017.
 T. Gebru, J. Hoffman, and L. Fei-Fei.
Fine-grained recognition in the wild: A multi-task domain adaptation approach. arXiv preprint arXiv:1709.02476, 2017.
 M. Gheisari and M. S. Baghshah. Unsupervised domain adaptation via representation learning and adaptive classifier learning. Neurocomputing, 165:300–311, 2015.
 M. Ghifary, W. Bastiaan Kleijn, M. Zhang, and D. Balduzzi. Domain generalization for object recognition with multi-task autoencoders. In
Proceedings of the IEEE international conference on computer vision, pages 2551–2559, 2015.
 M. Ghifary, W. B. Kleijn, and M. Zhang. Domain adaptive neural networks for object recognition. In Pacific Rim International Conference on Artificial Intelligence, pages 898–904. Springer, 2014.
 M. Ghifary, W. B. Kleijn, M. Zhang, D. Balduzzi, and W. Li. Deep reconstruction-classification networks for unsupervised domain adaptation. In European Conference on Computer Vision, pages 597–613.
Springer, 2016.
 R. Girshick.
Fast r-cnn.
In Proceedings of the IEEE international conference on computer vision, pages 1440–1448, 2015.
 R. Girshick, J. Donahue, T. Darrell, and J. Malik.
Rich feature hierarchies for accurate object detection and semantic segmentation. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pages 580–587, 2014.
 X. Glorot, A. Bordes, and Y. Bengio. Domain adaptation for large-scale sentiment classification: A deep learning approach.
In Proceedings
Manuscript accepted by Neurocomputing 2018
18 of the 28th international conference on machine learning (ICML-11), pages 513–520, 2011.
 B. Gong, K. Grauman, and F. Sha. Connecting the dots with landmarks:
Discriminatively learning domain-invariant features for unsupervised domain adaptation. In International Conference on Machine Learning, pages 222–230, 2013.
 B. Gong, Y. Shi, F. Sha, and K. Grauman.
Geodesic flow kernel for unsupervised domain adaptation. In Computer Vision and Pattern
Recognition (CVPR), 2012 IEEE Conference on, pages 2066–2073.
IEEE, 2012.
 I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In
Advances in neural information processing systems, pages 2672–2680, R. Gopalan, R. Li, and R. Chellappa. Domain adaptation for object recognition: An unsupervised approach. In Computer Vision (ICCV), 2011 IEEE International Conference on, pages 999–1006. IEEE, 2011.
 S. Gupta, J. Hoffman, and J. Malik.
Cross modal distillation for supervision transfer.
In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pages 2827–2836, 2016.
 P. Haeusser, T. Frerix, A. Mordvintsev, and D. Cremers. Associative domain adaptation. In International Conference on Computer Vision(ICCV), volume 2, page 6, 2017.
 D. He, Y. Xia, T. Qin, L. Wang, N. Yu, T. Liu, and W.-Y. Ma. Dual learning for machine translation. In Advances in Neural Information
Processing Systems, pages 820–828, 2016.
 K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.
 G. Hinton, O. Vinyals, and J. Dean.
Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.
 G. E. Hinton, S. Osindero, and Y.-W. Teh. A fast learning algorithm for deep belief nets. Neural computation, 18(7):1527–1554, 2006.
 J. Hoffman, S. Guadarrama, E. S. Tzeng, R. Hu, J. Donahue, R. Girshick, T. Darrell, and K. Saenko. Lsda: Large scale detection through adaptation. In Advances in Neural Information Processing Systems, pages 3536–3544, 2014.
 J. Hoffman, S. Gupta, J. Leong, S. Guadarrama, and T. Darrell. Crossmodal adaptation for rgb-d detection.
In Robotics and Automation(ICRA), 2016 IEEE International Conference on, pages 5032–5039.
IEEE, 2016.
 J. Hoffman, E. Tzeng, J. Donahue, Y. Jia, K. Saenko, and T. Darrell.
One-shot adaptation of supervised deep convolutional models. arXiv preprint arXiv:1312.6204, 2013.
 J. Hoffman, D. Wang, F. Yu, and T. Darrell.
Fcns in the wild:
Pixel-level adversarial and constraint-based adaptation. arXiv preprint arXiv:1612.02649, 2016.
 S. Hong, W. Im, J. Ryu, and H. S. Yang. Sspp-dan: Deep domain adaptation network for face recognition with single sample per person. arXiv preprint arXiv:1702.04069, 2017.
 S. Hong, J. Oh, H. Lee, and B. Han. Learning transferrable knowledge for semantic segmentation with deep convolutional neural network. In
Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 3204–3212, 2016.
 J. Hu, J. Lu, and Y.-P. Tan.
Deep transfer metric learning.
In
Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 325–333, 2015.
 X. Huang and S. Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. arXiv preprint arXiv:1703.06868, N. Inoue, R. Furuta, T. Yamasaki, and K. Aizawa.
Cross-domain weakly-supervised object detection through progressive domain adaptation. arXiv preprint arXiv:1803.11365, 2018.
 S. Ioffe and C. Szegedy.
Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International
Conference on Machine Learning, pages 448–456, 2015.
 P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros.
Image-to-image translation with conditional adversarial networks. arXiv preprint arXiv:1611.07004, 2016.
 M. Kan, S. Shan, and X. Chen. Bi-shifting auto-encoder for unsupervised domain adaptation. In Proceedings of the IEEE International
Conference on Computer Vision, pages 3846–3854, 2015.
 T. Kim, M. Cha, H. Kim, J. Lee, and J. Kim. Learning to discover crossdomain relations with generative adversarial networks. arXiv preprint arXiv:1703.05192, 2017.
 A. Kolesnikov and C. H. Lampert. Seed, expand and constrain: Three principles for weakly-supervised image segmentation.
In European
Conference on Computer Vision, pages 695–711. Springer, 2016.
 P. Kontschieder, M. Fiterau, A. Criminisi, and S. Rota Bulo. Deep neural decision forests.
In Proceedings of the IEEE International
Conference on Computer Vision, pages 1467–1475, 2015.
 A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks.
In Advances in neural information processing systems, pages 1097–1105, 2012.
 B. Kulis, K. Saenko, and T. Darrell. What you saw is not what you get:
Domain adaptation using asymmetric kernel transforms. In Computer
Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 1785–1792. IEEE, 2011.
 C. H. Lampert, H. Nickisch, and S. Harmeling. Learning to detect unseen object classes by between-class attribute transfer. In Computer
Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 951–958. IEEE, 2009.
 C. Li and M. Wand.
Precomputed real-time texture synthesis with markovian generative adversarial networks. In European Conference on Computer Vision, pages 702–716. Springer, 2016.
 D. Li, Y. Yang, Y.-Z. Song, and T. M. Hospedales. Deeper, broader and artier domain generalization. In Computer Vision (ICCV), 2017
IEEE International Conference on, pages 5543–5551. IEEE, 2017.
 Y. Li, K. Swersky, and R. Zemel.
Generative moment matching networks.
In Proceedings of the 32nd International Conference on
Machine Learning (ICML-15), pages 1718–1727, 2015.
 Y. Li, N. Wang, J. Liu, and X. Hou. Demystifying neural style transfer. arXiv preprint arXiv:1701.01036, 2017.
 Y. Li, N. Wang, J. Shi, J. Liu, and X. Hou. Revisiting batch normalization for practical domain adaptation. arXiv preprint arXiv:1603.04779, M.-Y. Liu and O. Tuzel. Coupled generative adversarial networks. In
Advances in neural information processing systems, pages 469–477, W. Liu, Z. Wang, X. Liu, N. Zeng, Y. Liu, and F. E. Alsaadi.
A survey of deep neural network architectures and their applications.
Neurocomputing, 234:11–26, 2017.
 X. Liu, L. Song, X. Wu, and T. Tan. Transferring deep representation for nir-vis heterogeneous face recognition. In Biometrics (ICB), 2016
International Conference on, pages 1–8. IEEE, 2016.
 M. Long, Y. Cao, J. Wang, and M. Jordan.
Learning transferable features with deep adaptation networks. In International Conference on Machine Learning, pages 97–105, 2015.
 M. Long, J. Wang, and M. I. Jordan. Deep transfer learning with joint adaptation networks. arXiv preprint arXiv:1605.06636, 2016.
 M. Long, H. Zhu, J. Wang, and M. I. Jordan. Unsupervised domain adaptation with residual transfer networks.
In Advances in Neural
Information Processing Systems, pages 136–144, 2016.
 H. Lu, L. Zhang, Z. Cao, W. Wei, K. Xian, C. Shen, and A. van den
Hengel. When unsupervised domain adaptation meets tensor representations. In The IEEE International Conference on Computer Vision(ICCV), volume 2, 2017.
 P. Mittal, M. Vatsa, and R. Singh. Composite sketch recognition via deep network-a transfer learning approach. In Biometrics (ICB), 2015
International Conference on, pages 251–256. IEEE, 2015.
 S. Motiian, Q. Jones, S. Iranmanesh, and G. Doretto.
Few-shot adversarial domain adaptation.
In Advances in Neural Information
Processing Systems, pages 6673–6683, 2017.
 S. Motiian, M. Piccirilli, D. A. Adjeroh, and G. Doretto.
Unified deep supervised domain adaptation and generalization. In The IEEE
International Conference on Computer Vision (ICCV), volume 2, 2017.
 H. V. Nguyen, H. T. Ho, V. M. Patel, and R. Chellappa. Dash-n: Joint hierarchical domain adaptation and feature learning. IEEE Transactions on Image Processing, 24(12):5479–5491, 2015.
 S. Pachori, A. Deshpande, and S. Raman. Hashing in the zero shot framework with domain adaptation. Neurocomputing, 2017.
 S. J. Pan, I. W. Tsang, J. T. Kwok, and Q. Yang. Domain adaptation via transfer component analysis. IEEE Transactions on Neural Networks, 22(2):199–210, 2011.
 S. J. Pan and Q. Yang.
A survey on transfer learning.
IEEE
Transactions on knowledge and data engineering, 22(10):1345–1359, V. M. Patel, R. Gopalan, R. Li, and R. Chellappa.
Visual domain adaptation: A survey of recent advances.
IEEE signal processing magazine, 32(3):53–69, 2015.
 K.-C. Peng, Z. Wu, and J. Ernst. Zero-shot deep domain adaptation. arXiv preprint arXiv:1707.01922, 2017.
 X. Peng, J. Hoffman, X. Y. Stella, and K. Saenko.
Fine-to-coarse knowledge transfer for low-res image classification. In Image Processing (ICIP), 2016 IEEE International Conference on, pages 3683–3687.
IEEE, 2016.
 X. Peng and K. Saenko. Synthetic to real adaptation with deep generative correlation alignment networks. arXiv preprint arXiv:1701.05524, Manuscript accepted by Neurocomputing 2018
 A. Raj, V. P. Namboodiri, and T. Tuytelaars. Subspace alignment based domain adaptation for rcnn detector. arXiv preprint arXiv:1507.05578, S.-A. Rebuffi, H. Bilen, and A. Vedaldi.
Learning multiple visual domains with residual adapters. arXiv preprint arXiv:1705.08045, S. Reed, Z. Akata, X. Yan, L. Logeswaran, B. Schiele, and H. Lee.
Generative adversarial text to image synthesis. arXiv preprint arXiv:1605.05396, 2016.
 S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in neural information processing systems, pages 91–99, 2015.
 M. Rochan and Y. Wang. Weakly supervised localization of novel objects using appearance transfer. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4315–4324, 2015.
 O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on
Medical Image Computing and Computer-Assisted Intervention, pages
234–241. Springer, 2015.
 S. Rota Bulo and P. Kontschieder. Neural decision forests for semantic image labelling. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 81–88, 2014.
 A. Rozantsev, M. Salzmann, and P. Fua. Beyond sharing weights for deep domain adaptation. arXiv preprint arXiv:1603.06432, 2016.
 A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick, K. Kavukcuoglu, R. Pascanu, and R. Hadsell.
Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016.
 K. Saenko, B. Kulis, M. Fritz, and T. Darrell. Adapting visual category models to new domains. In European conference on computer vision, pages 213–226. Springer, 2010.
 K. Saito, Y. Ushiku, and T. Harada.
Asymmetric tri-training for unsupervised domain adaptation. arXiv preprint arXiv:1702.08400, K. Saito, K. Watanabe, Y. Ushiku, and T. Harada. Maximum classifier discrepancy for unsupervised domain adaptation. arXiv preprint arXiv:1712.02560, 2017.
 S. Sankaranarayanan, Y. Balaji, A. Jain, S. N. Lim, and R. Chellappa.
Learning from synthetic data: Addressing domain shift for semantic segmentation. 2017.
 L. Shao, F. Zhu, and X. Li. Transfer learning for visual categorization:
A survey. IEEE transactions on neural networks and learning systems, 26(5):1019–1034, 2015.
 J. Shen, Y. Qu, W. Zhang, and Y. Yu. Wasserstein distance guided representation learning for domain adaptation. 2017.
 W. Shimoda and K. Yanai. Distinct class-specific saliency maps for weakly supervised semantic segmentation. In European Conference on
Computer Vision, pages 218–234. Springer, 2016.
 A. Shrivastava, T. Pfister, O. Tuzel, J. Susskind, W. Wang, and R. Webb.
Learning from simulated and unsupervised images through adversarial training. arXiv preprint arXiv:1612.07828, 2016.
 X. Shu, G.-J. Qi, J. Tang, and J. Wang. Weakly-shared deep transfer networks for heterogeneous-domain knowledge propagation. In Proceedings of the 23rd ACM international conference on Multimedia, pages 35–44. ACM, 2015.
 K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.
 K. Sohn, S. Liu, G. Zhong, X. Yu, M.-H. Yang, and M. Chandraker.
Unsupervised domain adaptation for face recognition in unlabeled videos. arXiv preprint arXiv:1708.02191, 2017.
 B. Sun, J. Feng, and K. Saenko. Return of frustratingly easy domain adaptation. In AAAI, volume 6, page 8, 2016.
 B. Sun and K. Saenko. Deep coral: Correlation alignment for deep domain adaptation. In Computer Vision–ECCV 2016 Workshops, pages
443–450. Springer, 2016.
 C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions.
In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1–9, 2015.
 Y. Taigman, A. Polyak, and L. Wolf. Unsupervised cross-domain image generation. arXiv preprint arXiv:1611.02200, 2016.
 Y. Taigman, M. Yang, M. Ranzato, and L. Wolf. Deepface: Closing the gap to human-level performance in face verification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1701–1708, 2014.
 B. Tan, Y. Song, E. Zhong, and Q. Yang. Transitive transfer learning.
In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1155–1164. ACM, B. Tan, Y. Zhang, S. J. Pan, and Q. Yang. Distant domain transfer learning. In AAAI, pages 2604–2610, 2017.
 Y. Tang, J. Wang, B. Gao, E. Dellandr´ea, R. Gaizauskas, and L. Chen.
Large scale semi-supervised object detection using visual and semantic knowledge transfer.
In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pages 2119–2128, 2016.
 J.-C. Tsai and J.-T. Chien. Adversarial domain separation and adaptation. In Machine Learning for Signal Processing (MLSP), 2017 IEEE
27th International Workshop on, pages 1–6. IEEE, 2017.
 E. Tzeng, C. Devin, J. Hoffman, C. Finn, P. Abbeel, S. Levine, K. Saenko, and T. Darrell. Adapting deep visuomotor representations with weak pairwise constraints. CoRR, vol. abs/1511.07111, 2015.
 E. Tzeng, J. Hoffman, T. Darrell, and K. Saenko.
Simultaneous deep transfer across domains and tasks. In Proceedings of the IEEE
International Conference on Computer Vision, pages 4068–4076, 2015.
 E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell.
Adversarial discriminative domain adaptation. arXiv preprint arXiv:1702.05464, E. Tzeng, J. Hoffman, N. Zhang, K. Saenko, and T. Darrell. Deep domain confusion: Maximizing for domain invariance. arXiv preprint arXiv:1412.3474, 2014.
 D. Ulyanov, A. Vedaldi, and V. Lempitsky. Improved texture networks:
Maximizing quality and diversity in feed-forward stylization and texture synthesis. arXiv preprint arXiv:1701.02096, 2017.
 P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol.
Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion.
Journal of Machine
Learning Research, 11(Dec):3371–3408, 2010.
 R. Volpi, P. Morerio, S. Savarese, and V. Murino. Adversarial feature augmentation for unsupervised domain adaptation. arXiv preprint arXiv:1711.08561, 2017.
 C. Wang and S. Mahadevan.
Heterogeneous domain adaptation using manifold alignment.
In IJCAI proceedings-international joint conference on artificial intelligence, volume 22, page 1541, 2011.
 L. Wang, V. A. Sindagi, and V. M. Patel. High-quality facial photosketch synthesis using multi-adversarial networks. arXiv preprint arXiv:1710.10182, 2017.
 X. Wang, X. Duan, and X. Bai. Deep sketch feature for cross-domain image retrieval. Neurocomputing, 207:387–397, 2016.
 Y. Xia, D. Huang, and Y. Wang. Detecting smiles of young children via deep transfer learning. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pages 1673–1681, 2017.
 T. Xiao, H. Li, W. Ouyang, and X. Wang. Learning deep feature representations with domain guided dropout for person re-identification. In
Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 1249–1258, 2016.
 M. Xie, N. Jean, M. Burke, D. Lobell, and S. Ermon. Transfer learning from deep features for remote sensing and poverty mapping. 2015.
 H. Yan, Y. Ding, P. Li, Q. Wang, Y. Xu, and W. Zuo. Mind the class weight bias: Weighted maximum mean discrepancy for unsupervised domain adaptation. arXiv preprint arXiv:1705.00609, 2017.
 Z. Yi, H. Zhang, P. T. Gong, et al. Dualgan: Unsupervised dual learning for image-to-image translation. arXiv preprint arXiv:1704.02510, 2017.
 D. Yoo, N. Kim, S. Park, A. S. Paek, and I. S. Kweon. Pixel-level domain transfer. In European Conference on Computer Vision, pages
517–532. Springer, 2016.
 J. Yosinski, J. Clune, Y. Bengio, and H. Lipson. How transferable are features in deep neural networks? In Advances in neural information processing systems, pages 3320–3328, 2014.
 W.
Zellinger, T.
Grubinger, E.
Lughofer, T.
Natschl¨ager, and S. Saminger-Platz.
Central moment discrepancy (cmd) for domaininvariant representation learning. arXiv preprint arXiv:1702.08811, H. Zhang, T. Xu, H. Li, S. Zhang, X. Huang, X. Wang, and D. Metaxas.
Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks. In IEEE Int. Conf. Comput. Vision (ICCV), pages 5907–5915, 2017.
 J. Zhang, Z. Ding, W. Li, and P. Ogunbona.
Importance weighted adversarial nets for partial domain adaptation. arXiv preprint arXiv:1803.09210, 2018.
 J. Zhang, W. Li, and P. Ogunbona. Transfer learning for cross-dataset recognition: A survey. 2017.
 L. Zhang, Z. He, and Y. Liu.
Deep object recognition across domains based on adaptive extreme learning machine. Neurocomputing, 239:194–203, 2017.
 X. Zhang, F. X. Yu, S.-F. Chang, and S. Wang. Deep transfer network:
Unsupervised domain adaptation. arXiv preprint arXiv:1503.00591, Y. Zhang, P. David, and B. Gong. Curriculum domain adaptation for semantic segmentation of urban scenes.
In The IEEE International
Manuscript accepted by Neurocomputing 2018
Conference on Computer Vision (ICCV), volume 2, page 6, 2017.
 W. Zhao, W. Xu, M. Yang, J. Ye, Z. Zhao, Y. Feng, and Y. Qiao. Dual learning for cross-domain image captioning. In Proceedings of the 2017
ACM on Conference on Information and Knowledge Management, pages 29–38. ACM, 2017.
 J. T. Zhou, I. W. Tsang, S. J. Pan, and M. Tan. Heterogeneous domain adaptation for multiple classes. In Artificial Intelligence and Statistics, pages 1095–1103, 2014.
 J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. arXiv preprint arXiv:1703.10593, 2017.
 F. Zhuang, X. Cheng, P. Luo, S. J. Pan, and Q. He.
Supervised representation learning: Transfer learning with deep autoencoders. In
IJCAI, pages 4119–4125, 2015.Towards Robust Pattern Recognition: A Review
Xu-Yao Zhang, Cheng-Lin Liu, Ching Y. Suen
Abstract—The accuracies for many pattern recognition tasks have increased rapidly year-by-year, achieving or even outperforming human performance. From the perspective of accuracy, pattern recognition seems to be a nearly-solved problem.
However, once launched in real applications, the high-accuracy pattern recognition systems may become unstable and unreliable, due to the lack of robustness in open and changing environments.
In this paper, we present a comprehensive review of research towards robust pattern recognition from the perspective of breaking three basic and implicit assumptions: closed-world assumption, independent and identically distributed assumption, and clean and big data assumption, which form the foundation of most pattern recognition models. Actually, our brain is robust at learning concepts continually and incrementally, in complex, open and changing environments, with different contexts, modalities and tasks, by showing only a few examples, under weak or noisy supervision. These are the major differences between human intelligence and machine intelligence, which are closely related to the above three assumptions. After witnessing the significant progress in accuracy improvement nowadays, this review paper will enable us to analyze the shortcomings and limitations of current methods and identify future research directions for robust pattern recognition.
Index Terms—Robust pattern recognition, closed world, independent and identically distributed, clean and big data.
I. INTRODUCTION
In human intelligence, the ability of recognizing patterns is the most fundamental cognitive skill in the brain and serves as the building block for other high-level decision making, which is historically shown to be crucial for our survival and evolution in complex environments. On the other hand, for the purpose of machine intelligence, pattern recognition is also an essential goal for both machine learning and artificial intelligence, where the solving of many high-level intelligent problems relies heavily on the success of automatic and accurate pattern recognition.
During the past decades(see the survey papers in 1968, 1980 and 2000 ), many exciting achievements in pattern recognition have been reported, and most successful methods are statistical approaches, such as parametric and nonparametric Bayes decision rules, support vector machines, boosting algorithms, and so on. To guarantee high accuracy, these models are usually built on some well-designed hand-crafted features. In traditional approaches, the choice of the feature representation strongly influences the classification performance. Since
2006, the end-to-end approach of deep learning, which simultaneously learns the feature and classifier directly from the raw data, has become the new cutting-edge solution for many pattern recognition tasks.
Xu-Yao Zhang, Cheng-Lin Liu, Ching Y. Suen. Towards Robust Pattern
Recognition: A Review. Proceedings of the IEEE, vol. 108, no. 6, pp. 894922, June 2020.
The accuracies on many problems have been increased significantly and rapidly from time to time. For example, on the MNIST (10-class handwritten digit) dataset, with convolutional neural networks, it is easy to achieve more than 99% accuracy without traditional hand-crafted features. On the more challenging task of 1000-class
ImageNet large scale visual recognition, the accuracy was improved year-by-year, for example, AlexNet (2012, 84.7%), GoogLeNet (2015, 93.33%), ResNet (2016, 96.43%), and so on. The newest accuracies have already surpassed human-level performance with large margins. Actually, this kind of accuracy improvement and recordbreaking phenomena happen all the time for different pattern recognition tasks, such as face recognition,, speech recognition,, handwriting recognition,, and so on. It seems that: from the perspective of accuracy, pattern recognition has become a well-solved problem.
However, accuracy is only one aspect of measuring performance. When launching a high-accuracy pattern recognition system into real applications, many unsatisfying and unexpected results may still happen, making the system unstable in robustness, and the reason causing these problems is usually a mixture of different factors. For example, Nguyen et al. reveal that the state-of-the-art deep neural networks (DNNs) are easily fooled by assigning high confidence predictions for unrecognizable images, indicating that: although the accuracy of DNN is very high, it is not as robust as human vision when dealing with outliers. Moreover, as shown by, a small perturbation (particularly designed) on the input sample will cause a large perturbation (incorrect prediction) on the output of a pattern recognition system, leading to great adversarial risk when using such system in real applications with stringent safety requirement. Moreover, in traditional pattern recognition, the class set is usually assumed to be closed.
However, in real world, the open set problem with dynamically changed class set is much more common. When re-contextualized into open set problems, many once solved tasks become significant challenging tasks again.
Another phenomenon causing significant performance drop for pattern recognition is the distribution mismatch. It has been shown that even a very small distribution shift can lead to a great performance drop for high accuracy pattern recognition systems. Therefore, besides accuracy, the adaptability and transferability of a pattern recognition system become very important in real-life applications. Most pattern recognition systems only have single input and output, however, an effective strategy for improving robustness is to increase the diversity on both the input and the output of a system. Therefore, the multi-modal learning and multi-task learning are also important issues for robust pattern recognition. In real world, the patterns seldom occur arXiv:2006.06976v1 [cs.CV] 12 Jun 2020
Raw Data
Classification
Statistical Pattern Recognition:
•
Dimension Reduction: PCA, LDA, ICA, ISOMAP, LLE …
•
Feature Selection: Wrappers, Filters, Embedded …
•
Bayes Decision Theory: Gaussian, Parzen, KNN, Mixture …
•
Neural Network: MLP, RBF, Polynomial …
•
Decision Tree: ID3, C4.5, CART, Random Forests …
•
Kernel Method: Support Vector Machine …
•
Ensemble Method: Bagging, Boosting …
•
Clustering: K-means, Hierarchical, Spectral …
•
…
Feature Space
Domain-Specific Feature Representation:
•
Preprocessing
•
Feature Extraction
•
Reducing Within-class Variance
•
Enlarging Between-class Variance
End-to-end Deep Learning:
•
Auto-encoder
•
Restricted Boltzmann Machine
•
Convolutional Neural Network
•
Recurrent Neural Network
•
Figure 1.
A brief overview of pattern recognition methods. in isolation, but instead, usually appear with rich contextual information. Learning from the dependencies between patterns to improve the robustness of decision making is an important problem in pattern recognition.
Most pattern recognition systems are actually data-hungry, and their high accuracies rely highly on both the quantity and quality of the training data. Any disturbances on the number or the labeling condition of the data will usually lead to great changes on the final performance. However, in real applications, it is usually difficult to collect large databases and produce accurate manual labeling. Therefore, the few-shot or even zero-shot learning abilities of pattern recognition systems are of great value for real applications. On the other hand, in order to reduce the dependence on data quality, the pattern recognition system should be robust and learnable with noisy data. Moreover, besides supervised training, other strategies like unsupervised, self-supervised, and semi-supervised learning are also valuable for pattern recognition systems to learn from abundant unlabeled data and easily-obtained surrogate supervisory signals.
Based on the above observations, the problem of pattern recognition is far from being solved when considering different requirements in real applications. Besides accuracy, more attention should be paid on improving the robustness of pattern recognition. There are many previous works on robust pattern recognition (see Section III), however, most of them are driven from a single view point of robustness. Currently, there is no clear definition on robust pattern recognition. In order to give a comprehensive understanding of robustness, and more importantly, reduce the gap between pattern recognition research and the requirements of real applications, in this paper we study and review different perspectives that are crucial for robust pattern recognition.
Actually, most pattern recognition models are derived from three implicit assumptions: closed-world assumption, independent and identically distributed assumption, and clean and big data assumption. These assumptions are reasonable in a controlled laboratory environment and will simplify the complexity of the problem, therefore, they are fundamental for most pattern recognition models. However, in real-world, these assumptions are usually not satisfied, and in most cases, the performance of models built under these assumptions will deteriorate significantly. Therefore, to build robust systems for real environments, we should try to break these assumptions and develop new models and algorithms by reconsidering the essentials of pattern recognition.
In the rest of this paper, we first give a brief overview of pattern recognition methods in Section 2. After that, we define robustness for pattern recognition in Section 3. Then, we present detailed overviews on different attempts trying to break the three basic assumptions in Sections 4, 5 and 6, respectively. Lastly, we draw concluding remarks in Section 7.
For readers who want to acquire some background knowledge before reading this paper, please refer to the appendix.
II. A BRIEF OVERVIEW OF PATTERN RECOGNITION
METHODS
As shown in Fig. 1, pattern recognition methods can be divided into two main categories: two-stage and end-to-end.
Most traditional methods are two-stage, i.e., with cascaded handcrafted feature representation and pattern classification.
The feature representation is to transform the raw data to a feature space with the property of within-class compactness and between-class separability. Preprocessing (like removing noise and normalizing data) is firstly applied to reduce within-class variance, while feature extraction further enlarges betweenclass variance, and this procedure is usually domain-specific.
Actually, for solving new pattern recognition problems, the first thing is the design of feature representation, and a good feature will significantly reduce the burden on subsequent classifier learning. This kind of efforts can be found in different applications like iris recognition, gait recognition, action recognition, and so on.
After feature representation, the second stage is pattern classification, which is a much more general problem. Actually, classification is the main focus of many textbooks including
Fukunaga, Duda et al., Bishop, and so on. This stage is also known as statistical pattern recognition, where many different issues are considered from different perspectives. Firstly, dimensionality reduction is widely adopted to derive a lower dimensional representation to facilitate subsequent classification task. Another approach of feature selection can be viewed as a discrete dimensionality reduction. After that, many classical classification models can be applied. The most fundamental one is the Bayes decision theory which integrates class-conditional density estimation with prior probability for maximum posterior probability classification. Artificial neural network is also widely used for pattern classification, including MLP (multilayer perceptron), RBF (radial basis function), polynomial networks, and so on. Decision tree based methods use a tree structure to represent the classification rule. Kernel methods, have been widely applied to extend linear models to nonlinear ones by performing linear operations on higher or even infinite dimensional space transformed implicitly by a kernel mapping function, and the most representative method is SVM (support vector machine). Ensemble methods can further improve the performance by combining predictions from multiple complementary models. Clustering is widely used as an unsupervised strategy for pattern recognition.
In two-stage methods, we usually have multiple choices for both feature representation and classifier learning. It is hard to predict which combination will lead to the best performance, and in practice, different pattern recognition problems usually have different optimal configurations according to domainspecific experiences. Contrarily, deep learning methods are end-to-end by learning the feature representation and classification jointly from the raw data. In this way, the learned features and classifiers are more cooperative toward the given task in a data-driven manner, which is more flexible and discriminative than two-stage methods.
Formerly, deep neural networks are usually layer-wise pretrained by unsupervised models like auto-encoder and restricted Boltzmann machine. Nowadays, deeper and deeper neural networks can be trained end-to-end due to many improved strategies such as better initialization, activation, optimization, normalization, architecture, and so on. Due to shared-weights architecture and local connectivity characteristic, the convolutional neural network has been successfully used in many visual recognition tasks like image classification, detection, segmentation, and so on. Moreover, due to the ability of dealing with arbitrary-length sequences, the recurrent neural network has been widely used for sequence-based pattern recognition like speech recognition, scene text recognition, and so on. Furthermore, the attention mechanism can further improve deep learning performance by focusing on the most relevant information. Nowadays, deep learning has become the cutting-edge solution for numerous pattern recognition tasks.
Besides the broad class of statistical pattern recognition
Table I
REPRESENTATIVE STUDIES WITH "ROBUST RECOGNITION", "ROBUST
CLASSIFICATION", OR "ROBUSTNESS" IN THEIR TITLES.
Year
Ref.
Definition of Robustness
Type
 
Heterogeneous sources
II
 
Small-sample effects, distortion of samples
III
 
Environmental differences
II
 
Train/test condition mismatch
II
 
Imprecise and changing environments
II
 
New class discovery, outlier rejection
I
 
Clutter, learn from a few examples
III
 
Noise corruption and occlusion, outliers
III
 
Small number of training data
III
 
Distribution difference
II
 
Adversarial attack
I
 
Adversarial, random noise
I
 
Outlier, feature noise, label noise
III approaches, structural pattern recognition has been developed for exploiting and understanding the rich structural information in patterns. Unlike statistical feature representation, the structure of patterns is of variable dimensionality, and can be viewed as in non-Euclidean space. String matching and graph matching are basic problems in structural pattern recognition. To improve the learning ability of structural pattern recognition problems, kernel methods (with graph kernel ), probabilistic graphical models, and graph neural networks have been used. Overall, the research and application of structural pattern recognition is less popular than that of statistical methods.
III. ROBUSTNESS IN PATTERN RECOGNITION
To build a pattern recognition system, there should be some training samples Dtrain = {xi, yi}n i=1 and test samples
Dtest = {ˆxi, ˆyi}ˆn i=1 where x is the observed pattern and y is the corresponding label (the hat on the symbol is used to differentiate training and test samples). The purpose of pattern recognition is to learn the joint distribution p(x, y) or conditional distribution p(y|x) from the training set Dtrain and then evaluate the learned model on a different test set Dtest.
During this process, there are usually some basic assumptions.
Assumption I: Closed-world assumption. The output space is assumed to be composed of a fixed number (e.g., k) of classes which are pre-defined as a prior Ω = {1, 2,..., k}, and all samples are assumed to come from these classes yi ∈ Ω, ˆyi ∈ Ω, ∀i. Under this assumption, we can clearly and easily define the decision boundaries since the whole space is partitioned into k regions. However, in real world applications, this assumption does not always hold, and there is an open space O much larger than Ω. The samples in O can be outliers not belonging to any classes, unknown samples from some new classes not shown in the training set, or even adversarial samples from the confusing area. In these cases, the pattern recognition system will produce over-confident wrong predictions, because in its opinion there are only k options and the winning class is highly reliable.
Clean and big data assumption
IID assumption
Closed-world assumption
Noisy and small data robustness
Non-iid robustness
Open-world robustness
Type I
Type II
Type III
Known known: empirical risk
Known unknown: outlier risk
Unknown known: adversarial risk
Unknown unknown: open class risk
Learning with interdependent data
Domain adaptation and transfer learning
Multi-task learning
Multi-modal learning
Supervised learning with noisy data
Unsupervised (self-supervised) learning
Semi-supervised learning
Few-shot and zero-shot learning
Figure 2.
Basic assumptions and robustness issues in pattern recognition.
Assumption II: IID assumption. The samples are assumed to be independent considering the joint distribution of observations and labels p(x1, y1, x2, y2,..., xn, yn) = p(x1, y1)p(x2, y2)... p(xn, yn) or the marginal distribution of the observations p(x1, x2,..., xn) = p(x1)p(x2)... p(xn), while the training and test data are assumed to be identically distributed p(x) ≈ p(ˆx) and p(x, y) ≈ p(ˆx, ˆy).
Under the independent assumption, we can then define the empirical loss as the summation of individual losses, e.g., − log p(x1, x2,..., xn) = �n i=1 − log p(xi). Under the identical distribution assumption, we can therefore hope that minimizing the training error on Dtrain will yield good generalization performance on Dtest. However, in real world, the IID assumption is often violated: the data collected from multiple sources or conditions can not be simply viewed as independent, and moreover, a small mismatch between the training and test environments will cause significant performance degradation.
Assumption III: Clean and big data assumption. The training data are assumed to be well labeled, and the volume of data is assumed to be large enough for covering different variations. Under this assumption, the only requirement is the capacity of the model, and supervised learning can be used to achieve good generalization performance. However, in real world applications, it is hard to collect a large number of training samples and also impossible to label all of them perfectly. How to effectively build pattern recognition systems from noisy data under small sample size (labeled or unlabeled) is a fundamental difference between machine intelligence and human intelligence.
As long as these assumptions remain stable, we can count on a reliable system to do its job time after time. However, when the assumptions no longer hold and the conditions start drifting, we want the system to keep its performance and be insensitive to these variations, this is called the robustness for a pattern recognition system. Actually, in the literature, there are already a lot of research on robust pattern recognition giving different definitions for robustness from diverse perspectives. We show some representative definitions in Table I, and actually they can also be partitioned into three types corresponding to the above three assumptions.
Usually, the lack of robustness in these studies is caused by the dissatisfaction of the assumptions. On the other hand, there are many other research works in the literature focusing on breaking the above assumptions but not using the terminology of robust pattern recognition, which are also of great values to this field. Therefore, to better understand the current state and identify directions for future research, this paper surveys recent advances in robust pattern recognition and presents them in a common taxonomy according to the three assumptions.
The main contents are organized as shown in Fig. 2. Under each assumption, taxonomic sub-classification is presented to partition the content into four sub-topics, resulting in totally twelve issues. A brief and comprehensive overview is presented for each of them accompanied by a discussion on current and future research directions. For some of the topics there already exist good review papers, however, we differ from them by focusing more on recent advances and the relations to robust pattern recognition. Although these topics are interrelated, the review and discussion on each of them are made to be as self-contained as possible. Readers can choose to start at anywhere according to their own interests.
However, it should always be remembered that the purpose is to break the three basic assumptions and realize robust pattern recognition.
IV. BREAKING CLOSED-WORLD ASSUMPTION
Most pattern recognition methods are based on the closedworld assumption: although we only have finite observations of samples and categories, we still try to find a full partition of the whole space, this of course is unwise and improper. For example, the support vector machine seeks a hyperplane to partition the whole space into two half spaces under the principle of maximum margin. In deep neural networks, the softmax layer partitions the whole space into a fixed number of classes and the summation of class probabilities is assumed to be one. These closed-world models will make overconfident errors on outliers and new category samples.
Actually, there are massive unknown regions in pattern classification due to the finite set of training samples. To avoid making ridiculous mistakes, we must find methods to deal with these open and unseen spaces. In this paper, motivated by the "known and unknown" statement in, we summarize
5 known known
Empirical Risk known unknown unknown known unknown unknown
Outlier Risk
Adversarial Risk
Open Class Risk(a)(b)(c)(d)
Figure 3.
An illustration on breaking closed-world assumption. The whole open space is partitioned into four parts: known known, known unknown, unknown known, and unknown unknown. the approaches on breaking closed-world assumption into the following perspectives.
A. Known Known: Empirical Risk
As shown in Fig. 3a, in closed-world recognition, we usually assume that we can observe some samples from some predefined categories, and we denote this case as "known known"(things we know that we know). A straightforward strategy in this case is to minimize the empirical risk, which is estimated as the misclassification rate on observed samples. However, since the number of samples is finite, minimizing empirical risk can not guarantee good generalization performance due to over-fitting. For example, it is common for the nearest neighbor decision rule to achieve perfect accuracy on training data but unsatisfactory performance on test data, and k-nearest neighbor is used to improve generalization with k searched by cross-validation. In decision trees, overfitting occurs when the tree is designed to perfectly fit all training samples, and pruning methods are applied to trim off unnecessary branches. Multilayer perceptron is able to approximate any decision boundary to arbitrary accuracy, and different tricks (like early stopping, weight decay) are used to avoid over-fitting.
The theoretical research of VC-dimension suggests minimizing the structural risk instead of empirical risk, by balancing the complexity of model against its success at fitting training data. Under this principle, the support vector machine seeks a hyperplane that has the largest distance to the nearest training data point of any class, resulting in the large-margin regularization. From then on, many other regularization strategies have also been proposed, like sparsity, low-rank, manifold, and so on. These regularization operations are usually combined with the empirical loss to build a better objective function. Other strategies(not integrated into the objective function) can also be viewed as implicit regularization, such as training with noise, regularized parameter estimation, dropout, and so on. As a conclusion, a common strategy for "known known" is empirical risk minimization with a well-defined regularization to improve generalization performance.
B. Known Unknown: Outlier Risk
As shown in Fig. 3b, besides known known, there is also
"known unknown" in the open space (we know there are some things we do not know). In open-world recognition, the things we do not know are often denoted as outliers. The simplest way to deal with outlier is extending the k-class problem to k + 1 classes by adding a new class representing outliers.
However, the drawback of this approach is that we need to collect outlier samples, and the distribution of outlier class is usually too complex to model. A more general case is that we do not have outlier samples, and the problem now becomes outlier detection, anomaly detection, or novelty detection.
1) Pattern rejection: The first solution we should consider is to integrate some rejection strategies into the traditional pattern classifiers, and many such attempts can be found in the literature. For Bayes decision theory, Chow showed that the optimal rule (for rejecting ambiguous patterns) is to reject the pattern if the maximum of the a posteriori probabilities maxi p(i|x) is less than some threshold. Dubuisson and Masson proposed a modified rejection for the case
� i pip(x|i) being smaller than some threshold, which is suitable for rejecting outliers (not belonging to pre-defined classes). For many other classical models, the rejection option needs to be specifically designed according to the structure of classifier, like support vector machine, nearest neighbor, sparse representation, multilayer perceptron, and so on. Ensemble learning of multiple classifiers can also be used for rejection. It was shown that different classifier structures and learning algorithms affect the rejection performance significantly.
2) Softmax and extensions: In many pattern recognition systems like deep neural networks, the softmax function is widely used for classification: p(i|x) = ezi(x)
�k j=1 ezj(x) ∈, y = arg k max i=1 p(i|x), (1) where zi(x) is the discriminant function for class i and y is the predicted class for x. Let p1
= p(y|x) and p2 = maxi̸=y p(i|x) be the top-1 and top-2 probabilities.
To reject outliers, a straightforward strategy is to set some thresholds on p1 (confidence on predicted class) or △ = p1 − p2 (ambiguity between the top two classes), and the sample should be rejected if either p1 or △ is below some threshold. Actually, this kind of operation rejects uncertain predictions rather than unknown classes. Due to the closedworld property �k i=1 p(i|x) = 1, it can be easily fooled with outliers: a sample from a novel class (not predefined k classes) may still have large values for both p1 and △. This means although the prediction is wrong, the classifier is still very confident (known as overconfident error ), making the system hard to apply a threshold for rejection. A simple and straightforward modification is using the sigmoid function p(i|x) =
1 + e−zi(x) ∈ (2) to break the sum-to-one assumption and adopting the one-vsall training, to improve outlier rejection. In this case, for each class, the training samples from other classes are viewed as outliers, and a sample can be efficiently rejected if ∀i : p(i|x) < threshold, since it does not belong to any known classes. Transforming sigmoid (binary) probabilities to multi-class probabilities satisfying �k i=1 p(i|x) ≤ 1 by the Dempster-Shafer theory of evidence can make outlier probability measurable as 1 − �k i=1 p(i|x).
To extend softmax for open set recognition, the openmax fits a Weibull distribution on the distances between samples and class-means to give a parametric estimation of the probability for an input being an outlier with respect to each class. Another extension called generative openmax employs generative adversarial network for novel category data synthesis to explicitly model the outlier class.
3) One-class classification: In the literature, another solution for outlier detection is the one-class classification, where all training samples are assumed to come from only one class. The support vector data description uses a hyper-sphere with minimum volume to encompass as many training points as possible. The one-class SVM treats the origin in feature space as the representation for open space and maximizes the margin of training samples with respect to it using a kernel-based method. To use one-class models in multi-class recognition tasks, each class can be modeled with an individual one-class classifier, and then the outputs for different classes can be combined and normalized to grow a multi-class classifier with the reject option.
4) Open space risk: Recently, more and more attentions on this old and important issue are actually awakened due to the work of, which defined the open space risk as:
RO(f) =
�
O f(x)dx
�
So f(x)dx, (3) where f is a measurable recognition function: f(x) > 0 for recognition of the class of interest and f(x) = 0 when it is not recognized. The O is the "open space" and So is a ball that includes all of the known training samples as well as the open space O. The RO(f) is considered to be the relative measure of the open space compared to the whole space, and the challenge on using this theory lies on how to define O and get a computationally tractable open space risk term. In, the 1-vs-set machine is proposed as an extension of traditional SVM by using a slab defined by two parallel hyper-planes to define the open space. Similar idea has also been studied by under open space hyper-plane classifiers. The work of further introduces a new model called compact abating probability (CAP) by defining the open space O as the space sufficiently far from any known training sample:
O = So −
� i
Br(xi), (4) where Br(xi) is a closed ball of radius r centered around training sample xi. A technique called Weibull-calibrated SVM has been proposed by combining CAP with statistical extreme value theory for score calibration to improve multi-class open set recognition.
5) Discussion: Consider an expert pattern recognition system which can classify digits from 0 to 9 perfectly, when we feed an image of "apple" into the system, it said "this is a 6 and I am very confident", this will immediately change our feeling about this system from intelligent to foolish. The ability of learning to reject is a major difference between closed-world and open-world recognition. Besides particular designed methods, theoretical analysis on this problem is particularly important, and although some studies have made good attempts on this direction, it is still worth further exploration. In many approaches, a threshold is usually used to distinguish normal and abnormal patterns, and different thresholds will lead to different tradeoffs between the adopted measurements (like precision and recall). Therefore, the choice of the threshold is usually task-dependent (different tasks will require different tradeoffs), and to evaluate the overall performance of a particular method, the threshold-independent metric should be used, like the AUROC (area under receiver operating characteristic curve), AUPR (area under precision-recall curve), and so on.
C. Unknown Known: Adversarial Risk
An intriguing phenomenon in open space is "unknown known": things we think we know but it turns out we do not. Different from the known unknown in Fig. 3b which denotes open space far away from training data and hence we know they are unknown, the unknown known in Fig. 3c represents open space near decision boundaries where we are supposed to know but actually not, due to the limited number of training data not covering this space. Ambiguous prediction
7 will happen for points close to the decision boundaries, for example, visually we think a sample is from one class but the system classifies it to another class. Since it is hard to sample such observation (low frequency in real world), the story is started by generating such samples to fool the system, which is known as adversarial examples.
1) Generation of adversarial examples: At the beginning, Szegedy et al. show that by applying an imperceptible perturbation to an image, it is possible to arbitrarily change its prediction. Given any sample x, an adversarial example x′ = x + η can be found through constrained optimization.
Since the perturbation η is small, we can not find any obvious difference between x and x′ visually, but their predicted labels are different, indicating the system is not robust: small perturbation on input causes large perturbation on output. An efficient method to generate adversarial examples called fast gradient sign is proposed by : let θ be parameters of a model, x and y be input and ground truth, and J(θ, x, y) be the cost used to train the model, the perturbation is then defined as: η = ϵ · sign (∇xJ(θ, x, y)), (5) where ϵ > 0 is a step-parameter. The elements in η correspond to the sign of the gradient of cost function with respect to input. Since η is on the direction of gradient, moving x along
+η will increase J, and consequently, a large-enough ϵ will cause x′ = x + η to be misclassified. The iterative gradient sign is proposed as a refinement to fast gradient sign.
After that, DeepFool is proposed to search a minimal perturbation that is sufficient to change the label.
2) Threat of adversarial examples: As shown in, using adversarial examples as attacks will be great threats for many applications, such as self-driving cars, voice commands, robots, and so on. It has been shown in that a perturbation with 1/1000 magnitude as the original image is sufficient to fool state-of-the-art deep neural networks. Moreover, many new attack methods are still being gradually proposed,. The work of showed that it is even possible to fool the system by only modifying one pixel of natural images.
Furthermore, the method of is proposed to attack a system which is viewed as a black-box. More surprisingly, the existence of a single small image-agnostic perturbation (called universal perturbation) that fools state-of-the-art classifiers on most natural images is also found. All these attempts have posed significant challenges on the robustness of pattern recognition systems.
3) Defense methods: To deal with adversarial attacks, many defense methods have been proposed. A typical approach is to augment the training set with adversarial examples and then retrain the model on the augmented data set,.
The defensive distillation is proposed to smooth the model during training for making it less sensitive to adversarial samples. The defense-GAN is trained to model the distribution of unperturbed real data, and at inference time it finds a close output to a given sample which does not contain the adversarial changes. Besides making the system robust to adversarial examples, Metzen et al. show that adversarial perturbations can also be detected by augmenting
=
"panda"
"gibbon"
=
"whale"
"turtle"(a) fooling examples(b) adversarial examples
0.007 �
Figure 4.(a): Fooling examples in which are classified to digits 0-9 with 99.99% confidence. (b): Adversarial examples in and. the system with a detector network which is trained on the binary classification task of distinguishing genuine samples from perturbed ones. The deep contractive network adopts an objective function min θ
L(x) +
H
� i=1 λi∥ ∂hi
∂hi−1
∥2, (6) where L(x) is a standard loss function and hi is the hidden representation for layer i in a deep neural network. Since adversarial examples can be produced using the gradient sign method, regularizing the smoothness of gradient is therefore helpful to avoid them.
Since there are open spaces near the decision boundaries, data augmentation can be used to fill them for improving robustness. The stability training adds pixel-wise uncorrelated Gaussian noise for each input x to produce an augmented sample x′, and then forces the outputs of the system on x and x′ to be as close as possible, thus improving its robustness against small perturbations. The robust optimization uses an alternating min-max procedure to increase local stability min θ n
� i=1 max x′ i∈Ui L(x′ i, yi), (7) where (xi, yi) is a sample-label pair and L is the loss function, the Ui is a ball around xi with some radius. In the inside max procedure, the x′ i can be viewed as an augmented worst-case sample, and in the outside min problem, the loss on (x′ i, yi) is minimized, thus making the system to be stable in a small neighborhood around every training point.
An interesting work of mixup proposes a data-agnostic method to produce augmented data points x′ = λxi + (1 − λ)xj, y′ = λyi + (1 − λ)yj, (8) where xi, xj are two examples drawn at random from training data and yi, yj are their corresponding one-hot label vectors.1 The λ ∈ is a random parameter to produce the augmented sample x′ and a new soft label y′ (not one-hot anymore). This is based on the assumption that: linear interpolations of feature vectors should lead to linear interpolations
1Previously, we use y to denote the label which is an integer from
{1, 2,..., k}. Here we use y (in bold) to represent a one-hot label vector: a vector (with length k) filled with 1 at the index of the labeled class and with
0 everywhere else.
8 of the associated labels. Although this approach is simple, it is very effective to produce augmented samples spreading not only within the same class but also between different classes, and therefore, the open space near decision boundaries is well handled with these augmented samples. Similar approach is also adopted in a work of between-class learning.
Although a mixture of two examples may not make sense for humans visually, it will make sense for machines as suggested by, and it is shown by that this can not only improve the generalization performance but can also increase the robustness to adversarial examples.
4) Discussion: As shown in Fig. 4a, another related concept is fooling examples which are produced to be completely unrecognizable to human eyes but the pattern recognition system will still classify them into particular classes with high confidence. This is different from adversarial examples shown in Fig. 4b. Actually, the phenomenon of fooling examples is the result of outliers with closed-world assumption which is discussed in Section IV-B. It is shown by that retraining of the system by viewing fooling examples as a newly added class is not sufficient to solve this problem, and a new batch of fooling images can be produced to fool the new system even after many retraining iterations. This is because there are massive open spaces for outliers and it is impossible to model them completely. Contrarily, augmenting training data with adversarial examples was shown to significantly increase the robustness even with only one extra epoch, this is because the open spaces near decision boundaries are limited and constrained, and therefore giving us possibility to model them. Since many people continue to propose new attack methods for producing adversarial examples, the research of novel defense strategy becomes particularly important to guarantee the safety of pattern recognition.
D. Unknown Unknown: Open Class Risk
As shown in Fig. 3d, the last case in open space is "unknown unknown": situations where a lot of unknown samples (out of this world) are grouping into different unknown (unseen) categories. In this case, we should not simply mark them as a single and large category of unknown (like Section IV-B), but also need to identify the newly emerged categories in a fine-grained manner. This is a common situation in real applications, where the datasets are dynamic and novel categories must be continuously detected and added, which is denoted as open world recognition in and class-incremental learning in.
1) Definition of the problem: During continuous use of a pattern recognition system, abundant or even infinite test data will come in a streaming manner. As shown in Fig. 5, the open-world recognition process can be decomposed into three steps. The first step is detecting unknown samples and placing them in a buffer, which requires the system to reject samples from unseen classes and keep high accuracy for seen classes.
The second step is labeling unknown samples in the buffer into new categories, which can be either finished by humans or automatically implemented. The last step is then updating the classifier with augmented categories and samples, which
�����
����������
����
��������
�������
�������
�����
������
������
������
������
������
Figure 5.
Open-world recognition with class-incremental learning. requires the classifier to be efficiently trainable in a classincremental manner where different classes occur at different times. Step 1 has already been discussed in Section IV-B, therefore, this section focuses on steps 2 and 3.
2) Labeling unknown samples:
A simple and accurate approach for step 2 is seeking help from human beings, either in a batch manner when the buffer size reaches some threshold, or immediately when the users encounter some strange outputs from the system and then try to give some feedback. Moreover, a strategy to make this process more efficient is using active learning to reduce the labeling cost by selecting the most valuable data to query their labels.
On the contrary, a more challenging task is automatic new class discovery without human labeling. Unsupervised clustering is an efficient and effective solution for finding new classes. A cluster can be seen as a new category if the number of samples falling in this cluster is large enough, and otherwise, it should only be viewed as an outlier and ignored.
However, a difficulty for this approach is the model selection problem, i.e., how many novel classes are contained in the data? To deal with this, the clustering algorithms should have the ability of automatic model selection.
3) Class incremental learning: The solution of step 3 requires us to rethink the relationship between discriminative and generative models. A pattern recognition system usually contains class-independent feature extraction φ(x) and classspecific decision function gi(x).2 The classification is then: x ∈ arg maxi gi(x). In discriminative model, all the decision functions gi(x), ∀i are trained jointly (like hinge loss, softmax loss, and so on), which can be viewed as the competition between different classes to adjust decision boundaries. On the contrary, in generative model, gi(x) is usually used to model each class independently (like negative log-likelihood loss for some distribution). Discriminative model usually has higher accuracy, however, since g1(x),..., gk(x) are coupled in training, adding a new class gk+1(x) will affect others, requiring retraining of them with all data available. Contrarily, in generative model, class-incremental learning will become much simpler, since the training of gk+1(x) is independent of other classes. However, the drawback is that generative model usually leads to lower accuracy. Therefore, hybrid discriminative and generative models become necessary: φ(x) is discriminative while gi(x) is generative. Actually, many recent works are already using this principle.
2For example, in deep learning, the φ(x) is a multi-layer neural network, and gi(x) is usually a linear function on φ(x) like gi(x) = w⊤ i φ(x) + bi.
4) Prototype based approaches: The nearest class mean(NCM) can generalize to new classes at near-zero cost: gi(x) = −∥φ(x) − µi∥2
2, µi = 1 ni
� j:yj=i φ(xj), (9) where µi is the mean of training samples (totally ni) in φ(x) space for class i, and gi(x) is based on the Euclidean distance to class-mean. Different criteria can be defined, such as softmax or sigmoid, to learn a discriminative φ(x) for high accuracy, and the updating of µi is always a generative mean calculation. When new class arrives, it is efficient to compute µnew and augment the model with a new decision function gnew(x), without affecting other classes.
The class-independent feature extraction φ(x) can be either a linear dimensionality reduction or a nonlinear deep neural network. Similar idea is also adopted in where a lot of exemplar samples are selected dynamically to represent each class in CNN transformed space, and a nearestmean-of-exemplars strategy is used for classification. Another work of prototypical network is also a NCM classifier in deep neural network transformed space. A more general analysis on representing each class as Gaussian is given in. A work of convolutional prototype learning uses automatically learned prototypes to represent each class by regularizing the deviation of prototypes from class-means.
As shown in, when normalized to lie on a sphere, NCM is also equal to the traditional linear classifier in neural networks.
5) Discussion: Since it is hard to enumerate all categories at once, how to smoothly update the system to learn more and more concepts over time is therefore an important and challenging task. Although using class-means or prototypes to represent each class is a simple generative model, it is effective for class distribution modeling, because a powerful φ(x) (e.g., deep neural networks) can be learned to transform complex intra-class distributions into simplified Gaussian distributions, which will then be efficient and effective for class-incremental learning. Other classical classifiers can also be modified for class-incremental learning like random forest, support vector machine, and so on. For class-incremental learning of deep neural networks, the newly learned classes may erase the knowledge of old classes, due to the joint updating of φ(x) and gi(x), resulting in catastrophic forgetting. A remedy is to review the historical data of old classes occasionally to prevent forgetting, and many other recent advances are gradually proposed on this topic like the evolving neural structure (learning to grow), dynamic generative memory (learning to remember), and so on. More efficient class-incremental learning models that can handle forgetting problem effectively will be the focus of future research.
V. BREAKING IID ASSUMPTION
Independent and identically distributed (IID) random variable is a fundamental assumption for most pattern recognition methods. However, in practical applications, this assumption is often violated. On the Dagstuhl seminar organized by Darrell
Domain adaptation
Multi-modal learning
Cross-class transfer learning
Multi-task learning
Multi-modal multi-task learning(a)(b)(c)(d)
Figure 6.
Different tasks where the IID assumption no longer holds. et al. in 2015, it was the agreement of all participants that learning with interdependent and non-identically distributed data should be the focus of future research. Moreover, it was shown by that even a very small mismatch between training and test distributions will make state-of-the-art models dropping their performance significantly.
In pattern recognition, a labeled sample (x, y) is usually assumed to come from a feature space x ∈ X and a label space y ∈ Y. A specific combination of feature space and label space can be viewed as an environment E = X × Y, where a learner p(y|x) is defined and performed. The learner should be adjustable when the environment starts to change
E ̸= E′, which can be summarized in four cases:
• X = X ′ and Y = Y′. This is the most-widely considered case, where the feature spaces and label spaces are identical, and the environmental change comes from the conditional distribution p(y|x) ̸= p(y′|x′).
• X = X ′ and Y ̸= Y′. The feature spaces are identical but the label spaces are different, for example, cross-class transfer learning and multi-task learning.
• X ̸= X ′ and Y = Y′. The feature spaces are different while the label spaces are identical, which happens often in multi-modal learning.
• X ̸= X ′ and Y ̸= Y′. Both the feature spaces and label spaces are different, which is the most difficult situation, and can be viewed as multi-modal multi-task learning.
A. Learning with Interdependent Data
In traditional pattern recognition, the samples are assumed to be independent. However, in real world, we usually have some group information (also denoted as set, bag or field in the literature) for the samples, implying statistical dependencies among them. Let
Xi = {xi
1, xi
2,..., xi ni}, Yi = {yi
1, yi
2,..., yi ni}(10) denote groups of samples and their corresponding labels. The purpose now is to learn the classifier and make decision with grouped data {Xi, Yi}N i=1:
• In each group, the samples are no longer independent.
• Different groups may not be identically distributed.
• Different groups can have different cardinalities.
1) Content consistency:
A straightforward and widelyconsidered case is that the samples in each group have the same label yi
1 = yi
2 = · · · = yi ni, ∀i, which is known as image set classification or group-based classification in the literature. This kind of content consistency in a group is very common in practice, for example, the temporal coherence between consecutive images in videos, the same object captured by multi-angle camera networks, classification based on long term observations, and so on. Each group Xi can be viewed as an unordered set of samples, and therefore the task is to define the similarities between different sets, for example by: viewing each set as a linear subspace and defining the similarity between two sets as canonical correlation, describing each set with the Grassmann and Stiefel manifolds and using geodesic distances as metrics, representing each set as an affine hull and calculating between-set distance from the sparse approximated nearest points, and so on. Besides viewing each set to lie on a certain geometric surface, deep learning framework based on minimum reconstruction error can be used to automatically discover the underlying geometric structure for image set classification. The multiple samples in the same group (or set) will provide complementary information from different aspects like appearance variations, view-points, illumination changes, nonrigid deformations, and so on. Therefore, it offers new opportunities to improve the classification accuracy compared with single example based classification.
2) Style consistency: Besides content consistency, another situation is style consistency : the samples in a group are isogenous or generated by the same source. For example, in handwriting recognition, a group of characters produced by a certain writer are homogeneous with his/her individual writing style; in face recognition, face images can appear as different groups according to different poses or illumination conditions; in speech recognition, different speakers have different accents, and so on. These situations provide important group information, and in each group the style is consistent(other than content). Moreover, a new group unnecessarily enjoys the same style as the training groups, which means style transfer exists between training and test groups. In the literature, this problem is studied under the terminology of pattern field classification by,, where a field is a group of isogenous patterns. Specifically, in a class-style conditional mixture of Gaussians is used to model the isogenous patterns, in the dependencies among samples is modeled by second-order statistics with normally distributed styles, and in the intraclass and interclass styles are studied under adaptive classification. The traditional
Bayes decision theory can also be extended to pattern field classification. By utilizing style consistency, classifying groups of patterns is shown to be much more accurate than classifying single patterns on various tasks like multipose face recognition, multi-speaker vowel classification, and multi-writer handwriting recognition.
3) Group-level supervision: A useful strategy to realize weakly-supervised learning is that there is only group-level supervision and the labels for the individual samples are not provided, which is a natural fit for numerous real-world applications and is denoted as multi-instance learning (MIL) in the literature. A group of instances is denoted as a bag, and although each bag has an associated label, the labels of the individual instances that conform the bag are not known.
For example, in drug activity prediction, a molecule (bag) can adopt a wide range of shapes (instances) by rotating some of its internal bonds, and knowing a previously-synthesized molecule has desired drug effect does not directly provide information on the shapes. In image classification, a single image (bag) can be represented by a collection of regions, blocks or patches (instances), and the labels are only attached to images instead of the low-level segments. The instances in each bag can be treated as non-IID samples, and not all of them are necessarily relevant : some instances may not convey any information about the bag class, or even come from other classes, thus providing confusing information. MIL usually deals with binary classification, and it is initially defined as : a bag is considered to be positive if and only if it contains at least one positive instance. Relaxed and alternative definitions for MIL are presented by to extend applications for different domains. Due to the property of weak supervision, MIL has found wide applications like image categorization, object localization, computeraided diagnosis, and so on.
4) Decision making in context: In above discussed situations, the order of the samples in each group is actually ignored. However, the organizational structure of the samples gives us a very important information of context which is historically shown to be crucial in pattern recognition. For example, the linguistic context takes place very naturally during the process of human reading. By using a language model, the performance of many related tasks like speech recognition, character recognition and text processing can be significantly improved. Moreover, the spatial arrangement of the samples, known as geometric context, is also an important piece of information for different pattern recognition tasks like and. A widely-used strategy to learn from the context is viewing the samples as a sequence, and many methods like hidden Markov models (HMMs), conditional random fields (CRFs) and recurrent neural networks(RNNs) can be used to model the dependencies among samples from the perspectives of Markov chain, conditional joint probability and long-short-term dependency respectively.
Besides sequence, graph is another useful representation for contextual learning, and recently graph neural networks,, have gained increasing popularity in various domains by modeling the dependencies between nodes in a graph via efficient and effective message passing among them. Moreover, other than structured input representation, the dependencies can also happen in the output space, known as structured output learning, which tries to predict more complex outputs such as trees, strings or lattices other than a group of independent labels. More details on this important issue can be found in.
5) Discussion: By utilizing the dependencies among data, assigning labels for a group of patterns simultaneously will
Feature
Extractor ��
Loss
Task
Classifier ��
Domain
Classifier ��
Source
Feature
Extractor ��
Target
��� ��� � �� source/target
��
��
Figure 7.
An illustration of adversarial domain adaptation. be more accurate and robust than labeling them separately.
The key problem is how to define and learn from the group information. Content and style are two important factors in pattern recognition. The dependencies derived from content consistency and style consistency are useful information to improve the performance of group-based pattern recognition.
Multi-instance learning which only requires group-level supervision is an effective strategy for weakly-supervised learning.
The contextual information embedded in the order or arrangement of the samples is proved to be important for structured prediction. Besides using dependency to improve performance, automatic discovery of the relationship of samples (relational reasoning) is also an important direction.
B. Domain Adaptation and Transfer Learning
As shown in Fig. 6a, when both the feature space and label space are identical, the non-IIDness may happen on the conditional distribution. In this case, the domain adaptation and transfer learning are actually dealing with the same thing: there is usually a source domain with sufficient labeled data and a target domain with a small amount of labeled or unlabeled data, and the purpose is to reduce the distribution mismatch between two domains in supervised, unsupervised, or semi-supervised manners.
1) Supervised fine-tuning: When there exist some labeled data in target domain (supervised domain adaptation), a simple and straightforward solution is to fine-tune the model on these extra labeled data. Actually, every pattern classifier which can be trained via incremental or online learning can be used in this way for supervised domain adaptation. The model trained on the source domain can be viewed as not only a good initialization but also a regularization, and fine-tuning on target data will gradually reduce the distribution shift between two domains. Many classifiers can essentially be learned incrementally like neural networks trained with back-propagation.
For non-incremental classifiers, we can also develop some counterpart algorithms for them, such as incremental decision tree, incremental SVM, and so on.
2) Cross-domain mapping: Another widely used strategy is learning cross-domain mappings to reduce the distribution shift. Let {xs, ys} and {xt, yt} denote source and target data, and θ denotes the parameters in classifier. The cross-domain mapping φ(·) can be defined in various ways:
Parameter mapping: p(ys|xs; θ) = p(yt|xt; φ(θ)), Source mapping: p(ys|φ(xs); θ) = p(yt|xt; θ), Target mapping: p(ys|xs; θ) = p(yt|φ(xt); θ), Co-mapping: p(ys|φ(xs); θ) = p(yt|φ(xt); θ).
In the first approach of parameter mapping, the source and target distributions are matched using transformed parameters φ(θ). For example, Leggetter and Woodland use a linear transformation on the mean parameters of the hidden
Markov model for speaker adaptation, and in a residual transformation network is used to map source parameters into target parameters. Another strategy is the source mapping applied on the source data φ(xs), and the transformed source data can be used together with the target data to train the classifier. Meanwhile, the target mapping applies the mapping on target data φ(xt),, and the advantage is that the adaptation (learning of φ) can happen after the training of the source classifier. At last, we can also define the co-mapping by projecting both the source and target data to a shared common space. The mapping φ can be either linear,, or nonlinear,,. Different criteria can be used to learn φ, like maximum likelihood, minimum earth mover distance, minimum regularized Euclidean distance, discriminative training,, component analysis, and so on.
3) Distribution matching: The purpose of domain adaptation and transfer learning is to match the distributions of source and target domains. The importance re-weighting is a widely used strategy for distribution matching: each source sample is weighted by the importance factor w(x) = pt(x)/ps(x) where pt(x) and ps(x) are target and source densities. Using weighted source samples to train the classifier will work well on target domain. However, density estimation is known to be a hard problem especially in high-dimensional spaces, therefore, directly estimating the importance without going through density estimation would be more promising as shown by and. Another problem in distribution matching is how to measure the discrepancy of two distributions. The maximum mean discrepancy (MMD) is a widely-used strategy by measuring the distance between the means of two distributions in a reproducing kernel Hilbert space (RKHS). It is shown that MMD will asymptotically approach zero if and only if the two distributions are the same. Since MMD is easy to calculate and does not require the label information, it has been widely used as a regularization term for unsupervised domain adaptation. Other than only using the distance between first-order means as the measurement, Zhang et al. propose aligning the secondorder covariance matrices in RKHS for distribution matching.
Besides MMD, many other kinds of distances, divergences, and information theoretical measurements can also be used for distribution matching, as discussed in the survey paper of.
4) Adversarial learning: Recently, an increasingly popular idea of adversarial learning tries to make the features from both domains to be as indistinguishable as possible. As shown
12 in Fig. 7, the whole framework is composed of four components. The feature extractors θ1 (for source) and θ2 (for target) are usually defined as deep neural networks. The task classifier θ3 is used to perform the original K-way classification for both source and target data. Importantly, a domain classifier θ4 is used to judge whether a sample is from source or target domain (binary classification). Since we have two classifiers, here we can define two standard classification losses ℓ1 and ℓ2. The key of adversarial learning is that these losses are optimized like playing a min-max game: θ1 ⇒ min ℓ1, max ℓ2, (15) θ2 ⇒ min ℓ1 (optional), max ℓ2, (16) θ3 ⇒ min ℓ1, (17) θ4 ⇒ min ℓ2.
The purpose of min ℓ1 is to guarantee classification accuracy while max ℓ2 aims to confuse the domain classifier and make the feature distributions over two domains similar, thus resulting in domain-invariant features. To efficiently seek maxθ1,θ2 ℓ2, multiple strategies can be used, like using gradient reversal layer to reverse the gradient in backpropagation, adopting inverted labels to calculate another surrogate fooled loss, and minimizing the cross-entropy loss against a uniform distribution. Moreover, the feature extractor modules can be designed as shared (θ1 = θ2), partially shared, or independent (θ1 ̸= θ2). Adversarial learning is efficient and effective for domain adaptation and transfer learning, and many subsequent improvements are still being gradually proposed.
5) Multi-source problem: In the above approaches, we assume that there is only a single source domain, but in practice, multiple sources may exist during data collection, which is related to the style consistent pattern field classification problem in Section V-A. The above discussed single-source methods can be extended accordingly to multisource case. For example, the cross-domain mapping can be extended to multiple source-mappings with a shared Bayes classifier, the adversarial based method can be modified by replacing the binary domain classifier with a multi-way classifier representing multiple sources, and so on.
6) Discussion: Domain adaptation and transfer learning are useful for many applications like speaker adaptation in speech recognition, writer adaptation in handwriting recognition, view adaptation in face recognition, and so on. Fine-tuning is a straightforward and effective strategy for supervised adaptation, while cross-domain mapping is a general approach for supervised, unsupervised, and semi-supervised adaptation. Traditional approaches usually focus on distribution matching, and adversarial learning has become the new trend for deep learning based adaptation.
Multi-source phenomenon is common in practice, and how to discover latent domains in mixed-multi-source data is an important and challenging problem.
C. Multi-task Learning
As shown in Fig. 6b, another case is that the feature spaces are identical but the label spaces are changed. For example, a ������
������
������
������
������
������
������
�����������
������
������
���
���
���
���
��
��
��
��
��������������
�����
��
��
������
��
��
������
��
��
���������������������������
���������������������������
����������������������
�����������������
Figure 8.
An illustration of multi-task representation learning. face image can be classified into different races, ages, genders, and so on. These tasks are not independent, instead, they are complementary to each other, and learning one task is helpful for solving another. How to efficiently and effectively learn from multiple related tasks is known as multi-task learning.
1) Transferable representation learning: The first question for multi-task learning is that: can we find a generic feature representation that is transferable among different tasks? The traditional hand-crafted feature representations are usually task-specific, and new features need to be designed for new tasks. It has been shown by that the features extracted from the deep neural network pre-trained with a large dataset are transferable to different tasks. The usual method is to train a base network, and then copy its first few layers to a target network, for which the remaining layers are randomly initialized and trained toward target task. There are multiple layers in deep neural networks, and the first layers usually learn low-level features whereas the latter layers learn semantic or high-level features. The low-level features are more general while the high-level features are more specific. Therefore, the transferabilities of features from bottom, middle, or top of a neural network are different, depending on the distance between the base task and target task : for similar tasks the later layers are more transferable, while for dissimilar tasks the earlier layers are preferred.
2) Multi-task representation learning: The second question for multi-task learning is that: can dealing with multiple tasks simultaneously be used to integrate different supervisory signals for learning an invariant representation? Since each task will produce a task-specific loss function, generally, as soon as you find yourself optimizing more than one loss function, you are effectively doing multi-task learning.
To learn multiple tasks jointly, there should be some shared and task-specific parameters in the architecture, and sharing what is learned during training different tasks in parallel is the central idea in multi-task learning. A straightforward approach is the hard parameter sharing (Fig. 8a), which shares the bottom layers for all tasks and keeps several task-specific output layers. This is the most widely-used strategy in real applications due to its simplicity and effectiveness. However, 13 it needs intensive experiments to find the optimal split position for shared and task-specific layers. Another approach is the soft parameter sharing in Fig. 8b, where each task has its own parameters which are regularized with some constraints to encourage similarity between them, like minimizing their ℓ2 distances or with some partially shared structure.
Other than using fixed sharing mechanism, another strategy is learning to share. For example, the cross-stitch network proposes to learn an optimal combination of shared and task-specific representations as shown in Fig. 8c. The above approaches are based on sharing features among tasks, and the decision-making processes of each task are still independent.
However, the solution of one task may be useful for solving other related tasks, indicating that we need task feedback to update the representation as shown in Fig. 8d. In this way, the performance of different tasks can be improved recurrently by utilizing the solutions (not only features) from other tasks.
3) Task relationship learning: Finding the relationship between different tasks will make information sharing among tasks to be more selective and smooth. A straightforward strategy is using task clustering to partition multiple tasks into several clusters, and the tasks in the same cluster are assumed to be similar to each other. It is also possible to dynamically widen a thin neural network in a greedy manner to create a tree-like deep architecture for clustering similar tasks in the same branch as shown in. Besides task clustering, many studies have also tried to learn both the pertask model parameters and the inter-task relationships simultaneously, where the task relationship can be formulated to be a matrix, tensor, or a nonlinear structure. This topic has attracted much attention in the research community, and the work of taskonomy has won the CVPR2018 best paper award. The taskonomy (task taxonomy) is a directed hyper-graph that captures the transferability among tasks: an edge between two tasks represents a feasible transfer, while the weight is the prediction of the transfer performance. With task relationship, the transfer learning performance would be improved, due to better transfer path from most related tasks to a target task, which can not only reduce the computational complexity of using all tasks but can also avoid the phenomenon of negative transfer caused by dissimilar tasks.
4) Discussion: When multiple related tasks can be defined naturally, multi-task learning will significantly improve the performance for many problems like computer vision and natural language processing. However, this requires labeled data for each task. A more efficient strategy is to use some auxiliary tasks where data can be collected without manual labeling (to be discussed in Section VI-B). When multiple tasks are learned jointly, how to balance their loss functions becomes the key problem. A dominant approach is to assign each task a pre-defined weight. However, the optimal weight is expensive and time-consuming to find empirically. Therefore, Kendall et al. propose to learn the optimal weights automatically for multiple loss functions by considering the homoscedastic uncertainty of each task. Furthermore, gradient normalization can also be used to automatically balance training in deep multi-task models by dynamically tuning the gradient magnitudes.
D. Multi-modal Learning
The world surrounding us involves multiple modalities: we see objects, hear sounds, feel texture, smell odors, and taste flavors. Different from multi-task learning (Fig. 6b) where multiple tasks are performed with the same input, the purpose of multi-modal learning is to utilize the supplementary and complementary information in different modalities to complete a shared task (Fig. 6c) or multiple related tasks (Fig. 6d).
1) Multi-modal representation and fusion: In multi-modal learning, each instance can be viewed by multiple modalities, which can be fused at different levels. The first approach we can consider is signal-level fusion, for example, the pansharpening of multi-resolution satellite images. However, different modalities usually have different data structures with different sizes or dimensions such as images, sound waves and texts, making them hard to be fused at raw-data level.
Therefore, we must design some modality-wise representations, which could be either handcrafted features or learned with deep neural networks. For example, 2D/3D convolutional neural networks can be used for spatial structured signals like image, CT, fMRI, video, and so on, while recurrent neural networks can be used for temporal data like speech and text.
With modality-wise feature extractions, different modalities are transformed into a unified space, and a straightforward fusion strategy is therefore feature-level fusion, for example, the modality-wise representations can be concatenated into a longer representation or averaged (with learnable modalitywise weights) to a new representation. After that, any traditional model can be learned on the fused feature representation for a given task. Another common strategy is decision-level fusion which is widely-investigated in multiple classifier systems. This fusion strategy is often favored because different models are used on different modalities, making the system more flexible and robust to modalitymissing as the predictions are made independently. Recently, due to the development of deep learning which learns hierarchical representations, the intermediate-level fusion is used to dynamically integrate modalities at different levels with an automatically learned and optimized fusion structure.
Moreover, another important strategy is the learning-based fusion, for example, using cross weights to gradually learn interactions of modalities, using multiple-kernel learning to learn optimal feature fusion, learning sharable and specific features for different modalities, and so on.
A widely-occurring problem in multi-modal learning is modality-missing, i.e., some modalities are unaccessible for some instances during inference. The generative model such as deep Boltzmann machine can be used to handle modality-missing by sampling the absent modality from the conditional distribution. We can also apply the modality-wise dropout during training to improve the generalization performance for modality-missing.
2) Cross-modal matching, alignment, and generation: Besides fusing multiple modalities to make accurate and robust predictions, another vibrant research direction causing increasing attentions is the cross-modal learning. In this case, different modalities are embedded into a coordinated and well���������������
����������
����������
����������
�
����������������
������
������
������
�
������
����������
����������
����������
�
������
������
������
�
Figure 9.
Two types of multi-modal multi-task learning. aligned space, for example, the maximum correlated space by canonical correlation analysis (CCA), the semantic preserved space by joint embedding, and so on. The embedded space enables cross-modal matching tasks, such as retrieving the images that are relevant to a given textual query, deciding which face image is the speaker given an audio clip of someone speaking, and so on. A more difficult problem is cross-modal alignment for finding correspondences between sub-items of instances from multiple modalities. For example, aligning the steps in a recipe to a video showing the dish being made, aligning a movie to the script or the book chapters. According to, the cross-modal alignment can be achieved either explicitly like using dynamic time warping and CCA, or implicitly like using the attention mechanism in deep neural networks. Another task cross-modal generation, which seeks a mapping from one modality to another, has become very popular with an emphasis on language and vision, for example, generating the text description of an input image, inversely generating the image given a text description, and so on. The difficulty is increasing from matching, alignment, to generation, requiring much better understanding and high-level capture of the interaction and relationship between modalities.
3) Multi-modal multi-task learning: The last case we shall discuss is the multi-modal multi-task learning (Fig. 6d), which can be partitioned into two types as shown in Fig. 9. The first and also much simpler setting is the the synchronous case, where all the modalities are available for solving each task.
In this case, a fused representation of all modalities can be learned efficiently during joint training multiple related tasks, which is widely used in different applications such as disease diagnosis, traffic sign recognition, autonomous driving, emotion recognition, and so on. As shown in Fig. 9b, the second and also more challenging setting is the asynchronous case, where different tasks may only rely on their own modalities. For example, image classification works on image, speech recognition deals with sound waves, while machine translation handles text data. Intuitively, it is hard to consider these problems jointly, since both their inputs and outputs are different. Moreover, it is also not clear about the common knowledge shared among these seemingly unrelated problems, and how much benefit we can get from combining them. An interesting work named "one model to learn them all" shows us such possibility and potential, where a single deep neural network to learn multiple tasks from various modalities is designed using modality-specific encoders, I/O mixer, and task-specific decoders. The joint training of diverse tasks with asynchronous image, speech, and text modalities is shown to benefit from shared architecture and parameters. Amazingly, although seemed to be unrelated, incorporating image classification in training would help to improve the performance of language parsing, indicating that some computational primitives can be shared between different modalities and even unrelated tasks.
4) Discussion: There are many practical problems that can benefit from multi-modal learning. In biometric applications, a person can be identified by face, fingerprint, iris, or voice.
Although each modality is already distinguishable, their combination will improve both the accuracy and robustness. In autonomous driving, the fusion of multiple sensors (radar, camera, LIDAR, GPS, and so on) is necessary and important to make robust decisions. Multi-modal analysis will also make the decision making to be more explainable.
Moreover, the human brain is essentially both a multi-modal and a multi-task system: it continuously receives stimuli from various modes of the surrounding world and performs various perceptual and cognitive tasks. For a pattern recognition system, multi-modal perception increases the diversity on input while multi-task improves the diversity on output, and usually diversity will bring robustness. Therefore, joint multi-modal multi-task learning will be an inspiring and important future direction.
VI. BREAKING CLEAN AND BIG DATA ASSUMPTION
Pattern recognition systems usually have strong abilities to memorize training data. As shown in, even if we randomly change the labels of data completely, neural networks can still achieve near zero training error, indicating the strong capacity to fit training data. This is valuable if we have a clean(well-labeled) and large-enough (covering different variations) dataset, and fitting the training data in this case will usually also lead to good generalization performance. However, this assumption is hard to satisfy in real applications. Actually, clean data and big data are contradictory: it is easy to collect a well-labeled small dataset, but it is impossible to manually label a big dataset without any error. Therefore, in order to improve the robustness on both the quality and quantity of data, first of all, the training process should be robust to noisy data, and second, particular learning strategies should be considered to reduce the dependence on large amounts of data.
To reach this goal, we present discussions and summarizations from the following four perspectives.
A. Supervised Learning with Noisy Data
In supervised learning, the noises in data can be partitioned into three types: (1) label noise: the sample is valid but the label is wrong due to mislabeling; (2) sample noise (or attribute noise): the sample is noisy but the label is valid, for example, samples caused by corruption, occlusion, distortion, and so on; (3) outlier noise: both the sample and label are invalid, for example, samples from a new not-care class or a totally noisy signal, but still labeled as one of the classes to be classified. To deal with noisy data, different approaches have been proposed in the literature. Frenay and Verleysen 
15 have surveyed many methods for label noise before the year of 2014. Complementarily, we consider all the three noise types and focus more on methods developed in recent years.
1) Robust loss: The unbounded loss function will usually over-emphasize the noisy data, and hence, the decision boundaries will deviate severely from the optimal one. Therefore, the first solution for learning with noisy data is to redefine the loss function to be bounded. The convex functions are usually unbounded, and therefore, most redefined robust loss functions are non-convex. For example, the ramp loss and truncated hinge loss set an upper bound on hinge loss by allowing a maximum error for each training observation, resulting in a non-convex but robust SVM. The correntropyinduced loss with properties of bounded, smooth, and non-convex is shown to be robust when combined with kernel classifiers. For deep neural networks, as suggested by, the categorical cross entropy loss is sensitive to label noise, and a comparison of different loss functions tolerant to label noise is given in.
2) Noise transition: For a sample x with annotation y(either correct or wrong), we use ˆy to denote its groundtruth (clean) label. Now, the labeling noises can be modeled probabilistically by p(y|ˆy, x) which is usually a complex process. However, we can assume p(y|ˆy, x) = p(y|ˆy): noisy label depends only on true label and not on the sample. This is an approximation of real-world labeling process and can still be useful in some certain scenarios, for example, there are usually some confusable (similar) categories which are hard for human labelers to distinguish, regardless of the specific samples. In this case, we can simply use a noise transition matrix T to specify the probability of one label being wrongly annotated to another Tij = p(y = j|ˆy = i).
Since p(y|x) = �
ˆy p(ˆy|x)p(y|ˆy), we can modify the loss function to be :
L = − 1 n n
� i=1 log
�
� k
� j=1 p(j|xi)Tyi,j
�
�, (19) where k is the number of classes. The matrix T can be estimated from data and subsequently fixed during classifier training, jointly estimated with the classifier,, or estimated with human-assistance. Moreover, Vahdat proposes an undirected graphical model to directly model p(y|ˆy, x) other than p(y|ˆy).
3) Cleaning: Another approach is explicitly detecting and removing the noisy data. An effective strategy is using ensemble learning to filter noisy data. In ensemble learning, different classifiers are complementary to each other, hence, examples which are in contradiction with most learners can be identified confidently as noisy. With this kind of approach, the data pruning method is shown to significantly improve generalization performance. For large-scale dataset cleaning, the partitioning filter is proposed for noise identification from large distributed datasets. Another approach is directly incorporating the noise detection into the objective function of the learning machine, for example, the robust SVM approach uses a binary indicator variable for each sample to explicitly mark it as noisy or clean. In this way, the noisy
Table II
METHODS ON SUPERVISED LEARNING WITH NOISY DATA.
Label Noise
Sample Noise
Outlier Noise
Robust Loss
!
!
!
Noise Transition
!
#
#
Cleaning
!
!
!
Reweighting
!
!
!
Relabeling
!
#
# data can be automatically suppressed and no loss is charged for them during training. Similar idea is also used for learning distance metric from the noisy side information.
4) Reweighting: Reweighting is a soft-version of cleaning: assigning small weights for noisy data other than completely removing them. The work of proposes a reweighting module by a Siamese network to distinguish clean labels and noisy labels under iterative learning. Moreover, the cleanNet assigns weights as the sample-to-label relevance calculated from a joint neural embedding network for measuring the similarity between a sample and its noisy labeled class. The work of mentorNet treats the base model as the studentNet and a mentorNet is used to provide a curriculum (the reweighting scheme) for studentNet to focus on samples with probably correct labels, which is shown to significantly improve the performance on real-world largescale noisy dataset of WebVision.
5) Relabeling: We can also correct the labels of noisy data by relabeling in the learning process. In, a probabilistic graphical model is used to simulate the relationship between samples, labels and noises, for deducing the true label with an EM-like algorithm. The bootstrapping is also used for relabeling the noisy data by updating the labels as convex combination of original noisy label and current prediction of classifier iteratively. Similarly, Tanaka et al. propose to learn model parameters and true labels alternatively under a joint optimization framework.
6) Discussion: As shown in Table II, different methods can handle different data noises. Although the technical details are different, the purposes of robust loss, cleaning, and reweighting are actually similar, i.e., reducing the influence of noisy data in learning process. Therefore, they can be used for all the three noise types. The noise transition and relabeling methods are only suitable for the case of label noise, however, they are efficient and effective strategies to improve robustness when the noises in data are mainly caused by mislabeling.
B. Unsupervised (Self-supervised) Learning
In traditional pattern recognition, unsupervised learning is usually referred to data clustering, however, nowadays, more emphasis is actually placed on unsupervised representation learning, where good and transferrable feature representation is learned from large amounts of unlabeled data.
A widely-used strategy is self-supervised learning, a specific instance of supervised learning where the targets are directly generated from the data and therefore no need for labeling.
1) Reconstruction-based: Since the data are unlabeled, a straightforward strategy is to use them as both the inputs and targets to learn a compressed representation with an encoder f(x) and decoder g(x) to minimize the reconstruction error: min f,g
� x
∥g (f(x)) − x∥.
The first approach following this idea is principal component analysis (PCA) which learns a linear subspace via f(x) = Wx and g(x) = W ⊤x with an orthogonal matrix
W for projection. The restricted Boltzmann machine and auto-encoder are also reconstruction based methods and can be viewed as nonlinear extensions of PCA. From then on, various improvements have been proposed such as denoising auto-encoder, contractive auto-encoder, variational auto-encoder, and so on. The split-brain auto-encoder splits the model into two disjoint subnetworks for cross-channel prediction, which transforms the reconstruction objective to a prediction based one, making the learned feature representation more semantic and meaningful.
2) Pseudo label with clustering: We can also assign some pseudo labels to the data, and then transform the problem to a supervised learning task. For unlabeled data, a natural idea is to use some clustering algorithm to partition the data into different clusters, and then the cluster identities can be viewed as the pseudo label to learn representations. However, a challenge is that the clustering relies heavily on a good representation, and conversely the learning of representation also requires good clustering results as supervision, resulting in a chicken-or-egg-first problem. To solve this problem, the alternative learning strategy, can be used for joint unsupervised learning of the representations and clusters.
3) Pseudo label with exemplar learning: Other than clustering, another method of exemplar learning views each sample as a particular class, the pseudo label now is the sample identity, and the purpose is to separate all training samples from each other as much as possible. The exemplarCNN treats each patch (with random transformations) in an unlabeled image as a particular class, and the classifier is trained to separate all these classes. In this way, the learned representation not only ensures that different patches can be distinguished but also enforces invariance to specified transformations. Another approach uses noise as targets to learn the representation and the one-to-one matching of training samples to uniformly sampled vectors for separating every training instance. In exemplar learning, each instance is treated as a distinct class of its own, therefore, the number of classes is the size of the entire training set. The computational challenges imposed by large number of classes need to be carefully considered in exemplar learning. Recently, the momentum contrast (MoCo) proposes using a dictionary as a queue and a momentum update mechanism to efficiently and effectively realize the idea of exemplar learning, and shows that the gap between unsupervised and supervised representation learning can be closed in many vision tasks.
4) Surrogate tasks for computer vision: Recently, another interesting trend for unsupervised learning is seeking help from some surrogate tasks for which the labels or targets come
Colorization
?
Inpainting
Context Prediction
Rotation Prediction
Figure 10.
Different types of self-supervision for visual tasks. for "free" with the data. For example, as shown in Fig. 10, learning to colorize grayscale image,, learning by inpainting (to generate the contents of an missing area in image conditioned on its surroundings), learning by context prediction (to predict the position of one patch relative to another patch in image), learning by solving jigsaw puzzles (geometric rearrangement of random-permutated patches), learning by predicting image rotations, and so on. For all these tasks, the supervisory signal can be easily obtained automatically, and therefore, there is no need to worry about the insufficiency and labeling of the training data.
Although these tasks seem to be simply defined, doing well on these tasks requires the model to learn meaningful and semantic representations. For example, for inpainting the model needs to understand the content of the image to produce a plausible hypothesis for the missing part, in context prediction the model should learn to recognize objects and also their parts to predict relative positions. Therefore, the learned representations from these surrogate tasks can transfer well to more complicated tasks. Similar approaches can also be found on video related tasks,.
5) Surrogate tasks for natural language: The idea of using surrogate tasks to learn representations from unlabeled data can also be used for other tasks like natural language processing, such as the language model for predicting what comes next in a sequence,. The work of BERT proposes two novel surrogate tasks for unsupervised learning. The first is masked language model which randomly masks some of the tokens from the input and the objective is to predict the original vocabulary identity of the masked word based on its context. The second task is next sentence prediction which is a binary classification task to predict whether a sentence is next to another sentence or not. These strategies have achieved new benchmark performance on eleven language tasks, indicating unsupervised pre-training has become an important integral part for language understanding.
6) Discussion: Due to the abundantly available unlabeled data, unsupervised or self-supervised learning is a long pursued objective for representation learning. In addition to the methods discussed above, it is hopeful to see more and more effective and interesting self-supervised methods in the future.
Besides classification, self-supervised learning can also be
17 used for regression problems. Since different approaches have been proposed from different aspects, the combination of multiple self-supervised methods through multi-task learning (Section V-C) is an inspiring direction.
C. Semi-supervised Learning
Semi-supervised learning (SSL) deals with a small number of labeled data and a large amount of unlabeled data simultaneously, and therefore, can be viewed as a combination of supervised and unsupervised learning. In the literature, a wide variety of methods have been proposed for SSL, and comprehensive surveys can be found in and. Nowadays, new progress especially deep learning based approaches have become the new state-of-the-art, therefore, in this section we focus on recent advances in SSL.
1) Reconstruction-based: A straightforward strategy for
SSL is combining the supervised loss on labeled data and unsupervised loss on unlabeled data to build a new objective function. The ladder network combines the denoising auto-encoder (as unsupervised learning for every layer) with supervised learning at the top layer for SSL. The stacked whatwhere auto-encoder uses a convolutional net for encoding and a deconvolutional net for decoding to simultaneously minimize a combination of supervised and reconstruction losses. Similarly, Zhang et al. take a segment of the classification network as encoder and use mirrored architecture as decoding pathway to build several auto-encoders for SSL.
2) Self/co/tri-training: Another approach is using initial classifier to predict pseudo labels for unlabeled data and then retraining classifier with all data. This process is repeated iteratively for boosting both the accuracy of pseudo labels and the performance of classifier. Following this, the first idea is self-training where a single model is used to predict the pseudo label as the class with maximum predicted probability, which is equivalent to the entropy minimization in SSL.
The co-training uses two different models to label unlabeled data for each other. Moreover, the tri-training utilizes bootstrap sampling to get three different training sets for building three different models. For example, in the trinet approach, three different modules are learned and if two modules agree on the prediction of the unlabeled sample confidently, the two modules will teach the third module on this sample. The strategies of self/co/tri-training are efficient to implement and also effective for SSL.
3) Generative model: Generative model is another widely used strategy for SSL. For example, the Gaussian mixture model can be used to maximize the joint likelihood of both labeled and unlabeled data using EM algorithm. The variational auto-encoder (VAE) can be used for SSL by treating the labels as additional latent variables. A recent trend is using generative adversarial network (GAN) for SSL by setting up an adversarial game between a discriminator D and generator G. In original GAN, D is a binary classifier.
To apply GAN for SSL, D is modified to be a k-class model or extended to k + 1 classes (k real classes and one fake class). For labeled data, D should minimize their supervised loss. For unlabeled data, D is then trained to minimize their uncertainty (e.g. by entropy minimization) to k classes. Moreover, D should also try to distinguish the generated samples by either maximizing their entropy(uncertainty) to k classes or classifying them to the additional fake class. Meanwhile, G is trained from the opposite direction to generate realistic samples for the k classes. By using GAN for SSL, the advantages are two-fold.
First, it can generate synthetic samples for different classes which serve as additional training data. Second, even bad examples from the generator will benefit SSL, because they are lying in low-density areas of the manifold which will guide the classifier to better locate decision boundary.
4) Perturbation-based: Most deep learning models utilize randomness to improve generalization. Therefore, multiple passes of an individual sample through the network might lead to different predictions, and the inconsistency between them can be used as the loss for unlabeled data. Let f be a model with parameter θ, and η, η′ as different randomness(data augmentation, dropout, and so on). The mean squared error is used by to minimize the inconsistency of the predictions ∥f(x|θ, η) − f(x|θ, η′)∥2
2. This is denoted as Πmodel in, where each sample is evaluated twice and the difference of predictions is minimized.
Actually, this can also be explained from the teacher-student viewpoint. For each unlabeled sample, a teacher T (x) is used to guide the learning of the student f by minimizing
∥f(x|θ, η) − T (x)∥2
In Π-model the teacher is another evaluation with a different perturbation T (x) = f(x|θ, η′). However, a single evaluation can be very noisy, therefore, the temporal ensembling proposes the use of exponentially moving average (EMA) of the predictions to form a teacher:
T (x) ← αT (x) + (1 − α)f(x|θ, η′), (22) where 0 < α < 1 is a momentum, and each unlabeled sample has a teacher that is the temporal ensembling of previous predictions. Moreover, the mean teacher proposes to average model parameters other than predictions, i.e., the teacher uses EMA parameters θ′ of student model θ: θ′ ← αθ′ + (1 − α)θ, (23) and now the teacher is T (x) = f(x|θ′, η′), which is the same model with student but using historically-averaged parameters.
The perturbation-based approaches will smooth the predictions on unlabeled data. Moreover, other than random perturbations, the virtual adversarial training can be used to find a worst-case perturbation for better SSL.
5) Global consistency: The perturbation-based approach is actually seeking a kind of local consistency: samples that are close in input space (due to perturbation) should also be close in output space. However, a more important idea is global consistency: samples forming an underlying structure should have similar predictions. To better utilize global consistency, the traditional graph-based SSL is combined with deep learning, by dynamically creating a graph in the latent space in each iteration batch to model data manifold, 18 and then regularizing with the manifold for a more favorable state of class separation. Another interesting approach of learning by association considers global consistency from a different perspective: imagine a walker from labeled data to unlabeled data according to the similarity calculated from latent representation, and then the walker will go back from unlabeled data to labeled data. Correct walks that start and end at the same class are encouraged, and wrong walks that end at a different class are penalized. The cycle-consistent association from labeled data to unlabeled ones and back can be efficiently modeled using transition probabilities, and therefore is an effective strategy to pursue global consistency in SSL.
6) Discussion: In SSL, the massive unlabeled data and the scarce labeled data reveal the underlying manifold of the entire dataset, and by letting the predictions for all samples to be smooth on the manifold, more accurate decision boundary can be obtained compared to purely supervised learning. Reconstruction-based methods can learn a better representation from unlabeled data, while pseudo labels can be used for iterative training on unlabeled data. Recently, a new trend is using GAN to model the distribution of labeled and unlabeled data for SSL. Moreover, perturbation-based methods utilize the randomness in deep neural network to seek local consistency on unlabeled data, and how to effectively consider global consistency in deep learning based SSL still needs more exploration.
D. Few-shot and Zero-shot Learning
In human intelligence, we can instantly learn a novel concept by observing only a few examples from a particular class. However, the state-of-the-art approaches in machine intelligence are usually highly data-hungry. This difference has inspired an important research topic of few-shot learning(FSL). In FSL, we have a many-shot dataset M and few-shot dataset F (usually k-shot n-way: k labeled samples for each of the n classes, and k is small like 1 or 5). The samples in M have disjoint label space with F. The purpose of FSL is to extract transferrable knowledge from M to help us perform better on F, as illustrated in Fig. 11.
1) Metric learning: Under the principle that test and training conditions must match, the episode based training is used to mimic the k-shot n-way setting. Specifically, in each training iteration, an episode is formed by randomly selecting n classes with k samples per class from M to act as the support set S = {(xi, yi)}kn i=1, and meanwhile a fraction of the remainder samples from those n classes are selected as the query set. This support/query split is designed to simulate the real application situation on F. The purpose now is to define the probability P(y|x, S). Although the underlying classes are different between M and F, the learned P(y|x, S) is hoped to be transferable between them, which can be viewed as a point-to-set metric. For example, in, a deep neural network embedded space is learned to calculate such a metric.
To learn a better embedding (or metric), a memory module can be used to explicitly encode the whole support set S into memory for defining P. Moreover, different criteria can be class �� class �� class ��... class �� class �� class ��
?
? class �� class ��
Transfer with few samples:
� Metric
� Meta-learning
� Imagination
Transfer with side information:
� Attribute
� Class name
� Text description
Few-shot Many-shot Zero-shot
Figure 11.
From many-shot to few-shot and zero-shot learning. used to learn the metric like the mean square error and the ranking loss. Since each support set S is designed to be few-shot, the metric learned in this way can be transferred to unseen categories which also have few examples.
2) Learning to learn (meta-learning): The learning to learn or meta-learning is to train another learner at a higher level for guiding the original learner. In, the meta-learner is defined as the transformation from the model parameters learned from few samples to the model parameters learned from large enough samples, which can be viewed as a modelmodel regression. Another approach instead uses the sample-model regression as the meta-learner by transforming each single sample directly to the classifier parameters. Denote original model as ϕ(x; W) where W represents the parameters to be learned, instead of learning W directly, a meta-learner ω(x; W ′) is used in to map x to W. Now, the model becomes ϕ(x; ω(x; W ′)) and the task is changed to learn W ′.
The meta-learner ω(x; W ′) can be used to predict any parameters of another network ϕ like linear layers or convolutional layers, and once learned, the parameters for any novel category can be predicted by a simple forward pass. Besides predicting parameters, other meta-learning methods such as learning the optimization algorithm or initialization can also be used for FSL. Since the tasks considered in metalearning is category-agnostic, they can be well transferred to new few-shot categories.
3) Learning with imagination: Another explanation for the FSL ability of humans is that we can easily visualize or imagine what novel objects should look like from different views although we only see very few examples. This has inspired the learning with imagination to produce additional training examples. As pointed out by, the challenge of FSL is that the few examples can only capture very little of the category's intra-class variation. To solve this, we can use the many-shot dataset M to learn the intra-class transformations of samples and then augment the samples in F along these transformations. In another work, a hallucinator is trained by taking a single example of a category and producing other examples to expand the training set. The hallucinator is trained jointly with the classification model, and the goal is to help the algorithm to learn a better classifier, which is different from other data generation model like GAN 
Table III
EVALUATION METRIC AND REPRESENTATIVE PERFORMANCE FOR DIFFERENT ROBUSTNESS ISSUES IN PATTERN RECOGNITION.
Open-world
Evaluation Metric
Representative Performance
Section IV-A
Classification accuracy: the ratio of number of correctly classified patterns to the total number of patterns, evaluated on a test dataset different from the training dataset.
On a benchmark 1000-class ImageNet dataset, humanlevel accuracy is 94.9% (top-5), and ResNet could achieve
96.43% accuracy. On a smaller 10-class MNIST dataset, it is common to achieve more than 99% accuracy (top-1).
Section IV-B
Rejection performance: a threshold is used to distinguish normal and abnormal patterns, and to evaluate overall performance, threshold-independent metric is used like area under curve.
To detect notMNIST from MNIST, the AUROC (area under receiver operating characteristic curve) is 85% and AUPR(area under precision-recall curve) is 86%.
Section IV-C
Adversarial robustness: Let △(x, f) = minη ∥η∥2 subject to f(x + η) ̸= f(x), the robustness of classifier f is ρ(f) = Ex
△(x,f)
∥x∥2 where Ex is expectation over data.
On ILSVRC the adversarial robustness is 2.7 × 10−3 for CaffeNet and 1.9 × 10−3 for GoogLeNet, indicating that: a perturbation with 1/1000 magnitude as the original image is sufficient to fool state-of-the-art deep neural networks.
Section IV-D
Class-incremental capacity: the changing trend of classification accuracy as the number of classes increased.
On ILSVRC as the number of classes incremented from 100 to
1000, the accuracy is reduced from about 90% to 45%.
Non-iid
Evaluation Metric
Representative Performance
Section V-A
Contextual learning ability: the performance improvement caused by learning from a group of interdependent patterns.
By integrating geometric and linguistic contexts, improved the correct rate of handwritten Chinese text recognition from
69% to 91%.
Section V-B
Adaptability and transferability: the performance of adaptation and transfer between different (i.e., source and target) domains which have different data distributions.
Through writer adaptation with style transfer mapping, achieved more than 30% error reduction rate on a large scale handwriting recognition database CASIA-OLHWDB.
Section V-C
Multi-task cooperation ability: the gain of considering multiple tasks simultaneously compared to handling them independently.
The taskonomy reduced the number of labeled samples needed for solving 10 tasks by roughly 2
3 (compared to training independently) while keeping the performance nearly the same.
Section V-D
Multi-modal fusion ability: the boost of performance by utilizing the complementary information in different modalities.
By fusing multiple modalities at several spatial and temporal scales, won the first place out of 17 teams on the ChaLearn
2014 "looking at people challenge gesture recognition track".
Noisy Small Data
Evaluation Metric
Representative Performance
Section VI-A
Noisy data tolerance: the stability of classification performance when the training data contain a certain percentage of noises.
On CIFAR10, with clean data the accuracy is 93%, when 30% of the data are noisy, the accuracy is deteriorated to 72%, and a noise-robust model can recover the accuracy to 91%.
Section VI-B
Self-supervised capability: performance of learning from purely unlabeled data under some self-supervised mechanisms.
Using instance discrimination as the self-supervision, the unsupervised MoCo outperformed its supervised pre-training counterpart in 7 vision tasks on many datasets.
Section VI-C
Semi-supervised capability: performance of joint learning from massive unlabeled data and scarce labeled data.
On ImageNet, the accuracy of supervised learning (100% labeled data) is 96%, a semi-supervised model (10% labeled and 90% unlabeled data) could achieve 91% accuracy.
Section VI-D
Few-shot generalization: knowledge transfer ability from learning of old classes to new classes with few or even zero data.
On miniImageNet with 5-class, the 1-shot (one sample per class) accuracy is 49% and the 5-shot accuracy is 68%.
On CUB with 50-class, the 0-shot (no sample but side information of attribute is available) accuracy is 55%. whose goal is to generate realistic examples. Actually, the effectiveness of learning with imagination comes from the recovery of the intra-class variation missed in FSL.
4) Zero-shot learning: An extreme case of FSL is zeroshot learning (ZSL) where there is no example for the novel categories. In this case, some side information is needed to transfer the knowledge from previously learned categories to novel categories (Fig. 11), including attributes, class names, word vector, text description, and so on.
In attribute-based ZSL, attributes are typically nameable properties that are present or not for a certain category. In this way, multiple binary attribute-specific classifiers can be trained independently. After that, for a new class, training samples are no longer required, we only need the attribute associations for this class, and a test sample can be effectively classified by checking its predicted attributes. Besides userdefined attributes, learning the latent attributes and the class-attribute associations can further improve performance. A more general approach for ZSL suitable for different side information is embedding based approach, where two embedding networks are learned for both samples and side information, and the similarity between them are measured using Euclidean, cosine, or manifold distance.
In the embedded space, nearest neighbor search (cross-modal match) can then be efficiently used for ZSL. The last approach for ZSL is synthesizing class-specific samples conditioned on their side information for unseen classes, which can be implemented in various ways like: data generation at feature-level or sample-level, using variational auto-encoder or generative adversarial network, conditioned on attributes or text descriptions, and so on.
5) Discussion: Building a good model totally from scratch with a small number of observations is difficult, and actually, the FSL abilities of human beings are based on our abundant prior experiences of dealing with related tasks. Therefore, as pointed out by : the key insight for FSL is that the categories we have already learned can give us information that helps us to learn new categories with fewer examples.
Therefore, FSL can be viewed as cross-class transfer learning.
Moreover, humans are good at ZSL because we have other knowledge sources (like book and Internet) from which we can infer what a new category looks like. Therefore, ZSL is more like cross-modal learning (Section V-D). Although many approaches have been proposed in the literature, few-shot and zero-shot learning are still urgently needed skills for machine intelligence.
VII. CONCLUDING REMARKS
This paper considers the robustness of pattern recognition from the perspective of three basic assumptions, which are reasonable in controlled laboratory environments for pursuing high accuracies, however, will become unstable and unreliable in real-life applications. To improve robustness, we present a comprehensive literature review of the approaches trying to break these assumptions:
• For breaking closed-world assumption, we partition the open-space into four components: the known known corresponding to the empirical risk, the known unknown corresponding to the outlier risk, the unknown known corresponding to the adversarial risk, and the unknown unknown corresponding to the open class risk.
• For breaking independent and identically distributed assumption, we first discuss the problems in learning with interdependent data, then review recent advances in domain adaptation and transfer learning, and finally analyse the multi-task and multi-modal learning for increasing the diversity on both output and input of the system.
• For breaking clean and big data assumption, we first introduce supervised learning with noisy data, then review un/self-supervised and semi-supervised learning to learn from unlabeled data and surrogate supervision, and lastly discuss few-shot and zero-shot learning to transfer knowledge from big-data to small-data.
With the above approaches, we can improve the robustness of a pattern recognition system by: growing continuously with changing concepts in open world, adapting smoothly with changing environments under non-identical conditions, and learning stably with changing resources under different data quality and quantity. Actually, these are fundamental issues in robust pattern recognition, because these changing factors will usually greatly affect the stability of final performance in practice. Furthermore, in continuous use of a pattern recognition system, other than being a static model, how to make it evolvable during lifelong learning, or never-ending learning is an important step towards real intelligence.
Through breaking the three basic assumptions, we can actually eliminate the main obstacles in reaching this goal.
Unlike the traditional closed-world classification which is usually evaluated by accuracy, how to evaluate the recognition performance in open and changing environments is a big issue.
Besides accuracy, other evaluation measurements reflecting the ability in dealing with the changing factors are more important. As shown in Table III, when considering many other evaluation metrics (different from the classification accuracy), it is obvious that pattern recognition is far from solved.
Moreover, in the research community, different tasks are usually evaluated with different metrics and databases. How to build a general benchmark for evaluating the robustness by integrating different metrics together is an important future task for pattern recognition.
Different from the widely-used empirical risk minimization, theoretical analysis to unify different open-world risks will become the foundation for future classifier design. A future pattern recognition system should acquire complementary information from interdependent data in different modalities and boost itself through the cooperation of multiple tasks by adaptively learning from a few labeled, unlabeled or noisy data.
Although many attempts have been proposed in the literature, most of them try to solve a single problem from a single perspective. However, the three basic assumptions are actually related, and through joint consideration many new research problems can be raised, such as open-world domain adaptation, open-world semi-supervised learning, crossmodal domain adaptation, multi-task self-supervised learning, few-shot domain adaptation, and so on.
Future research of a unified framework to deal with the openworld, non-i.i.d., noisy and small data issues simultaneously is the ultimate goal of robust pattern recognition.
Besides the robustness issues, many other problems are also important for pattern recognition. For example, the interpretability of the model: besides high accuracy, the system also needs to explain why such a prediction is made, for increasing our confidence and safety in trusting the result.
Some traditional classifiers like decision tree and logistic regression are interpretable, but how to make other models especially black-box deep neural networks explainable is an important task. Another important issue is computational efficiency. Besides big data, strong computing power is also a key for the success of modern pattern recognition technologies. In order to widen the application scope and also reduce resource consumption, the compression and acceleration of pattern recognition models are of great values for practical applications. Since pattern recognition can be viewed as the simulation of human brain perception ability which enables machine to recognize objects or events in sensing data, how to effectively learn from neuroscience for developing brain-inspired, biologically-plausible or psychophysics-driven pattern recognition models is an inspiring future direction. With more attentions and efforts paid to these important issues in pattern recognition, the gap between human intelligence and machine intelligence can be narrowed in the foreseeable future.
APPENDIX A
BACKGROUND REFERENCES
• Pattern Recognition 
• Deep Learning 
• Outlier Detection 
• Adversarial Example 
• Open Set Recognition 
• Class-incremental Learning 
• Contextual Learning 
• Domain Adaptation 
• Transfer Learning 
• Multi-task Learning 
• Multi-modal Learning 
• Learning with Noise 
• Representation Learning 
• Self-supervised Learning 
• Semi-supervised Learning 
• Few-shot Learning 
• Zero-shot Learning 
REFERENCES
 Z. Akata, F. Perronnin, Z. Harchaoui, and C. Schmid. Label-embedding for image classification.
IEEE Trans. Pattern Anal. Mach. Intell., 38(7):1425–1438, 2016.
 Z. Al-Halah, M. Tapaswi, and R. Stiefelhagen. Recovering the missing link: Predicting class-attribute associations for unsupervised zero-shot learning. In IEEE Conf. Comput. Vis. Pattern Recognit., pages 5975–
 T. Almaev, B. Martinez, and M. Valstar. Learning to transfer: transferring latent task structures and its application to person-specific facial action unit detection. In Int. Conf. Comput. Vis., pages 3774–3782, J. Amores.
Multiple instance classification: Review, taxonomy and comparative study. Artificial Intelligence, 201:81–105, 2013.
 G. Andrew, R. Arora, J. Bilmes, and K. Livescu.
Deep canonical correlation analysis.
In Int. Conf. Mach. Learn., pages 1247–1255, A. Angelova, Y. Abu-Mostafam, and P. Perona. Pruning training sets for learning of object categories. In IEEE Conf. Comput. Vis. Pattern
Recognit., pages 494–501, 2005.
 R. Arandjelovic and A. Zisserman. Look, listen and learn. In Int. Conf.
Comput. Vis., pages 609–617, 2017.
 H. Azizpour, A. Razavian, J. Sullivan, A. Maki, and S. Carlsson.
Factors of transferability for a generic ConvNet representation. IEEE
Trans. Pattern Anal. Mach. Intell., 38(9):1790–1802, 2016.
 T. Baltrusaitis, C. Ahuja, and L. Morency.
Multimodal machine learning: A survey and taxonomy. IEEE Trans. Pattern Anal. Mach.
Intell., 41(2):423–443, 2019.
 M. Bautista, A. Sanakoyeu, E. Sutter, and B. Ommer. CliqueCNN:
Deep unsupervised exemplar learning. In Advances Neural Inf. Process.
Syst., pages 3846–3854, 2016.
 A. Bendale and T. Boult. Towards open world recognition. In IEEE
Conf. Comput. Vis. Pattern Recognit., pages 1893–1902, 2015.
 A. Bendale and T. Boult. Towards open set deep networks. In IEEE
Conf. Comput. Vis. Pattern Recognit., pages 1563–1572, 2016.
 Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives. IEEE Trans. Pattern Anal. Mach. Intell., 35(8):1798–1828, 2013.
 Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin.
A neural probabilistic language model.
J. Mach. Learn. Res., 3:1137–1155, Y. Bengio, D.-H. Lee, J. Bornschein, T. Mesnard, and Z. Lin. Towards biologically plausible deep learning. arXiv:1502.04156, 2015.
 L. Bertinetto, J. Henriques, J. Valmadre, P. Torr, and A. Vedaldi.
Learning feed-forward one-shot learners.
In Advances Neural Inf.
Process. Syst., pages 523–531, 2016.
 H. Bilen and A. Vedaldi. Integrated perception with recurrent multitask neural networks. In Advances Neural Inf. Process. Syst., pages
235–243, 2016.
 C. Bishop. Neural Networks for Pattern Recognition. Oxford university press, 1995.
 C. Bishop. Training with noise is equivalent to Tikhonov regularization.
Neural Computation, 7(1):108–116, 1995.
 C. Bishop. Pattern Recognition and Machine Learning. Springer-Verlag
New York, 2006.
 A. Blum and T. Mitchell. Combining labeled and unlabeled data with co-training. In Annual conf. Comput. Learn. Theory, pages 92–100, P. Bojanowski and A. Joulin.
Unsupervised learning by predicting noise. In Int. Conf. Mach. Learn., pages 517–526, 2017.
 L. Breiman. Classification and Regression Trees. Routledge, 2017.
 C. Brodley and M. Friedl. Identifying mislabeled training data. J. Artif.
Intell. Res., 11:131–167, 1999.
 J. Brooks. Support vector machines with the ramp loss and the hard margin loss. Operations Research, 59(2):467–479, 2011.
 H. Bunke and K. Riesen.
Recent advances in graph-based pattern recognition with applications in document analysis. Pattern Recognition, 44(5):1057–1067, 2011.
 H. Bunke and A. Sanfeliu. Syntactic and Structural Pattern Recognition
- Theory and Applications. World Scientific, 1990.
 P. Busto and J. Gall. Open set domain adaptation. In Int. Conf. Comput.
Vis., pages 754–763, 2017.
 Q. Cai, Y. Pan, T. Yao, C. Yan, and T. Mei. Memory matching networks for one-shot image recognition. In IEEE Conf. Comput. Vis. Pattern
Recognit., pages 4080–4088, 2018.
 J. Cao, Y. Li, and Z. Zhang. Partially shared multi-task convolutional neural network with local constraint for face attribute learning. In IEEE
Conf. Comput. Vis. Pattern Recognit., pages 4290–4299, 2018.
 N. Carlini and D. Wagner. Towards evaluating the robustness of neural networks. In IEEE Symposium Security Privacy, pages 39–57, 2017.
 R. Caruana. Multitask learning. Machine Learning, 28(1):41–75, 1997.
 H. Cevikalp. Best fitting hyperplanes for classification. IEEE Trans.
Pattern Anal. Mach. Intell., 39(6):1076–1088, 2017.
 V. Chandola, A. Banerjee, and V. Kumar. Anomaly detection: A survey.
ACM Computing Surveys, 41(3):1–58, 2009.
 I. Chang and M. Loew. Pattern recognition with new class discovery.
In IEEE Conf. Comput. Vis. Pattern Recognit., pages 438–443, 1991.
 O. Chapelle, B. Scholkopf, and A. Zien. Semi-Supervised Learning.
MIT Press, 2006.
 D.-D. Chen, W. Wang, W. Gao, and Z.-H. Zhou. Tri-net for semisupervised deep learning. In Int. Joint Conf. Artif. Intell., 2018.
 S. Chen, Q. Jin, J. Zhao, and S. Wang. Multimodal multi-task learning for dimensional and continuous emotion recognition. In ACM Annual
Workshop on Audio/Visual Emotion Challenge, pages 19–26, 2017.
 Y. Chen, J. Bi, and J. Wang. MILES: Multiple-instance learning via embedded instance selection. IEEE Trans. Pattern Anal. Mach. Intell., 28(12):1931–1947, 2006.
 Z. Chen, V. Badrinarayanan, C.-Y. Lee, and A. Rabinovich. GradNorm:
Gradient normalization for adaptive loss balancing in deep multitask networks. In Int. Conf. Mach. Learn., pages 1–10, 2018.
 C. Chibelushi, F. Deravi, and J. Mason. Adaptive classifier integration for robust pattern recognition.
IEEE Trans. Systems, Man, and Cybernetics, 29(6):902–907, 1999.
 K. Cho, A. Courville, and Y. Bengio. Describing multimedia content using attention-based encoder-decoder networks. IEEE Trans. Multimedia, 17(11):1875–1886, 2015.
 C. Chow. On optimum recognition error and reject tradeoff. IEEE
Trans. Information Theory, 16(1):41–46, 1970.
 S. Chowdhuri, T. Pankaj, and K. Zipser. MultiNet: Multi-modal multitask learning for autonomous driving. arXiv:1709.05581v4, 2019.
 C. Ciliberto, A. Rudi, L. Rosasco, and M. Pontil. Consistent multitask learning with nonlinear output relations.
In Advances Neural Inf.
Process. Syst., pages 1986–1996, 2017.
 R. Cinbis, J. Verbeek, and C. Schmid.
Weakly supervised object localization with multi-fold multiple instance learning. IEEE Trans.
Pattern Anal. Mach. Intell., 39(1):189–203, 2017.
 R. Collobert and J. Weston. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Int. Conf.
Mach. Learn., pages 160–168, 2008.
 C. Cortes and V. Vapnik. Support vector machine. Machine Learning, 20(3):273–297, 1995.
 N. Courty, R. Flamary, D. Tuia, and A. Rakotomamonjy.
Optimal transport for domain adaptation.
IEEE Trans. Pattern Anal. Mach.
Intell., 39(9):1853–1865, 2017.
 G.E. Dahl, D. Yu, L. Deng, and A. Acero. Context-dependent pretrained deep neural networks for large-vocabulary speech recognition.
IEEE Trans. Audio, Speech, Langu. Process., 20(1):30–42, 2012.
 A. Dai and Q. Le. Semi-supervised sequence learning. In Advances
Neural Inf. Process. Syst., pages 3079–3087, 2015.
 Z. Dai, Z. Yang, F. Yang, W. Cohen, and R. Salakhutdinov. Good semisupervised learning that requires a bad GAN. In Advances Neural Inf.
Process. Syst., pages 6510–6520, 2017.
 T. Darrell, M. Kloft, M. Pontil, G. Ratsch, and E. Rodner.
Machine learning with interdependent and non-identically distributed data.
Dagstuhl Reports (Dagstuhl Seminar 15152), 5(4):18–55, 2015.
 J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova.
BERT: Pretraining of deep bidirectional transformers for language understanding. arXiv:1810.04805, 2018.
 T. Dietterich. Steps toward robust artificial intelligence. AI Magazine, 38(3):3–24, 2017.
 T. Dietterich, R. Lathrop, and T. Lozano-Perez. Solving the multiple instance problem with axis-parallel rectangles. Artificial Intelligence, 89:31–71, 1997.
 C. Doersch, A. Gupta, and A. Efros. Unsupervised visual representation learning by context prediction. In Int. Conf. Comput. Vis., pages 1422–
 C. Doersch and A. Zisserman.
Multi-task self-supervised visual learning. In Int. Conf. Comput. Vis., pages 2051–2060, 2017.
 J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and T. Darrell. DeCAF: A deep convolutional activation feature for generic visual recognition. In Int. Conf. Mach. Learn., pages 647–655, 2014.
 W. Dong, G. Shi, X. Li, Y. Ma, and F. Huang. Compressive sensing via nonlocal low-rank regularization.
IEEE Trans. Image Process., 23(8):3618–3632, 2014.
 A. Dosovitskiy, P. Fischer, J. Springenberg, M. Riedmiller, and T. Brox.
Discriminative unsupervised feature learning with exemplar convolutional neural networks.
IEEE Trans. Pattern Anal. Mach. Intell., 38(9):1734–1747, 2016.
 B. Dubuisson and M. Masson.
A statistical decision rule with incomplete knowledge about classes. Pattern Recognition, 26(1):155–
 R. Duda, P. Hart, and D. Stork. Pattern Classification. John Wiley &
Sons, 2001.
 L. Duong, T. Cohn, S. Bird, and P. Cook. Low resource dependency parsing: Cross-lingual parameter sharing in a neural network parser. In
Int. Joint Conf. Natural Language Processing, pages 845–850, 2015.
 E. Elhamifar and R. Vidal. Robust classification using structured sparse representation. In IEEE Conf. Comput. Vis. Pattern Recognit., pages
1873–1879, 2011.
 S. Ertekin, L. Bottou, and C. Giles. Nonconvex online support vector machines.
IEEE Trans. Pattern Anal. Mach. Intell., 33(2):368–381, A. Fawzi, S. Moosavi-Dezfooli, and P. Frossard. The robustness of deep networks: A geometrical perspective. IEEE Signal Process. Magazine, 34(6):50–62, 2017.
 L. Fei-Fei, R. Fergus, and P. Perona.
One-shot learning of object categories. IEEE Trans. Pattern Anal. Mach. Intell., 28(4):594–611, P. Felzenszwalb, R. Girshick, D. McAllester, and D. Ramanan. Object detection with discriminatively trained part-based models. IEEE Trans.
Pattern Anal. Mach. Intell., 32(9):1627–1645, 2010.
 C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In Int. Conf. Mach. Learn., 2017.
 J. Foulds and E. Frank. A review of multi-instance learning assumptions. Knowledge Engineering Review, 25(1):1–25, 2010.
 B. Frenay and M. Verleysen. Classification in the presence of label noise: a survey. IEEE Trans. Neural Netw. Learn. Syst., 25(5):845–
 Y. Freund and R. Schapire. Experiments with a new boosting algorithm.
In Int. Conf. Mach. Learn., pages 148–156, 1996.
 J. Friedman. Regularized discriminant analysis. J. American Statistical
Association, 84(405):165–175, 1989.
 A. Frome, G. Corrado, J. Shlens, S. Bengio, J. Dean, M. Ranzato, and T. Mikolov. DeViSE: A deep visual-semantic embedding model. In
Advances Neural Inf. Process. Syst., pages 2121–2129, 2013.
 K.-S. Fu. Recent developments in pattern recognition. IEEE Trans.
Comput., 29(10):845–854, 1980.
 Z. Fu, T. Xiang, E. Kodirov, and S. Gong.
Zero-shot learning on semantic class prototype graph.
IEEE Trans. Pattern Anal. Mach.
Intell., 40(8):2009–2022, 2018.
 K. Fukunaga. Introduction to Statistical Pattern Recognition. Academic
Press, 1990.
 Y. Ganin and V. Lempitsky.
Unsupervised domain adaptation by backpropagation. In Int. Conf. Mach. Learn., pages 1180–1189, 2015.
 Z. Ge, S. Demyanov, Z. Chen, and R. Garnavi. Generative openmax for multi-class open set classification. arXiv:1707.07418, 2017.
 A. Ghosh, H. Kumar, and P. Sastry. Robust loss functions under label noise for deep neural networks.
In AAAI Conf. Artif. Intell., pages
1919–1925, 2017.
 A. Ghosh, N. Manwani, and P. Sastry.
Making risk minimization tolerant to label noise. Neurocomputing, 160:93–107, 2015.
 S. Gidaris, P. Singh, and N. Komodakis. Unsupervised representation learning by predicting image rotations. In Int. Conf. Learn. Representations, 2018.
 X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks. In Int. Conf. Artif. Intell. Stat., pages
249–256, 2010.
 M. Gonen and E. Alpaydin. Multiple kernel learning algorithms. J.
Mach. Learn. Res., 12:2211–2268, 2011.
 I. Goodfellow, Y. Bengio, and A. Courville.
Deep Learning.
MIT
Press, 2016. http://www.deeplearningbook.org.
 I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In
Advances Neural Inf. Process. Syst., pages 2672–2680, 2014.
 I. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. In Int. Conf. Learn. Representations, 2015.
 Y. Grandvalet and Y. Bengio.
Semi-supervised learning by entropy minimization. In Advances Neural Inf. Process. Syst., pages 529–536, Y. Grandvalet, A. Rakotomamonjy, J. Keshet, and S. Canu. Support vector machines with a reject option. In Advances Neural Inf. Process.
Syst., pages 537–544, 2009.
 A. Graves, S. Fernandez, F. Gomez, and J. Schmidhuber. Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks. In Int. Conf. Mach. Learn., pages 369–376, A. Graves, M. Liwicki, S. Fernandez, R. Bertolami, H. Bunke, and J. Schmidhuber.
A novel connectionist system for unconstrained handwriting recognition.
IEEE Trans. Pattern Anal. Mach. Intell., 31(5):855–868, 2009.
 A. Gretton, K. Borgwardt, M. Rasch, B. Scholkopf, and A. Smola. A kernel method for the two-sample-problem. In Advances Neural Inf.
Process. Syst., pages 513–520, 2007.
 J. Gu, J. Cai, S. Joty, L. Niu, and G. Wang. Look, imagine and match:
Improving textual-visual cross-modal retrieval with generative models.
In IEEE Conf. Comput. Vis. Pattern Recognit., pages 7181–7189, 2018.
 S. Gu and L. Rigazio. Towards deep neural network architectures robust to adversarial examples. arXiv:1412.5068, 2014.
 S. Guerriero, B. Caputo, and T. Mensink. Deep nearest class mean classifiers. In Worskhop Int. Conf. Learn. Representations, 2018.
 I. Guyon and A. Elisseeff.
An introduction to variable and feature selection. J. Mach. Learn. Res., 3:1157–1182, 2003.
 P. Haeusser, A. Mordvintsev, and D. Cremers. Learning by association: a versatile semi-supervised training method for neural networks. In
IEEE Conf. Comput. Vis. Pattern Recognit., pages 89–98, 2017.
 J. Hampshire and A. Waibel. The meta-pi network: Building distributed knowledge representations for robust multisource pattern recognition.
IEEE Trans. Pattern Anal. Mach. Intell., 14(7):751–769, 1992.
 B. Han, J. Yao, G. Niu, M. Zhou, I. Tsang, Y. Zhang, and M. Sugiyama.
Masking: A new perspective of noisy supervision. arXiv:1805.08193, S. Han, H. Mao, and W. Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding.
In Int. Conf. Learn. Representations, 2016.
 R. Haralick. Decision making in context. IEEE Trans. Pattern Anal.
Mach. Intell., 5(4):417–428, 1983.
 B. Hariharan and R. Girshick. Low-shot visual recognition by shrinking and hallucinating features. In Int. Conf. Comput. Vis., pages 3018–
 M. Hayat, M. Bennamoun, and S. An. Deep reconstruction models for image set classification. IEEE Trans. Pattern Anal. Mach. Intell., 37(4):713–727, 2015.
 C. He, R. Wang, S. Shan, and X. Chen. Exemplar-supported generative reproduction for class incremental learning. In British Machine Vision
Conf., 2018.
 K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick. Momentum contrast for unsupervised visual representation learning. arXiv:1911.05722, 2019.
 K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In IEEE Conf. Comput. Vis. Pattern Recognit., pages 770–
 R. He, W.-S. Zheng, B.-G. Hu, and X.-W. Kong. A regularized correntropy framework for robust pattern recognition. Neural Computation, 23(8):2074–2100, 2011.
 D. Hendrycks and K. Gimpel. A baseline for detecting misclassified and out-of-distribution examples in neural networks.
In Int. Conf.
Learn. Representations, 2017.
 G. Hinton and R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504–507, 2006.
 T. Ho, J. Hull, and S. Srihari.
Decision combination in multiple classifier systems. IEEE Trans. Pattern Anal. Mach. Intell., 16(1):66–
 V. Hodge and J. Austin. A survey of outlier detection methodologies.
Artif. Intell. Review, 22(2):85–126, 2004.
 J. Hoffman, S. Gupta, J. Leong, S. Guadarrama, and T. Darrell. Crossmodal adaptation for RGB-D detection. In IEEE Int. Conf. Robotics
Automation, pages 5032–5039, 2016.
 J. Hoffman, B. Kulis, T. Darrell, and K. Saenko. Discovering latent domains for multisource domain adaptation.
In European Conf.
Computer Vision, pages 702–715, 2012.
 J. Hoffman, E. Rodner, J. Donahue, T. Darrell, and K. Saenko. Efficient learning of domain-invariant image representations. In Int. Conf. Learn.
Representations, pages 1–9, 2013.
 C. Hu, Y. Chen, L. Hu, and X. Peng. A novel random forests based class incremental learning method for activity recognition.
Pattern
Recognition, 78:277–290, 2018.
 Y. Hu, A. Mian, and R. Owens.
Face recognition using sparse approximated nearest points between image sets. IEEE Trans. Pattern
Anal. Mach. Intell., 34(10):1992–2004, 2012.
 J. Huang, A. Smola, A. Gretton, K. Borgwardt, and B. Scholkopf.
Correcting sample selection bias by unlabeled data. In Advances Neural
Inf. Process. Syst., pages 601–608, 2007.
 K. Huang, R. Jin, Z. Xu, and C.-L. Liu. Robust metric learning by smooth optimization. In Conf. Uncertain. Artif. Intell., 2010.
 S.-J. Huang, R. Jin, and Z.-H. Zhou.
Active learning by querying informative and representative examples. IEEE Trans. Pattern Anal.
Mach. Intell., 36(10):1936–1949, 2014.
 H. Daume III and D. Marcu.
Domain adaptation for statistical classifiers. J. Artif. Intell. Res., 26:101–126, 2006.
 ILSVRC.
ImageNet large scale visual recognition challenge. http:
//www.image-net.org/challenges/LSVRC.
 S. Ioffe and C. Szegedy.
Batch normalization: Accelerating deep network training by reducing internal covariate shift.
In Int. Conf.
Mach. Learn., 2015.
 A. Jain. Data clustering: 50 years beyond K-means. Pattern Recognit.
Lett., 31(8):651–666, 2010.
 A.K. Jain, R.P.W. Duin, and J. Mao. Statistical pattern recognition: A review. IEEE Trans. Pattern Anal. Mach. Intell., 22(1):4–37, 2000.
 L. Jiang, Z. Zhou, T. Leung, L.-J. Li, and L. Fei-Fei.
Mentornet:
Learning data-driven curriculum for very deep neural networks on corrupted labels. In Int. Conf. Mach. Learn., pages 2309–2318, 2018.
 I. Jindal, M. Nokleby, and X. Chen. Learning deep networks from noisy labels with dropout regularization. In Int. Conf. Data Mining, pages 967–972, 2016.
 P. Junior, R. Souza, R. Werneck, B. Stein, D. Pazinato, W. Almeida, O. Penatti, R. Torres, and A. Rocha. Nearest neighbors distance ratio open-set classifier. Machine Learning, 106(3):359–386, 2017.
 L. Kaiser, A. Gomez, N. Shazeer, A. Vaswani, N. Parmar, L. Jones, and J. Uszkoreit. One model to learn them all. arXiv:1706.05137, K. Kamnitsas, D. Castro, L. Folgoc, I. Walker, R. Tanno, D. Rueckert, B. Glocker, A. Criminisi, and A. Nori. Semi-supervised learning via compact latent space clustering. In Int. Conf. Mach. Learn., 2018.
 M. Kan, J. Wu, S. Shan, and X. Chen. Domain adaptation for face recognition: Targetize source domain bridged by common subspace.
Int. Journal of Computer Vision, 109(1):94–109, 2014.
 M. Kandemir and F. Hamprecht. Computer-aided diagnosis from weak supervision: A benchmarking study. Comput. Med. Imaging Graph., 42:44–50, 2015.
 X. Kang, S. Li, and J.A. Benediktsson. Pansharpening with matting model.
IEEE Trans. Geoscience and Remote Sensing, 52(8):5088–
 B. Karmakar and N. Pal. How to make a neural network say "Don't know". Information Sciences, 430:444–466, 2018.
 A. Kendall, Y. Gal, and R. Cipolla. Multi-task learning using uncertainty to weigh losses for scene geometry and semantics.
In IEEE
Conf. Comput. Vis. Pattern Recognit., pages 7482–7491, 2018.
 Y. Kharin.
Robustness in Statistical Pattern Recognition.
Springer
Science & Business Media, 1996.
 T.-K. Kim, J. Kittler, and R. Cipolla.
Discriminative learning and recognition of image set classes using canonical correlations. IEEE
Trans. Pattern Anal. Mach. Intell., 29(6):1005–1018, 2007.
 D. Kingma and J. Ba. Adam: A method for stochastic optimization.
In Int. Conf. Learn. Representations, 2015.
 D. Kingma, S. Mohamed, D. Rezende, and M. Welling.
Semisupervised learning with deep generative models. In Advances Neural
Inf. Process. Syst., pages 3581–3589, 2014.
 D. Kingma and M. Welling.
Auto-encoding variational bayes. arXiv:1312.6114, 2013.
 T. Kipf and M. Welling.
Semi-supervised classification with graph convolutional networks. In Int. Conf. Learn. Representations, 2017.
 A. Kolesnikov, X. Zhai, and L. Beyer. Revisiting self-supervised visual representation learning. In IEEE Conf. Comput. Vis. Pattern Recognit., pages 1920–1929, 2019.
 D. Koller and N. Friedman.
Probabilistic Graphical Models.
MIT
Press, 2009.
 S. Kotz and S. Nadarajah. Extreme Value Distributions: Theory and Applications. World Scientific, 2000.
 J. Kozerawski and M. Turk. CLEAR: Cumulative learning for oneshot one-class image recognition. In IEEE Conf. Comput. Vis. Pattern
Recognit., pages 3446–3455, 2018.
 A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classification with deep convolutional neural networks. In Advances Neural Inf. Process.
Syst., pages 1097–1105, 2012.
 L. Kuncheva. Combining Pattern Classifiers: Methods and Algorithms.
John Wiley & Sons, 2004.
 A. Kurakin, I. Goodfellow, and S. Bengio. Adversarial examples in the physical world. arXiv:1607.02533, 2016.
 I. Kuzborskij, F. Orabona, and B. Caputo. From n to n+ 1: Multiclass transfer incremental learning.
In IEEE Conf. Comput. Vis. Pattern
Recognit., pages 3358–3365, 2013.
 J. Lafferty, A. McCallum, and F. Pereira. Conditional random fields:
Probabilistic models for segmenting and labeling sequence data. In Int.
Conf. Mach. Learn., pages 282–289, 2001.
 S. Laine and T. Aila.
Temporal ensembling for semi-supervised learning. In Int. Conf. Learn. Representations, 2017.
 B. Lake, R. Salakhutdinov, and J. Tenenbaum.
Human-level concept learning through probabilistic program induction.
Science, 350(6266):1332–1338, 2015.
 C. Lampert.
Kernel methods in computer vision.
Foundations and Trends in Computer Graphics and Vision, 4(3):193–285, 2009.
 C. Lampert, H. Nickisch, and S. Harmeling. Attribute-based classification for zero-shot visual object categorization. IEEE Trans. Pattern
Anal. Mach. Intell., 36(3):453–465, 2014.
 G. Larsson, M. Maire, and G. Shakhnarovich. Learning representations for automatic colorization. In Eur. Conf. Comput. Vis., pages 577–593, P. Laskov, C. Gehl, S. Kruger, and K.-R. Muller. Incremental support vector learning: Analysis, implementation and applications. J. Mach.
Learn. Res., 7:1909–1936, 2006.
 Y. LeCun, Y. Bengio, and G. Hinton.
Deep learning.
Nature, 521(7553):436–444, 2015.
 Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner.
Gradient-based learning applied to document recognition. Proc. IEEE, 86(11):2278–
 D.-H. Lee.
Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks.
In Workshop Int. Conf.
Mach. Learn., 2013.
 K.-H. Lee, X. He, L. Zhang, and L. Yang. CleanNet: Transfer learning for scalable image classifier training with label noise. In IEEE Conf.
Comput. Vis. Pattern Recognit., pages 5447–5456, 2018.
 C.J. Leggetter and P.C. Woodland.
Maximum likelihood linear regression for speaker adaptation of continuous density hidden markov models. Computer Speech and Language, 9(2):171–185, 1995.
 X. Li, Y. Zhou, T. Wu, R. Socher, and C. Xiong.
Learn to grow:
A continual structure learning framework for overcoming catastrophic forgetting. In Int. Conf. Mach. Learn., 2019.
 Z. Li, L.-F. Cheong, S. Yang, and K.-C. Toh. Simultaneous clustering and model selection: Algorithm, theory and applications. IEEE Trans.
Pattern Anal. Mach. Intell., 40(8):1964–1978, 2018.
 Z. Li and D. Hoiem. Learning without forgetting. IEEE Trans. Pattern
Anal. Mach. Intell., 40(12):2935–2947, 2018.
 Z. Li and D. Hoiem. G-distillation: Reducing overconfident errors on novel samples. arXiv:1804.03166, 2018.
 A. Liu and B. Ziebart. Robust classification under sample selection bias. In Advances Neural Inf. Process. Syst., pages 37–45, 2014.
 C.-L. Liu. Classifier combination based on confidence transformation.
Pattern Recognition, 38(1):11–28, 2005.
 C.-L. Liu.
One-vs-all training of prototype classifiers for pattern classification and retrieval. In Int. Conf. Pattern Recognition, pages
3328–3331, 2010.
 C.-L. Liu, K. Nakashima, H. Sako, and H. Fujisawa.
Handwritten digit recognition: benchmarking of state-of-the-art techniques. Pattern
Recognition, 36(10):2271–2285, 2003.
 C.-L. Liu, H. Sako, and H. Fujisawa. Performance evaluation of pattern classifiers for handwritten character recognition. Int. J. Document Anal.
Recognit., 4(3):191–204, 2002.
 K. Liu, Y. Li, N. Xu, and P. Natarajan. Learn to combine modalities in multimodal deep learning. arXiv:1805.11730, 2018.
 T. Liu and D. Tao.
Classification with noisy labels by importance reweighting. IEEE Trans. Pattern Anal. Mach. Intell., 38(3):447–461, X. Liu, J. Weijer, and A. Bagdanov. Exploiting unlabeled data in CNNs by self-supervised learning to rank. IEEE Trans. Pattern Anal. Mach.
Intell., 41(8):1862–1878, 2019.
 J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In IEEE Conf. Comput. Vis. Pattern Recognit., pages 3431–3440, 2015.
 M. Long, Y. Cao, J. Wang, and M. Jordan.
Learning transferable features with deep adaptation networks. In Int. Conf. Mach. Learn., pages 1–9, 2015.
 Y. Long, L. Liu, F. Shen, L. Shao, and X. Li. Zero-shot learning using synthesised unseen visual data with diffusion regularisation.
IEEE
Trans. Pattern Anal. Mach. Intell., 40(10):2498–2512, 2018.
 X. Lu, Y. Wang, X. Zhou, Z. Zhang, and Z. Ling. Traffic sign recognition via multi-modal tree-structure embedded multi-task learning. IEEE
Trans. Intelligent Transportation Systems, 18(4):960–972, 2017.
 Y. Lu, A. Kumar, S. Zhai, Y. Cheng, T. Javidi, and R. Feris. Fullyadaptive feature sharing in multi-task networks with applications in person attribute classification.
In IEEE Conf. Comput. Vis. Pattern
Recognit., pages 5334–5343, 2017.
 A. Maas, A. Hannun, and A. Ng.
Rectifier nonlinearities improve neural network acoustic models. In Int. Conf. Mach. Learn., 2013.
 J. Malmaud, J. Huang, V. Rathod, N. Johnston, A. Rabinovich, and K. Murphy. What's cookin'? interpreting cooking videos using text, speech and vision. arXiv:1503.01558, 2015.
 R. Mammone, X. Zhang, and R. Ramachandran. Robust speaker recognition: A feature-based approach. IEEE Signal Processing Magazine, 13(5):58–71, 1996.
 Y. Mansour, M. Mohri, and A. Rostamizadeh. Domain adaptation with multiple sources. In Advances Neural Inf. Process. Syst., pages 1041–
 H. Masnadi-Shirazi and N. Vasconcelos. On the design of loss functions for classification: theory, robustness to outliers, and savageboost. In
Advances Neural Inf. Process. Syst., pages 1049–1056, 2009.
 M. Masud, J. Gao, L. Khan, J. Han, and B. Thuraisingham. Classification and novel class detection in concept-drifting data streams under time constraints. IEEE Trans. Know. Data Eng., 23(6):859–874, 2011.
 T. Mensink, J. Verbeek, F. Perronnin, and G. Csurka. Distance-based image classification: Generalizing to new classes at near-zero cost.
IEEE Trans. Pattern Anal. Mach. Intell., 35(11):2624–2637, 2013.
 J. Metzen, T. Genewein, V. Fischer, and B. Bischoff. On detecting adversarial perturbations. In Int. Conf. Learn. Representations, 2017.
 D. Miller and J. Browning. A mixture model and EM-based algorithm for class discovery, robust classification, and outlier rejection in mixed labeled/unlabeled data sets. IEEE Trans. Pattern Anal. Mach. Intell., 25(11):1468–1483, 2003.
 I. Misra, A. Shrivastava, A. Gupta, and M. Hebert.
Cross-stitch networks for multi-task learning. In IEEE Conf. Comput. Vis. Pattern
Recognit., pages 3994–4003, 2016.
 T. Mitchell, W. Cohen, E. Hruschka, and et al. Never-ending learning.
Communications of ACM, 61(5):103–115, 2018.
 T. Miyato, S. Maeda, M. Koyama, and S. Ishii. Virtual adversarial training: a regularization method for supervised and semi-supervised learning. IEEE Trans. Pattern Anal. Mach. Intell., 41(8):1979–1993, C. Molnar.
Interpretable Machine Learning: A Guide for Making
Black Box Models Explainable.
2019. https://christophm.github.io/ interpretable-ml-book/.
 S. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and P. Frossard. Universal adversarial perturbations. In IEEE Conf. Comput. Vis. Pattern Recognit., pages 1765–1773, 2017.
 S. Moosavi-Dezfooli, A. Fawzi, and P. Frossard. DeepFool: a simple and accurate method to fool deep neural networks.
In IEEE Conf.
Comput. Vis. Pattern Recognit., pages 2574–2582, 2016.
 S. Motiian, Q. Jones, S. Iranmanesh, and G. Doretto.
Few-shot adversarial domain adaptation. In Advances Neural Inf. Process. Syst., pages 6670–6680, 2017.
 K. Muller, S. Mika, G. Riitsch, K. Tsuda, and B. Scholkopf.
An introduction to kernel-based learning algorithms. IEEE Trans. Neural
Netw., 12:181–201, 2001.
 K. Murugesan, H. Liu, J. Carbonell, and Y. Yang. Adaptive smoothed online multi-task learning.
In Advances Neural Inf. Process. Syst., pages 4296–4304, 2016.
 A. Nagrani, S. Albanie, and A. Zisserman. Seeing voices and hearing faces: Cross-modal biometric matching. In IEEE Conf. Comput. Vis.
Pattern Recognit., pages 8427–8436, 2018.
 G. Nagy. State of the art in pattern recognition. Proc. IEEE, 56(5):836–
 N. Neverova, C. Wolf, G. Taylor, and F. Nebout. ModDrop: adaptive multi-modal gesture recognition.
IEEE Trans. Pattern Anal. Mach.
Intell., 38(8):1692–1706, 2016.
 A. Nguyen, J. Yosinski, and J. Clune. Deep neural networks are easily fooled: High confidence predictions for unrecognizable images.
In
IEEE Conf. Comput. Vis. Pattern Recognit., pages 427–436, 2015.
 M. Noroozi and P. Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In Eur. Conf. Comput. Vis., pages
69–84, 2016.
 S. Nowozin and C. Lampert.
Structured learning and prediction in computer vision. Foundations and Trends in Computer Graphics and Vision, 6(3-4):185–365, 2011.
 A. Oliver, A. Odena, C. Raffel, E. Cubuk, and I. Goodfellow. Realistic evaluation of deep semi-supervised learning algorithms. In Advances
Neural Inf. Process. Syst., 2018.
 O. Ostapenko, M. Puscas, T. Klein, P. Jahnichen, and M. Nabi.
Learning to remember: A synaptic plasticity driven framework for continual learning. In IEEE Conf. Comput. Vis. Pattern Recognit., 2019.
 S. Pan, I. Tsang, J. Kwok, and Q. Yang. Domain adaptation via transfer component analysis. IEEE Trans. Neural Netw., 22(2):199–210, 2011.
 S. Pan and Q. Yang. A survey on transfer learning. IEEE Trans. Know.
Data Eng., 22(10):1345–1359, 2009.
 N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. Celik, and A. Swami. Practical black-box attacks against machine learning. In
ACM Asia Conf. Comput. Communi. Secur., pages 506–519, 2017.
 N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. Celik, and A. Swami. The limitations of deep learning in adversarial settings.
In IEEE Eur. Symposium Security Privacy, pages 372–387, 2016.
 N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami. Distillation as a defense to adversarial perturbations against deep neural networks.
In IEEE Symposium Security Privacy, pages 582–597, 2016.
 G. Parisi, R. Kemker, J. Part, C. Kanan, and S. Wermter. Continual lifelong learning with neural networks: A review.
Neural Netw., 113:54–71, 2019.
 D. Park, L. Hendricks, Z. Akata, A. Rohrbach, B. Schiele, T. Darrell, and M. Rohrbach. Multimodal explanations: Justifying decisions and pointing to the evidence. In IEEE Conf. Comput. Vis. Pattern Recognit., pages 8779–8788, 2018.
 D. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell, and A. Efros. Context encoders: Feature learning by inpainting. In IEEE Conf. Comput. Vis.
Pattern Recognit., pages 2536–2544, 2016.
 G. Patrini, A. Rozza, A. Menon, R. Nock, and L. Qu. Making deep neural networks robust to label noise: A loss correction approach. In
IEEE Conf. Comput. Vis. Pattern Recognit., pages 1944–1952, 2017.
 P. Peng, Y. Tian, T. Xiang, Y. Wang, M. Pontil, and T. Huang. Joint semantic and latent attribute modelling for cross-class transfer learning.
IEEE Trans. Pattern Anal. Mach. Intell., 40(7):1625–1638, 2018.
 A. Pentina and C. Lampert. Lifelong learning with non-i.i.d. tasks. In
Advances Neural Inf. Process. Syst., pages 1540–1548, 2015.
 M. Pimentel, D. Clifton, L. Clifton, and L. Tarassenko. A review of novelty detection. Signal Processing, 99:215–249, 2014.
 M. Poo, J. Du, N. Ip, Z. Xiong, B. Xu, and T. Tan. China brain project: basic neuroscience, brain diseases, and brain-inspired computing. Neuron, 92(3):591–596, 2016.
 F. Provost and T. Fawcett. Robust classification for imprecise environments. Machine Learning, 42(3):203–231, 2001.
 H. Qi, M. Brown, and D. Lowe. Low-shot learning with imprinted weights. In IEEE Conf. Comput. Vis. Pattern Recognit., pages 5822–
 R. Qiao, L. Liu, C. Shen, and A. Hengel.
Less is more: zero-shot learning from online textual documents with noise suppression.
In
IEEE Conf. Comput. Vis. Pattern Recognit., pages 2249–2257, 2016.
 L. Rabiner.
A tutorial on hidden Markov models and selected applications in speech recognition. Proc. IEEE, 77(2):257–286, 1989.
 A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever. Improving language understanding by generative pre-training. OpenAI Technical report, 2018.
 D. Ramachandram and G. Taylor. Deep multimodal learning: A survey on recent advances and trends.
IEEE Signal Processing Magazine, 34(6):96–108, 2017.
 A. Rasmus, H. Valpola, M. Honkala, M. Berglund, and T. Raiko. Semisupervised learning with ladder networks.
In Advances Neural Inf.
Process. Syst., pages 3546–3554, 2015.
 S. Rastegar, M. Baghshah, H. Rabiee, and S. Shojaee.
MDL-CW:
A multimodal deep learning framework with cross weights. In IEEE
Conf. Comput. Vis. Pattern Recognit., pages 2601–2609, 2016.
 S. Ravi and H. Larochelle.
Optimization as a model for few-shot learning. In Int. Conf. Learn. Representations, 2017.
 S. Rebuffi, A. Kolesnikov, G. Sperl, and C. Lampert. iCaRL: Incremental classifier and representation learning. In IEEE Conf. Comput.
Vis. Pattern Recognit., pages 2001–2010, 2017.
 B. Recht, R. Roelofs, L. Schmidt, and V. Shankar.
Do CIFAR-10 classifiers generalize to CIFAR-10? arXiv:1806.00451, 2018.
 S. Reed, Z. Akata, X. Yan, L. Logeswaran, H. Lee, and B. Schiele.
Generative adversarial text to image synthesis.
In Int. Conf. Mach.
Learn., pages 1060–1069, 2016.
 S. Reed, H. Lee, D. Anguelov, C. Szegedy, D. Erhan, and A. Rabinovich. Training deep neural networks on noisy labels with bootstrapping. In Workshop Int. Conf. Learn. Representations, 2015.
 S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: towards realtime object detection with region proposal networks.
IEEE Trans.
Pattern Anal. Mach. Intell., 39(6):1137–1149, 2017.
 B. RichardWebster, S. Anthony, and W. Scheirer.
PsyPhy: A psychophysics driven evaluation framework for visual recognition. IEEE
Trans. Pattern Anal. Mach. Intell., 41(9):2280–2286, 2019.
 S. Rifai, P. Vincent, X. Muller, X. Glorot, and Y. Bengio. Contractive auto-encoders: Explicit invariance during feature extraction.
In Int.
Conf. Mach. Learn., page 2011, 833–840.
 A. Rozantsev, M. Salzmann, and P. Fua. Residual parameter transfer for deep domain adaptation. In IEEE Conf. Comput. Vis. Pattern Recognit., pages 4339–4348, 2018.
 A. Rozantsev, M. Salzmann, and P. Fua.
Beyond sharing weights for deep domain adaptation. IEEE Trans. Pattern Anal. Mach. Intell., 41(4):801–814, 2019.
 S. Ruder. An overview of multi-task learning in deep neural networks. arXiv:1706.05098, 2017.
 M. Sajjadi, M. Javanmardi, and T. Tasdizen.
Regularization with stochastic transformations and perturbations for deep semi-supervised learning. In Advances Neural Inf. Process. Syst., pages 1163–1171, T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen. Improved techniques for training GANs. In Advances Neural
Inf. Process. Syst., pages 2234–2242, 2016.
 P. Samangouei, M. Kabkab, and R. Chellappa. Defense-GAN: Protecting classifiers against adversarial attacks using generative models. In
Int. Conf. Learn. Representations, 2018.
 N. Samsudin and A. Bradley. Nearest neighbour group-based classification. Pattern Recognition, 43(10):3458–3467, 2010.
 A. Santoro, D. Raposo, D. Barrett, M. Malinowski, R. Pascanu, P. Battaglia, and T. Lillicrap.
A simple neural network module for relational reasoning.
In Advances Neural Inf. Process. Syst., pages
4967–4976, 2017.
 G. Saon and M. Picheny. Recent advances in conversational speech recognition using convolutional and recurrent neural networks. IBM J.
Research Development, 61(4):1–10, 2017.
 P. Sarkar and G. Nagy.
Style consistent classification of isogenous patterns. IEEE Trans. Pattern Anal. Mach. Intell., 27(1):88–98, 2005.
 F. Scarselli, M. Gori, A. Tsoi, M. Hagenbuchner, and G. Monfardini.
The graph neural network model.
IEEE Trans. Neural Networks, 20(1):61–80, 2009.
 W. Scheirer, L. Jain, and T. Boult. Probability models for open set recognition.
IEEE Trans. Pattern Anal. Mach. Intell., 36(11):2317–
 W. Scheirer, A. Rocha, A. Sapkota, and T. Boult. Toward open set recognition. IEEE Trans. Pattern Anal. Mach. Intell., 35(7):1757–1772, J. Schmidhuber.
Deep learning in neural networks: An overview.
Neural Netw., 61:85–117, 2015.
 B. Scholkopf, J. Platt, J. Shawe-Taylor, and A. Smola.
Estimating the support of a high-dimensional distribution. Neural Computation, 13(7):1443–1471, 2001.
 T. Serre, L. Wolf, S. Bileschi, M. Riesenhuber, and T. Poggio. Robust object recognition with cortex-like mechanisms. IEEE Trans. Pattern
Anal. Mach. Intell., 29(3):411–426, 2007.
 U. Shaham, Y. Yamada, and S. Negahban. Understanding adversarial training: Increasing local stability of neural nets through robust optimization. arXiv:1511.05432, 2015.
 G. Shakhnarovich, J. Fisher, and T. Darrell.
Face recognition from long-term observations. In Eur. Conf. Comput. Vis., pages 851–865, B. Shi, X. Bai, and C. Yao. An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition.
IEEE Trans. Pattern Anal. Mach. Intell., 39(11):2298–
 H. Shimodaira. Improving predictive inference under covariate shift by weighting the log-likelihood function. J. Stat. Plan. Infer., 90(2):227–
 L. Shu, H. Xu, and B. Liu.
DOC: Deep open classification of text documents.
In Conf. Empirical Methods in Natural Language
Processing, pages 2911–2916, 2017.
 A. Smola, A. Gretton, L. Song, and B. Scholkopf. A Hilbert space embedding for distributions. In Int. Conf. Algor. Learn. Theory, pages
13–31, 2007.
 J. Snell, K. Swersky, and R. Zemel. Prototypical networks for fewshot learning. In Advances Neural Inf. Process. Syst., pages 4080–4090, J. Springenberg.
Unsupervised and semi-supervised learning with categorical generative adversarial networks.
In Int. Conf. Learn.
Representations, 2016.
 N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov.
Dropout: A simple way to prevent neural networks from overfitting. J. Mach. Learn. Res., 15(1):1929–1958, 2014.
 N. Srivastava and R. Salakhutdinov. Multimodal learning with deep
Boltzmann machines. In Advances Neural Inf. Process. Syst., pages
2222–2230, 2012.
 J. Su, D. Vargas, and K. Sakurai. One pixel attack for fooling deep neural networks. arXiv:1710.08864, 2017.
 C. Suen. N-gram statistics for natural language understanding and text processing.
IEEE Trans. Pattern Anal. Mach. Intell., 1(2):164–172, M. Sugiyama, S. Nakajima, H. Kashima, P. Bunau, and M. Kawanabe.
Direct importance estimation with model selection and its application to covariate shift adaptation. In Advances Neural Inf. Process. Syst., pages 1433–1440, 2008.
 S. Sukhbaatar, J. Bruna, M. Paluri, L. Bourdev, and R. Fergus. Training convolutional networks with noisy labels.
In Workshop Int. Conf.
Learn. Representations, 2015.
 Y. Sun, X. Wang, and X. Tang.
Hybrid deep learning for face verification.
IEEE Trans. Pattern Anal. Mach. Intell., 38(10):1997–
 Z. Sun and T. Tan. Ordinal measures for iris recognition. IEEE Trans.
Pattern Anal. Mach. Intell., 31(12):2211–2226, 2009.
 F. Sung, Y. Yang, L. Zhang, T. Xiang, P. Torr, and T. Hospedales.
Learning to compare: Relation network for few-shot learning. In IEEE
Conf. Comput. Vis. Pattern Recognit., pages 1199–1208, 2018.
 I. Sutskever, O. Vinyals, and Q. Le. Sequence to sequence learning with neural networks. In Advances Neural Inf. Process. Syst., pages
3104–3112, 2014.
 C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In IEEE Conf. Comput. Vis. Pattern Recognit., 2015.
 C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing properties of neural networks. In Int.
Conf. Learn. Representations, 2014.
 Y. Taigman, M. Yang, M. Ranzato, and L. Wolf. Deepface: Closing the gap to human-level performance in face verification. In IEEE Conf.
Comput. Vis. Pattern Recognit., pages 1701–1708, 2014.
 D. Tanaka, D. Ikami, T. Yamasaki, and K. Aizawa. Joint optimization framework for learning with noisy labels. In IEEE Conf. Comput. Vis.
Pattern Recognit., pages 5552–5560, 2018.
 A. Tarvainen and H. Valpola. Mean teachers are better role models:
Weight-averaged consistency targets improve semi-supervised deep
26 learning results. In Advances Neural Inf. Process. Syst., pages 1195–
 D. Tax. One-class classification: concept-learning in the absence of counter-examples. Ph.D. Thesis, Delft University of Technology, 2001.
 D. Tax and R. Duin.
Support vector data description.
Machine
Learning, 54(1):45–66, 2004.
 D. Tax and R. Duin. Growing a multi-class classifier with a reject option. Pattern Recognit. Lett., 29(10):1565–1570, 2008.
 J. Tenenbaum and W. Freeman.
Separating style and content with bilinear models. Neural Computation, 12(6):1247–1283, 2000.
 S. Thrun and J. O'Sullivan. Discovering structure in multiple learning tasks: The TC algorithm. In Int. Conf. Mach. Learn., pages 489–497, Y. Tokozume, Y. Ushiku, and T. Harada. Between-class learning for image classification. In IEEE Conf. Comput. Vis. Pattern Recognit., pages 5486–5494, 2018.
 E. Triantafillou, R. Zemel, and R. Urtasun. Few-shot learning through an information retrieval lens. In Advances Neural Inf. Process. Syst., pages 2255–2265, 2017.
 I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun. Large margin methods for structured and interdependent output variables. J. Mach.
Learn. Res., 6:1453–1484, 2005.
 P. Turaga, A. Veeraraghavan, A. Srivastava, and R. Chellappa. Statistical computations on Grassmann and Stiefel manifolds for image and video-based recognition. IEEE Trans. Pattern Anal. Mach. Intell., 33(11):2273–2286, 2011.
 E. Tzeng, J. Hoffman, T. Darrell, and K. Saenko. Simultaneous deep transfer across domains and tasks. In Int. Conf. Comput. Vis., pages
4068–4076, 2015.
 E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell.
Adversarial discriminative domain adaptation. In IEEE Conf. Comput. Vis. Pattern
Recognit., pages 7167–7176, 2017.
 P. Utgoff. Incremental induction of decision trees. Machine Learning, 4:161–186, 1989.
 A. Vahdat.
Toward robustness against label noise in training deep discriminative neural networks. In Advances Neural Inf. Process. Syst., pages 5596–5605, 2017.
 V.N. Vapnik. Statistical Learning Theory. New York: John Wiley &
Sons, 1998.
 S. Veeramachaneni and G. Nagy.
Style context with second-order statistics. IEEE Trans. Pattern Anal. Mach. Intell., 27(1):14–22, 2005.
 S. Veeramachaneni and G. Nagy. Analytical results on style-constrained
Bayesian classification of pattern fields.
IEEE Trans. Pattern Anal.
Mach. Intell., 29(7):1280–1285, 2007.
 P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y. Bengio.
Graph attention networks.
In Int. Conf. Learn. Representations, 2018.
 V. Verma, G. Arora, A. Mishra, and P. Rai.
Generalized zero-shot learning via synthesized examples. In IEEE Conf. Comput. Vis. Pattern
Recognit., pages 4281–4289, 2018.
 P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol. Extracting and composing robust features with denoising autoencoders. In Int.
Conf. Mach. Learn., pages 1096–1103, 2008.
 O. Vinyals, C. Blundell, T. Lillicrap, K. Kavukcuoglu, and D. Wierstra.
Matching networks for one shot learning.
In Advances Neural Inf.
Process. Syst., page 2016, 3630–3638.
 W. Wan, Y. Zhong, T. Li, and J. Chen. Rethinking feature distribution for loss functions in image classification. In IEEE Conf. Comput. Vis.
Pattern Recognit., pages 9117–9126, 2018.
 A. Wang, J. Cai, J. Lu, and T.-J. Cham. MMSS: Multi-modal sharable and specific feature learning for RGB-D object recognition. In Int.
Conf. Comput. Vis., pages 1125–1133, 2015.
 H. Wang, A. Kl¨aser, C. Schmid, and C.-L. Liu. Dense trajectories and motion boundary descriptors for action recognition. Int. J. Comput.
Vis., 103(1):60–79, 2013.
 L. Wang, T. Tan, H. Ning, and W. Hu. Silhouette analysis-based gait recognition for human identification. IEEE Trans. Pattern Anal. Mach.
Intell., 25(12):1505–1518, 2003.
 M. Wang and W. Deng. Deep visual domain adaptation: A survey. arXiv:1802.03601, 2018.
 Q.-F. Wang, F. Yin, and C.-L. Liu. Handwritten Chinese text recognition by integrating multiple contexts. IEEE Trans. Pattern Anal. Mach.
Intell., 34(8):1469–1481, 2012.
 X. Wang and A. Gupta. Unsupervised learning of visual representations using videos. In Int. Conf. Comput. Vis., pages 2794–2802, 2015.
 Y. Wang, W. Liu, X. Ma, J. Bailey, H. Zha, L. Song, and S.-T. Xia.
Iterative learning with open-set noisy labels. In IEEE Conf. Comput.
Vis. Pattern Recognit., pages 8688–8696, 2018.
 Y.-X. Wang, R. Girshick, M. Hebert, and B. Hariharan.
Low-shot learning from imaginary data.
In IEEE Conf. Comput. Vis. Pattern
Recognit., pages 7278–7286, 2018.
 Y.-X. Wang and M. Hebert.
Learning to learn: Model regression networks for easy small sample learning. In Eur. Conf. Comput. Vis., pages 616–634, 2016.
 Y. Wu and Y. Liu. Robust truncated hinge loss support vector machines.
J. American Statistical Association, 102(479):974–983, 2007.
 Z. Wu, Y. Xiong, S. Yu, and D. Lin. Unsupervised feature learning via non-parametric instance discrimination. In IEEE Conf. Comput. Vis.
Pattern Recognit., pages 3733–3742, 2018.
 Y. Xian, C. Lampert, B. Schiele, and Z. Akata. Zero-shot learning:
A comprehensive evaluation of the good, the bad and the ugly. IEEE
Trans. Pattern Anal. Mach. Intell., 41(9):2251–2265, 2019.
 Y. Xian, T. Lorenz, B. Schiele, and Z. Akata.
Feature generating networks for zero-shot learning. In IEEE Conf. Comput. Vis. Pattern
Recognit., pages 5542–5551, 2018.
 T. Xiao, T. Xia, Y. Yang, C. Huang, and X. Wang. Learning from massive noisy labeled data for image classification.
In IEEE Conf.
Comput. Vis. Pattern Recognit., pages 2691–2699, 2015.
 G. Xu, B.-G. Hu, and J. Principe. Robust C-loss kernel classifiers.
IEEE Trans. Neural Netw. Learn. Syst., 29(3):510–522, 2018.
 K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhutdinov, R. Zemel, and Y. Bengio. Show, attend and tell: Neural image caption generation with visual attention.
In Int. Conf. Mach. Learn., pages
2048–2057, 2015.
 L. Xu, K. Crammer, and D. Schuurmans.
Robust support vector machine training via convex outlier ablation.
In AAAI Conf. Artif.
Intell., pages 536–542, 2006.
 R. Xu, Z. Chen, W. Zuo, J. Yan, and L. Lin. Deep cocktail network:
Multi-source unsupervised domain adaptation with category shift. In
IEEE Conf. Comput. Vis. Pattern Recognit., pages 3964–3973, 2018.
 S. Yan, D. Xu, B. Zhang, H.-J. Zhang, Q. Yang, and S. Lin. Graph embedding and extensions: A general framework for dimensionality reduction. IEEE Trans. Pattern Anal. Mach. Intell., 29(1):40–51, 2007.
 H.-M. Yang, X.-Y. Zhang, F. Yin, and C.-L. Liu. Robust classification with convolutional prototype learning.
In IEEE Conf. Comput. Vis.
Pattern Recognit., pages 3474–3482, 2018.
 J. Yang, D. Parikh, and D. Batra. Joint unsupervised learning of deep representations and image clusters. In IEEE Conf. Comput. Vis. Pattern
Recognit., pages 5147–5156, 2016.
 J. Yosinski, J. Clune, Y. Bengio, and H. Lipson. How transferable are features in deep neural networks? In Advances Neural Inf. Process.
Syst., pages 3320–3328, 2014.
 A. Zamir, A. Sax, W. Shen, L. Guibas, J. Malik, and S. Savarese.
Taskonomy: Disentangling task transfer learning.
In IEEE Conf.
Comput. Vis. Pattern Recognit., pages 3712–3722, 2018.
 C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals.
Understanding deep learning requires rethinking generalization. In Int. Conf.
Learn. Representations, 2017.
 D. Zhang and D. Shen.
Multi-modal multi-task learning for joint prediction of multiple regression and classification variables in Alzheimer's disease. NeuroImage, 59(2):895–907, 2012.
 H. Zhang, M. Cisse, Y. Dauphin, and D. Lopez-Paz. Mixup: Beyond empirical risk minimization. In Int. Conf. Learn. Representations, 2018.
 H. Zhang and V. Patel. Sparse representation-based open set recognition. IEEE Trans. Pattern Anal. Mach. Intell., 39(8):1690–1696, 2017.
 J. Zhang, W. Li, and P. Ogunbona. Transfer learning for cross-dataset recognition: A survey. arXiv:1705.04396, 2017.
 R. Zhang, P. Isola, and A. Efros. Colorful image colorization. In Eur.
Conf. Comput. Vis., pages 649–666, 2016.
 R. Zhang, P. Isola, and A. Efros. Split-brain autoencoders: Unsupervised learning by cross-channel prediction. In IEEE Conf. Comput.
Vis. Pattern Recognit., pages 1058–1067, 2017.
 T. Zhang.
Analysis of multi-stage convex relaxation for sparse regularization. J. Mach. Learn. Res., 11:1081–1107, 2010.
 X.-Y. Zhang, Y. Bengio, and C.-L. Liu. Online and offline handwritten Chinese character recognition: A comprehensive study and new benchmark. Pattern Recognition, 61:348–360, 2017.
 X.-Y. Zhang, K. Huang, and C.-L. Liu.
Pattern field classification with style normalized transformation.
In Int. Joint Conf. Artificial
Intelligence, pages 1621–1626, 2011.
 X.-Y. Zhang and C.-L. Liu.
Writer adaptation with style transfer mapping. IEEE Trans. Pattern Anal. Mach. Intell., 35(7):1773–1787, Y. Zhang, K. Lee, and H. Lee. Augmenting supervised neural networks with unsupervised objectives for large-scale image classification. In Int.
Conf. Mach. Learn., pages 612–621, 2016.
 Z. Zhang, M. Wang, Y. Huang, and A. Nehorai.
Aligning infinitedimensional covariance matrices in reproducing kernel hilbert spaces for domain adaptation. In IEEE Conf. Comput. Vis. Pattern Recognit., pages 3437–3445, 2018.
 Z. Zhang and K. Zhao. Low-rank matrix approximation with manifold regularization. IEEE Trans. Pattern Anal. Mach. Intell., 35(7):1717–
 J. Zhao, M. Mathieu, R. Goroshin, and Y. LeCun. Stacked what-where auto-encoders. In Workshop Int. Conf. Learn. Representations, 2016.
 S. Zheng, Y. Song, T. Leung, and I. Goodfellow.
Improving the robustness of deep neural networks via stability training.
In IEEE
Conf. Comput. Vis. Pattern Recognit., pages 4480–4488, 2016.
 B. Zhou, D. Bau, A. Oliva, and A. Torralba. Interpreting deep visual representations via network dissection.
IEEE Trans. Pattern Anal.
Mach. Intell., 41(9):2131–2145, 2019.
 D. Zhou, O. Bousquet, T. Lal, J. Weston, and B. Scholkopf. Learning with local and global consistency. In Advances Neural Inf. Process.
Syst., pages 321–328, 2004.
 J. Zhou, G. Cui, Z. Zhang, C. Yang, Z. Liu, and M. Sun. Graph neural networks: A review of methods and applications. arXiv:1812.08434, Z.-H. Zhou and M. Li. Tri-training: exploiting unlabeled data using three classifiers. IEEE Trans. Know. Data Eng., 17(11):1529–1541, Z.-H. Zhou, Y.-Y. Sun, and Y.-F. Li. Multi-instance learning by treating instances as non-iid samples. In Int. Conf. Mach. Learn., pages 1249–
 X. Zhu. Semi-supervised learning literature survey. Computer Science, University of Wisconsin-Madison, 2006.
 X. Zhu, X. Wu, and Q. Chen. Eliminating class noise in large datasets.
In Int. Conf. Mach. Learn., pages 920–927, 2003.
 Y. Zhu, M. Elhoseiny, B. Liu, X. Peng, and A. Elgammal. A generative adversarial approach for zero-shot learning from noisy texts. In IEEE
Conf. Comput. Vis. Pattern Recognit., pages 1004–1013, 2018.
 Y. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, and S. Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books.
In Int. Conf.
Comput. Vis., pages 19–27, 2015.Deep Learning of Representations:
Looking Forward
Yoshua Bengio
Department of Computer Science and Operations Research
Universit´e de Montr´eal, Canada
Abstract. Deep learning research aims at discovering learning algorithms that discover multiple levels of distributed representations, with higher levels representing more abstract concepts. Although the study of deep learning has already led to impressive theoretical results, learning algorithms and breakthrough experiments, several challenges lie ahead. This paper proposes to examine some of these challenges, centering on the questions of scaling deep learning algorithms to much larger models and datasets, reducing optimization difficulties due to ill-conditioning or local minima, designing more efficient and powerful inference and sampling procedures, and learning to disentangle the factors of variation underlying the observed data. It also proposes a few forward-looking research directions aimed at overcoming these challenges.
Background on Deep Learning
Deep learning is an emerging approach within the machine learning research community. Deep learning algorithms have been proposed in recent years to move machine learning systems towards the discovery of multiple levels of representation. They have had important empirical successes in a number of traditional AI applications such as computer vision and natural language processing.
See (Bengio, 2009; Bengio et al., 2013d) for reviews and Bengio (2013c) and the other chapters of the book by Montavon and Muller (2012) for practical guidelines. Deep learning is attracting much attention both from the academic and industrial communities. Companies like Google, Microsoft, Apple, IBM and Baidu are investing in deep learning, with the first widely distributed products being used by consumers aimed at speech recognition. Deep learning is also used for object recognition(Google Goggles), image and music information retrieval (Google Image Search, Google Music), as well as computational advertising (Corrado, 2012). A deep learning building block (the restricted
Boltzmann machine, or RBM) was used as a crucial part of the winning entry of a million-dollar machine learning competition (the Netflix competition) (Salakhutdinov et al., 2007; T¨oscher et al., 2009). The New York Times covered the subject twice in 2012, with front-page articles.1 Another series of articles (including a third New York Times article) covered a more recent event showing off the application of deep learning in a major Kaggle competition for drug discovery (for example see "Deep Learning - The Biggest Data Science Breakthrough of the Decade"2. Much more recently, Google bought out ("acqui-hired") a company (DNNresearch) created by University of Toronto professor Geoffrey Hinton (the founder and leading researcher of deep learning) and two of his
PhD students, Ilya Sutskever and Alex Krizhevsky, with the press writing titles such as "Google
Hires Brains that Helped Supercharge Machine Learning" (Robert McMillan for Wired, March 13th, The performance of many machine learning methods is heavily dependent on the choice of data representation (or features) on which they are applied. For that reason, much of the actual effort in deploying machine learning algorithms goes into the design of preprocessing pipelines that result in a hand-crafted representation of the data that can support effective machine learning. Such feature
1 http://www.nytimes.com/2012/11/24/science/scientists-see-advancesin-deep-learning-a-part-of-artificial-intelligence.html
2 http://oreillynet.com/pub/e/2538
Y. Bengio engineering is important but labor-intensive and highlights the weakness of many traditional learning algorithms: their inability to extract and organize the discriminative information from the data.
Feature engineering is a way to take advantage of human ingenuity and prior knowledge to compensate for that weakness. In order to expand the scope and ease of applicability of machine learning, it would be highly desirable to make learning algorithms less dependent on feature engineering, so that novel applications could be constructed faster, and more importantly for the author, to make progress towards artificial intelligence (AI).
A representation learning algorithm discovers explanatory factors or features. A deep learning algorithm is a particular kind of representation learning procedure that discovers multiple levels of representation, with higher-level features representing more abstract aspects of the data. This area of research was kick-started in 2006 by a few research groups, starting with Geoff Hinton's group, who initially focused on stacking unsupervised representation learning algorithms to obtain deeper representations (Hinton et al., 2006; Bengio et al., 2007; Ranzato et al., 2007; Lee et al., 2008). Since then, this area has seen rapid growth, with an increasing number of workshops (now one every year at the NIPS and ICML conferences, the two major conferences in machine learning) and even a new specialized conference just created in 2013 (ICLR – the International Conference on Learning
Representations).
Transfer learning is the ability of a learning algorithm to exploit commonalities between different learning tasks in order to share statistical strength, and transfer knowledge across tasks. Among the achievements of unsupervised representation learning algorithms are the impressive successes they obtained at the two transfer learning challenges held in 2011. First, the Transfer Learning Challenge, presented at an ICML 2011 workshop of the same name, was won using unsupervised layer-wise pretraining (Bengio, 2011; Mesnil et al., 2011). A second Transfer Learning Challenge was held the same year and won by Goodfellow et al. (2011) using unsupervised representation learning. Results were presented at NIPS 2011's Challenges in Learning Hierarchical Models Workshop.
Quick Overview of Deep Learning Algorithms
The central concept behind all deep learning methodology is the automated discovery of abstraction, with the belief that more abstract representations of data such as images, video and audio signals tend to be more useful: they represent the semantic content of the data, divorced from the low-level features of the raw data (e.g., pixels, voxels, or waveforms). Deep architectures lead to abstract representations because more abstract concepts can often be constructed in terms of less abstract ones.
Deep learning algorithms are special cases of representation learning with the property that they learn multiple levels of representation. Deep learning algorithms often employ shallow (single-layer) representation learning algorithms as subroutines. Before covering the unsupervised representation learning algorithms, we quickly review the basic principles behind supervised representation learning algorithms such as the good old multi-layer neural networks. Supervised and unsupervised objectives can of course be combined (simply added, with a hyper-parameter as coefficient), like in Larochelle and Bengio (2008)'s discriminative RBM.
Deep Supervised Nets, Convolutional Nets, Dropout
Before 2006, it was believed that training deep supervised neural networks (Rumelhart et al., 1986) was too difficult (and indeed did not work). The first breakthrough in training them happened in Geoff Hinton's lab with unsupervised pre-training by RBMs (Hinton et al., 2006), as discussed in the next subsection. However, more recently, it was discovered that one could train deep supervised nets by proper initialization, just large enough for gradients to flow well and activations to convey useful information (Glorot and Bengio, 2010; Sutskever, 2012).3 Another interesting ingredient in the 3 and potentially with the use of momentum (Sutskever, 2012)
Deep Learning of Representations: Looking Forward
3 success of training the deep supervised networks of Glorot and Bengio (2010) (and later of Krizhevsky et al. (2012)) is the presence of rectifying non-linearities (such as max(0, x)) instead of sigmoidal nonlinearities (such as 1/(1+exp(−x)) or tanh(x)). See Jarrett et al. (2009); Nair and Hinton (2010) for earlier work on rectifier-like non-linearities. We return to this topic in Section 4. These good results with purely supervised training of deep nets seem to be especially clear when large quantities of labeled data are available, and it was demonstrated with great success for speech recognition (Seide et al., 2011a; Hinton et al., 2012a; Deng et al., 2013) and object recognition (Krizhevsky et al., 2012) with breakthroughs reducing the previous state-of-the-art error rates by 30% to 50% on difficult to beat benchmarks.
One of the key ingredients for success in the applications of deep learning to speech, images, and natural language processing (Bengio, 2008; Collobert et al., 2011) is the use of convolutional architectures (LeCun et al., 1998b), which alternate convolutional layers and pooling layers. Units on hidden layers of a convolutional network are associated with a spatial or temporal position and only depend on (or generate) the values in a particular window of the raw input. Furthermore, units on convolutional layers share parameters with other units of the same "type" located at different positions, while at each location one finds all the different types of units. Units on pooling layers aggregate the outputs of units at a lower layer, either aggregating over different nearby spatial positions (to achieve a form of local spatial invariance) or over different unit types. For example, a max-pooling unit outputs the maximum over some lower level units, which can therefore be seen to compete towards sending their signal forward.
Another key ingredient in the success of many recent breakthrough results in the area of object recognition is the idea of dropouts (Hinton et al., 2012b; Krizhevsky et al., 2012; Goodfellow et al., 2013b). Interestingly, it consists in injecting noise (randomly dropping out units with probability
1/2 from the neural network during training, and correspondingly multiplying by 1/2 the weights magnitude at test time) that prevents a too strong co-adaptation of hidden units: hidden units must compute a feature that will be useful even when half of the other hidden units are stochastically turned off (masked). This acts like a powerful regularizer that is similar to bagging aggregation but over an exponentially large number of models (corresponding to different masking patterns, i.e., subsets of the overall network) that share parameters.
Unsupervised or Supervised Layer-wise Pre-Training
One of the key results of recent years of research in deep learning is that deep compositions of non-linearities – such as found in deep feedforward networks or in recurrent networks applied over long sequences – can be very sensitive to initialization (some initializations can lead much better or much worse results after training). The first type of approaches that were found useful to reduce that sensitivity is based on greedy layer-wise pre-training (Hinton et al., 2006; Bengio et al., 2007).
The idea is to train one layer at a time, starting from lower layers (on top of the input), so that there is a clear training objective for the currently added layer (which typically avoids the need for back-propagating error gradients through many layers of non-linearities). With unsupervised pre-training, each layer is trained to model the distribution of values produced as output of the previous layer. As a side-effect of this training, a new representation is produced, which can be used as input for deeper layers. With the less common supervised pre-training (Bengio et al., 2007; Yu et al., 2010; Seide et al., 2011b), each additional layer is trained with a supervised objective (as part of a one hidden layer network). Again, we obtain a new representation (e.g., the hidden or output layer of the newly trained supervised model) that can be re-used as input for deeper layers.
The effect of unsupervised pre-training is apparently most drastic in the context of training deep auto-encoders (Hinton and Salakhutdinov, 2006), unsupervised learners that learn to reconstruct their input: unsupervised pre-training allows to find much lower training and test reconstruction error.
Y. Bengio
Directed and Undirected Graphical Models with Anonymous Latent Variables
Anonymous latent variables are latent variables that do not have a predefined semantics in terms of predefined human-interpretable concepts. Instead they are meant as a means for the computer to discover underlying explanatory factors present in the data. We believe that although non-anonymous latent variables can be very useful when there is sufficient prior knowledge to define them, anonymous latent variables are very useful to let the machine discover complex probabilistic structure: they lend flexibility to the model, allowing an otherwise parametric model to non-parametrically adapt to the amount of data when more anonymous variables are introduced in the model.
Principal components analysis (PCA), independent components analysis (ICA), and sparse coding all correspond to a directed graphical model in which the observed vector x is generated by first independently sampling some underlying factors (put in vector h) and then obtaining x by Wh plus some noise. They only differ in the type of prior put on h, and the corresponding inference procedures to recover h (its posterior P(h | x) or expected value E[h | x]) when x is observed. Sparse coding tends to yield many zeros in the estimated vector h that could have generated the observed x. See section 3 of Bengio et al. (2013d) for a review of representation learning procedures based on directed or undirected graphical models.4 Section 2.5 describes sparse coding in more detail.
An important thing to keep in mind is that directed graphical models tend to enjoy the property that in computing the posterior, the different factors compete with each other, through the celebrated explaining away effect. Unfortunately, except in very special cases (e.g., when the columns of W are orthogonal, which eliminates explaining away and its need), this results in computationally expensive inference. Although maximum a posteriori (MAP) inference5 remains polynomial-time in the case of sparse coding, this is still very expensive, and unnecessary in other types of models (such as the stacked auto-encoders discussed below). In fact, exact inference becomes intractable for deeper models, as discussed in section 5.
Although RBMs enjoy tractable inference, this is obtained at the cost of a lack of explaining away between the hidden units, which could potentially limit the representational power of E[h | x] as a good representation for the factors that could have generated x. However, RBMs are often used as building blocks for training deeper graphical models such as the deep belief network (DBN) (Hinton et al., 2006) and sthe deep Boltzmann machine (DBM) (Salakhutdinov and Hinton, 2009), which can compensate for the lack of explaining away in the RBM hidden units via a rich prior (provided by the upper layers) which can introduce potentially complex interactions and competition between the hidden units. Note that there is explaining away (and intractable exact inference) iin DBNs and something analogous in DBMs.
Regularized Auto-Encoders
Auto-encoders include in their training criterion a form of reconstruction oerror, such as ||r(x)−x||2, where r(·) is the learned reconstruction function, often decomposed as r(x) = g(f(x)) where f(·) is an encoding function and g(·) a decoding function. The idea is that auto-encoders should have low reconstruction error at the training examples, but high reconstruction error in most other configurations of the input. In the case of auto-encoders, good generalization means that test examples(sampled from the same distribution as training examples) also get low reconstruction error. Autoencoders have to be regularized to prevent them from simply learning the identity function r(x) = x, which would be useless. Regularized auto-encoders include the old bottleneck auto-encoders (like in PCA) with less hidden units than input, as well as the denoising auto-encoders (Vincent et al., 2008) and contractive auto-encoders (Rifai et al., 2011a). The denoising auto-encoder takes a noisy version N(x) of original input x and tries to reconstruct x, e.g., it minimizes ||r(N(x)) − x||2. The contractive auto-encoder has a regularization penalty in addition to the reconstruction error, trying to make hidden units f(x) as constant as possible with respect to x (minimizing the contractive
4 Directed and undirected: just two different views on the semantics of probabilistic models, not mutually exclusive, but views that are more convenient for some models than others.
5 finding h that approximately maximizes P(h | x)
Deep Learning of Representations: Looking Forward
5 penalty || ∂f(x)
∂x ||2
F ). A Taylor expansion of the denoising error shows that it is also approximately equivalent to minimizing reconstruction error plus a contractive penalty on r(·) (Alain and Bengio, 2013). As explained in Bengio et al. (2013d), the tug-of-war between minimization of reconstruction error and the regularizer means that the intermediate representation must mostly capture the variations necessary to distinguish training examples, i.e., the directions of variations on the manifold(a lower dimensional region) near which the data generating distribution concentrates. Score matching (Hyv¨arinen, 2005) is an inductive principle that can be an interesting alternative to maximum likelihood, and several nconnections have been drawn between reconstruction error in auto-encoders and score matching (Swersky et al., 2011). It has also been shown that denoising auto-encoders and some forms of contractive auto-encoders estimate the score6 of the underlying data generating distribution (Vincent, 2011; Alain and Bengio, 2013). This can be used to endow regularized auto-encoders with a probabilistic interpretation and to sample from the implicitly learned density models (Rifai et al., 2012b; Bengio et al., 2012; Alain and Bengio, 2013) through some variant of Langevin or Metropolis-Hastings Monte-Carlo Markov chains (MCMC). More recently, the results from Alain and Bengio (2013) have been generalized: whereas the score estimation result was only valid for asymptotically small Gaussian corruption noise, squared reconstruction error, and continuous inputs, the result from Bengio et al. (2013c) is applicable for any type of input, any form of reconstruction loss (so long as it is a negative log-likelihood), any form of corruption (so long as it prevents learning the identity mapping) and does not depend on the level of corruption noise going to zero.
Even though there is a probabilistic interpretation to regularized auto-encoders, this interpretation does not involve the definition of intermediate anonymous latent variables. Instead, they are based on the construction of a direct parametrization of an encoding function which immediately maps an input x to its representation f(x), and they are motivated by geometrical considerations in the spirit of manifold learning algorithms (Bengio et al., 2013d). Consequently, there is no issue of tractability of inference, even with deep auto-encoders obtained by stacking single-layer ones. This is true even in the recently proposed multi-layer versions of the denoising auto-encoders (Bengio and Thibodeau-Laufer, 2013) in which noise is injected not just in input, but in hidden units (like in the Gibbs chain of a deep Boltzmann machine).
It was previously believed (Ranzato et al., 2008), including by the author himself, that reconstruction error should only be small where the estimated density has a peak, e.g., near the data.
However, recent theoretical and empirical results (Alain and Bengio, 2013) show that the reconstruction error will be small where the estimated density has a peak (a mode) but also where it has a trough (a minimum). This is because the reconstruction error vector (reconstruction minus input) estimates the score ∂ log p(x)
∂x, i.e., the reconstruction error is small where || ∂ log p(x)
∂x
|| is small. This can happen at a local maximum but also at a local minimum (or saddle point) of the estimated density. This argues against using reconstruction error itself as an energy function,7 which should only be low near high probability points.
Sparse Coding and PSD
Sparse coding (Olshausen and Field, 1996) is a particular kind of directed graphical model with a linear relationship between visible and latent variables (like in PCA), but in which the latent variables have a prior (e.g., Laplace density) that encourages sparsity (many zeros) in the MAP posterior. Sparse coding is not actually very good as a generative model, but has been very successful for unsupervised feature learning (Raina et al., 2007; Coates and Ng, 2011; Yu et al., 2011; Grosse et al., 2007; Jenatton et al., 2009; Bach et al., 2011). See Bengio et al. (2013d) for a brief overview in the context of deep learning, along with connections to other unsupervised representation learning algorithms. Like other directed graphical models, it requires somewhat expensive inference, but the 6 derivative of the log-density with respect to the data; this is different from the usual definition of score in statistics, where the derivative is with respect to the parameters
7 To define energy, we write probability as the normalized exponential of minus the energy.
Y. Bengio good news is that for sparse coding, MAP inference is a convex optimization problem for which several fast approximations have been proposed (Mairal et al., 2009; Gregor and LeCun, 2010a). It is interesting to note the results obtained by Coates and Ng (2011) which suggest that sparse coding is a better encoder but not a better learning algorithm than RBMs and sparse auto-encoders (none of which has explaining away). Note also that sparse coding can be generalized into the spike-and-slab sparse coding algorithm (Goodfellow et al., 2012), in which MAP inference is replaced by variational inference, and that was used to win the NIPS 2011 transfer learning challenge (Goodfellow et al., Another interesting variant on sparse coding is the predictive sparse coding (PSD) algorithm (Kavukcuoglu et al., 2008) and its variants, which combine properties of sparse coding and of auto-encoders. Sparse coding can be seen as having only a parametric "generative" decoder (which maps latent variable values to visible variable values) and a non-parametric encoder (find the latent variables value that minimizes reconstruction error and minus the log-prior on the latent variable). PSD adds a parametric encoder (just an affine transformation followed by a non-linearity) and learns it jointly with the generative model, such that the output of the parametric encoder is close to the latent variable values that reconstructs well the input.
Scaling Computations
From a computation point of view, how do we scale the recent successes of deep learning to much larger models and huge datasets, such that the models are actually richer and capture a very large amount of information?
Scaling Computations: The Challenge
The beginnings of deep learning in 2006 have focused on the MNIST digit image classification problem (Hinton et al., 2006; Bengio et al., 2007), breaking the supremacy of SVMs (1.4% error) on this dataset.8 The latest records are still held by deep networks: Ciresan et al. (2012) currently claim the title of state-of-the-art for the unconstrained version of the task (e.g., using a convolutional architecture and stochastically deformed data), with 0.27% error.
In the last few years, deep learning has moved from digits to object recognition in natural images, and the latest breakthrough has been achieved on the ImageNet dataset.9 bringing down the state-of-the-art error rate (out of 5 guesses) from 26.1% to 15.3% (Krizhevsky et al., 2012)
To achieve the above scaling from 28×28 grey-level MNIST images to 256×256 RGB images, researchers have taken advantage of convolutional architectures (meaning that hidden units do not need to be connected to all units at the previous layer but only to those in the same spatial area, and that pooling units reduce the spatial resolution as we move from lower to higher layers). They have also taken advantage of GPU technology to speed-up computation by one or two orders of magnitude (Raina et al., 2009; Bergstra et al., 2010, 2011; Krizhevsky et al., 2012).
We can expect computational power to continue to increase, mostly through increased parallelism such as seen in GPUs, multicore machines, and clusters. In addition, computer memory has become much more affordable, allowing (at least on CPUs) to handle potentially huge models (in terms of capacity).
However, whereas the task of recognizing handwritten digits is solved to the point of achieving roughly human-level performance, this is far from true for tasks such as general object recognition, scene understanding, speech recognition, or natural language understanding. What is needed to nail those tasks and scale to even more ambitious ones?
8 for the knowledge-free version of the task, where no image-specific prior is used, such as image deformations or convolutions, where the current state-of-the-art is around 0.8% and involves deep learning (Rifai et al., 2011b; Hinton et al., 2012b).
9 The 1000-class ImageNet benchmark, whose results are detailed here: http://www.image-net.org/challenges/LSVRC/2012/ results.html
Deep Learning of Representations: Looking Forward
As we approach AI-scale tasks, it should become clear that our trained models will need to be much larger in terms of number of parameters. This is suggested by two observations. First, AI means understanding the world around us at roughly the same level of competence as humans.
Extrapolating from the current state of machine learning, the amount of knowledge this represents is bound to be large, many times more than what current models can capture. Second, more and more empirical results with deep learning suggest that larger models systematically work better (Coates et al., 2011; Hinton et al., 2012b; Krizhevsky et al., 2012; Goodfellow et al., 2013b), provided appropriate regularization is used, such as the dropouts technique described above.
Part of the challenge is that the current capabilities of a single computer are not sufficient to achieve these goals, even if we assume that training complexity would scale linearly with the complexity of the task. This has for example motivated the work of the Google Brain team (Le et al., 2012; Dean et al., 2012) to parallelize training of deep nets over a very large number of nodes.
As we will see in Section 4, we hypothesize that as the size of the models increases, our current ways of training deep networks become less and less efficient, so that the computation required to train larger models (to capture correspondingly more information) is likely to scale much worse than linearly (Dauphin and Bengio, 2013).
Another part of the challenge is that the increase in computational power has been mostly coming(and will continue to come) from parallel computing. Unfortunately, when considering very large datasets, our most efficient training algorithms for deep learning (such as variations on stochastic gradient descent or SGD) are inherently sequential (each update of the parameters requires having completed the previous update, so they cannot be trivially parallelized). Furthermore, for some tasks, the amount of available data available is becoming so large that it does not fit on a disk or even on a file server, so that it is not clear how a single CPU core could even scan all that data (which seems necessary in order to learn from it and exploit all of it, if training is inherently sequential).
Scaling Computations: Solution Paths
Parallel Updates: Asynchronous SGD. One idea that we explored in Bengio et al. (2003) is that of asynchronous SGD: train multiple versions of the model in parallel, each running on a different node and seeing different subsets of the data (on different disks), but with an asynchronous lock-free sharing mechanism which keeps the different versions of the model not too far from each other. If the sharing were synchronous, it would be too inefficient because most nodes would spend their time waiting for the sharing to be completed and would be waiting for the slowest of the nodes. This idea has been analyzed theoretically (Recht et al., 2011) and successfully engineered on a grand scale recently at Google (Le et al., 2012; Dean et al., 2012). However, current large-scale implementations(with thousands of nodes) are still very inefficient (in terms of use of the parallel resources), mostly because of the communication bottleneck requiring to regularly exchange parameter values between nodes. The above papers also take advantage of a way to train deep networks which has been very successful for GPU implementations, namely the use of rather large minibatches (blocks of examples after which an update is performed), making some parallelization (across the examples in the minibatch) easier. One option, explored by Coates et al. (2012) is to use as building blocks for learning features algorithms such as k-means that can be run efficiently over large minibatches (or the whole data) and thus parallelized easily on a cluster (they learned 150,000 features on a cluster with only 30 machines).
Another interesting consideration is the optimization of trade-off between communication cost and computation cost in distributed optimization algorithms, e.g., as discussed in Tsianos et al.
Sparse Updates. One idea that we propose here is to change the learning algorithms so as to obtain sparse updates, i.e., for any particular minibatch there is only a small fraction of parameters that are updated. If the amount of sparsity in the update is large, this would mean that a much smaller fraction of the parameters need to be exchanged between nodes when performing an asynchronous
Y. Bengio
SGD10. Sparse updates could be obtained simply if the gradient is very sparse. This gradient sparsity can arise with approaches that select paths in the neural network. We already know methods which produce slightly sparse updates, such as dropouts (Hinton et al., 2012b),11 maxout (Goodfellow et al., 2013b)12 and other hard-pooling mechanisms, such as the recently proposed and very successful stochastic pooling (Zeiler and Fergus, 2013). These methods do not provide enough sparsity, but this could be achieved in two ways. First of all, we could choose to only pay attention to the largest elements of the gradient vector. Second, we could change the architecture along the lines proposed next.
Conditional Computation. A central idea (that applies whether one parallelizes or not) that we put forward is that of conditional computation: instead of dropping out paths independently and at random, drop them in a learned and optimized way. Decision trees remain some of the most appealing machine learning algorithms because prediction time can be on the order of the logarithm of the number of parameters. Instead, in most other machine learning predictors, scaling is linear (i.e., much worse). This is because decision trees exploit conditional computation: for a given example, as additional computations are performed, one can discard a gradually larger set of parameters (and avoid performing the associated computation). In deep learning, this could be achieved by combining truly sparse activations (values not near zero like in sparse auto-encoders, but actual zeros) and multiplicative connections whereby some hidden units gate other hidden units (when the gater output is zero it turns off the output of the gated unit). When a group A of hidden units has a sparse activation pattern (with many actual zeros) and it multiplicatively gates other hidden units
B, then only a small fraction of the hidden units in B may need to be actually computed, because we know that these values will not be used. Such gating is similar to what happens when a decision node of a decision tree selects a subtree and turns off another subtree. More savings can thus be achieved if units in B themselves gate other units, etc. The crucial difference with decision trees (and e.g., the hard mixture of experts we introduced a decade ago (Collobert et al., 2003)) is that the gating units should not be mutually exclusive and should instead form a distributed pattern. Indeed, we want to keep the advantages of distributed representations and avoid the limited local generalization suffered by decision trees (Bengio et al., 2010). With a high level of conditional computation, some parameters are used often (and are well tuned) whereas other parameters are used very rarely, requiring more data to estimate. A trade-off and appropriate regularization therefore needs to be established which will depend on the amount of training signals going into each parameter. Interestingly, conditional computation also helps to achieve sparse gradients, and the fast convergence of hard mixtures of experts (Collobert et al., 2003) provides positive evidence that a side benefit of conditional computation will be easier and faster optimization.
Another existing example of conditional computation and sparse gradients is with the first layer of neural language models, deep learning models for text data (Bengio et al., 2003; Bengio, 2008). In that case, there is one parameter vector per word in the vocabulary, but each sentence only "touches" the parameters associated with the words in the sentence. It works because the input can be seen as extremely sparse. The question is how to perform conditional computation in the rest of the model.
One issue with the other example we mentioned, hard mixtures of experts (Collobert et al., 2003), is that its training mechanism only make sense when the gater operates at the output layer.
In that case, it is easy to get a strong and clean training signal for the gater output: one can just evaluate what the error would have been if a different expert had been chosen, and train the gater to produce a higher output for the expert that would have produced the smallest error (or to reduce computation and only interrogate two experts, require that the gater correctly ranks their
10 although the gain would be reduced considerably in a minibatch mode, roughly by the size of the minibatch
11 where half of the hidden units are turned off, although clearly, this is not enough sparsity for reaching our objective; unfortunately, we observed that randomly and independently dropping a lot more than half of the units yielded substantially worse results
12 where in addition to dropouts, only one out of k filters wins the competition in max-pooling units, and only one half of those survives the dropouts masking, making the sparsity factor 2k
Deep Learning of Representations: Looking Forward
9 probability of being the best one). The challenge is how to produce training signals for gating units that operate in the middle of the model. One cannot just enumerate all the gating configurations, because in a distributed setting with many gating units, there will be an exponential number of configurations. Interestingly, this suggests introducing randomness in the gating process itself, e.g., stochastically choosing one or two choices out of the many that a group of gating units could take. This is interesting because this is the second motivation (after the success of dropouts as a regularizer) for re-introducing randomness in the middle of deep networks. This randomness would allow configurations that would otherwise not be selected (if only a kind of "max" dictated the gating decision) to be sometimes selected, thus allowing to accumulate a training signal about the value of this configuration, i.e., a training signal for the gater. The general question of estimating or propagating gradients through stochastic neurons is treated in another exploratory article (Bengio, 2013a), where it is shown that one can obtain an unbiased (but noisy) estimator of the gradient of a loss through a discrete stochastic decision. Another interesting idea explored in that paper is that of adding noise just before the non-linearity (max-pooling (maxi xi) or rectifier (max(0, x))). Hence the winner is not always the same, and when a choice wins it has a smooth influence on the result, and that allows a gradient signal to be provided, pushing that winner closer or farther from winning the competition on another example.
Optimization
Optimization: The Challenge
As we consider larger and larger datasets (growing faster than the size of the models), training error and generalization error converge. Furthermore many pieces of evidence in the results of experiments on deep learning suggest that training deep networks (including recurrent networks) involves a difficult optimization (Bengio, 2013b; Gulcehre and Bengio, 2013; Bengio et al., 2013a). It is not yet clear how much of the difficulty is due to local minima and how much is due to ill-conditioning(the two main types of optimization difficulties in continuous optimization problems). It is therefore interesting to study the optimization methods and difficulties involved in deep learning, for the sake of obtaining better generalization. Furthermore, better optimization could also have an impact on scaling computations, discussed above.
One important thing to keep in mind, though, is that in a deep supervised network, the top two layers (the output layer and the top hidden layer) can rather easily be made to overfit, simply by making the top hidden layer large enough. However, to get good generalization, what we have found is that one needs to optimize the lower layers, those that are far removed from the immediate supervised training signal (Bengio et al., 2007). These observations mean that only looking at the training criterion is not sufficient to assess that a training procedure is doing a good job at optimizing the lower layers well. However, under constraints on the top hidden layer size, training error can be a good guide to the quality of the optimization of lower layers. Note that supervised deep nets are very similar (in terms of the optimization problem involved) to deep auto-encoders and to recurrent or recursive networks, and that properly optimizing RBMs (and more so deep Boltzmann machines) seems more difficult: progress on training deep nets is therefore likely to be a key to training the other types of deep learning models.
One of the early hypotheses drawn from experiments with layer-wise pre-training as well as of other experiments (semi-supervised embeddings (Weston et al., 2008) and slow feature analysis (Wiskott and Sejnowski, 2002a; Bergstra and Bengio, 2009)) is that the training signal provided by backpropagated gradients is sometimes too weak to properly train intermediate layers of a deep network. This is supported by the observation that all of these successful techniques somehow inject a training signal into the intermediate layers, helping them to figure out what they should do. However, the more recent successful results with supervised learning on very large labeled datasets suggest that with some tweaks in the optimization procedure (including initialization), it is sometimes possible to achieve as good results with or without unsupervised pre-training or semi-supervised embedding intermediate training signals.
Y. Bengio
Optimization: Solution Paths
In spite of these recent encouraging results, several more recent experimental results again point to a fundamental difficulty in training intermediate and lower layers.
Diminishing Returns with Larger Networks. First, Dauphin and Bengio (2013) show that with well-optimized SGD training, as the size of a neural net increases, the "return on investment"(number of training errors removed per added hidden unit) decreases, given a fixed number of training iterations, until the point where it goes below 1 (which is the return on investment that would be obtained by a brain-dead memory-based learning mechanism – such as Parzen Windows – which just copies an incorrectly labeled example into the weights of the added hidden unit so as to produce just the right answer for that example only). This suggests that larger models may be fundamentally more difficult to train, probably because there are now more second-order interactions between the parameters, increasing the condition number of the Hessian matrix (of second derivatives of model parameters with respect to the training criterion). This notion of return on investment may provide a useful metric by which to measure the effect of different methods to improve the scaling behavior of training and optimization procedures for deep learning.
Intermediate Concepts Guidance and Curriculum. Second, Gulcehre and Bengio (2013) show that there are apparently simple tasks on which standard black-box machine learning algorithms completely fail. Even supervised and pre-trained deep networks were tested and failed at these tasks.
These tasks have in common the characteristic that the correct labels are obtained by the composition of at least two levels of non-linearity and abstraction: e.g., the first level involves the detection of objects in a scene and the second level involves a non-linear logical operation on top of these (such as the detecting presence of multiple objects of the same category). On the other hand, the task becomes easily solvable by a deep network whose intermediate layer is first pre-trained to solve the first-level sub-task. This raises the question of how humans might learn even more abstract tasks, and Bengio (2013b) studies the hypothesis that the use of language and the evolution of culture could have helped humans reduce that difficulty (and gain a serious advantage over other less cultured animals). It would be interesting to explore multi-agent learning mechanisms inspired by the the mathematical principles behind the evolution of culture in order to bypass this optimization difficulty. The basic idea is that humans (and current learning algorithms) are limited to "local descent" optimization methods, that make small changes in the parameter values with the effect of reducing the expected loss in average. This is clearly prone to the presence of local minima, while a more global search (in the spirit of both genetic and cultural evolution) could potentially reduce this difficulty. One hypothesis is that more abstract learning tasks involve more challenging optimization difficulties, which would make such global optimization algorithms necessary if we want computers to learn such abstractions from scratch. Another option, following the idea of curriculum learning (Bengio et al., 2009), is to provide guidance ourselves to learning machines (as exemplified in the toy example of Gulcehre and Bengio (2013)), by "teaching them" gradually more complex concepts to help them understand the world around us (keeping in mind that we also have to do that for humans and that it takes 20 years to complete).
Changing the learning procedure and the architecture. Regarding the basic optimization difficulty of a single deep network, three types of solutions should be considered. First, there are solutions based on improved general-purpose optimization algorithms, such as for example the recent work on adaptive learning rates (Schaul et al., 2012), online natural gradient (Le Roux et al., 2008;
Pascanu and Bengio, 2013) or large-minibatch second order methods (Martens, 2010).
Another class of attacks on the optimization problem is based on changing the architecture (family of functions and its parametrization) or the way that the outputs are produced (for example by adding noise). As already introduced in LeCun et al. (1998a), changes in the preprocessing, training objective and architecture can change the difficulty of optimization, and in particularly improve the Deep Learning of Representations: Looking Forward
11 conditioning of the Hessian matrix (of second derivatives of the loss with respect to parameters).
With gradient descent, training time into a quadratic bowl is roughly proportional to the condition number of the Hessian matrix (ratio of largest to smallest eigenvalue). For example LeCun et al.(1998a) recommends centering and normalizing the inputs, an idea recently extended to hidden layers of Boltzmann machines with success (Montavon and Muller, 2012). A related idea that may have an impact on ill-conditioning is the idea of skip-connections, which forces both the mean output and the mean slope of each hidden unit of a deep multilayer network to be zero (Raiko et al., 2012), a centering idea which originates from Schraudolph (1998).
There has also been very successful recent work exploiting rectifier non-linearities for deep supervised networks (Glorot et al., 2011a; Krizhevsky et al., 2012). Interestingly, such non-linearities can produce rather sparse unit outputs, which could be exploited, if the amount of sparsity is sufficiently large, to considerably reduce the necessary computation (because when a unit output is 0, there is no need to actually multiply it with its outgoing weights). Very recently, we have discovered a variant on the rectifier non-linearity called maxout (Goodfellow et al., 2013b) which appears to open a very promising door towards more efficient training of deep networks. As confirmed experimentally (Goodfellow et al., 2013b), maxout networks can train deeper networks and allow lower layers to undergo more training. The more general principle at stake here may be that when the gradient is sparse, i.e., only a small subset of the hidden units and parameters is touched by the gradient, the optimization problem may become easier. We hypothesize that sparse gradient vectors have a positive effect on reducing the ill-conditioning difficulty involved in training deep nets.
The intuition is that by making many terms of the gradient vector 0, one also knocks off many off-diagonal terms of the Hessian matrix, making this matrix more diagonal-looking, which would reduce many of the ill-conditioning effects involved, as explained below. Indeed, gradient descent relies on an invalid assumption: that one can modify a parameter θi (in the direction of the gradient
∂C
∂θi ) without taking into account the changes in ∂C
∂θi that will take place when also modifying other parameters θj. Indeed, this is precisely the information that is captured (e.g. with second-order methods) by the off-diagonal entries
∂2C
∂θi∂θj =
∂
∂θj
∂C
∂θi, i.e., how changing θj changes the gradient on θi. Whereas second-order methods may have their own limitations13 it would be interesting if substantially reduced ill-conditioning could be achieved by modifying the architecture and training procedure. Sparse gradients would be just one weapon in this line of attack.
As we have argued above, adding noise in an appropriate way can be useful as a powerful regularizer (as in dropouts), and it can also be used to make the gradient vector sparser, which would reinforce the above positive effect on the optimization difficulty. If some of the activations are also sparse (as our suggestions for conditional computation would require), then more entries of the gradient vector will be zeroed out, also reinforcing that beneficial optimization effect. In addition, it is plausible that the masking noise found in dropouts (as well as in denoising auto-encoders) encourages a faster symmetry-breaking: quickly moving away from the condition where all hidden units of a neural network or a Boltzmann machine do the same thing (due to a form of symmetry in the signals they receive), which is a non-attractive fixed point with a flat (up to several orders) likelihood function. This means that gradient descent can take a lot of time to pull apart hidden units which are behaving in a very similar way. Furthermore, when starting from small weights, these symmetry conditions (where many hidden units do something similar) are actually attractive from far away, because initially all the hidden units are trying to grab the easiest and most salient job (explain the gradients on the units at the layer above). By randomly turning off hidden units we obtain a faster specialization which helps training convergence.
A related concept that has been found useful in understanding and reducing the training difficulty of deep or recurrent nets is the importance of letting the training signals (back-propagated gradients) flow, in a focused way. It is important that error signals flow so that credit and blame is clearly assigned to different components of the model, those that could change slightly to improve the training loss. The problem of vanishing and exploding gradients in recurrent nets (Hochreiter, 1991;
13 first, practical implementations never come close to actually inverting the Hessian, and second, they often require line searches that may be computationally inefficient if the optimal trajectory is highly curved
Y. Bengio
Bengio et al., 1994) arises because the effect of a long series of non-linear composition tends to produce gradients that can either be very small (and the error signal is lost) or very large (and the gradient steps diverge temporarily). This idea has been exploited to propose successful initialization procedures for deep nets (Glorot and Bengio, 2010). A composition of non-linearities is associated with a product of Jacobian matrices, and a way to reduce the vanishing problem would be to make sure that they have a spectral radius (largest eigenvalue) close to 1, like what is done in the weight initialization for Echo State Networks (Jaeger, 2007) or in the carousel self-loop of LSTM (Hochreiter and Schmidhuber, 1997) to help propagation of influences over longer paths. A more generic way to avoid gradient vanishing is to incorporate a training penalty that encourages the propagated gradient vectors to maintain their magnitude (Pascanu and Bengio, 2012). When combined with a gradient clipping14 heuristic (Mikolov, 2012) to avoid the detrimental effect of overly large gradients, it allows to train recurrent nets on tasks on which it was not possible to train them before (Pascanu and Bengio, 2012).
Inference and Sampling
All of the graphical models studied for deep learning except the humble RBM require a non-trivial form of inference, i.e., guessing values of the latent variables h that are appropriate for the given visible input x. Several forms of inference have been investigated in the past: MAP inference is formulated like an optimization problem (looking for h that approximately maximizes P(h | x));
MCMC inference attempts to sample a sequence of h's from P(h | x); variational inference looks for a simple (typically factorial) approximate posterior qx(h) that is close to P(h | x), and usually involves an iterative optimization procedure. See a recent machine learning textbook for more details (Bishop, 2006; Barber, 2011; Murphy, 2012).
In addition, a challenge related to inference is sampling (not just from P(h | x) but also from
P(h, x) or P(x)), which like inference is often needed in the inner loop of learning algorithms for probabilistic models with latent variables, energy-based models (LeCun et al., 2006) or Markov
Random Fields (Kindermann, 1980) (also known as undirected graphical models), where P(x) or P(h, x) is defined in terms of a parametrized energy function whose normalized exponential gives probabilities.
Deep Boltzmann machines (Salakhutdinov and Hinton, 2009) combine the challenge of inference(for the "positive phase" where one tries to push the energies associated with the observed x down) and the challenge of sampling (for the "negative phase" where one tries to push up the energies associated with x's sampled from P(x)). Sampling for the negative phase is usually done by MCMC, although some learning algorithms (Collobert and Weston, 2008; Gutmann and Hyvarinen, 2010;
Bordes et al., 2013) involve "negative examples" that are sampled through simpler procedures (like perturbations of the observed input). In Salakhutdinov and Hinton (2009), inference for the positive phase is achieved with a mean-field variational approximation.15
Inference and Sampling: The Challenge
There are several challenges involved with all of the these inference and sampling techniques.
The first challenge is practical and computational: these are all iterative procedures that can considerably slow down training (because inference and/or sampling is often in the inner loop of learning).
14 When the norm of the gradient is above a threshold τ, reduce it to τ
15 In the mean-field approximation, computation proceeds like in Gibbs sampling, but with stochastic binary values replaced by their conditional expected value (probability of being 1), given the outputs of the other units. This deterministic computation is iterated like in a recurrent network until convergence is approached, to obtain a marginal (factorized probability) approximation over all the units.
Deep Learning of Representations: Looking Forward
Potentially Huge Number of Modes. The second challenge is more fundamental and has to do with the potential existence of highly multi-modal posteriors: all of the currently known approaches to inference and sampling are making very strong explicit or implicit assumptions on the form the distribution of interest (P(h | x) or P(h, x)). As we argue below, these approaches make sense if this target distribution is either approximately unimodal (MAP), (conditionally) factorizes (variational approximations, i.e., the different factors hi are approximately independent16 of each other given x), or has only a few modes between which it is easy to mix (MCMC). However, approximate inference can be potentially hurtful, not just at test time but for training, because it is often in the inner loop of the learning procedure (Kulesza and Pereira, 2008).
Imagine for example that h represents many explanatory variables of a rich audio-visual scene with a highly ambiguous raw input x, including the presence of several objects with ambiguous attributes or categories, such that one cannot really disambiguate one of the objects independently of the others (the so-called "structured output" scenario, but at the level of latent explanatory variables). Clearly, a factorized or unimodal representation would be inadequate (because these variables are not at all independent, given x) while the number of modes could grow exponentially with the number of ambiguous factors present in the scene. For example, consider a visual scene x through a haze hiding most details, yielding a lot of uncertainty. Say it involves 10 objects (e.g., people), each having 5 ambiguous binary attributes (out of 20) (e.g., how they are dressed) and uncertainty between 100 categorical choices for each element (e.g., out of 10000 persons in the database, the marginal evidence allows to reduce the uncertainty for each person to about 100 choices). Furthermore, suppose that these uncertainties cannot be factorized (e.g., people tend to be in the same room with other people involved in the same activity, and friends tend to stand physically close to each other, and people choose to dress in a way that socially coherent). To make life hard on mean-field and other factorized approximations, this means that only a small fraction (say 1%) of these configurations are really compatible. So one really has to consider 1% × (25 × 100)10 ≈ 1033 plausible configurations of the latent variables. If one has to take a decision y based on x, e.g., P(y | x) = � h P(y | h)P(h | x) involves summing over a huge number of non-negligible terms of the posterior P(h | x), which we can consider as modes (the actual dimension of h is much larger, so we have reduced the problem from (220 × 10000)10 ≈ 10100 to about 1033, but that is still huge. One way or another, summing explicitly over that many modes seems implausible, and assuming single mode (MAP) or a factorized distribution (mean-field) would yield very poor results. Under some assumptions on the underlying data-generating process, it might well be possible to do inference that is exact or a provably good approximations, and searching for graphical models with these properties is an interesting avenue to deal with this problem. Basically, these assumptions work because we assume a specific structure in the form of the underlying distribution. Also, if we are lucky, a few
Monte-Carlo samples from P(h | x) might suffice to obtain an acceptable approximation for our y, because somehow, as far as y is concerned, many probable values of h yield the same answer y and a Monte-Carlo sample will well represent these different "types" of values of h. That is one form of regularity that could be exploited (if it exists) to approximately solve that problem. What if these assumptions are not appropriate to solve challenging AI problems? Another, more general assumption (and thus one more likely to be appropriate for these problems) is similar to what we usually do with machine learning: although the space of functions is combinatorially large, we are able to generalize by postulating a rather large and flexible family of functions (such as a deep neural net). Thus an interesting avenue is to assume that there exists a computationally tractable function that can compute P(y | x) in spite of the apparent complexity of going through the intermediate steps involving h, and that we may learn P(y | x) through (x, y) examples. This idea will be developed further in Section 5.2.
Mixing Between Modes. What about MCMC methods? They are hurt by the problem of mode mixing, discussed at greater length in Bengio et al. (2013b), and summarized here. To make the 16 this can be relaxed by considering tree-structured conditional dependencies (Saul and Jordan, 1996) and mixtures thereof
Y. Bengio mental picture simpler, imagine that there are only two kinds of probabilities: tiny and high. MCMC transitions try to stay in configurations that have a high probability (because they should occur in the chain much more often than the tiny probability configurations). Modes can be thought of as islands of high probability, but they may be separated by vast seas of tiny probability configurations. Hence, it is difficult for the Markov chain of MCMC methods to jump from one mode of the distribution to another, when these are separated by large low-density regions embedded in a high-dimensional space, a common situation in real-world data, and under the manifold hypothesis (Cayton, 2005;
Narayanan and Mitter, 2010). This hypothesis states that natural classes present in the data (e.g., visual object categories) are associated with low-dimensional regions17 (i.e., manifolds) near which the distribution concentrates, and that different class manifolds are well-separated by regions of very low density. Here, what we consider a mode may be more than a single point, it could be a whole (low-dimensional) manifold. Slow mixing between modes means that consecutive samples tend to be correlated (belong to the same mode) and that it takes a very large number of consecutive sampling steps to go from one mode to another and even more to cover all of them, i.e., to obtain a large enough representative set of samples (e.g. to compute an expected value under the sampled variables distribution). This happens because these jumps through the low-density void between modes are unlikely and rare events. When a learner has a poor model of the data, e.g., in the initial stages of learning, the model tends to correspond to a smoother and higher-entropy (closer to uniform) distribution, putting mass in larger volumes of input space, and in particular, between the modes (or manifolds). This can be visualized in generated samples of images, that look more blurred and noisy18. Since MCMCs tend to make moves to nearby probable configurations, mixing between modes is therefore initially easy for such poor models. However, as the model improves and its corresponding distribution sharpens near where the data concentrate, mixing between modes becomes considerably slower. Making one unlikely move (i.e., to a low-probability configuration) may be possible, but making N such moves becomes exponentially unlikely in N. Making moves that are far and probable is fundamentally difficult in a high-dimensional space associated with a peaky distribution (because the exponentially large fraction of the far moves would be to an unlikely configuration), unless using additional (possibly learned) knowledge about the structure of the distribution.
Inference and Sampling: Solution Paths
Going into a space where mixing is easier. The idea of tempering (Iba, 2001) for MCMCs is analogous to the idea of simulated annealing (Kirkpatrick et al., 1983) for optimization, and it is designed for and looks very appealing to solve the mode mixing problem: consider a smooth version (higher temperature, obtained by just dividing the energy by a temperature greater than
1) of the distribution of interest; it therefore spreads probability mass more uniformly so one can mix between modes at that high temperature version of the model, and then gradually cool to the target distribution while continuing to make MCMC moves, to make sure we end up in one of the "islands" of high probability. Desjardins et al. (2010); Cho et al. (2010); Salakhutdinov (2010b,a) have all considered various forms of tempering to address the failure of Gibbs chain mixing in RBMs.
Unfortunately, convincing solutions (in the sense of making a practical impact on training efficiency) have not yet been clearly demonstrated. It is not clear why this is so, but it may be due to the need to spend much time at some specific (critical) temperatures in order to succeed. More work is certainly warranted in that direction.
An interesting observation (Bengio et al., 2013b) which could turn out to be helpful is that after we train a deep model such as a DBN or a stack of regularized auto-encoders, we can observe that mixing between modes is much easier at higher levels of the hierarchy (e.g. in the top-level
RBM or top-level auto-encoder): mixing between modes is easier at deeper levels of representation.
17 e.g. they can be charted with a few coordinates
18 See examples of generated images with some of the current state-of-the-art in learned generative models of images (Courville et al., 2011; Luo et al., 2013)
Deep Learning of Representations: Looking Forward
This is achieved by running the MCMC in a high-level representation space and then projecting back in raw input space to obtain samples at that level. The hypothesis proposed (Bengio et al., 2013b) to explain this observation is that unsupervised representation learning procedures (such as for the RBM and contractive or denoising auto-encoders) tend to discover a representation whose distribution has more entropy (the distribution of vectors in higher layers is more uniform) and that better "disentangles" or separates out the underlying factors of variation (see next section for a longer discussion of the concept of disentangling). For example, suppose that a perfect disentangling had been achieved that extracted the factors out of images of objects, such as object category, position, foreground color, etc. A single Gibbs step could thus switch a single top-level variable (like object category) when that variable is resampled given the others, a very local move in that top-level disentangled representation but a very far move (going to a very different place) in pixel space. Note that maximizing mutual information between inputs and their learned deterministic representation, which is what auto-encoders basically do (Vincent et al., 2008), is equivalent to maximizing the entropy of the learned representation,19 which supports this hypothesis. An interesting idea20 would therefore be to use higher levels of a deep model to help the lower layers mix better, by using them in a way analogous to parallel tempering, i.e., to suggest configurations sampled from a different mode.
Another interesting potential avenue for solving the problem of sampling from a complex and rough (non-smooth) distribution would be to take advantage of quantum annealing effects (Rose and Macready, 2007) and analog computing hardware (such as produced by D-Wave). NP-hard problems (such as sampling or optimizing exactly in an Ising model) still require exponential time but experimental evidence has shown that for some problems, quantum annealing is far superior to standard digital computation (Brooke et al., 2001). Since quantum annealing is performed by essentially implementing a Boltzmann machine in analog hardware, it might be the case that drawing samples from a Boltzmann machine is one problem where quantum annealing would be dramatically superior to classical digital computing.
Learning a Computational Graph that Does What we Want If we stick to the idea of obtaining actual values of the latent variables (either through MAP, factorized variational inference or MCMC), then a promising path is based on learning approximate inference, i.e., optimizing a learned approximate inference mechanism so that it performs a better inference faster. This idea is not new and has been shown to work well in many settings. This idea was actually already present in the wake-sleep algorithm (Hinton et al., 1995; Frey et al., 1996; Hinton et al., 2006) in the context of variational inference for Sigmoidal Belief Networks and DBNs. Learned approximate inference is also crucial in the predictive sparse coding (PSD) algorithm (Kavukcuoglu et al., 2008). This approach is pushed further with Gregor and LeCun (2010b) in which the parametric encoder has the same structural form as a fast iterative sparse coding approximate inference algorithm. The important consideration in both cases is not just that we have fast approximate inference, but that (a) it is learned, and (b) the model is learned jointly with the learned approximate inference procedure.
See also Salakhutdinov and Larochelle (2010) for learned fast approximate variational inference in DBMs, or Bagnell and Bradley (2009); Stoyanov et al. (2011) for learning fast approximate inference(with fewer steps than would otherwise be required by standard general purpose inference) based on loopy belief propagation.
The traditional view of probabilistic graphical models is based on the clean separation between modeling (defining the model), optimization (tuning the parameters), inference (over the latent variables) and sampling (over all the variables, and possibly over the parameters as well in the Bayesian scenario). This modularization has clear advantages but may be suboptimal. By bringing learning into inference and jointly learning the approximate inference and the "generative model" itself, one can hope to obtain "specialized" inference mechanisms that could be much more efficient and accurate than generic purpose ones; this was the subject of a recent ICML workshop (Eisner, 19 Salah Rifai, personal communication
20 Guillaume Desjardins, personal communication
Y. Bengio
2012). The idea of learned approximate inference may help deal with the first (purely computational) challenge raised above regarding inference, i.e., it may help to speed up inference to some extent, but it generally keeps the approximate inference parameters separate from the model parameters.
But what about the challenge from a huge number of modes? What if the number of modes is too large and/or these are too well-separated for MCMC to visit efficiently or for variational/MAP inference to approximate satisfactorily? If we stick to the objective of computing actual values of the latent variables, the logical conclusion is that we should learn to approximate a posterior that is represented by a rich multi-modal distribution. To make things concrete, imagine that we learn(or identify) a function f(x) of the visible variable x that computes the parameters θ = f(x) of an approximate posterior distribution Qθ=f(x)(h) but where Qθ=f(x)(h) ≈ P(h | x) can be highly multimodal, e.g., an RBM with visible variables h (coupled with additional latent variables used only to represent the richness of the posterior over h itself). Since the parameters of the RBM are obtained through a parametric computation taking x as input,21 this is really a conditional RBM (Taylor et al., 2007; Taylor and Hinton, 2009). Whereas variational inference is usually limited to a nonparametric approximation of the posterior, Q(h) (one that is analytically and iteratively optimized for each given x) one could consider a parametric approximate posterior that is learned (or derived analytically) while allowing for a rich multi-modal representation (such as what an RBM can capture, i.e., up to an exponential number of modes).
Avoiding inference and explicit marginalization over latent variables altogether. We now propose to consider an even more radical departure from traditional thinking regarding probabilistic models with latent variables. It is motivated by the observation that even with the last proposal, something like a conditional RBM to capture the posterior P(h | x), when one has to actually make a decision or a prediction, it is necessary for optimal decision-making to marginalize over the latent variables. For example, if we want to predict y given x, we want to compute something like
� h P(y | h)P(h | x). If P(h | x) is complex and highly multi-modal (with a huge number of modes), then even if we can represent the posterior, performing this sum exactly is out of the question, and even an MCMC approximation may be either very poor (we can only visit at most N modes with N
MCMC steps, and that is very optimistic because of the mode mixing issue) or very slow (requiring an exponential number of terms being computed or a very very long MCMC chain). It seems that we have not really addressed the original "fundamental challenge with highly multi-modal posteriors" raised above.
To address this challenge, we propose to avoid explicit inference altogether by avoiding to sample, enumerate, or represent actual values of the latent variables h. In fact, our proposal is to completely skip the latent variables themselves. Instead, if we first consider the example of the previous paragraph, one can just directly learn to predict P(y | x). In general, what we seek is that the only approximation error we are left with is due to to function approximation. This might be important because the compounding of approximate inference with function approximation could be very hurtful (Kulesza and Pereira, 2008).
To get there, one may wish to mentally go through an intermediate step. Imagine we had a good approximate posterior Qθ=f(x)(h) as proposed above, with parameters θ = f(x). Then we could imagine learning an approximate decision model that approximates and skips the intractable sum over h, instead directly going from θ = f(x) to a prediction of y, i.e., we would estimate P(y | x) by g(f(x)). Now since we are already learning f(x), why learn g(θ) separately? We could simply directly learn to estimate π(x) = g(f(x)) ≈ P(y | x).
Now that may look trivial, because this is already what we do in discriminant training of deep networks or recurrent networks, for example. And don't we lose all the advantages of probabilistic models, such as, handling different forms of uncertainty, missing inputs, and being able to answer any "question" of the form "predict any variables given any subset of the others"? Yes, if we stick
21 for many models, such as deep Boltzmann machines, or bipartite discrete Markov random fields (Martens and Sutskever, 2010), f does not even need to be learned, it can be derived analytically from the form of P(h | x)
Deep Learning of Representations: Looking Forward
17 to the traditional deep (or shallow) neural networks like those discussed in Section 2.1.22 But there are other options.
We propose to get the advantages of probabilistic models without the need for explicitly going through many configurations of the latent variables. The general principle of what we propose to achieve this is to construct a family of computational graphs which perform the family of tasks we are interested in. A recent proposal (Goodfellow et al., 2013a) goes in this direction. Like previous work on learned approximate inference (Stoyanov et al., 2011), one can view the approach as constructing a computational graph associated to approximate inference (e.g. a fixed number of iterations of meanfield updates) in a particular setting (here, filling missing input with a variational approximation over hidden and unclamped inputs). An interesting property is that depending on which input variables are clamped and which are considered missing (either during training or at test time), we get a different computational graph, while all these computational graphs share the same parameters.
In Goodfellow et al. (2013a), training the shared parameters of these computational graph is achieved through a variational criterion that is similar to a generalized pseudo-likelihood, i.e., approximately maximizing log P(xv | xc) for randomly chosen partitions (v, c) of s.
This would be similar to dependency networks (Heckerman et al., 2000), but re-using the same parameters for every possible question-answer partition and training the system to answer for any subset of variables rather than singletons like in pseudo-likelihood. For the same reason, it raises the question of whether the different estimated conditionals are coherent with a global joint distribution.
In the case where the computational graph is obtained from the template of an inference mechanism for a joint distribution (such as variational inference), then clearly, we keep the property that these conditionals are coherent with a global joint distribution. With the mean-field variational inference, the computational graph looks like a recurrent neural network converging to a fixed point, and where we stop the iterations after a fixed number of steps or according to a convergence criterion. Such a trained parametrized computational graph is used in the iterative variational approach introduced in Goodfellow et al. (2013a) for training and missing value inference in deep Boltzmann machines, with an inpainting-like criterion in which arbitrary subsets of pixels are predicted given the others (a generalized pseudo-likelihood criterion). It has also been used in a recursion that follows the template of loopy belief propagation to fill-in the missing inputs and produce outputs (Stoyanov et al., 2011).
Although in these cases there is still a notion of latent variables (e.g. the latent variables of the deep Boltzmann machine) that motivate the "template" used for the learned approximate inference, what we propose here is to stop thinking about them as actual latent factors, but rather just as a way to parametrize this template for a question answering mechanism regarding missing inputs, i.e., the "generic conditional prediction mechanism" implemented by the recurrent computational graph that is trained to predict any subset of variables given any other subset. Although Goodfellow et al. (2013a) assume a factorial distribution across the predicted variables, we propose to investigate non-factorial posterior distributions over the observed variables, i.e., in the spirit of the recent flurry of work on structured output machine learning (Tsochantaridis et al., 2005). We can think of this parametrized computational graph as a family of functions, each corresponding to answering a different question (predict a specific set of variables given some others), but all sharing the same parameters. We already have examples of such families in machine learning, e.g., with recurrent neural networks or dynamic Bayes nets (where the functions in the family are indexed by the length of the sequence). This is also analogous to what happens with dropouts, where we have an exponential number of neural networks corresponding to different sub-graphs from input to output(indexed by which hidden units are turned on or off). For the same reason as in these examples, we obtain a form of generalization across subsets. Following the idea of learned approximate inference, the parameters of the question-answering inference mechanism would be taking advantage of the specific underlying structure in the data generating distribution. Instead of trying to do inference on the anonymous latent variables, it would be trained to do good inference only over observed
22 although, using something like these deep nets would be appealing because they are currently beating benchmarks in speech recognition, language modeling and object recognition
Y. Bengio variables or over high-level features learned by a deep architecture, obtained deterministically from the observed input.
An even more radically different solution to the problem of avoiding explicit latent variables was recently introduced in Bengio et al. (2013c) and Bengio and Thibodeau-Laufer (2013). These introduce training criteria respectively for generalized forms of denoising auto-encoders and for generative stochastic networks, with the property that maximum likelihood training of the reconstruction probabilities yields consistent but implicit estimation of the data generating distribution. These
Generative Stochastic Networks (GSNs) can be viewed as inspired by the Gibbs sampling procedure in deep Boltzmann machines (or deep belief networks) in the sense that one can construct computational graphs that perform similar computation, i.e., these are stochastic computational graphs (or equivalently, deterministic computational graphs with noise sources injected in the graph). These models are not explicitly trained to fill-in missing inputs but simply to produce a Markov chain whose asymptotic distribution estimates the data generating distribution. However, one can show that this chain can be manipulated in order to obtain samples of the estimated conditional distribution P(xv | xc), i.e., if one clamps some of the inputs, one can sample from a chain that stochastically fills-in from the missing inputs.
The approximate inference is not anymore an approximation of something else, it is the definition of the model itself. This is actually good news because we thus eliminate the issue that the approximate inference may be poor. The only thing we need to worry about is whether the parameterized computational graph is rich enough (or may overfit) to capture the unknown data generating distribution, and whether it makes it easy or difficult to optimize the parameters.
The idea that we should train with the approximate inference as part of the computational graph for producing a decision (and a loss) was first introduced by Stoyanov et al. (2011), and we simply push it further here, by proposing to allow the computational graph to depart in any way we care to explore from the template provided by existing inference or sampling mechanisms, i.e., potentially losing the connection and the reference to probabilistic latent variables. Once we free ourselves from the constraint of interpreting this parametrized question answering computational graph as corresponding to approximate inference or approximate sampling involving latent variables, all kinds of architectures and parametrizations are possible, where current approximate inference mechanisms can serve as inspiration and starting points. Interestingly, Bengio and Thibodeau-Laufer (2013) provides a proper training criterion for training any such stochastic computational graph simply using backprop over the computational graph, so as to maximize the probability of reconstructing the observed data under a reconstruction probability distribution that depends on the inner nodes of the computational graph. The noise injected in the computational graph must be such that the learner cannot get rid of the noise and obtain perfect reconstruction (a dirac at the correct observed input), just like in denoising auto-encoders. It is quite possible that this new freedom could give rise to much better models.
To go farther than Bengio and Thibodeau-Laufer (2013); Goodfellow et al. (2013a); Stoyanov et al. (2011) it would be good to go beyond the kind of factorized prediction common in variational and loopy belief propagation inference. We would like the reconstruction distribution to be able to capture multi-modal non-factorial distributions. Although the result from Alain and Bengio (2013) suggests that when the amount of injected noise is small, a unimodal distribution is sufficient, it is convenient to accomodate large amounts of injected noise to make training more efficient, as discussed by Bengio et al. (2013c). One idea is to obtain such multi-modal reconstruction distributions is to represent the estimated joint distribution of the predicted variables (possibly given the clamped variables) by a powerful model such as an RBM or a regularized auto-encoder, e.g., as has been done for structured output predictions when there is complex probabilistic structure between the output variables (Mnih et al., 2011; Li et al., 2013).
Although conditional RBMs have been already explored, conditional distributions provided by regularized auto-encoders remain to be studied. Since a denoising auto-encoder can be shown to estimate the underlying data generating distribution, making its parameters dependent on some other variables yields an estimator of a conditional distribution, which can also be trained by simple gradient-based methods (and backprop to obtain the gradients).
Deep Learning of Representations: Looking Forward
All these ideas lead to the question: what is the interpretation of hidden layers, if not directly of the underlying generative latent factors? The answer may simply be that they provide a better representation of these factors, a subject discussed in the next section. But what about the representation of uncertainty about these factors? The author believes that humans and other animals carry in their head an internal representation that implicitly captures both the most likely interpretation of any of these factors (in case a hard decision about some of them has to be taken) and uncertainty about their joint assignment. This is of course a speculation. Somehow, our brain would be operating on implicit representations of the joint distribution between these explanatory factors, generally without having to commit until a decision is required or somehow provoked by our attention mechanisms (which seem related to our tendancy to verbalize a discrete interpretation). A good example is foreign language understanding for a person who does not master that foreign language. Until we consciously think about it, we generally don't commit to a particular meaning for ambiguous word(which would be required by MAP inference), or even to the segmentation of the speech in words, but we can take a hard or a stochastic decision that depends on the interpretation of these words if we have to, without having to go through this intermediate step of discrete interpretation, instead treating the ambiguous information as soft cues that may inform our decision. In that example, a factorized posterior is also inadequate because some word interpretations are more compatible with each other.
To summarize, what we propose here, unlike in previous work on approximate inference, is to drop the pretense that the learned approximate inference mechanism actually approximates the latent variables distribution, mode, or expected value. Instead, we only consider the construction of a computational graph (deterministic or stochastic) which produces answers to the questions we care about, and we make sure that we can train a family of computational graphs (sharing parameters) whose elements can answer any of these questions. By removing the interpretation of approximately marginalizing over latent variables, we free ourselves from a strong constraint and the possible hurtful approxiations involved in approximate inference, especially when the true posterior would have a huge number of significant modes.
This discussion is of course orthogonal to the use of Bayesian averaging methods in order to produce better-generalizing predictions, i.e., handling uncertainty due to a small number of training examples. The proposed methods can be made Bayesian just like neural networks have their Bayesian variants (Neal, 1994), by somehow maintaining an implicit or explicit distribution over parameters.
A promising step in this direction was proposed by Welling and Teh (2011), making such Bayesian computation tractable by exploiting the randomness introduced with stochastic gradient descent to also produce the Bayesian samples over the uncertain parameter values.
Disentangling
Disentangling: The Challenge
What are "underlying factors" explaining the data? The answer is not obvious. One answer could be that these are factors that can be separately controlled (one could set up way to change one but not the others). This can actually be observed by looking at sequential real-world data, where only a small proportion of the factors typically change from t to t + 1. Complex data arise from the rich interaction of many sources. These factors interact in a complex web that can complicate AI-related tasks such as object classification. If we could identity and separate out these factors (i.e., disentangle them), we would have almost solved the learning problem. For example, an image is composed of the interaction between one or more light sources, the object shapes and the material properties of the various surfaces present in the image. It is important to distinguish between the related but distinct goals of learning invariant features and learning to disentangle explanatory factors. The central difference is the preservation of information. Invariant features, by definition, have reduced sensitivity in the directions of invariance. This is the goal of building features that are insensitive to variation in the data that are uninformative to the task at hand. Unfortunately, it is often difficult to determine a priori which set of features and variations will ultimately be relevant to the task at
Y. Bengio hand. Further, as is often the case in the context of deep learning methods, the feature set being trained may be destined to be used in multiple tasks that may have distinct subsets of relevant features. Considerations such as these lead us to the conclusion that the most robust approach to feature learning is to disentangle as many factors as possible, discarding as little information about the data as is practical.
Deep learning algorithms that can do a much better job of disentangling the underlying factors of variation would have tremendous impact. For example, suppose that the underlying factors can be
"guessed" (predicted) from a simple (e.g. linear) transformation of the learned representation, ideally a transformation that only depends on a few elements of the representation. That is what we mean by a representation that disentangles the underlying factors. It would clearly make learning a new supervised task (which may be related to one or a few of them) much easier, because the supervised learning could quickly learn those linear factors, zooming in on the parts of the representation that are relevant.
Of all the challenges discussed in this paper, this is probably the most ambitious, and success in solving it the most likely to have far-reaching impact. In addition to the obvious observation that disentangling the underlying factors is almost like pre-solving any possible task relevant to the observed data, having disentangled representations would also solve other issues, such as the issue of mixing between modes. We believe that it would also considerably reduce the optimization problems involved when new information arrives and has to be reconciled with the world model implicit in the current parameter setting. Indeed, it would allow only changing the parts of the model that involve the factors that are relevant to the new observation, in the spirit of sparse updates and reduced ill-conditioning discussed above.
Disentangling: Solution Paths
Deeper Representations Disentangle Better. There are some encouraging signs that our current unsupervised representation-learning algorithms are reducing the "entanglement" of the underlying factors23 when we apply them to raw data (or to the output of a previous representation learning procedure, like when we stack RBMs or regularized auto-encoders).
First, there are experimental observations suggesting that sparse convolutional RBMs and sparse denoising auto-encoders achieve in their hidden units a greater degree of disentangling than in their inputs (Goodfellow et al., 2009; Glorot et al., 2011b). What these authors found is that some hidden units were particularly sensitive to a known factor of variation while being rather insensitive (i.e., invariant) to others. For example, in a sentiment analysis model that sees unlabeled paragraphs of customer comments from the Amazon web site, some hidden units specialized on the topic of the paragraph (the type of product being evaluated, e.g., book, video, music) while other units specialized on the sentiment (positive vs negative). The disentanglement was never perfect, so the authors made quantitative measurements of sensitivity and invariance and compared these quantities on the input and the output (learned representation) of the unsupervised learners.
Another encouraging observation (already mentioned in the section on mixing) is that deeper representations were empirically found to be more amenable to quickly mixing between modes (Bengio et al., 2013b). Two (compatible) hypotheses were proposed to explain this observation: (1) RBMs and regularized auto-encoders deterministically transform24 their input distribution into one that is more uniform-looking, that better fills the space (thus creating easier paths between modes), and(2) these algorithms tend to discover representations that are more disentangled. The advantage of a higher-level disentangled representation is that a small MCMC step (e.g. Gibbs) in that space(e.g. flipping one high-level variable) can move in one step from one input-level mode to a distant one, e.g., going from one shape / object to another one, adding or removing glasses on the face of a person (which requires a very sharp coordination of pixels far from each other because glasses occupy a very thin image area), or replacing foreground and background colors (such as going into a "reverse video" mode).
23 as measured by how predictive some individual features are of known factors
24 when considering the features learned, e.g., the P(hi = 1 | x), for RBMs
Deep Learning of Representations: Looking Forward
Although these observations are encouraging, we do not yet have a clear understanding as to why some representation algorithms tend to move towards more disentangled representations, and there are other experimental observations suggesting that this is far from sufficient. In particular, Gulcehre and Bengio (2013) show an example of a task on which deep supervised nets (and every other black-box machine learning algorithm tried) fail, on which a completely disentangled input representation makes the task feasible (with a maxout network (Goodfellow et al., 2013b)). Unfortunately, unsupervised pre-training applied on the raw input images failed to produce enough disentangling to solve the task, even with the appropriate convolutional structure. What is interesting is that we now have a simple artificial task on which we can evaluate new unsupervised representation learning methods for their disentangling ability. It may be that a variant of the current algorithms will eventually succeed at this task, or it may be that altogether different unsupervised representation learning algorithms are needed.
Generic Priors for Disentangling Factors of Variation. A general strategy was outlined in Bengio et al. (2013d) to enhance the discovery of representations which disentangle the underlying and unknown factors of variation: it relies on exploiting priors about these factors. We are most interested in broad generic priors that can be useful for a large class of learning problems of interest in AI. We list these priors here:
• Smoothness: assumes the function f to be learned is s.t. x ≈ y generally implies f(x) ≈ f(y).
This most basic prior is present in most machine learning, but is insufficient to get around the curse of dimensionality.
• Multiple explanatory factors: the data generating distribution is generated by different underlying factors, and for the most part what one learns about one factor generalizes in many configurations of the other factors. The objective is to recover or at least disentangle these underlying factors of variation. This assumption is behind the idea of distributed representations. More specific priors on the form of the model can be used to enhance disentangling, such as multiplicative interactions between the factors (Tenenbaum and Freeman, 2000; Desjardins et al., 2012) or orthogonality of the features derivative with respect to the input (Rifai et al., 2011b, 2012a; Sohn et al., 2013). The parametrization and training procedure may also be used to disentangle discrete factors (e.g., detecting a shape) from associated continuous-valued factors (e.g., pose parameters), as in transforming auto-encoders (Hinton et al., 2011), spike-and-slab RBMs with pooled slab variables (Courville et al., 2011) and other pooling-based models that learn a feature subspace (Kohonen, 1996; Hyv¨arinen and Hoyer, 2000).
• A hierarchical organization of explanatory factors: the concepts that are useful for describing the world around us can be defined in terms of other concepts, in a hierarchy, with more abstract concepts higher in the hierarchy, defined in terms of less abstract ones. This assumption is exploited with deep representations. Although stacking single-layer models has been rather successful, much remains to be done regarding the joint training of all the layers of a deep unsupervised model.
• Semi-supervised learning: with inputs X and target Y to predict, given X, a subset of the factors explaining X's distribution explain much of Y, given X. Hence representations that are useful for spelling out P(X) tend to be useful when learning P(Y | X), allowing sharing of statistical strength between the unsupervised and supervised learning tasks. However, many of the factors that explain X may dominate those that also explain Y, which can make it useful to incorporate observations of Y in training the learned representations, i.e., by semi-supervised representation learning.
• Shared factors across tasks: with many Y 's of interest or many learning tasks in general, tasks(e.g., the corresponding P(Y | X, task)) are explained by factors that are shared with other tasks, allowing sharing of statistical strength across tasks, e.g. for multi-task and transfer learning or domain adaptation. This can be achieved by sharing embeddings or representation functions across tasks (Collobert and Weston, 2008; Bordes et al., 2013).
• Manifolds: probability mass concentrates near regions that have a much smaller dimensionality than the original space where the data lives. This is exploited with regularized auto-encoder algorithms, but training criteria that would explicitly take into account that we are looking for a Y. Bengio concentration of mass in an integral number directions remain to be developed.
• Natural clustering: different values of categorical variables such as object classes are associated with separate manifolds. More precisely, the local variations on the manifold tend to preserve the value of a category, and a linear interpolation between examples of different classes in general involves going through a low density region, i.e., P(X | Y = i) for different i tend to be well separated and not overlap much. For example, this is exploited in the Manifold Tangent Classifier (Rifai et al., 2011b). This hypothesis is consistent with the idea that humans have named categories and classes because of such statistical structure (discovered by their brain and propagated by their culture), and machine learning tasks often involves predicting such categorical variables.
• Temporal and spatial coherence: this prior introduced in Becker and Hinton (1992) is similar to the natural clustering assumption but concerns sequences of observations: consecutive (from a sequence) or spatially nearby observations tend to be easily predictable from each other. In the special case typically studied, e.g., slow feature analysis (Wiskott and Sejnowski, 2002b), one assumes that consecutive values are close to each other, or that categorical concepts remain either present or absent for most of the transitions. More generally, different underlying factors change at different temporal and spatial scales, and this could be exploited to sift different factors into different categories based on their temporal scale.
• Sparsity: for any given observation x, only a small fraction of the possible factors are relevant.
In terms of representation, this could be represented by features that are often zero (as initially proposed by Olshausen and Field (1996)), or more generally by the fact that most of the extracted features are insensitive to small variations of x. This can be achieved with certain forms of priors on latent variables (peaked at 0), or by using a non-linearity whose value is often flat at 0 (i.e., 0 and with a 0 derivative), or simply by penalizing the magnitude of the derivatives of the function mapping input to representation. A variant on that hypothesis is that for any given input, only a small part of the model is relevant and only a small subset of the parameters need to be updated.
• Simplicity of Factor Dependencies: in good high-level representations, the factors are related to each other through simple, typically linear, dependencies. This can be seen in many laws of physics, and is assumed when plugging a linear predictor on top of a learned representation.
Conclusion
Deep learning and more generally representation learning are recent areas of investigation in machine learning and recent years of research have allowed to clearly identify several major challenges for approaching the performance of these algorithms from that of humans. We have broken down these challenges into four major areas: scaling computations, reducing the difficulties in optimizing parameters, designing (or avoiding) expensive inference and sampling, and helping to learn representations that better disentangle the unknown underlying factors of variation. There is room for exploring many paths towards addressing all of these issues, and we have presented here a few appealing directions of research towards these challenges.
Acknowledgments
The author is extremely grateful for the feedback and discussions he enjoyed with collaborators
Ian Goodfellow, Guillaume Desjardins, Aaron Courville, Pascal Vincent, Roland Memisevic and Nicolas Chapados, which greatly contributed to help form the ideas presented here and fine-tune this manuscript. He is also grateful for the funding support from NSERC, CIFAR, the Canada
Research Chairs, and Compute Canada.
Bibliography
Alain, G. and Bengio, Y. (2013). What regularized auto-encoders learn from the data generating distribution.
In International Conference on Learning Representations (ICLR'2013).
Bach, F., Jenatton, R., Mairal, J., and Obozinski, G. (2011). Structured sparsity through convex optimization. Technical report, arXiv.1109.2397.
Bagnell, J. A. and Bradley, D. M. (2009). Differentiable sparse coding. In NIPS'2009, pages 113–120.
Barber, D. (2011). Bayesian Reasoning and Machine Learning. Cambridge University Press.
Becker, S. and Hinton, G. (1992). A self-organizing neural network that discovers surfaces in random-dot stereograms. Nature, 355, 161–163.
Bengio, Y. (2008). Neural net language models. Scholarpedia, 3(1).
Bengio, Y. (2009). Learning deep architectures for AI. Now Publishers.
Bengio, Y. (2011). Deep learning of representations for unsupervised and transfer learning. In JMLR W&CP:
Proc. Unsupervised and Transfer Learning.
Bengio, Y. (2013a). Estimating or propagating gradients through stochastic neurons. Technical Report arXiv:1305.2982, Universite de Montreal.
Bengio, Y. (2013b). Evolving culture vs local minima. In Growing Adaptive Machines: Integrating Development and Learning in Artificial Neural Networks, number also as ArXiv 1203.2990v1, pages T. Kowaliw, N. Bredeche & R. Doursat, eds. Springer-Verlag.
Bengio, Y. (2013c). Practical recommendations for gradient-based training of deep architectures. In K.-R.
M¨uller, G. Montavon, and G. B. Orr, editors, Neural Networks: Tricks of the Trade. Springer.
Bengio, Y. and Thibodeau-Laufer, E. (2013). Deep generative stochastic networks trainable by backprop.
Technical Report arXiv:1306.1091, Universite de Montreal.
Bengio, Y., Simard, P., and Frasconi, P. (1994). Learning long-term dependencies with gradient descent is difficult. IEEE Transactions on Neural Networks, 5(2), 157–166.
Bengio, Y., Ducharme, R., Vincent, P., and Jauvin, C. (2003). A neural probabilistic language model. JMLR, 3, 1137–1155.
Bengio, Y., Lamblin, P., Popovici, D., and Larochelle, H. (2007). Greedy layer-wise training of deep networks.
In NIPS'2006.
Bengio, Y., Louradour, J., Collobert, R., and Weston, J. (2009). Curriculum learning. In ICML'09.
Bengio, Y., Delalleau, O., and Simard, C. (2010). Decision trees do not generalize to new variations. Computational Intelligence, 26(4), 449–467.
Bengio, Y., Alain, G., and Rifai, S. (2012). Implicit density estimation by local moment matching to sample from auto-encoders. Technical report, arXiv:1207.0057.
Bengio, Y., Boulanger-Lewandowski, N., and Pascanu, R. (2013a). Advances in optimizing recurrent networks. In ICASSP'2013.
Bengio, Y., Mesnil, G., Dauphin, Y., and Rifai, S. (2013b). Better mixing via deep representations. In
ICML'2013.
Bengio, Y., Li, Y., Alain, G., and Vincent, P. (2013c). Generalized denoising auto-encoders as generative models. Technical Report arXiv:1305.6663, Universite de Montreal.
Bengio, Y., Courville, A., and Vincent, P. (2013d). Unsupervised feature learning and deep learning: A review and new perspectives. IEEE Trans. Pattern Analysis and Machine Intelligence (PAMI).
Bergstra, J. and Bengio, Y. (2009). Slow, decorrelated features for pretraining complex cell-like networks.
In NIPS'2009.
Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., Turian, J., Warde-Farley, D., and Bengio, Y. (2010). Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientific Computing Conference (SciPy).
Bergstra, J., Bastien, F., Breuleux, O., Lamblin, P., Pascanu, R., Delalleau, O., Desjardins, G., Warde-Farley, D., Goodfellow, I., Bergeron, A., and Bengio, Y. (2011). Theano: Deep learning on gpus with python. In
Big Learn workshop, NIPS'11.
Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.
Bordes, A., Glorot, X., Weston, J., and Bengio, Y. (2013). A semantic matching energy function for learning with multi-relational data. Machine Learning: Special Issue on Learning Semantics.
Brooke, J. J., Bitko, D., Rosenbaum, T. F., and Aeppli, G. (2001). Quantum annealing of a disordered magnet. Technical Report cond-mat/0105238.
Y. Bengio
Cayton, L. (2005). Algorithms for manifold learning. Technical Report CS2008-0923, UCSD.
Cho, K., Raiko, T., and Ilin, A. (2010). Parallel tempering is efficient for learning restricted Boltzmann machines. In IJCNN'2010.
Ciresan, D., Meier, U., and Schmidhuber, J. (2012). Multi-column deep neural networks for image classification. Technical report, arXiv:1202.2745.
Coates, A. and Ng, A. Y. (2011). The importance of encoding versus training with sparse coding and vector quantization. In ICML'2011.
Coates, A., Lee, H., and Ng, A. Y. (2011). An analysis of single-layer networks in unsupervised feature learning. In AISTATS'2011.
Coates, A., Karpathy, A., and Ng, A. (2012). Emergence of object-selective features in unsupervised feature learning. In NIPS'2012.
Collobert, R. and Weston, J. (2008). A unified architecture for natural language processing: Deep neural networks with multitask learning. In ICML'2008.
Collobert, R., Bengio, Y., and Bengio., S. (2003). Scaling large learning problems with hard parallel mixtures.
International Journal of Pattern Recognition and Artificial Intelligence, 17(3), 349–365.
Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., and Kuksa, P. (2011). Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12, 2493–2537.
Corrado, G. (2012). Deep networks for predicting ad click through rates. In ICML'2012 Online Advertising
Workshop.
Courville, A., Bergstra, J., and Bengio, Y. (2011). Unsupervised models of images by spike-and-slab RBMs.
In ICML'2011.
Dauphin, Y. and Bengio, Y. (2013). Big neural networks waste capacity. Technical Report arXiv:1301.3583, Universite de Montreal.
Dean, J., Corrado, G., Monga, R., Chen, K., Devin, M., Le, Q., Mao, M., Ranzato, M., Senior, A., Tucker, P., Yang, K., and Ng, A. Y. (2012). Large scale distributed deep networks. In NIPS'2012.
Deng, L., Li, J., Huang, J.-T., Yao, K., Yu, D., Seide, F., Seltzer, M., Zweig, G., He, X., Williams, J., Gong, Y., and Acero, A. (2013). Recent advances in deep learning for speech research at Microsoft. In ICASSP
Desjardins, G., Courville, A., Bengio, Y., Vincent, P., and Delalleau, O. (2010). Tempered Markov chain
Monte Carlo for training of restricted Boltzmann machine. In AISTATS, volume 9, pages 145–152.
Desjardins, G., Courville, A., and Bengio, Y. (2012).
Disentangling factors of variation via generative entangling.
Eisner, J. (2012).
Learning approximate inference policies for fast prediction.
Keynote talk at ICML
Workshop on Inferning: Interactions Between Search and Learning.
Frey, B. J., Hinton, G. E., and Dayan, P. (1996). Does the wake-sleep algorithm learn good density estimators? In NIPS'95, pages 661–670. MIT Press, Cambridge, MA.
Glorot, X. and Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural networks.
In AISTATS'2010.
Glorot, X., Bordes, A., and Bengio, Y. (2011a). Deep sparse rectifier neural networks. In AISTATS.
Glorot, X., Bordes, A., and Bengio, Y. (2011b). Domain adaptation for large-scale sentiment classification:
A deep learning approach. In ICML'2011.
Goodfellow, I., Le, Q., Saxe, A., and Ng, A. (2009). Measuring invariances in deep networks. In NIPS'09, pages 646–654.
Goodfellow, I., Courville, A., and Bengio, Y. (2011). Spike-and-slab sparse coding for unsupervised feature discovery. In NIPS Workshop on Challenges in Learning Hierarchical Models.
Goodfellow, I., Courville, A., and Bengio, Y. (2012). Large-scale feature learning with spike-and-slab sparse coding. In ICML'2012.
Goodfellow, I. J., Courville, A., and Bengio, Y. (2013a). Joint training of deep Boltzmann machines for classification. In International Conference on Learning Representations: Workshops Track.
Goodfellow, I. J., Warde-Farley, D., Mirza, M., Courville, A., and Bengio, Y. (2013b). Maxout networks. In
ICML'2013.
Gregor, K. and LeCun, Y. (2010a).
Learning fast approximations of sparse coding.
In L. Bottou and M. Littman, editors, Proceedings of the Twenty-seventh International Conference on Machine Learning(ICML-10). ACM.
Gregor, K. and LeCun, Y. (2010b). Learning fast approximations of sparse coding. In ICML'2010.
Grosse, R., Raina, R., Kwong, H., and Ng, A. Y. (2007). Shift-invariant sparse coding for audio classification.
In UAI'2007.
Deep Learning of Representations: Looking Forward
Gulcehre, C. and Bengio, Y. (2013). Knowledge matters: Importance of prior information for optimization.
Technical Report arXiv:1301.4083, Universite de Montreal.
Gutmann, M. and Hyvarinen, A. (2010).
Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In AISTATS'2010.
Heckerman, D., Chickering, D. M., Meek, C., Rounthwaite, R., and Kadie, C. (2000). Dependency networks for inference, collaborative filtering, and data visualization. Journal of Machine Learning Research, 1, 49–75.
Hinton, G., Krizhevsky, A., and Wang, S. (2011). Transforming auto-encoders. In ICANN'2011.
Hinton, G., Deng, L., Dahl, G. E., Mohamed, A., Jaitly, N., Senior, A., Vanhoucke, V., Nguyen, P., Sainath, T., and Kingsbury, B. (2012a). Deep neural networks for acoustic modeling in speech recognition. IEEE
Signal Processing Magazine, 29(6), 82–97.
Hinton, G. E. and Salakhutdinov, R. (2006). Reducing the dimensionality of data with neural networks.
Science, 313(5786), 504–507.
Hinton, G. E., Dayan, P., Frey, B. J., and Neal, R. M. (1995). The wake-sleep algorithm for unsupervised neural networks. Science, 268, 1558–1161.
Hinton, G. E., Osindero, S., and Teh, Y. (2006). A fast learning algorithm for deep belief nets. Neural
Computation, 18, 1527–1554.
Hinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2012b).
Improving neural networks by preventing co-adaptation of feature detectors. Technical report, arXiv:1207.0580.
Hochreiter, S. (1991).
Untersuchungen zu dynamischen neuronalen Netzen. Diploma thesis, Institut f¨ur
Informatik, Lehrstuhl Prof. Brauer, Technische Universit¨at M¨unchen.
Hochreiter, S. and Schmidhuber, J. (1997). Long short-term memory. Neural Computation, 9(8), 1735–1780.
Hyv¨arinen, A. (2005). Estimation of non-normalized statistical models using score matching. J. Machine
Learning Res., 6.
Hyv¨arinen, A. and Hoyer, P. (2000). Emergence of phase and shift invariant features by decomposition of natural images into independent feature subspaces. Neural Computation, 12(7), 1705–1720.
Iba, Y. (2001). Extended ensemble monte carlo. International Journal of Modern Physics, C12, 623–656.
Jaeger, H. (2007). Echo state network. Scholarpedia, 2(9), 2330.
Jarrett, K., Kavukcuoglu, K., Ranzato, M., and LeCun, Y. (2009). What is the best multi-stage architecture for object recognition? In ICCV'09.
Jenatton, R., Audibert, J.-Y., and Bach, F. (2009). Structured variable selection with sparsity-inducing norms. Technical report, arXiv:0904.3523.
Kavukcuoglu, K., Ranzato, M., and LeCun, Y. (2008).
Fast inference in sparse coding algorithms with applications to object recognition. CBLL-TR-2008-12-01, NYU.
Kindermann, R. (1980). Markov Random Fields and Their Applications (Contemporary Mathematics ; V.
1). American Mathematical Society.
Kirkpatrick, S., Jr., C. D. G.,, and Vecchi, M. P. (1983). Optimization by simulated annealing. Science, 220, 671–680.
Kohonen, T. (1996). Emergence of invariant-feature detectors in the adaptive-subspace self-organizing map.
Biological Cybernetics, 75, 281–291. 10.1007/s004220050295.
Krizhevsky, A., Sutskever, I., and Hinton, G. (2012). ImageNet classification with deep convolutional neural networks. In NIPS'2012.
Kulesza, A. and Pereira, F. (2008). Structured learning with approximate inference. In NIPS'2007.
Larochelle, H. and Bengio, Y. (2008). Classification using discriminative restricted Boltzmann machines. In
ICML'2008.
Le, Q., Ranzato, M., Monga, R., Devin, M., Corrado, G., Chen, K., Dean, J., and Ng, A. (2012). Building high-level features using large scale unsupervised learning. In ICML'2012.
Le Roux, N., Manzagol, P.-A., and Bengio, Y. (2008). Topmoumoute online natural gradient algorithm. In
NIPS'07.
LeCun, Y., Bottou, L., Orr, G. B., and M¨uller, K. (1998a). Efficient backprop. In Neural Networks, Tricks of the Trade.
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998b). Gradient based learning applied to document recognition. Proc. IEEE.
LeCun, Y., Chopra, S., Hadsell, R., Ranzato, M.-A., and Huang, F.-J. (2006). A tutorial on energy-based learning. In G. Bakir, T. Hofman, B. Scholkopf, A. Smola, and B. Taskar, editors, Predicting Structured
Data, pages 191–246. MIT Press.
Lee, H., Ekanadham, C., and Ng, A. (2008). Sparse deep belief net model for visual area V2. In NIPS'07.
Y. Bengio
Li, Y., Tarlow, D., and Zemel, R. (2013). Exploring compositional high order pattern potentials for structured output learning. In CVPR'2013.
Luo, H., Carrier, P. L., Courville, A., and Bengio, Y. (2013). Texture modeling with convolutional spikeand-slab RBMs and deep extensions. In AISTATS'2013.
Mairal, J., Bach, F., Ponce, J., and Sapiro, G. (2009). Online dictionary learning for sparse coding. In
ICML'2009.
Martens, J. (2010). Deep learning via Hessian-free optimization. In L. Bottou and M. Littman, editors, Proceedings of the Twenty-seventh International Conference on Machine Learning (ICML-10), pages 735–
742. ACM.
Martens, J. and Sutskever, I. (2010). Parallelizable sampling of Markov random fields. In AISTATS'2010.
Mesnil, G., Dauphin, Y., Glorot, X., Rifai, S., Bengio, Y., Goodfellow, I., Lavoie, E., Muller, X., Desjardins, G., Warde-Farley, D., Vincent, P., Courville, A., and Bergstra, J. (2011).
Unsupervised and transfer learning challenge: a deep learning approach. In JMLR W&CP: Proc. Unsupervised and Transfer Learning, volume 7.
Mikolov, T. (2012). Statistical Language Models based on Neural Networks. Ph.D. thesis, Brno University of Technology.
Mnih, V., Larochelle, H., and Hinton, G. (2011). Conditional restricted Boltzmann machines for structure output prediction. In Proc. Conf. on Uncertainty in Artificial Intelligence (UAI).
Montavon, G. and Muller, K.-R. (2012). Deep Boltzmann machines and the centering trick. In G. Montavon, G. Orr, and K.-R. M¨uller, editors, Neural Networks: Tricks of the Trade, volume 7700 of Lecture Notes in Computer Science, pages 621–637.
Murphy, K. P. (2012). Machine Learning: a Probabilistic Perspective. MIT Press, Cambridge, MA, USA.
Nair, V. and Hinton, G. E. (2010).
Rectified linear units improve restricted Boltzmann machines.
In
ICML'10.
Narayanan, H. and Mitter, S. (2010). Sample complexity of testing the manifold hypothesis. In NIPS'2010.
Neal, R. M. (1994). Bayesian Learning for Neural Networks. Ph.D. thesis, Dept. of Computer Science, University of Toronto.
Olshausen, B. A. and Field, D. J. (1996). Emergence of simple-cell receptive field properties by learning a sparse code for natural images. Nature, 381, 607–609.
Pascanu, R. and Bengio, Y. (2012). On the difficulty of training recurrent neural networks. Technical Report arXiv:1211.5063, Universite de Montreal.
Pascanu, R. and Bengio, Y. (2013).
Revisiting natural gradient for deep networks.
Technical report, arXiv:1301.3584.
Raiko, T., Valpola, H., and LeCun, Y. (2012).
Deep learning made easier by linear transformations in perceptrons. In AISTATS'2012.
Raina, R., Battle, A., Lee, H., Packer, B., and Ng, A. Y. (2007). Self-taught learning: transfer learning from unlabeled data. In ICML'2007.
Raina, R., Madhavan, A., and Ng, A. Y. (2009). Large-scale deep unsupervised learning using graphics processors. In L. Bottou and M. Littman, editors, ICML 2009, pages 873–880, New York, NY, USA.
ACM.
Ranzato, M., Poultney, C., Chopra, S., and LeCun, Y. (2007). Efficient learning of sparse representations with an energy-based model. In NIPS'2006.
Ranzato, M., Boureau, Y.-L., and LeCun, Y. (2008). Sparse feature learning for deep belief networks. In
NIPS'07, pages 1185–1192, Cambridge, MA. MIT Press.
Recht, B., Re, C., Wright, S., and Niu, F. (2011). Hogwild: A lock-free approach to parallelizing stochastic gradient descent. In NIPS'2011.
Rifai, S., Vincent, P., Muller, X., Glorot, X., and Bengio, Y. (2011a). Contractive auto-encoders: Explicit invariance during feature extraction. In ICML'2011.
Rifai, S., Dauphin, Y., Vincent, P., Bengio, Y., and Muller, X. (2011b). The manifold tangent classifier. In
NIPS'2011.
Rifai, S., Bengio, Y., Courville, A., Vincent, P., and Mirza, M. (2012a). Disentangling factors of variation for facial expression recognition. In Proceedings of the European Conference on Computer Vision (ECCV
6), pages 808–822.
Rifai, S., Bengio, Y., Dauphin, Y., and Vincent, P. (2012b). A generative process for sampling contractive auto-encoders. In ICML'2012.
Rose, G. and Macready, W. (2007).
An introduction to quantum annelaing.
Technical report, D-Wave
Systems.
Deep Learning of Representations: Looking Forward
Rumelhart, D., Hinton, G., and Williams, R. (1986). Learning representations by back-propagating errors.
Nature, 323, 533–536.
Salakhutdinov, R. (2010a). Learning deep Boltzmann machines using adaptive MCMC. In ICML'2010.
Salakhutdinov, R. (2010b). Learning in Markov random fields using tempered transitions. In NIPS'2010.
Salakhutdinov, R. and Hinton, G. (2009). Deep Boltzmann machines. In AISTATS'2009, pages 448–455.
Salakhutdinov, R. and Larochelle, H. (2010).
Efficient learning of deep Boltzmann machines.
In AISTATS'2010.
Salakhutdinov, R., Mnih, A., and Hinton, G. (2007).
Restricted Boltzmann machines for collaborative filtering. In ICML'2007, pages 791–798.
Saul, L. K. and Jordan, M. I. (1996). Exploiting tractable substructures in intractable networks. In NIPS'95.
MIT Press, Cambridge, MA.
Schaul, T., Zhang, S., and LeCun, Y. (2012). No More Pesky Learning Rates. Technical report, New York
University, arxiv 1206.1106.
Schraudolph, N. N. (1998). Centering neural network gradient factors. In G. B. Orr and K.-R. Muller, editors, Neural Networks: Tricks of he Trade, pages 548–548. Springer.
Seide, F., Li, G., and Yu, D. (2011a). Conversational speech transcription using context-dependent deep neural networks. In Interspeech 2011, pages 437–440.
Seide, F., Li, G., and Yu, D. (2011b). Feature engineering in context-dependent deep neural networks for conversational speech transcription. In ASRU'2011.
Sohn, K., Zhou, G., and Lee, H. (2013).
Learning and selecting features jointly with point-wise gated
Boltzmann machines. In ICML'2013.
Stoyanov, V., Ropson, A., and Eisner, J. (2011). Empirical risk minimization of graphical model parameters given approximate inference, decoding, and model structure. In AISTATS'2011.
Sutskever, I. (2012). Training Recurrent Neural Networks. Ph.D. thesis, CS Dept., U. Toronto.
Swersky, K., Ranzato, M., Buchman, D., Marlin, B., and de Freitas, N. (2011). On autoencoders and score matching for energy based models. In ICML'2011. ACM.
Taylor, G. and Hinton, G. (2009). Factored conditional restricted Boltzmann machines for modeling motion style. In L. Bottou and M. Littman, editors, ICML 2009, pages 1025–1032. ACM.
Taylor, G., Hinton, G. E., and Roweis, S. (2007). Modeling human motion using binary latent variables. In
NIPS'06, pages 1345–1352. MIT Press, Cambridge, MA.
Tenenbaum, J. B. and Freeman, W. T. (2000). Separating style and content with bilinear models. Neural
Computation, 12(6), 1247–1283.
Tsianos, K., Lawlor, S., and Rabbat, M. (2012). Communication/computation tradeoffs in consensus-based distributed optimization. In NIPS'2012.
Tsochantaridis, I., Joachims, T., Hofmann, T., and Altun, Y. (2005). Large margin methods for structured and interdependent output variables. J. Mach. Learn. Res., 6, 1453–1484.
T¨oscher, A., Jahrer, M., and Bell, R. M. (2009). The bigchaos solution to the netflix grand prize.
Vincent, P. (2011). A connection between score matching and denoising autoencoders. Neural Computation, 23(7), 1661–1674.
Vincent, P., Larochelle, H., Bengio, Y., and Manzagol, P.-A. (2008).
Extracting and composing robust features with denoising autoencoders. In ICML 2008.
Welling, M. and Teh, Y.-W. (2011).
Bayesian learning via stochastic gradient Langevin dynamics.
In
ICML'2011, pages 681–688.
Weston, J., Ratle, F., and Collobert, R. (2008). Deep learning via semi-supervised embedding. In ICML
Wiskott, L. and Sejnowski, T. (2002a). Slow feature analysis: Unsupervised learning of invariances. Neural
Computation, 14(4), 715–770.
Wiskott, L. and Sejnowski, T. J. (2002b). Slow feature analysis: Unsupervised learning of invariances. Neural
Computation, 14(4), 715–770.
Yu, D., Wang, S., and Deng, L. (2010). Sequential labeling using deep-structured conditional random fields.
IEEE Journal of Selected Topics in Signal Processing.
Yu, K., Lin, Y., and Lafferty, J. (2011). Learning image representations from the pixel level via hierarchical sparse coding. In CVPR'2011.
Zeiler, M. D. and Fergus, R. (2013).
Stochastic pooling for regularization of deep convolutional neural networks. In International Conference on Learning Representations.arXiv:1305.0445v2 [cs.LG] 7 Jun 2013Semantic Matching in Search
Hang Li and Jun Xu
Noah's Ark Lab, Huawei Technologies, Hong Kong
ABSTRACT
In this talk, we will give a high level introduction to our book entitled "Semantic Matching in Search", which was published recently in Foundations and Trends in Information Retrieval. We will start our talk by pointing out the importance of semantic matching for natural language processing and information retrieval. Most of the tasks in natural language processing and information retrieval, including search, question answering, and machine translation, are based on matching between language expressions.
This approach works quite well in practice; its limitation is also obvious, however. Sometimes mismatch can occur. We argue that 'semantic matching' is an effective approach to overcome the challenge, that is to conduct more semantic analysis on the language expressions and perform matching between language expressions at semantic level. We will then introduce the major approaches to semantic matching in search, developed in the research community in recent
SMIR'14, July 11, 2014, Gold Coast, Queensland, Australia.. years, including matching by query reformulation, matching with term dependency model, matching with translation model, matching with topic model, and matching with latent space model. We will conclude the talk by explaining our view on the open problems and future directions of semantic matching.
Categories and Subject Descriptors
H.4.0 [Information Systems Applications]: General
Keywords semantic matching, information retrieval, web search
REFERENCES
 H. Li and J. Xu. Semantic Matching in Search. Foundations and Trends in Information Retrieval, 7(5):343–469, 2013.Unsupervised Learning of Visual Representations using Videos
Xiaolong Wang, Abhinav Gupta
Robotics Institute, Carnegie Mellon University
Abstract
Is strong supervision necessary for learning a good visual representation?
Do we really need millions of semantically-labeled images to train a Convolutional Neural Network (CNN)? In this paper, we present a simple yet surprisingly powerful approach for unsupervised learning of CNN. Specifically, we use hundreds of thousands of unlabeled videos from the web to learn visual representations.
Our key idea is that visual tracking provides the supervision. That is, two patches connected by a track should have similar visual representation in deep feature space since they probably belong to the same object or object part. We design a Siamese-triplet network with a ranking loss function to train this CNN representation. Without using a single image from ImageNet, just using 100K unlabeled videos and the VOC 2012 dataset, we train an ensemble of unsupervised networks that achieves 52% mAP (no bounding box regression). This performance comes tantalizingly close to its ImageNet-supervised counterpart, an ensemble which achieves a mAP of 54.4%. We also show that our unsupervised network can perform competitively in other tasks such as surface-normal estimation.
1. Introduction
What is a good visual representation and how can we learn it? At the start of this decade, most computer vision research focused on "what" and used hand-defined features such as SIFT and HOG as the underlying visual representation. Learning was often the last step where these low-level feature representations were mapped to semantic/3D/functional categories. However, the last three years have seen the resurgence of learning visual representations directly from pixels themselves using the deep learning and Convolutional Neural Networks (CNNs).
At the heart of CNNs is a completely supervised learning paradigm. Often millions of examples are first labeled using Mechanical Turk followed by data augmentation to create tens of millions of training instances. CNNs are then trained using gradient descent and back propagation. But one question still remains: is strong-supervision necessary for training these CNNs? Do we really need millions of semantically-labeled images to learn a good representation?
…
…
…
…
Learning to Rank
Conv
Net
Conv
Net
Conv
Net
Query(First Frame)
Tracked(Last Frame)
Negative(Random)(a) Unsupervised Tracking in Videos
𝐷, 𝐷, 𝐷, 𝐷, 𝐷: Distance in deep feature space(b) Siamese-triplet Network(c) Ranking Objective
Figure 1. Overview of our approach. (a) Given unlabeled videos, we perform unsupervised tracking on the patches in them. (b)
Triplets of patches including query patch in the initial frame of tracking, tracked patch in the last frame, and random patch from other videos are fed into our siamese-triplet network for training. (c) The learning objective: Distance between the query and tracked patch in feature space should be smaller than the distance between query and random patches.
It seems humans can learn visual representations using little or no semantic supervision but our approaches still remain completely supervised.
In this paper, we explore the alternative: how we can exploit the unlabeled visual data on the web to train CNNs(e.g. AlexNet )? In the past, there have been several attempts at unsupervised learning using millions of static images or frames extracted from videos.
The most common architecture used is an auto-encoder which learns representations based on its ability to reconstruct the input images. While these approaches have been able to automatically learn V1-like filters given unlabeled data, they are still far away from supervised approaches on tasks such as object detection. So, what is the missing link? We argue that static images themselves might not have enough information to learn a good
1 arXiv:1505.00687v2 [cs.CV] 6 Oct 2015 visual representation. But what about videos? Do they have enough information to learn visual representations? In fact, humans also learn their visual representations not from millions of static images but years of dynamic sensory inputs.
Can we have similar learning capabilities for CNNs?
We present a simple yet surprisingly powerful approach for unsupervised learning of CNNs using hundreds of thousands of unlabeled videos from the web. Visual tracking is one of the first capabilities that develops in infants and often before semantic representations are learned1. Taking a leaf from this observation, we propose to exploit visual tracking for learning CNNs in an unsupervised manner. Specifically, we track millions of "moving" patches in hundreds of thousands of videos. Our key idea is that two patches connected by a track should have similar visual representation in deep feature space since they probably belong to the same object. We design a Siamese-triplet network with ranking loss function to train the CNN representation. This ranking loss function enforces that in the final deep feature space the first frame patch should be much closer to the tracked patch than any other randomly sampled patch. We demonstrate the strength of our learning algorithm using extensive experimental evaluation. Without using a single image from ImageNet, just 100K unlabeled videos and VOC 2012 dataset, we train an ensemble of AlexNet networks that achieves 52% mAP (no bounding box regression). This performance is similar to its ImageNet-supervised counterpart, an ensemble which achieves 54.4% mAP. We also show that our network trained using unlabeled videos achieves similar performance to its completely supervised counterpart on other tasks such as surface normal estimation. We believe this is the first time an unsupervised-pretrained CNN has been shown so competitive; that too on varied datasets and tasks. Specifically for VOC, we would like to put our results in context: this is the best results till-date by using only PASCAL-provided annotations (next best is scratch at
2. Related Work
Unsupervised learning of visual representations has a rich history starting from original auto-encoders work of Olhausen and Field. Most of the work in this area can be broadly divided into three categories.
The first class of algorithms focus on learning generative models with strong priors. These algorithms essentially capture co-occurrence statistics of features.
The second class of algorithms use manually defined features such as
SIFT or HOG and perform clustering over training data to discover semantic classes. Some of these recent algorithms also focus on learning mid-level representations rather than discovering semantic classes them1http://www.aoa.org/patients-and-public/good-vision-throughoutlife/childrens-vision/infant-vision-birth-to-24-months-of-age selves. The third class of algorithms and more related to our paper is unsupervised learning of visual representations from the pixels themselves using deep learning approaches. Starting from the seminal work of Olhausen and Field, the goal is to learn visual representations which are (a) sparse and(b) reconstructive. Olhausen and Field showed that using this criteria they can learn V1-like filters directly from the data. However, this work only focused on learning a single layer. This idea was extended by Hinton and Salakhutdinov to train a deep belief network in an unsupervised manner via stacking layer-by-layer RBMs. Similar to this, Bengio et al. investigated stacking of both RBMs and autoencoders. As a next step, Le et al. scaled up the learning of multi-layer autoencoder on large-scale unlabeled data. They demonstrated that although the network is trained in an unsupervised manner, the neurons in high layers can still have high responses on semantic objects such as human heads and cat faces. Sermanet et al. applied convolutional sparse coding to pre-train the model layer-bylayer in unsupervised manner. The model is then fine-tuned for pedestrian detection. In a contemporary work, Doersch et al. explored to use spatial context as a cue to perform unsupervised learning for CNNs.
However, it is not clear if static images is the right way to learn visual representations. Therefore, researchers have started focusing on learning feature representations using videos.
Early work such as focused on inclusion of constraints via video to autoencoder framework. The most common constraint is enforcing learned representations to be temporally smooth.
Similar to this, Goroshin et al. proposed to learn autoencoders based on the slowness prior. Other approaches such as Taylor et al. trained convolutional gated RBMs to learn latent representations from pairs of successive images. This was extended in a recent work by Srivastava et al. where they proposed to learn a LSTM model in an unsupervised manner to predict future frames.
Finally, our work is also related to metric learning via deep networks. For example, Chopra et al. proposed to learn convolutional networks in a siamese architecture for face verification.
Wang et al. introduced a deep triplet ranking network to learn fine-grained image similarity. Zhang et al. optimized the max-margin loss on triplet units to learn deep hashing function for image retrieval. However, all these methods required labeled data.
Our work is also related to, which used CNN pre-trained on ImageNet classification and detection dataset as initialization, and performed semisupervised learning in videos to tackle object detection in target domain. However, in our work, we propose an unsupervised approach instead of semi-supervised algorithm.
3. Overview
Our goal is to train convolutional neural networks using hundreds of thousands of unlabeled videos from the Internet. We follow the AlexNet architecture to design our base network. However, since we do not have labels, it is not clear what should be the loss function and how we should optimize it. But in case of videos, we have another supervisory information: time. For example, we all know that the scene does not change drastically within a short time in a video and same object instances appear in multiple frames of the video. So, how do we exploit this information to train a CNN-based representation?
We sample millions of patches in these videos and track them over time. Since we are tracking these patches, we know that the first and last tracked frames correspond to the same instance of the moving object or object part. Therefore, any visual representation that we learn should keep these two data points close in the feature space. But just using this constraint is not sufficient: all points can be mapped to a single point in feature space. Therefore, for training our
CNN, we sample a third patch which creates a triplet. For training, we use a loss function that enforces that the first two patches connected by tracking are closer in feature space than the first one and a random one.
Training a network with such triplets converges fast since the task is easy to overfit to. One way is to increase the number of training triplets. However, after initial convergence most triplets satisfy the loss function and therefore back-propagating gradients using such triplets is inefficient.
Instead, analogous to hard-negative mining, we select the third patch from multiple patches that violates the constraint(loss is maximum).
Selecting this patch leads to more meaningful gradients for faster learning.
4. Patch Mining in Videos
Given a video, we want to extract patches of interest(patches with motion in our case) and track these patches to create training instances. One obvious way to find patches of interest is to compute optical flow and use the high magnitude flow regions. However, since YouTube videos are noisy with a lot of camera motion, it is hard to localize moving objects using simple optical flow magnitude vectors. Thus we follow a two-step approach: in the first step, we obtain SURF interest points and use Improved Dense
Trajectories (IDT) to obtain motion of each SURF point. Note that since IDT applies a homography estimation(video stabilization) method, it reduces the problem caused by camera motion. Given the trajectories of SURF interest points, we classify these points as moving if the flow magnitude is more than 0.5 pixels. We also reject frames if (a) very few (< 25%) SURF interest points are classified as moving because it might be just noise; (b) majority of SURF interest points (> 75%) are classified as moving as
…
…
Query(First Frame)
Tracked(Last Frame)
Sliding Window Searching
Tracking
Small Motion
Camera Motion
Figure 2.
Given the video about buses (the "bus" label are not utilized), we perform IDT on it. red points represents the SURF feature points, green represents the trajectories for the points. We reject the frames with small and large camera motions (top pairs).
Given the selected frame, we find the bounding box containing most of the moving SURF points. We then perform tracking. The first and last frame of the track provide pair of patches for training
CNN. it corresponds to moving camera. Once we have extracted moving SURF interest points, in the second step, we find the best bounding box such that it contains most of the moving
SURF points. The size of the bounding box is set as h × w, and we perform sliding window with it in the frame. We take the bounding box which contains the most number of moving SURF interest points as the interest bounding box.
In the experiment, we set h = 227, w = 227 in the frame with size 448 × 600. Note that these patches might contain objects or part of an object as shown in Figure 2.
Tracking.
Given the initial bounding box, we perform tracking using the KCF tracker. After tracking along 30 frames in the video, we obtain the second patch. This patch acts as the similar patch to the query patch in the triplet.
Note that the KCF tracker does not use any supervised information except for the initial bounding box.
5. Learning Via Videos
In the previous section, we discussed how we can use tracking to generate pairs of patches. We use this procedure to generate millions of such pairs (See Figure 3 for examples of pairs of patches mined). We now describe how we use these as training instances for our visual representation learning.
5.1. Siamese Triplet Network
Our goal is to learn a feature space such that the query patch is closer to the tracked patch as compared to any other randomly sampled patch. To learn this feature space we design a Siamese-triplet network. A Siamese-triplet network consist of three base networks which share the same paramQuery(First Frame)
Tracked(Last Frame)
Query(First Frame)
Tracked(Last Frame)
Patch
Pairs
Patch
Pairs
Figure 3. Examples of patch pairs we obtain via patch mining in the videos. eters (see Figure 4). For our experiments, we take the image with size 227 × 227 as input. The base network is based on the AlexNet architecture for the convolutional layers. Then we stack two fully connected layers on the pool5 outputs, whose neuron numbers are 4096 and 1024 respectively. Thus the final output of each single network is 1024 dimensional feature space f(·). We define the loss function on this feature space.
5.2. Ranking Loss Function
Given the set of patch pairs S sampled from the video, we propose to learn an image similarity model in the form of CNN. Specifically, given an image X as an input for the network, we can obtain its feature in the final layer as f(X).
Then, we define the distance of two image patches X1, X2 based on the cosine distance in the feature space as, D(X1, X2) = 1 − f(X1) · f(X2)
∥f(X1)∥∥f(X2)∥.
We want to train a CNN to obtain feature representation f(·), so that the distance between query image patch and the tracked patch is small and the distance between query patch and other random patches is encouraged to be larger. Formally, given the patch set S, where Xi is the original query patch (first patch in tracked frames), X+ i is the tracked patch and X− i is a random patch from a different video, we want to enforce D(Xi, X− i ) > D(Xi, X+ i ). Therefore, the loss of our ranking model is defined by hinge loss as, L(Xi, X+ i, X− i ) = max{0, D(Xi, X+ i ) − D(Xi, X− i ) + M}, (2) where M represents the gap parameters between two distances. We set M = 0.5 in the experiment. Then our objective function for training can be represented as, min
W λ
2 ∥ W ∥2
N
� i=1 max{0, D(Xi, X+ i ) − D(Xi, X− i ) + M}, (3) where W is the parameter weights of the network, i.e., parameters for function f(·). N is the number of the triplets of samples. λ is a constant representing weight decay, which is set to λ = 0.0005.
5.3. Hard Negative Mining for Triplet Sampling
One non-trivial part for learning to rank is the process of selecting negative samples. Given a pair of similar images
Xi, X+ i, how can we select the patch X− i, which is a negative match to Xi, from the large pool of patches? Here we first select the negative patches randomly, and then find hard examples (in a process analogous to hard negative mining).
Random Selection:
During learning, we perform mini-batch Stochastic Gradient Descent (SGD). For each
Xi, X+ i, we randomly sample K negative matches in the same batch B, thus we have K sets of triplet of samples.
For every triplet of samples, we calculate the gradients over three of them respectively and perform back propagation.
Note that we shuffle all the images randomly after each epoch of training, thus the pair of patches Xi, X+ i can look at different negative matches each time.
Hard Negative Mining: While one can continue to sample random patches for creating the triplets, it is more efficient to search the negative patches smartly. After 10 epochs of training using negative data selected randomly, we want to make the problem harder to get more robust feature representations. Analogous to hard-negative mining procedure in SVM, where gradient descent learning is only performed on hard-negatives (not all possible negative), we search for
𝑋𝑖
𝑋𝑖
−
𝑋𝑖 𝑓(𝑋𝑖
+) 𝑓(𝑋𝑖
−) 𝑓(𝑋𝑖)
Ranking
Loss
Layer
Shared Weights
Shared Weights
Figure 4.
Siamese-triplet network. Each base network in the Siamese-triplet network share the same architecture and parameter weights. The architecture is rectified from AlexNet by using only two fully connected layers. Given a triplet of training samples, we obtain their features from the last layer by forward propagation and compute the ranking loss.
Figure 5.
Top response regions for the pool5 neurons of our unsupervised-CNN. Each row shows top response of one neuron. the negative patch such that the loss is maximum and use that patch to compute and back propagate gradients.
Specifically, the sampling of negative matches is similar as random selection before, except that this time we select according to the loss(Eq. 2). For each pair Xi, X+ i, we calculate the loss of all other negative matches in batch B, and select the top K ones with highest losses. We apply the loss on these K negative matches as our final loss and calculate the gradients over them. Since the feature of each sample is already computed after the forward propagation, we only need to calculate the loss over these features, thus the extra computation for hard negative mining is very small. For the experiments, we use K = 4. Note that while some of the negatives might be semantically similar patches, our embedding constraint only requires same instance examples to be closer than category examples (which can be closer than other negatives in the space).
5.4. Adapting for Supervised Tasks
Given the CNN learned by using unsupervised data, we want to transfer the learned representations to the tasks with supervised data. In our experiments, we apply our model to two different tasks including object detection and surface normal estimation. In both tasks we take the base network from our Siamese-triplet network and adjust the fully connected layers and outputs accordingly.
We introduce two ways to fine-tune and transfer the information obtained from unsupervised data to supervised learning.
One straight forward approach is directly applying our ranking model as a pre-trained network for the target task.
More specifically, we use the parameters of the convolutional layers in the base network of our triplet architecture as initialization for the target task. For the fully connected layers, we initialize them randomly. This method of transferring feature representation is very similar to the approach applied in RCNN. However, RCNN uses the network pre-trained with ImageNet Classification data. In our case, the unsupervised ranking task is quite different from object detection and surface normal estimation. Thus, we need to adapt the learning rate to the fine-tuning procedure introduced in RCNN. We start with the learning rate with ϵ = 0.01 instead of 0.001 and set the same learning rate for all layers. This setting is crucial since we want the pretrained features to be used as initialization of supervised learning, and adapting the features to the new task.
In this paper, we explore one more approach to transfer/fine-tune the network. Specifically, we note that there might be more juice left in the millions of unsupervised training data (which could not be captured in the initial learning stage).
Therefore, we use an iterative finetuning scheme. Given the initial unsupervised network, we first fine-tune using the PASCAL VOC data. Given the new fine-tuned network, we use this network to re-adapt to ranking triplet task. Here we again transfer convolutional parameters for re-adapting. Finally, this re-adapted network is fine-tuned on the VOC data yielding a better trained model.
We show in the experiment that this circular approach gives improvement in performance. We also notice that after two iterations of this approach the network converges.
5.5. Model Ensemble
We proposed an approach to learn CNNs using unlabeled videos. However, there is absolutely no limit to generating training instances and pairs of tracked patches (YouTube has more than billions of videos). This opens up the possibility of training multiple CNNs using different sets of data.
Once we have trained these CNNs, we append the fc7 features from each of these CNNs to train the final SVM. Note that the ImageNet trained models also provide initial boost for adding more networks (See Table 1).
5.6. Implementation Details
We apply mini-batch SGD in training. As the 3 networks share the same parameters, instead of inputting 3 samples to the triplet network, we perform the forward propagation for the whole batch by a single network and calculate the loss based on the output feature. Given a pair of patches
Xi, X+ i, we randomly select another patch X− i ∈ B which is extracted in a different video from Xi, X+ i. Given their features from forward propagation f(Xi), f(X+ i ), f(X− i ), we can compute the loss as Eq. 2.
For unsupervised learning, we download 100K videos from YouTube using the URLs provided by. used thousands of keywords to retrieve videos from YouTube.
Note we drop the labels associated with each video. By performing our patch mining method on the videos, we obtain
8 million image patches. We train three different networks separately using 1.5M, 5M and 8M training samples. We report numbers based on these three networks. To train our siamese-triplet networks, we set the batch size as |B| = 100, (a) Unsupervised Pre-trained(b) Fine-tuned
Figure 6.
Conv1 filters visualization. (a) The filters of the first convolutional layer of the siamese-triplet network trained in unsupervised manner. (b) By fine-tuning the unsupervised pre-trained network on PASCAL VOC 2012, we obtain sharper filters. the learning rate starting with ϵ0 = 0.001. We first train our network with random negative samples at this learning rate for 150K iterations, and then we apply hard negative mining based on it. For training on 1.5M patches, we reduce the learning rate by a factor of 10 at every 80K iterations and train for 240K iterations. For training on 5M and 8M patches, we reduce the learning rate by a factor of 10 at every 120K iterations and train for 350K iterations.
6. Experiments
We demonstrate the quality of our learned visual representations with qualitative and quantitative experiments.
Qualitatively, we show the convolutional filters learned in layer 1 (See Figure 6). Our learned filters are similar to V1 though not as strong. However, after fine-tuning on PASCAL VOC 2012, these filters become quite strong. We also show that the underlying representation learns a reasonable nearness metric by showing what the units in Pool5 layers represent (See Figure 5). Ignoring boundary effects, each pool5 unit has a receptive field of 195 × 195 pixels in the original 227 × 227 pixel input. A central pool5 unit has a nearly global view, while one near the edge has a smaller, clipped support. Each row displays top 6 activations for a pool5 unit. We have chosen 5 pool5 units for visualization.
For example, the first neuron represents animal heads, second represents potted plant, etc. This visualization indicates the nearness metric learned by the network since each row corresponds to similar firing patterns inside the CNN. Our unsupervised networks are available for download.
6.1. Unsupervised CNNs without Fine-tuning
First, we demonstrate that the unsupervised-CNN representation learned using videos (without fine-tuning) is reasonable.
We perform Nearest Neighbors (NN) using ground-truth (GT) windows in VOC 2012 val set as query.
The retrieval-database consists of all selective search windows (more than 0.5 overlap with GT windows) in VOC
2012 train set. See Figure 7 for qualitative results. Our unsupervised-CNN is far superior to a random AlexNet architecture and the results are quite comparable to AlexNet trained on ImageNet.
Quantitatively, we measure the retrieval rate by counting number of correct retrievals in top-K (K=20) retrievals. A retrieval is correct if the semantic class for retrieved patch and query patch are the same. Using our unsupervised-CNN(Pool5 features) without fine-tuning and cosine distance, we obtain 40% retrieval rate. Our performance is significantly better as compared to 24% by ELDA on HOG and 19% by AlexNet with random parameters (our initialization). This clearly demonstrates our unsupervised network learns a good visual representation compared to a random parameter CNN. As a baseline, ImageNet CNN performs
62% (but note it already learns on semantics).
We also evaluate our unsupervised-CNN without finetuning for scene classification task on MIT Indoor 67.
We train a linear classifier using softmax loss.
Using pool5 features from unsupervised-CNN without fine-tuning gives 41% classification accuracy compared to 21% for
GIST+SVM and 16% for random AlexNet.
ImageNettrained AlexNet has 54% accuracy. We also provide object detection results without fine-tuning in the next section.
6.2. Unsupervised CNNs with Fine-tuning
Next, we evaluate our approach by transferring the feature representation learned in unsupervised manner to the tasks with labeled data. We focus on two challenging problems: object detection and surface normal estimation.
Object Detection
For object detection, we perform our experiments on PASCAL VOC 2012 dataset.
We follow the detection pipeline introduced in RCNN, which borrowed the CNNs pre-trained on other datasets and fine-tuned on it with the VOC data. The fine-tuned CNN was then used to extract features followed by training SVMs for each object class.
However, instead of using ImageNet pre-trained network as initialization in RCNN, we use our unsupervised-CNN. We fine-tune our network with the trainval set (11540 images) and train SVMs with them. Evaluation is performed in the standard test set (10991 images).
At the fine-tuning stage, we change the output to 21 classes and initialize the convolutional layers with our unsupervised pre-trained network. To fine-tune the network, we start with learning rate as ϵ = 0.01 and reduce the learning rate by a factor of 10 at every 80K iterations. The network is fine-tuned for 200K iterations. Note that for all the experiments, no bounding box regression is performed.
Query(a) Random AlexNet(b) Imagenet AlexNet(c) Unsupervised AlexNet
Figure 7. Nearest neighbors results. Given the query object from VOC 2012 val, we retrieve the NN from VOC 2012 train via calculating the cosine distance on pool5 feature space. We compare the results of 3 different models: (a) AlexNet with random parameters; (b) AlexNet trained with Imagenet data; (c) AlexNet trained using our unsupervised method on 8M data.
Table 1. mean Average Precision (mAP) on VOC 2012. "external" column shows the number of patches used to pre-train unsupervised-CNN.
VOC 2012 test external aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAP scratch
44.0 scratch (3 ensemble)
47.3 unsup + ft
1.5M
46.2 unsup + ft
5M
47.0 unsup + ft
8M
47.5 unsup + ft (2 ensemble)
6.5M
50.5 unsup + ft (3 ensemble)
8M
52.0 unsup + iterative ft
5M
RCNN 70K
RCNN 70K (2 ensemble)
RCNN 70K (3 ensemble)
RCNN 200K (big stepsize)
We compare our method with the model trained from scratch as well as using ImagNet pre-trained network. Notice that the results for VOC 2012 reported in RCNN are obtained by only fine-tuning on the train set without using the val set. For fair comparison, we fine-tuned the ImageNet pre-trained network with VOC 2012 trainval set.
Moreover, as the step size of reducing learning rate in RCNN is set to 20K and iterations for fine-tuning is 70K, we also try to enlarge the step size to 50K and finetune the network for 200K iterations. We report the results for both of these settings.
Single Model. We show the results in Table 1. As a baseline, we train the network from scratch on VOC 2012 dataset and obtain 44% mAP. Using our unsupervised network pre-trained with 1.5M pair of patches and then finetuned on VOC 2012, we obtain mAP of 46.2% (unsup+ft, external data = 1.5M). However, using more data, 5M and 8M patches in pre-training and then fine-tune, we can achieve 47% and 47.5% mAP. These results indicate that our unsupervised network provides a significant boost as compared to the scratch network. More importantly, when more unlabeled data is applied, we can get better performance ( 3.5% boost compared to training from scratch).
Model Ensemble. We also try combining different models using different sets of unlabeled data in pre-training. By ensembling two fine-tuned networks which are pre-trained using 1.5M and 5M patches, we obtained a boost of 3.5% comparing to the single model, which is 50.5%(unsup+ft(2 ensemble)). Finally, we ensemble all three different networks pre-trained with different sets of data, whose size are
1.5M, 5M and 8M respectively. We get another boost and reach 52% mAP (unsup+ft (3 ensemble)).
Baselines. We compare our approach with RCNN which uses ImageNet pre-trained models. Following the procedure in, we obtain 50.1% mAP (RCNN 70K) by setting the step size to 20K and fine-tuning for 70K iterations. To generate a model ensemble, the CNNs are first trained on the ImageNet dataset separately, and then they are fine-tuned with the VOC 2012 dataset. The result of ensembling two of these networks is 53.6% mAP (RCNN
70K (2 ensemble)). If we ensemble three networks, we get a mAP of 54.4%. For fair of comparison, we also finetuned the ImageNet pre-trained model with larger step size(50K) and more iterations (200K). The result is 52.3% mAP(RCNN 200K (big stepsize)). Note that while ImageNet network shows diminishing returns with ensembling since the training data remains similar, in our case since every network in the ensemble looks at different sets of data, we get huge performance boosts.
Exploring a better way to transfer learned representation. Given our fine-tuned model using 5M patches in pre-training (unsup+ft, external = 5M), we use it to re-learn and re-adapt to the unsupervised triplet task. After that, the network is re-applied to fine-tune on VOC 2012. The final
Table 2. Results on NYU v2 for per-pixel surface normal estimation, evaluated over valid pixels.(Lower Better)(Higher Better)
Mean
Median 11.25◦ 22.5◦ 30◦ scratch
46.8 52.5 unsup + ft
ImageNet + ft
UNFOLD 
Discr. 
3DP (MW) 
52.0 57.8 result for this single model is 48% mAP (unsup + iterative ft), which is 1% better than the initial fine-tuned network.
Unsupervised network without fine-tuning: We also perform object detection without fine-tuning on VOC 2012.
We extract pool5 features using our unsupervised-CNN and train SVM on top of it. We obtain mAP of 26.1% using our unsupervised network (training with 8M data). The ensemble of two unsupervised-network (training with 5M and 8M data) gets mAP of 28.2%. As a comparison, Imagenet pretrained network without fine-tuning gets mAP of 40.4%.
Surface Normal Estimation
To illustrate that our unsupervised representation can be generalized to different tasks, we adapt the unsupervised
CNN to the task of surface normal estimation from a RGB image.
In this task, we want to estimate the orientation of the pixels.
We perform our experiments on the NYUv2 dataset, which includes 795 images for training and 654 images for testing. Each image is has corresponding depth information which can be used to generate groundtruth surface normals. For evaluation and generating the groundtruth, we adopt the protocols introduced in which is used by different methods on this task.
To apply deep learning to this task, we followed the same form of outputs and loss function as the coarse network mentioned in. Specifically, we first learn a codebook by performing k-means on surface normals and generate 20 codewords. Each codeword represents one class and thus we transform the problem to 20-class classification for each pixel. Given a 227 × 227 image as input, our network generates surface normals for the whole scene. The output of our network is 20 × 20 pixels, each of which is represented by a distribution over 20 codewords. Thus the dimension of output is 20 × 20 × 20 = 8000.
The network architecture for this task is also based on the AlexNet. To relieve over-fitting, we only stack two fully connected layers with 4096 and 8000 neurons on the pool5 layer. During training, we initialize the network with the unsupervised pre-trained network (single network using 8M external data). We use the same learning rate 1.0 × 10−6 as and fine-tune with 10K iterations given the small number of training data. Note that unlike, we do not utilize any data from the videos in NYU dataset for training.
Figure 8. Surface normal estimation results on NYU dataset. For visualization, we use green for horizontal surface, blue for facing right and red for facing left, i.e., blue → X; green → Y; red → Z.
For comparison, we also trained networks from scratch as well as using ImageNet pre-trained. For evaluation, we report mean and median error (in degrees). We also report percentage of pixels with less than 11.25, 22.5 and 30 degree errors. We show our qualitative results in in Figure 8. and quantitative results in Table 2. Our approach (unsup + ft) is significantly better than network trained from scratch and comes very close to Imagenet-pretrained CNN (∼ 1%).
7. Discussion and Conclusion
We have presented an approach to train CNNs in an unsupervised manner using videos. Specifically, we track millions of patches and learn an embedding using CNN that keeps patches from same track closer in the embedding space as compared to any random third patch. Our unsupervised pre-trained CNN fine-tuned using VOC training data outperforms CNN trained from scratch by 3.5%. An ensemble version of our approach outperforms scratch by 4.7% and comes tantalizingly close to an Imagenet-pretrained
CNN (within 2.5%). We believe this is an extremely surprising result since until recently semantic supervision was considered a strong requirement for training CNNs. We believe our successful implementation opens up a new space for designing unsupervised learning algorithms for CNN training.
Acknowledgement: This work was partially supported by ONR MURI
N000141010934 and NSF IIS 1320083. This material was also based on research partially sponsored by DARPA under agreement number FA875014-2-0244. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA or the U.S. Government. The authors would like to thank Yahoo! and Nvidia for the compute cluster and GPU donations respectively.
References
 H. Bay, T. Tuytelaars, and L. V. Gool.
Surf: Speeded up robust features. In ECCV, 2006. 3
 Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives. TPAMI, 35(8):1798–1828, 2013. 2
 Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. Greedy layerwise training of deep networks. In NIPS, 2007. 1, 2
 S. Chopra, R. Hadsell, and Y. LeCun. Learning a similarity metric discriminatively, with application to face verification. In CVPR, N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In CVPR, 2005. 1
 C. Doersch, A. Gupta, and A. A. Efros. Mid-level visual element discovery as discriminative mode seeking. In NIPS, 2013. 2
 C. Doersch, A. Gupta, and A. A. Efros. Context as supervisory signal: Discovering objects with predictable context. In ECCV, 2014.
 C. Doersch, A. Gupta, and A. A. Efros. Unsupervised visual representation learning by context prediction. In ICCV, 2015. 2
 S. M. A. Eslami, N. Heess, and J. Winn. The shape boltzmann machine: a strong model of object shape. In CVPR, 2012. 2
 M. Everingham, L. V. Gool, C. K. Williams, J. Winn,, and A. Zisserman.
The pascal visual object classes (voc) challenge.
IJCV, 88(2):303–338, 2010. 6
 P. Foldiak. Learning invariance from transformation sequences. Neural Computation, 3(2):194–200, 1991. 2
 D. F. Fouhey, A. Gupta, and M. Hebert. Data-driven 3D primitives for single image understanding. In ICCV, 2013. 8
 D. F. Fouhey, A. Gupta, and M. Hebert. Unfolding an indoor origami world. In ECCV, 2014. 8
 R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In
CVPR, 2014. 5, 6, 7
 Y. Gong, Y. Jia, T. K. Leung, A. Toshev, and S. Ioffe. Deep convolutional ranking for multilabel image annotation. In ICLR, 2007.
 R. Goroshin, J. Bruna, J. Tompson, D. Eigen, and Y. LeCun. Unsupervised learning of spatiotemporally coherent metrics.
CoRR, abs/1412.6056, 2015. 2
 R. Hadsell, S. Chopra, and Y. LeCun. Dimensionality reduction by learning an invariant mapping. In CVPR, 2006. 2
 B. Hariharan, J. Malik, and D. Ramanan. Discriminative decorrelation for clustering and classification. In ECCV, 2012. 6
 J. F. Henriques, R. Caseiro, P. Martins, and J. Batista. High-speed tracking with kernelized correlation filters. TPAMI, 2015. 3
 G. E. Hinton, P. Dayan, B. J. Frey, and R. M. Neal.
The" wake-sleep" algorithm for unsupervised neural networks. Science, 268(5214):1158–1161, 1995. 2
 G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313:504–507, 2006. 2
 E. Hoffer and N. Ailon. Deep metric learning using triplet network.
CoRR, /abs/1412.6622, 2015. 2
 Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for fast feature embedding. CoRR, /abs/1408.5093, 2014. 1
 A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012. 1, L. Ladick´y, B. Zeisl, and M. Pollefeys.
Discriminatively trained dense surface normal estimation. In ECCV, 2014. 8
 Q. V. Le, M. A. Ranzato, R. Monga, M. Devin, K. Chen, G. S. Corrado, J. Dean, and A. Y. Ng. Building high-level features using large scale unsupervised learning. In ICML, 2012. 1, 2
 Q. V. Le, W. Y. Zou, S. Y. Yeung, and A. Y. Ng. Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis. In CVPR, 2011. 2
 Y. LeCun, B. Boser, J. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Handwritten digit recognition with a backpropagation network. In NIPS, 1990. 1
 H. Lee, R. Grosse, R. Ranganath, and A. Y. Ng. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. In ICML, 2009. 2
 X. Liang, S. Liu, Y. Wei, L. Liu, L. Lin, and S. Yan. Computational baby learning. CoRR, abs/1411.2861, 2014. 2, 5
 S. Liu, X. Liang, L. Liu, X. Shen, J. Yang, C. Xu, X. Cao, and S. Yan. Matching-cnn meets knn: Quasi-parametric human parsing.
In CVPR, 2015. 2
 D. Lowe.
Distinctive Image Features from Scale-Invariant Keypoints. IJCV, 60(2):91–110, 2004. 1
 P. Luo, X. Wang, and X. Tang. Hierarchical face parsing via deep learning. In CVPR, 2012. 2
 H. Mobahi, R. Collobert, and J. Weston. Deep learning from temporal coherence in video. In ICML, 2009. 1, 2
 B. A. Olshausen and D. J. Field. Sparse coding with an overcomplete basis set: A strategy employed by v1? Vision research, 1997. 1, 2
 A. Quattoni and A.Torralba. Recognizing indoor scenes. In CVPR, M. A. Ranzato, F. J. Huang, Y.-L. Boureau, and Y. LeCun. Unsupervised learning of invariant feature hierarchies with applications to object recognition. In CVPR, 2007. 1
 B. C. Russell, A. A. Efros, J. Sivic, W. T. Freeman, and A. Zisserman.
Using multiple segmentations to discover objects and their extent in image collections. In CVPR, 2006. 2
 P. Sermanet, K. Kavukcuoglu, S. Chintala, and Y. LeCun. Pedestrian detection with unsupervised multi-stage feature learning. In CVPR, N. Silberman, D. Hoiem, P. Kohli, and R. Fergus. Indoor segmentation and support inference from RGBD images. In ECCV, 2012.
 S. Singh, A. Gupta, and A. A. Efros. Unsupervised discovery of mid-level discriminative patches. In ECCV, 2012. 2
 J. Sivic, B. C. Russell, A. A. Efros, A. Zisserman, and W. T. Freeman.
Discovering objects and their location in images. In ICCV, 2005. 2
 N. Srivastava, E. Mansimov, and R. Salakhutdinov.
Unsupervised learning of video representations using lstms.
CoRR, abs/1502.04681, 2015. 2
 N. Srivastava and R. R. Salakhutdinov. Multimodal learning with deep boltzmann machines. In NIPS, 2012. 1, 2
 D. Stavens and S. Thrun. Unsupervised learning of invariant features using video. In CVPR, 2010. 2
 E. B. Sudderth, A. Torralba, W. T. Freeman, and A. S. Willsky.
Describing visual scenes using transformed dirichlet processes. In
NIPS, 2005. 2
 Y. Tang, R. Salakhutdinov, and G. Hinton. Robust boltzmann machines for recognition and denoising. In CVPR, 2012. 2
 G. W. Taylor, R. Fergus, Y. LeCun, and C. Bregler. Convolutional learning of spatio-temporal features. In ECCV, 2010. 1, 2
 P. Vincent, H. Larochelle, Y. Bengio, and P. Manzagol.
Extracting and composing robust features with denoising autoencoders. In
ICML, 2008. 1, 2
 H. Wang and C. Schmid. Action recognition with improved trajectories. In ICCV, 2013. 3
 J. Wang, Y. Song, T. Leung, C. Rosenberg, J. Wang, J. Philbin, B. Chen, and Y. Wu. Learning fine-grained image similarity with deep ranking. In CVPR, 2014. 2, 3
 X. Wang, D. F. Fouhey, and A. Gupta. Designing deep networks for surface normal estimation. In CVPR, 2015. 8
 L. Wiskott and T. J. Sejnowski. Slow feature analysis:unsupervised learning of invariances. Neural Computation, 14:715–770, 2002. 2
 P. Wohlhart and V. Lepetit. Learning descriptors for object recognition and 3d pose estimation. In CVPR, 2015. 2
 R. Zhang, L. Lin, R. Zhang, W. Zuo, and L. Zhang. Bit-scalable deep hashing with regularized similarity learning for image retrieval and person re-identification. TIP, 24(12):4766–4779, 2015. 2
 W. Y. Zou, S. Zhu, A. Y. Ng, and K. Yu. Deep learning of invariant features via simulated fixations in video. In NIPS, 2012. 1, 2Densely Connected Convolutional Networks
Gao Huang∗
Cornell University gh349@cornell.edu
Zhuang Liu∗
Tsinghua University liuzhuang13@mails.tsinghua.edu.cn
Laurens van der Maaten
Facebook AI Research lvdmaaten@fb.com
Kilian Q. Weinberger
Cornell University kqw4@cornell.edu
Abstract
Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections—one between each layer and its subsequent layer—our network has
L(L+1)
2 direct connections.
For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet.
1. Introduction
Convolutional neural networks (CNNs) have become the dominant machine learning approach for visual object recognition. Although they were originally introduced over 20 years ago, improvements in computer hardware and network structure have enabled the training of truly deep
CNNs only recently. The original LeNet5 consisted of 5 layers, VGG featured 19, and only last year Highway
∗Authors contributed equally x0 x1
H1 x2
H2
H3
H4 x3 x4
Figure 1: A 5-layer dense block with a growth rate of k = 4.
Each layer takes all preceding feature-maps as input.
Networks and Residual Networks (ResNets) have surpassed the 100-layer barrier.
As CNNs become increasingly deep, a new research problem emerges: as information about the input or gradient passes through many layers, it can vanish and "wash out" by the time it reaches the end (or beginning) of the network. Many recent publications address this or related problems. ResNets and Highway Networks bypass signal from one layer to the next via identity connections. Stochastic depth shortens ResNets by randomly dropping layers during training to allow better information and gradient flow. FractalNets repeatedly combine several parallel layer sequences with different number of convolutional blocks to obtain a large nominal depth, while maintaining many short paths in the network.
Although these different approaches vary in network topology and training procedure, they all share a key characteristic: they create short paths from early layers to later layers.
In this paper, we propose an architecture that distills this insight into a simple connectivity pattern: to ensure maximum information flow between layers in the network, we connect all layers (with matching feature-map sizes) directly with each other. To preserve the feed-forward nature, each layer obtains additional inputs from all preceding layers and passes on its own feature-maps to all subsequent layers. Figure 1 illustrates this layout schematically. Crucially, in contrast to ResNets, we never combine features through summation before they are passed into a layer; instead, we combine features by concatenating them. Hence, the ℓth layer has ℓ inputs, consisting of the feature-maps of all preceding convolutional blocks. Its own feature-maps are passed on to all L−ℓ subsequent layers. This introduces
L(L+1)
2 connections in an L-layer network, instead of just
L, as in traditional architectures. Because of its dense connectivity pattern, we refer to our approach as Dense Convolutional Network (DenseNet).
A possibly counter-intuitive effect of this dense connectivity pattern is that it requires fewer parameters than traditional convolutional networks, as there is no need to relearn redundant feature-maps. Traditional feed-forward architectures can be viewed as algorithms with a state, which is passed on from layer to layer. Each layer reads the state from its preceding layer and writes to the subsequent layer.
It changes the state but also passes on information that needs to be preserved. ResNets make this information preservation explicit through additive identity transformations.
Recent variations of ResNets show that many layers contribute very little and can in fact be randomly dropped during training. This makes the state of ResNets similar to (unrolled) recurrent neural networks, but the number of parameters of ResNets is substantially larger because each layer has its own weights. Our proposed DenseNet architecture explicitly differentiates between information that is added to the network and information that is preserved.
DenseNet layers are very narrow (e.g., 12 filters per layer), adding only a small set of feature-maps to the "collective knowledge" of the network and keep the remaining featuremaps unchanged—and the final classifier makes a decision based on all feature-maps in the network.
Besides better parameter efficiency, one big advantage of DenseNets is their improved flow of information and gradients throughout the network, which makes them easy to train. Each layer has direct access to the gradients from the loss function and the original input signal, leading to an implicit deep supervision. This helps training of deeper network architectures. Further, we also observe that dense connections have a regularizing effect, which reduces overfitting on tasks with smaller training set sizes.
We evaluate DenseNets on four highly competitive benchmark datasets (CIFAR-10, CIFAR-100, SVHN, and ImageNet). Our models tend to require much fewer parameters than existing algorithms with comparable accuracy.
Further, we significantly outperform the current state-ofthe-art results on most of the benchmark tasks.
2. Related Work
The exploration of network architectures has been a part of neural network research since their initial discovery. The recent resurgence in popularity of neural networks has also revived this research domain. The increasing number of layers in modern networks amplifies the differences between architectures and motivates the exploration of different connectivity patterns and the revisiting of old research ideas.
A cascade structure similar to our proposed dense network layout has already been studied in the neural networks literature in the 1980s. Their pioneering work focuses on fully connected multi-layer perceptrons trained in a layerby-layer fashion. More recently, fully connected cascade networks to be trained with batch gradient descent were proposed. Although effective on small datasets, this approach only scales to networks with a few hundred parameters. In, utilizing multi-level features in CNNs through skip-connnections has been found to be effective for various vision tasks. Parallel to our work, derived a purely theoretical framework for networks with cross-layer connections similar to ours.
Highway Networks were amongst the first architectures that provided a means to effectively train end-to-end networks with more than 100 layers. Using bypassing paths along with gating units, Highway Networks with hundreds of layers can be optimized without difficulty. The bypassing paths are presumed to be the key factor that eases the training of these very deep networks. This point is further supported by ResNets, in which pure identity mappings are used as bypassing paths. ResNets have achieved impressive, record-breaking performance on many challenging image recognition, localization, and detection tasks, such as ImageNet and COCO object detection. Recently, stochastic depth was proposed as a way to successfully train a 1202-layer ResNet. Stochastic depth improves the training of deep residual networks by dropping layers randomly during training. This shows that not all layers may be needed and highlights that there is a great amount of redundancy in deep (residual) networks. Our paper was partly inspired by that observation. ResNets with pre-activation also facilitate the training of state-of-the-art networks with > 1000 layers.
An orthogonal approach to making networks deeper(e.g., with the help of skip connections) is to increase the network width. The GoogLeNet uses an "Inception module" which concatenates feature-maps produced by filters of different sizes. In, a variant of ResNets with wide generalized residual blocks was proposed.
In fact, simply increasing the number of filters in each layer of C o n v o l u t i o n
P o o l i n g
Dense Block 1
C o n v o l u t i o n
P o o l i n g
P o o l i n g
L i n e a r
C o n v o l u t i o n
Input
Prediction
�horse�
Dense Block 2
Dense Block 3
Figure 2: A deep DenseNet with three dense blocks. The layers between two adjacent blocks are referred to as transition layers and change feature-map sizes via convolution and pooling.
ResNets can improve its performance provided the depth is sufficient. FractalNets also achieve competitive results on several datasets using a wide network structure.
Instead of drawing representational power from extremely deep or wide architectures, DenseNets exploit the potential of the network through feature reuse, yielding condensed models that are easy to train and highly parameterefficient. Concatenating feature-maps learned by different layers increases variation in the input of subsequent layers and improves efficiency. This constitutes a major difference between DenseNets and ResNets. Compared to Inception networks, which also concatenate features from different layers, DenseNets are simpler and more efficient.
There are other notable network architecture innovations which have yielded competitive results. The Network in Network (NIN) structure includes micro multi-layer perceptrons into the filters of convolutional layers to extract more complicated features. In Deeply Supervised Network (DSN), internal layers are directly supervised by auxiliary classifiers, which can strengthen the gradients received by earlier layers. Ladder Networks introduce lateral connections into autoencoders, producing impressive accuracies on semi-supervised learning tasks.
In, Deeply-Fused Nets (DFNs) were proposed to improve information flow by combining intermediate layers of different base networks. The augmentation of networks with pathways that minimize reconstruction losses was also shown to improve image classification models.
3. DenseNets
Consider a single image x0 that is passed through a convolutional network. The network comprises L layers, each of which implements a non-linear transformation Hℓ(·), where ℓ indexes the layer. Hℓ(·) can be a composite function of operations such as Batch Normalization (BN), rectified linear units (ReLU), Pooling, or Convolution (Conv). We denote the output of the ℓth layer as xℓ.
ResNets.
Traditional convolutional feed-forward networks connect the output of the ℓth layer as input to the(ℓ + 1)th layer, which gives rise to the following layer transition: xℓ = Hℓ(xℓ−1).
ResNets add a skip-connection that bypasses the non-linear transformations with an identity function: xℓ = Hℓ(xℓ−1) + xℓ−1.
An advantage of ResNets is that the gradient can flow directly through the identity function from later layers to the earlier layers. However, the identity function and the output of Hℓ are combined by summation, which may impede the information flow in the network.
Dense connectivity.
To further improve the information flow between layers we propose a different connectivity pattern: we introduce direct connections from any layer to all subsequent layers. Figure 1 illustrates the layout of the resulting DenseNet schematically. Consequently, the ℓth layer receives the feature-maps of all preceding layers, x0,..., xℓ−1, as input: xℓ = Hℓ([x0, x1,..., xℓ−1]), (2) where [x0, x1,..., xℓ−1] refers to the concatenation of the feature-maps produced in layers 0,..., ℓ−1. Because of its dense connectivity we refer to this network architecture as
Dense Convolutional Network (DenseNet). For ease of implementation, we concatenate the multiple inputs of Hℓ(·) in eq. (2) into a single tensor.
Composite function.
Motivated by, we define Hℓ(·) as a composite function of three consecutive operations: batch normalization (BN), followed by a rectified linear unit (ReLU) and a 3 × 3 convolution (Conv).
Pooling layers.
The concatenation operation used in Eq. (2) is not viable when the size of feature-maps changes.
However, an essential part of convolutional networks is down-sampling layers that change the size of feature-maps.
To facilitate down-sampling in our architecture we divide the network into multiple densely connected dense blocks; see Figure 2. We refer to layers between blocks as transition layers, which do convolution and pooling. The transition layers used in our experiments consist of a batch normalization layer and an 1×1 convolutional layer followed by a 2×2 average pooling layer.
Growth rate.
If each function Hℓ produces k featuremaps, it follows that the ℓth layer has k0 +k ×(ℓ−1) input feature-maps, where k0 is the number of channels in the input layer. An important difference between DenseNet and existing network architectures is that DenseNet can have very narrow layers, e.g., k = 12. We refer to the hyperparameter k as the growth rate of the network. We show in Section 4 that a relatively small growth rate is sufficient to
Layers
Output Size
DenseNet-121(k = 32)
DenseNet-169(k = 32)
DenseNet-201(k = 32)
DenseNet-161(k = 48)
Convolution
112 × 112
7 × 7 conv, stride 2
Pooling
56 × 56
3 × 3 max pool, stride 2
Dense Block
56 × 56
� 1 × 1 conv
3 × 3 conv
�
× 6
� 1 × 1 conv
3 × 3 conv
�
× 6
� 1 × 1 conv
3 × 3 conv
�
× 6
� 1 × 1 conv
3 × 3 conv
�
× 6
Transition Layer
56 × 56
1 × 1 conv
28 × 28
2 × 2 average pool, stride 2
Dense Block
28 × 28
� 1 × 1 conv
3 × 3 conv
�
× 12
� 1 × 1 conv
3 × 3 conv
�
× 12
� 1 × 1 conv
3 × 3 conv
�
× 12
� 1 × 1 conv
3 × 3 conv
�
× 12
Transition Layer
28 × 28
1 × 1 conv
14 × 14
2 × 2 average pool, stride 2
Dense Block
14 × 14
� 1 × 1 conv
3 × 3 conv
�
× 24
� 1 × 1 conv
3 × 3 conv
�
× 32
� 1 × 1 conv
3 × 3 conv
�
× 48
� 1 × 1 conv
3 × 3 conv
�
× 36
Transition Layer
14 × 14
1 × 1 conv
7 × 7
2 × 2 average pool, stride 2
Dense Block
7 × 7
� 1 × 1 conv
3 × 3 conv
�
× 16
� 1 × 1 conv
3 × 3 conv
�
× 32
� 1 × 1 conv
3 × 3 conv
�
× 32
� 1 × 1 conv
3 × 3 conv
�
× 24
Classification
Layer
1 × 1
7 × 7 global average pool
1000D fully-connected, softmax
Table 1: DenseNet architectures for ImageNet. The growth rate for the first 3 networks is k = 32, and k = 48 for DenseNet-161. Note that each "conv" layer shown in the table corresponds the sequence BN-ReLU-Conv. obtain state-of-the-art results on the datasets that we tested on. One explanation for this is that each layer has access to all the preceding feature-maps in its block and, therefore, to the network's "collective knowledge". One can view the feature-maps as the global state of the network. Each layer adds k feature-maps of its own to this state. The growth rate regulates how much new information each layer contributes to the global state. The global state, once written, can be accessed from everywhere within the network and, unlike in traditional network architectures, there is no need to replicate it from layer to layer.
Bottleneck layers.
Although each layer only produces k output feature-maps, it typically has many more inputs. It has been noted in that a 1×1 convolution can be introduced as bottleneck layer before each 3×3 convolution to reduce the number of input feature-maps, and thus to improve computational efficiency. We find this design especially effective for DenseNet and we refer to our network with such a bottleneck layer, i.e., to the BN-ReLU-Conv(1×
1)-BN-ReLU-Conv(3×3) version of Hℓ, as DenseNet-B. In our experiments, we let each 1×1 convolution produce 4k feature-maps.
Compression.
To further improve model compactness, we can reduce the number of feature-maps at transition layers. If a dense block contains m feature-maps, we let the following transition layer generate ⌊θm⌋ output featuremaps, where 0 <θ ≤1 is referred to as the compression factor. When θ = 1, the number of feature-maps across transition layers remains unchanged. We refer the DenseNet with θ<1 as DenseNet-C, and we set θ = 0.5 in our experiment.
When both the bottleneck and transition layers with θ < 1 are used, we refer to our model as DenseNet-BC.
Implementation Details.
On all datasets except ImageNet, the DenseNet used in our experiments has three dense blocks that each has an equal number of layers. Before entering the first dense block, a convolution with 16 (or twice the growth rate for DenseNet-BC) output channels is performed on the input images. For convolutional layers with kernel size 3×3, each side of the inputs is zero-padded by one pixel to keep the feature-map size fixed. We use 1×1 convolution followed by 2×2 average pooling as transition layers between two contiguous dense blocks. At the end of the last dense block, a global average pooling is performed and then a softmax classifier is attached. The feature-map sizes in the three dense blocks are 32× 32, 16×16, and 8×8, respectively. We experiment with the basic DenseNet structure with configurations {L = 40, k = 12}, {L =
100, k = 12} and {L = 100, k = 24}.
For DenseNetBC, the networks with configurations {L = 100, k = 12}, {L=250, k=24} and {L=190, k=40} are evaluated.
In our experiments on ImageNet, we use a DenseNet-BC structure with 4 dense blocks on 224×224 input images.
The initial convolution layer comprises 2k convolutions of size 7×7 with stride 2; the number of feature-maps in all other layers also follow from setting k. The exact network configurations we used on ImageNet are shown in Table 1.
4. Experiments
We empirically demonstrate DenseNet's effectiveness on several benchmark datasets and compare with state-of-theart architectures, especially with ResNet and its variants.
4.1. Datasets
CIFAR.
The two CIFAR datasets consist of colored natural images with 32×32 pixels. CIFAR-10 (C10) conMethod
Depth
Params
C10
C10+
C100
C100+
SVHN
Network in Network All-CNN Deeply Supervised Net Highway Network FractalNet 
38.6M
2.01 with Dropout/Drop-path
38.6M
ResNet 
1.7MResNet (reported by )
1.7M
ResNet with Stochastic Depth 
1.7M
10.2MWide ResNet 
11.0M36.5M20.50 with Dropout
2.7MResNet (pre-activation) 
1.7M
11.26∗
35.58∗10.2M
10.56∗
33.47∗DenseNet (k = 12)
1.0M
DenseNet (k = 12)
7.0M
DenseNet (k = 24)
27.2M
DenseNet-BC (k = 12)
0.8M
DenseNet-BC (k = 24)
15.3M
DenseNet-BC (k = 40)
25.6MTable 2: Error rates (%) on CIFAR and SVHN datasets. k denotes network's growth rate. Results that surpass all competing methods are bold and the overall best results are blue. "+" indicates standard data augmentation (translation and/or mirroring). ∗ indicates results run by ourselves. All the results of DenseNets without data augmentation (C10, C100, SVHN) are obtained using Dropout. DenseNets achieve lower error rates while using fewer parameters than ResNet. Without data augmentation, DenseNet performs better by a large margin. sists of images drawn from 10 and CIFAR-100 (C100) from
100 classes. The training and test sets contain 50,000 and 10,000 images respectively, and we hold out 5,000 training images as a validation set. We adopt a standard data augmentation scheme (mirroring/shifting) that is widely used for these two datasets. We denote this data augmentation scheme by a "+" mark at the end of the dataset name (e.g., C10+). For preprocessing, we normalize the data using the channel means and standard deviations. For the final run we use all 50,000 training images and report the final test error at the end of training.
SVHN.
The Street View House Numbers (SVHN) dataset
 contains 32×32 colored digit images. There are 73,257 images in the training set, 26,032 images in the test set, and 531,131 images for additional training. Following common practice we use all the training data without any data augmentation, and a validation set with 6,000 images is split from the training set. We select the model with the lowest validation error during training and report the test error. We follow and divide the pixel values by
255 so they are in the range.
ImageNet.
The ILSVRC 2012 classification dataset consists 1.2 million images for training, and 50,000 for validation, from 1, 000 classes. We adopt the same data augmentation scheme for training images as in, and apply a single-crop or 10-crop with size 224×224 at test time. Following, we report classification errors on the validation set.
4.2. Training
All the networks are trained using stochastic gradient descent (SGD). On CIFAR and SVHN we train using batch size 64 for 300 and 40 epochs, respectively.
The initial learning rate is set to 0.1, and is divided by 10 at 50% and 75% of the total number of training epochs. On ImageNet, we train models for 90 epochs with a batch size of 256. The learning rate is set to 0.1 initially, and is lowered by 10 times at epoch 30 and 60. Due to GPU memory constraints, our largest model (DenseNet-161) is trained with a mini-batch size 128. To compensate for the smaller batch size, we train this model for 100 epochs, and divide the learning rate by
10 at epoch 90.
Following, we use a weight decay of 10−4 and a Nesterov momentum of 0.9 without dampening. We adopt the weight initialization introduced by. For the three datasets without data augmentation, i.e., C10, C100 and SVHN, we add a dropout layer after each convolutional layer (except the first one) and set the dropout rate to
0.2. The test errors were only evaluated once for each task and model setting.
Model top-1 top-5
DenseNet-121 (k=32) 25.02 (23.61) 7.71 (6.66)
DenseNet-169 (k=32) 23.80 (22.08) 6.85 (5.92)
DenseNet-201 (k=32) 22.58 (21.46) 6.34 (5.54)
DenseNet-161 (k=48) 22.33 (20.85) 6.15 (5.30)
Table 3: The top-1 and top-5 error rates on the ImageNet validation set, with single-crop (10crop) testing.
8 x 10
#parameters validation error
ResNet−34
ResNet−101
ResNet−152
DenseNet−121
ResNet−50
DenseNet−169
DenseNet−201
DenseNet−161(k=48)
ResNets
DenseNets−BC
2.5 x 10
#FLOPs validation error
ResNet−34
DenseNet−121
ResNet−50
DenseNet−169
DenseNet−201
ResNets
DenseNets−BC
ResNet−152
DenseNet−161(k=48)
ResNet−101
Figure 3: Comparison of the DenseNets and ResNets top-1 error rates (single-crop testing) on the ImageNet validation dataset as a function of learned parameters (left) and FLOPs during test-time (right).
4.3. Classification Results on CIFAR and SVHN
We train DenseNets with different depths, L, and growth rates, k. The main results on CIFAR and SVHN are shown in Table 2. To highlight general trends, we mark all results that outperform the existing state-of-the-art in boldface and the overall best result in blue.
Accuracy.
Possibly the most noticeable trend may originate from the bottom row of Table 2, which shows that
DenseNet-BC with L = 190 and k = 40 outperforms the existing state-of-the-art consistently on all the CIFAR datasets. Its error rates of 3.46% on C10+ and 17.18% on
C100+ are significantly lower than the error rates achieved by wide ResNet architecture.
Our best results on
C10 and C100 (without data augmentation) are even more encouraging: both are close to 30% lower than FractalNet with drop-path regularization. On SVHN, with dropout, the DenseNet with L = 100 and k = 24 also surpasses the current best result achieved by wide ResNet.
However, the 250-layer DenseNet-BC doesn't further improve the performance over its shorter counterpart. This may be explained by that SVHN is a relatively easy task, and extremely deep models may overfit to the training set.
Capacity.
Without compression or bottleneck layers, there is a general trend that DenseNets perform better as
L and k increase. We attribute this primarily to the corresponding growth in model capacity. This is best demonstrated by the column of C10+ and C100+. On C10+, the error drops from 5.24% to 4.10% and finally to 3.74% as the number of parameters increases from 1.0M, over 7.0M to 27.2M. On C100+, we observe a similar trend. This suggests that DenseNets can utilize the increased representational power of bigger and deeper models. It also indicates that they do not suffer from overfitting or the optimization difficulties of residual networks.
Parameter Efficiency.
The results in Table 2 indicate that
DenseNets utilize parameters more efficiently than alternative architectures (in particular, ResNets). The DenseNetBC with bottleneck structure and dimension reduction at transition layers is particularly parameter-efficient. For example, our 250-layer model only has 15.3M parameters, but it consistently outperforms other models such as FractalNet and Wide ResNets that have more than 30M parameters. We also highlight that DenseNet-BC with L = 100 and k = 12 achieves comparable performance (e.g., 4.51% vs 4.62% error on C10+, 22.27% vs 22.71% error on C100+) as the 1001-layer pre-activation ResNet using 90% fewer parameters. Figure 4 (right panel) shows the training loss and test errors of these two networks on C10+. The 1001-layer deep
ResNet converges to a lower training loss value but a similar test error. We analyze this effect in more detail below.
Overfitting.
One positive side-effect of the more efficient use of parameters is a tendency of DenseNets to be less prone to overfitting. We observe that on the datasets without data augmentation, the improvements of DenseNet architectures over prior work are particularly pronounced. On C10, the improvement denotes a 29% relative reduction in error from 7.33% to 5.19%. On C100, the reduction is about 30% from 28.20% to 19.64%. In our experiments, we observed potential overfitting in a single setting: on C10, a 4× growth of parameters produced by increasing k =12 to k =24 lead to a modest increase in error from 5.77% to 5.83%. The DenseNet-BC bottleneck and compression layers appear to be an effective way to counter this trend.
4.4. Classification Results on ImageNet
We evaluate DenseNet-BC with different depths and growth rates on the ImageNet classification task, and compare it with state-of-the-art ResNet architectures. To ensure a fair comparison between the two architectures, we eliminate all other factors such as differences in data preprocessing and optimization settings by adopting the publicly available Torch implementation for ResNet by 1. We simply replace the ResNet model with the DenseNet-BC network, and keep all the experiment settings exactly the same as those used for ResNet. The only exception is our largest
DenseNet model is trained with a mini-batch size of 128
1https://github.com/facebook/fb.resnet.torch
#parameters
×105
16 test error (%)
DenseNet
DenseNet-C
DenseNet-B
DenseNet-BC
#parameters
×105
16 test error (%)
ResNet
DenseNet-BC
3x fewer parameters
300 epoch
16 test error (%)
Test error: ResNet-1001 (10.2M)
Test error: DenseNet-BC-100 (0.8M)
Training loss: ResNet-1001 (10.2M)
Training loss: DenseNet-BC-100 (0.8M)
10−3
10−2
10−1
100 training loss
Figure 4: Left: Comparison of the parameter efficiency on C10+ between DenseNet variations. Middle: Comparison of the parameter efficiency between DenseNet-BC and (pre-activation) ResNets. DenseNet-BC requires about 1/3 of the parameters as ResNet to achieve comparable accuracy. Right: Training and testing curves of the 1001-layer pre-activation ResNet with more than 10M parameters and a 100-layer DenseNet with only 0.8M parameters. because of GPU memory limitations; we train this model for 100 epochs with a third learning rate drop after epoch
90 to compensate for the smaller batch size.
We report the single-crop and 10-crop validation errors of DenseNets on ImageNet in Table 3.
Figure 3 shows the single-crop top-1 validation errors of DenseNets and ResNets as a function of the number of parameters (left) and FLOPs (right). The results presented in the figure reveal that
DenseNets perform on par with the state-of-the-art ResNets, whilst requiring significantly fewer parameters and computation to achieve comparable performance. For example, a DenseNet-201 with 20M parameters model yields similar validation error as a 101-layer ResNet with more than 40M parameters. Similar trends can be observed from the right panel, which plots the validation error as a function of the number of FLOPs: a DenseNet that requires as much computation as a ResNet-50 performs on par with a ResNet-101, which requires twice as much computation.
It is worth noting that our experimental setup implies that we use hyperparameter settings that are optimized for
ResNets but not for DenseNets. It is conceivable that more extensive hyper-parameter searches may further improve the performance of DenseNet on ImageNet.2
5. Discussion
Superficially, DenseNets are quite similar to ResNets:
Eq. (2) differs from Eq. (1) only in that the inputs to Hℓ(·) are concatenated instead of summed. However, the implications of this seemingly small modification lead to substantially different behaviors of the two network architectures.
Model compactness.
As a direct consequence of the input concatenation, the feature-maps learned by any of the DenseNet layers can be accessed by all subsequent layers.
This encourages feature reuse throughout the network, and leads to more compact models.
2Our DenseNet implementation contains some memory inefficiencies which temporarily precludes experiments with over 30M parameters.
The left two plots in Figure 4 show the result of an experiment that aims to compare the parameter efficiency of all variants of DenseNets (left) and also a comparable
ResNet architecture (middle). We train multiple small networks with varying depths on C10+ and plot their test accuracies as a function of network parameters.
In comparison with other popular network architectures, such as
AlexNet or VGG-net, ResNets with pre-activation use fewer parameters while typically achieving better results. Hence, we compare DenseNet (k = 12) against this architecture. The training setting for DenseNet is kept the same as in the previous section.
The graph shows that DenseNet-BC is consistently the most parameter efficient variant of DenseNet. Further, to achieve the same level of accuracy, DenseNet-BC only requires around 1/3 of the parameters of ResNets (middle plot). This result is in line with the results on ImageNet we presented in Figure 3. The right plot in Figure 4 shows that a DenseNet-BC with only 0.8M trainable parameters is able to achieve comparable accuracy as the 1001-layer(pre-activation) ResNet with 10.2M parameters.
Implicit Deep Supervision.
One explanation for the improved accuracy of dense convolutional networks may be that individual layers receive additional supervision from the loss function through the shorter connections. One can interpret DenseNets to perform a kind of "deep supervision".
The benefits of deep supervision have previously been shown in deeply-supervised nets (DSN; ), which have classifiers attached to every hidden layer, enforcing the intermediate layers to learn discriminative features.
DenseNets perform a similar deep supervision in an implicit fashion: a single classifier on top of the network provides direct supervision to all layers through at most two or three transition layers. However, the loss function and gradient of DenseNets are substantially less complicated, as the same loss function is shared between all layers.
Stochastic vs. deterministic connection.
There is an interesting connection between dense convolutional net4706 works and stochastic depth regularization of residual networks. In stochastic depth, layers in residual networks are randomly dropped, which creates direct connections between the surrounding layers. As the pooling layers are never dropped, the network results in a similar connectivity pattern as DenseNet: there is a small probability for any two layers, between the same pooling layers, to be directly connected—if all intermediate layers are randomly dropped. Although the methods are ultimately quite different, the DenseNet interpretation of stochastic depth may provide insights into the success of this regularizer.
Feature Reuse.
By design, DenseNets allow layers access to feature-maps from all of its preceding layers (although sometimes through transition layers). We conduct an experiment to investigate if a trained network takes advantage of this opportunity. We first train a DenseNet on
C10+ with L = 40 and k = 12. For each convolutional layer ℓ within a block, we compute the average (absolute) weight assigned to connections with layer s. Figure 5 shows a heat-map for all three dense blocks. The average absolute weight serves as a surrogate for the dependency of a convolutional layer on its preceding layers. A red dot in position(ℓ, s) indicates that the layer ℓ makes, on average, strong use of feature-maps produced s-layers before. Several observations can be made from the plot:
1. All layers spread their weights over many inputs within the same block. This indicates that features extracted by very early layers are, indeed, directly used by deep layers throughout the same dense block.
2. The weights of the transition layers also spread their weight across all layers within the preceding dense block, indicating information flow from the first to the last layers of the DenseNet through few indirections.
3. The layers within the second and third dense block consistently assign the least weight to the outputs of the transition layer (the top row of the triangles), indicating that the transition layer outputs many redundant features (with low weight on average). This is in keeping with the strong results of DenseNet-BC where exactly these outputs are compressed.
4. Although the final classification layer, shown on the very right, also uses weights across the entire dense block, there seems to be a concentration towards final feature-maps, suggesting that there may be some more high-level features produced late in the network.
6. Conclusion
We proposed a new convolutional network architecture, which we refer to as Dense Convolutional Network(DenseNet). It introduces direct connections between any
Dense Block 1
Source layer (s)
Dense Block 2
Dense Block 3
Target layer ()
Transition layer 1
Transition layer 2
Classification layer
Target layer ()
Target layer ()
Figure 5: The average absolute filter weights of convolutional layers in a trained DenseNet. The color of pixel (s, ℓ) encodes the average L1 norm (normalized by number of input feature-maps) of the weights connecting convolutional layer s to ℓ within a dense block. Three columns highlighted by black rectangles correspond to two transition layers and the classification layer. The first row encodes weights connected to the input layer of the dense block. two layers with the same feature-map size. We showed that
DenseNets scale naturally to hundreds of layers, while exhibiting no optimization difficulties. In our experiments, DenseNets tend to yield consistent improvement in accuracy with growing number of parameters, without any signs of performance degradation or overfitting.
Under multiple settings, it achieved state-of-the-art results across several highly competitive datasets.
Moreover, DenseNets require substantially fewer parameters and less computation to achieve state-of-the-art performances. Because we adopted hyperparameter settings optimized for residual networks in our study, we believe that further gains in accuracy of DenseNets may be obtained by more detailed tuning of hyperparameters and learning rate schedules.
Whilst following a simple connectivity rule, DenseNets naturally integrate the properties of identity mappings, deep supervision, and diversified depth. They allow feature reuse throughout the networks and can consequently learn more compact and, according to our experiments, more accurate models. Because of their compact internal representations and reduced feature redundancy, DenseNets may be good feature extractors for various computer vision tasks that build on convolutional features, e.g.,. We plan to study such feature transfer with DenseNets in future work.
Acknowledgements.
The authors are supported in part by the III-1618134, III-1526012, IIS-1149882 grants from the National Science Foundation, and the Bill and Melinda
Gates foundation. Gao Huang is supported by the International Postdoctoral Exchange Fellowship Program of China
Postdoctoral Council (No.20150015). Zhuang Liu is supported by the National Basic Research Program of China
Grants 2011CBA00300, 2011CBA00301, the National Natural Science Foundation of China Grant 61361136003. We also thank Daniel Sedra, Geoff Pleiss and Yu Sun for many insightful discussions.
References
 C. Cortes, X. Gonzalvo, V. Kuznetsov, M. Mohri, and S. Yang. Adanet: Adaptive structural learning of artificial neural networks. arXiv preprint arXiv:1607.01097, 2016. 2
 J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. FeiFei. Imagenet: A large-scale hierarchical image database. In
CVPR, 2009. 5
 S. E. Fahlman and C. Lebiere. The cascade-correlation learning architecture. In NIPS, 1989. 2
 J. R. Gardner, M. J. Kusner, Y. Li, P. Upchurch, K. Q.
Weinberger, and J. E. Hopcroft. Deep manifold traversal:
Changing labels with convolutional features. arXiv preprint arXiv:1511.06421, 2015. 8
 L. Gatys, A. Ecker, and M. Bethge. A neural algorithm of artistic style. Nature Communications, 2015. 8
 X. Glorot, A. Bordes, and Y. Bengio. Deep sparse rectifier neural networks. In AISTATS, 2011. 3
 I. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, and Y. Bengio. Maxout networks. In ICML, 2013. 5
 S. Gross and M. Wilber. Training and investigating residual nets, 2016. 5, 6
 B. Hariharan, P. Arbeláez, R. Girshick, and J. Malik. Hypercolumns for object segmentation and fine-grained localization. In CVPR, 2015. 2
 K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In ICCV, 2015. 5
 K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, 2016. 1, 2, 3, 4, 5, 6
 K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. In ECCV, 2016. 2, 3, 5, 7
 G. Huang, Y. Sun, Z. Liu, D. Sedra, and K. Q. Weinberger.
Deep networks with stochastic depth. In ECCV, 2016. 1, 2, S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In
ICML, 2015. 3
 A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Tech Report, 2009. 4
 A. Krizhevsky, I. Sutskever, and G. E. Hinton.
Imagenet classification with deep convolutional neural networks. In
NIPS, 2012. 3, 7
 G. Larsson, M. Maire, and G. Shakhnarovich. Fractalnet:
Ultra-deep neural networks without residuals. arXiv preprint arXiv:1605.07648, 2016. 1, 3, 5, 6
 Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E.
Howard, W. Hubbard, and L. D. Jackel. Backpropagation applied to handwritten zip code recognition. Neural computation, 1(4):541–551, 1989. 1
 Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradientbased learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. 1, 3
 C.-Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu. Deeplysupervised nets. In AISTATS, 2015. 2, 3, 5, 7
 Q. Liao and T. Poggio. Bridging the gaps between residual learning, recurrent neural networks and visual cortex. arXiv preprint arXiv:1604.03640, 2016. 2
 M. Lin, Q. Chen, and S. Yan. Network in network. In ICLR, J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In CVPR, 2015. 2
 Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y.
Ng. Reading digits in natural images with unsupervised feature learning, 2011. In NIPS Workshop, 2011. 5
 M. Pezeshki, L. Fan, P. Brakel, A. Courville, and Y. Bengio.
Deconstructing the ladder network architecture. In ICML, A. Rasmus, M. Berglund, M. Honkala, H. Valpola, and T. Raiko. Semi-supervised learning with ladder networks.
In NIPS, 2015. 3
 A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and Y. Bengio. Fitnets: Hints for thin deep nets. In ICLR, O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al.
Imagenet large scale visual recognition challenge.
IJCV. 1, 7
 P. Sermanet, S. Chintala, and Y. LeCun. Convolutional neural networks applied to house numbers digit classification. In
ICPR, pages 3288–3291. IEEE, 2012. 5
 P. Sermanet, K. Kavukcuoglu, S. Chintala, and Y. LeCun.
Pedestrian detection with unsupervised multi-stage feature learning. In CVPR, 2013. 2
 J. T. Springenberg, A. Dosovitskiy, T. Brox, and M. Riedmiller.
Striving for simplicity: The all convolutional net. arXiv preprint arXiv:1412.6806, 2014. 5
 N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. JMLR, 2014. 5
 R. K. Srivastava, K. Greff, and J. Schmidhuber. Training very deep networks. In NIPS, 2015. 1, 2, 5
 I. Sutskever, J. Martens, G. Dahl, and G. Hinton. On the importance of initialization and momentum in deep learning.
In ICML, 2013. 5
 C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In CVPR, 2015. 2, 3
 C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna.
Rethinking the inception architecture for computer vision. In
CVPR, 2016. 2, 3, 4
 S. Targ, D. Almeida, and K. Lyman.
Resnet in resnet: Generalizing residual architectures. arXiv preprint arXiv:1603.08029, 2016. 2
 J. Wang, Z. Wei, T. Zhang, and W. Zeng. Deeply-fused nets. arXiv preprint arXiv:1605.07716, 2016. 3
 B. M. Wilamowski and H. Yu.
Neural network learning without backpropagation. IEEE Transactions on Neural Networks, 21(11):1793–1803, 2010. 2
 S. Yang and D. Ramanan. Multi-scale recognition with dagcnns. In ICCV, 2015. 2
 S. Zagoruyko and N. Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016. 3, 5, 6
 Y. Zhang, K. Lee, and H. Lee. Augmenting supervised neural networks with unsupervised objectives for large-scale image classification. In ICML, 2016. 3Segmentation as Selective Search for Object Recognition
Koen E. A. van de Sande∗
Jasper R. R. Uijlings†
Theo Gevers∗
Arnold W. M. Smeulders∗
∗University of Amsterdam
†University of Trento
Amsterdam, The Netherlands
Trento, Italy ksande@uva.nl, jrr@disi.unitn.it, th.gevers@uva.nl, a.w.m.smeulders@uva.nl
Abstract
For object recognition, the current state-of-the-art is based on exhaustive search. However, to enable the use of more expensive features and classifiers and thereby progress beyond the state-of-the-art, a selective search strategy is needed. Therefore, we adapt segmentation as a selective search by reconsidering segmentation: We propose to generate many approximate locations over few and precise object delineations because (1) an object whose location is never generated can not be recognised and (2) appearance and immediate nearby context are most effective for object recognition. Our method is class-independent and is shown to cover 96.7% of all objects in the Pascal
VOC 2007 test set using only 1,536 locations per image.
Our selective search enables the use of the more expensive bag-of-words method which we use to substantially improve the state-of-the-art by up to 8.5% for 8 out of 20 classes on the Pascal VOC 2010 detection challenge.
1. Introduction
Object recognition, i.e. determining the position and the class of an object within an image, has made impressive progress over the past few years, see the Pascal VOC challenge.
The state-of-the-art is based on exhaustive search over the image to find the best object positions. However, as the total number of images and windows to evaluate in an exhaustive search is huge and growing, it is necessary to constrain the computation per location and the number of locations considered. The computation is currently reduced by using a weak classifier with simple-to-compute features, and by reducing the number of locations on a coarse grid and with fixed window sizes. This comes at the expense of overlooking some object locations and misclassifying others. Therefore, we propose selective search, greatly reducing the number of locations to consider. Specifically, we propose to use segmentation to generate a limited set of locations, permitting the more powerful yet expensive bag-of(a)(c)(d)(b)
Figure 1. Given an image (a) our aim is to find its objects for which the ground truth is shown in (b). To achieve this, we adapt segmentation as a selective search strategy: We aim for high recall by generating locations at all scales and account for many different scene conditions by employing multiple invariant colour spaces.
Example object hypotheses are visualised in (d). words features.
Selective search has been exploited successfully by for object delineation, i.e. creating a pixel-wise classification of the image. Both concentrate on 10-100 possibly overlapping segments per image, which best correspond to an object. They focus on finding accurate object contours, which is why both references use a powerful, specialized contour detector. In this paper, we reconsider segmentation to use as an instrument to select the best locations for object recognition. Rather than aiming for 10-100 accurate locations, we aim to generate 1,000-10,000 approximate locations. For boosting object recognition, (1) generating several thousand locations per image guarantees the inclusion of virtually all objects, and (2) rough segmentation includes the local context known to be beneficial for object classification. Hence we place our computational attention precisely on these parts of the image which bear the most information for object classification.
Emphasizing recall (encouraging to include all image fragments of potential relevance) was earlier proposed by
Hoiem et al. for surface layout classification and adopted by Russell et al. for latent object discovery.
In the references its use is limited to changing the scale of the segmentation, while its potential for finding objects has yet to be investigated. Malisiewicz and Efros investigated how well segments capture objects as opposed to the bounding boxes of an exhaustive search. They also mainly change the scale of the segmentation. In contrast, this paper uses a full segmentation hierarchy and accounts for as many different scene conditions as possible, such as shadows, shading, and highlights, by using a variety of invariant colour spaces. Furthermore, we demonstrate the power of segmentation as selective search on the challenging Pascal
VOC dataset in terms of both recall and recognition accuracy.
To summarize, we make the following contributions: (1)
We reconsider segmentation by adapting it as an instrument to select the best locations for object recognition. We put most emphasis on recall and prefer good object approximations over exact object boundaries. (2) We demonstrate that accounting for scene conditions through invariant colour spaces results in a powerful selective search strategy with high recall. (3) We show that our selective search enables the use of more expensive features such as bag-of-words and substantially improves the state-of-the-art on the Pascal
VOC 2010 detection challenge for 8 out of 20 classes.
2. Related Work
In Figure 2, the relation of this paper with other work is visualized. Research within localisation can generally be divided into two categories. 1) Work with emphasis on recognition (Section 2.1). Here determining the object class is more important than finding the exact contours and an exhaustive search is the norm. 2) Work with emphasis on object delineation (Section 2.2). Here object contours are most important and the use of segmentation is the norm.
There are two exceptions to these categories. Vedaldi et al. use jumping windows, in which the relation between individual visual words and the object location is learned to predict the object location in new images.
Maji and Malik combine multiple of these relations to predict the object location using a Hough-transform, after which they randomly sample windows close to the Hough maximum. Both methods can be seen as a selective search.
In contrast to learning, we adopt segmentation as selective search to generate class independent object hypotheses.
2.1. Exhaustive Search for Recognition
As an object can be located at any position and scale in the image, it is natural to search everywhere.
However, the visual search space is huge, making an exhaustive search computationally expensive. This imposes constraints on the evaluation cost per location and/or the Localisation
Exhaustive search
Selective search
Object
Recognition
Object
Recognition
Object
Delineation
100,000-10,000,000
Coarse
Weak/Cascade
Weak (appearance)
Recall
 
1,000-10,000
Approximate
Strong
Strong (appearance)
Recall, This paper
10-100
Precise
Strong
Strong (shape, contour)
Precision
 
#Locations
Location
Classifiers
Features
Focus
References
Figure 2. Positioning of this paper with respect to related work. number of locations considered. Hence most of these sliding window techniques use a coarse search grid and fixed aspect ratios, using weak classifiers and economic image features such as HOG. This method is often used as a preselection step in a cascade of classifiers.
Related to the sliding window technique is the highly successful part-based object localisation method of Felzenszwalb et al.. Their method also performs an exhaustive search using a linear SVM and HOG features. However, they search for objects and object parts, whose combination results in an impressive object detection performance.
Lampert et al. developed a branch and bound technique to directly search for the optimal window within an image. While they obtain impressive results for linear classifiers, found that for non-linear classifiers the method in practice still visits over a 100,000 windows per image.
While the previous methods are all class-specific, Alexe et al. propose to search for any object, independent of its class. They train a classifier on the object windows of those objects which have a well-defined shape (as opposed to e.g. grass). Then instead of a full exhaustive search they randomly sample boxes to which they apply their classifier.
The boxes with the highest "objectness" measure serve as a set of object hypotheses. This set is then used to greatly reduce the number of windows evaluated by class-specific object detectors.
Instead of an exhaustive search, in this paper, we propose to do segmentation as a selective search enabling the immediate use of expensive and potentially more powerful recognition techniques. In contrast to all exhaustive methods except, our method yields an object hypotheses set which is completely class independent.
2.2. Selective Search for Object Delineation
In the domain of object delineation, both Carreira et al. and Endres and Hoiem propose to generate a set of class independent object hypotheses using segmentation.
Both methods generate multiple foreground/background segmentations, learn to predict the likelihood that a fore(a)(b)
Figure 3. Two examples of our hierarchical grouping algorithm showing the necessity of different scales. On the left we find many objects at different scales. On the right we necessarily find the objects at different scales as the girl is contained by the tv. ground segment is a complete object, and use this to rank the segments. Both algorithms show a promising ability to accurately delineate objects within images, confirmed by
 who achieve state-of-the-art results on pixel-wise image classification using. This paper uses selective search for object recognition, hence we put more emphasis on recall and welcome rough object locations instead of precise object delineations. We can omit the excellent yet expensive contour detector of included in, making our algorithm computationally feasible on large datasets. In contrast to, we use a hierarchical grouping algorithm instead of multiple foreground/background segmentations.
Gu et al. address the problem of carefully segmenting and recognizing objects based on their parts. They first generate a set of part hypotheses using a grouping method based on. Each part hypothesis is described by both appearance and shape features. Then an object is recognized and carefully delineated by using its parts, achieving good results for shape recognition. In their work, the segmentation is limited to a single hierarchy while its power of discovering parts or objects is not evaluated. In this paper, we use multiple hierarchical segmentations diversified through employing a variety of colour spaces, and evaluate their potential to find complete objects.
3. Segmentation as Selective Search
In this section, we adapt segmentation as selective search for object recognition. This adaptation leads to the following considerations:
High recall. Objects whose locations are not generated can never be recognized. Recall is therefore the most important criterion. To obtain a high recall we observe the following: (1) Objects can occur at any scale within an image.
Moreover, some objects are contained within other objects.
Hence it is necessary to generate locations at all scales, as illustrated in Figure 3. (2) There is no single best strategy to group regions together: An edge may represent an object boundary in one image, while the same edge in another image may be the result of shading. Hence rather than aiming for the single best segmentation, it is important to combine multiple complementary segmentations, i.e. we want to diversify the set of segmentations used.
Coarse locations are sufficient. As the state-of-the-art in object recognition uses appearance features, the exact object contours of the object hypotheses are less important.
Hence instead of a strong focus on object boundaries (e.g.
 ), the evaluation should focus on finding reasonable approximations of the object locations, such as is measured by the Pascal overlap criterion.
Fast to compute. The generation of the object hypotheses should not become a bottleneck when performing object localisation on a large dataset.
3.1. Our Segmentation Algorithm
The most natural way to generate locations at all scales is to use all locations from a hierarchical segmentation algorithm (illustrated in Figure 1). Our algorithm uses size and appearance features which are efficiently propagated throughout the hierarchy, making it reasonably fast. Note that we keep the algorithm basic to ensure repeatability and make clear that our results do not stem from parameter tuning but from rethinking the goal of segmentation.
As regions can yield richer information than pixels, we start with an oversegmentation, i.e. a set of small regions which do not spread over multiple objects. We use the fast method of as our starting point, which found wellsuited for generating an oversegmentation.
Starting from the initial regions, we use a greedy algorithm which iteratively groups the two most similar regions together and calculates the similarities between this new region and its neighbours. We continue until the whole image becomes a single region. As potential object locations, we consider either all segments throughout the hierarchy (including initial segments), or we consider the tight bounding boxes around these segments.
We define the similarity S between region a and b as
S(a, b) = Ssize(a, b) + Stexture(a, b). Both components result in a number in range and are weighed equally.
Ssize(a,b) is defined as the fraction of the image that the segment a and b jointly occupy. This measure encourages small regions to merge early and prevents a single region from gobbling up all others one by one.
Stexture(a, b) is defined as the histogram intersection between SIFT-like texture measurements. For these measurements, we aggregate the gradient magnitude in 8 directions over a region, just like in a single subregion of SIFT with no Gaussian weighting. As we use colour, we follow and do texture measurements in each colour channel separately and concatenate the results.
3.2. Shadow, Shading and Highlight Edges
To obtain multiple segmentations which are complementary, we perform our segmentation in a variety of colour channels with different invariance properties. Specifically, we consider multiple colour spaces with different degrees of sensitivity to shadow, shading and highlight edges.
Standard RGB is the most sensitive. The opponent colour space is insensitive to highlight edges, but sensitive to shadows and shading edges. The normalized RGB space is insensitive to shadow and shading edges but still sensitive to highlights. The hue H is the most invariant and is insensitive to shadows, shading and highlights. Note that we always perform each segmentation in a single colour space, including the initial segmentation of.
An alternative approach to multiple colour spaces would be the use of different thresholds for the starting segmentation. We evaluate this approach as well.
3.3. Discussion
Our adaptation of segmentation as selective search for object recognition is designed to obtain high recall by considering all levels of a hierarchical grouping of image segments. Furthermore, by considering multiple colour spaces with increasing levels of invariance to imaging conditions, we are robust to the additional edges introduced into an image by shadows, shading and highlights. Finally, our approach is fast which makes it applicable to large datasets.
4. Object Recognition System
In this section, we detail how to use the selective search strategy from Section 3 for a complete object recognition system. As feature representation, two types of features are dominant: histograms of oriented gradients (HOG) and bag-of-words. HOG has been shown to be successful in combination with the part-based model by Felzenszwalb et al.. However, as they use an exhaustive search, HOG features in combination with a linear classifier is the only feasible choice.
To show that our selective search strategy enables the use of more expensive and potentially more powerful features, we use Bag-of-Words for object recognition. We use a more powerful (and expensive) implementation than by employing multiple colour spaces and a finer spatial pyramid division.
Specifically we sample descriptors at each pixel on a single scale.
We extract SIFT and two recommended colour SIFTs from, OpponentSIFT and RGB-SIFT.
Software from is used. We use a visual codebook of size 4,096 and a spatial pyramid with 4 levels. Because a spatial pyramid results in a coarser spatial subdivision than the cells which make up a HOG descriptor, our features contain less information about the specific spatial layout of the object. Therefore, HOG is better suited for rigid objects and our features are better suited for deformable object types.
As classifier we employ a Support Vector Machine with a histogram intersection kernel using. We use the fast, approximate classification strategy of.
Our training procedure is illustrated in Figure 4. The initial positive examples consist of all ground truth object windows. As initial negative examples we use all object locations generated by our selective search that have an overlap of 20% to 50% with a positive example, unless they have more than 70% overlap with another negative, i.e. we avoid near duplicates. This selection of training examples gives reasonably good initial classification models.
Then we enter a retraining phase to iteratively add hard negative examples (e.g. ): We apply the learned models to the training set using the locations generated by our selective search. For each negative image we add the highest scoring location. As our initial training set already yields good models, our models converge in only two iterations.
For the test set, the final model is applied to all locations generated by our selective search. The windows are sorted by classifier score while windows which have more than
30% overlap with a higher scoring window are considered near-duplicates and are removed.
5. Evaluation
To evaluate the quality of our selective search strategy, we perform the following four experiments:
• Experiment 1 evaluates how to adapt segmentation for selective search. Specifically we compare multiple flat segmentations against a hierarchy and evaluate the use of increasingly invariant colour spaces.
• Experiment 2 compares segmentation as selective search on the task of generating good object locations for recognition with.
• Experiment 3 compares segmentation as selective search on the task of generating good object delineations for segmentation with.
• Experiment 4 evaluates the use of our object hypotheses in the object recognition system of Section 4, on the widely accepted object localisation method of and compares it to the state-of-the-art.
Positive examples
Object hypotheses
Ground truth
Difficult negatives if overlap with positive 20-50%
Training Examples
Train
SVM(Histogram Intersection
Kernel)
Model
Search for false positives
False Positives
Add to training examples
Training Examples
Retrain
Figure 4. The training procedure of our object recognition pipeline. As positive learning examples we use the ground truth. As negatives we use examples that have a 20-50% overlap with the positive examples. We iteratively add hard negatives using a retraining phase.
In all experiments, we report results on the challenging
Pascal VOC 2007 or 2010 datasets. These datasets contain images of twenty object categories and the ground truth in terms of object labels, the location in terms of bounding boxes, and for a subset of the data the object location in terms of a pixel-wise segmentation.
As in, the quality of the hypotheses is defined in terms of the average recall over all classes versus the number of locations retrieved. We use the standard Pascal overlap criterion where an object is considered found if the area of the intersection of a candidate location and the ground truth location, divided by the area of their union is larger than 0.5. Note that in the first two experiments the location is a bounding box, and in the third it is a segment.
Any parameter selection was done on the training set only, while results in this paper are reported on the test set.
5.1. Exp. 1: Segmentation for Selective Search
In this experiment, we evaluate how to adapt segmentation for selective search. First, we compare multiple flat segmentations against a hierarchical segmentation. Second, we evaluate the use of a variety of colour spaces.
Flat versus Hierarchy. As our segmentation algorithm starts with the initial oversegmentation of, we compare our hierarchical version with multiple flat segmentations by. We do this in RGB colour space. We vary the scale of by setting the threshold k from 100 to 1000 both in steps of 10 and in steps of 50. For our hierarchical algorithm we use the smallest threshold 100. Varying the threshold k results in many more segments than a single hierarchical grouping, because in the segment boundaries resulting from a high threshold are not a subset of those from a small threshold. Therefore we additionally consider two hierarchical segmentations using a threshold of 100 and 200.
Experiment 1: Multiple Flat segmentations versus Hierarchy
Max. recall (%)
# windows
 k = 100, 150... 1000
 k = 100, 110... 1000
Hierarchical k = 100
Hierarchical k = 100, 200
Table 1. Comparison of multiple flat segmentations versus a hierarchy in terms of recall and the number of windows per image.
As can be seen from Table 1, multiple flat segmentations yield a higher recall than a single hierarchical grouping but using many more locations. However, if we choose two initial thresholds and combine results, our algorithm yields recall of 89.4 instead of 87.7, while using only 511 locations instead of 1159. Hence a hierarchical approach is preferable over multiple flat segmentations as it yields better results, fewer parameters, and selects all scales naturally. Additionally, we found it to be much faster.
Multiple Colour Spaces. We now test two diversification strategies to obtain higher recall. As seen in the previous experiment it is beneficial to use multiple starting segmentations. Furthermore we test how combining different colour spaces with different invariance properties can increase the number of objects found. Specifically, we take a segmentation in RGB colour space, and subsequently add the segmentation in Opponent colour space, normalized rgb colour space, and the Hue channel. We do this for a single initial segmentation with k = 100, two initial segmentations with k = 100, 200, and four initial segmentations with k = 100, 150, 200, 250. Results are shown in Figure 5.
Experiment 1: Influence of Multiple Colour Spaces
RGB
RGB+Opp
RGB+Opp+rgb
RGB+Opp+rgb+H
Colour Spaces
Recall k=100,150,200,250 k=100,200 k=100
Figure 5. Using multiple colour spaces clearly improves recall; along the horizontal axis increasingly invariant colour spaces are added.
As can be seen, both changing the initial segmentation and using a variety of different colour channels yield complementary object locations. Note that using four different colour spaces works better than using four different initial segmentations. Furthermore, when using all four colour spaces the difference between two and four initial segmentations is negligible. We conclude that varying the colour spaces with increasing invariances is better than varying the threshold of the initial segmentation. In subsequent experiments we always use these two initial segmentations.
On the sensitivity of parameters. In preliminary experiments on the training set we used other colour spaces such as HSV, HS, normalized rg plus intensity, intensity only, etc. However, we found that as long as one selects colour spaces with a range of invariance properties, the outcome is very similar. For illustration purposes we used in this paper the colour spaces with the most clear invariance properties.
Furthermore, we found that as long as a good oversegmentation is generated, the exact choice for k is unimportant.
Finally, different implementations of the texture histogram yielded little changes overall. We conclude that the recall obtained in this paper is not caused by parameter tuning but rather by having a good diversification of segmentation strategies through different colour invariance properties.
5.2. Exp. 2: Selective Search for Recognition
We now compare our selective search method to the sliding windows of, the jumping windows of, and the 'objectness' measure of. Table 2 shows the maximum recall obtained for each method together with the average number of locations generated per image.
Our method achieves the best results with a recall of 96.7% with on average 1,536 windows per image. The jumping windows of come second with 94% recall but uses 10,000 windows instead. Moreover, their method is specifically trained for each class whereas our method is completely class-independent. Hence, with only a limited number of object locations our method yields the highest recall.
Experiment 2: Maximum Recall of Selective Search for Recognition
Max. recall (%)
# windows
Sliding Windows 
200 per class
Jumping Windows 
10,000 per class
'Objectness' 
Our hypotheses
Table 2. Comparison of maximum recall between our method and. We achieve the highest recall of 96.7%. Second comes
 with 94.0% but using an order of magnitude more locations.
We also compare the trade-off between recall and the number of windows in Figure 6. As can be seen, our method gives a higher recall using fewer windows than. The method of seems to need only few windows to obtain their maximum recall of 83%. However, they use 200 windows per image per class, which means they generate 4,000 windows per image. Moreover, the ordering of their hypotheses is based on a class specific recognition score while the ordering of our hypotheses is imposed by the inclusion of segmentations in increasingly invariant colour spaces.
In conclusion, our selective search outperforms other methods in terms of maximum recall while using fewer locations. Additionally, our method is completely classindependent. This shows that segmentation, when adapted
Experiment 2: Recall of Selective Search for Recognition
Number of candidate windows
Recall
Sliding Windows (# per class)
Jumping Windows (# per class)
Objectness
Our locations
Figure 6. The trade-off between the number of retrieved windows and recall on the Pascal VOC 2007 object detection dataset. Note that for the reported number of locations is per class; the total number of windows per image is a factor 20 higher. for high recall by using all scales and a variety of colour spaces with different invariance properties, is a highly effective selective search strategy for object recognition.
5.3.Exp. 3: Selective Search for Object Delineation
The methods of are designed for object delineation and computationally too expensive to apply them to the VOC 2007 detection dataset. Instead we compare to them on the much smaller segmentation dataset using not boxes but the segments instead. We generated candidate segments for by using their publicly available code. Note that we excluded the background category in the evaluation.
Results are shown in Table 3. The method of achieves the best recall of 82.2% using 1,989 windows. Our method comes second with a recall of 79.8% using 1973 segments.
The method of results in a recall of 78.2% using only
697 windows. However, our method is 28 times faster than
 and 54 times faster than.
We conclude that our method is competitive in terms of recall while still computationally feasible on large datasets.
Experiment 3: Recall of Selective Search for Segmentation
Max. recall (%)
# windows
Time (s)
Carreira 
Endres 
Our hypotheses
Combination
Table 3. Comparison of our paper with in terms of recall on the Pascal VOC 2007 segmentation task. Our method has competitive recall while being more than an order of magnitude faster.
Interestingly, we tried to diversify the selective search by combining all three methods.
The resulting recall is 90.1%(!), much higher than any single method. We conclude that for the purpose of recognition, instead of aiming for the best segmentation, it is prudent to investigate how segmentations can complement each other.
Experiment 4: Object Recognition Accuracy on VOC2007 Test Set
Average Precision bicycle car horse bus motorbike train person tv/monitor sofa aeroplane bottle cow dining table chair cat sheep boat potted plant dog bird
Object Category
Search strategies using part-based models
Part-based + Exhaustive search (baseline)
Part-based + Our selective search
Average Precision bicycle car horse bus motorbike train person tv/monitor sofa aeroplane bottle cow dining table chair cat sheep boat potted plant dog bird
Object Category
Part-based models versus bag-of-words models
Part-based + Exhaustive search (baseline)
Our Bag-of-Words + Our selective search
Figure 7. Object recognition results on the PASCAL VOC 2007 test set. For the left plot, object models are trained using the part-based
Felzenszwalb system, which uses exhaustive search by default. For the right plot, object models are trained using more expensive bag-of-words features and classifiers; exhaustive search is not feasible with these models.
5.4. Exp. 4: Object Recognition Accuracy
In this experiment, we evaluate our object hypotheses on a widely accepted part-based object recognition method and inside the object recognition system described in Section 4. The latter is compared to the state-of-the-art on the challenging Pascal VOC 2010 detection task.
Search strategies using part-based models. We compare various search strategies on the method of Felzenszwalb. We consider the exhaustive search of to be our baseline. We use our selective search boxes as a filter on the output of, as facilitated by their code, where we discard all locations whose Pascal Overlap is smaller than
0.8. In practice this reduces the number of considered windows from around 100,000 per image per class to around
5,000. Results are shown on the left in Figure 7. Overall using our boxes as a filter reduces Mean Average Precision from 0.323 MAP to 0.296 MAP, 0.03 MAP less while evaluating 20 times fewer boxes. Note that for some concepts like aeroplane, dog, dining table, and sheep there is even a slight improvement, suggesting a trade-off between high recall and precision for object detection accuracy.
If we use all 10,000 boxes of in the same manner on, the MAP reduces to 0.215. But in they have an additional hill-climbing step which enables them to consider only 2,000 windows at the expense of 0.04 MAP. This suggest that a hill-climbing step as suggested by could improve results further when using our boxes.
Part-based HOG versus bag-of-words. A major advantage of selective search is that it enables the use of more expensive features and classifiers. To evaluate the potential of better features and classifiers, we compare the bag-ofwords recognition pipeline described in Section 4 with the baseline of which uses HOG and linear classifiers. Results on the right in Figure 7 show improvements for 10 out of 20 object categories. Especially significant are the improvements the object categories cat, cow, dog, sheep, diningtable, and aeroplane, which we improve with 11% to 20%. Except aeroplane, these object categories all have flexible shape on which bag-of-words is expected to work well (Section 4). The baseline achieves a higher accuracy for object categories with rigid shape characteristics such as bicycle, car, bottle, person and chair. If we select the best method for each class, instead of a MAP of 0.323 of the baseline, we get a MAP of 0.378, a significant, absolute improvement of 5% MAP.
To check whether the differences on the right in Figure 7 originate mainly from the different features, we combined bag-of-words features with the exhaustive search of for the concepts cat and car.
With cat, bag-of-words gives
0.392 AP for selective and 0.375 AP for exhaustive search, compared to 0.193 AP for part-based HOG features. With car, bag-of-words gives 0.547 for selective and 0.535 for exhaustive search, and 0.579 for part-based HOG features.
Comparison to the state-of-the-art. To compare our results to the current state-of-the-art in object recognition, we have submitted our bag-of-words models for the Pascal
VOC 2010 detection task to the official evaluation server.
Results are shown in Table 4, together with the top-4 from the competition. In this independent evaluation, our system improves the state-of-the-art by up to 8.5% for 8 out of 20 object categories compared to all other competition entries.
In conclusion, our selective search yields good object locations for part-based models, as even without the hillclimbing step of we need to evaluate 20 times fewer windows at the expense of 0.03 MAP in average precision.
More importantly, our selective search enables the use of Experiment 4: Object Recogntion Accuracy on VOC2010 Test Set
System plane bike bird boat bottle bus car cat chair cow table dog horse motor person plant sheep sofa train tv
NLPR
MIT UCLA 
NUS
UoCTTI 
This paper
Table 4. Results from the Pascal VOC 2010 detection task test set, comparing the approach from this paper to the current state-of-the-art.
We improve the state-of-the-art up to 0.085 AP for 8 categories and equal the state-of-the-art for one more category. expensive features and classifiers which allow us to substantially improve the state-of-the-art for 8 out of 20 classes on the VOC2010 detection challenge.
6. Conclusions
In this paper, we have adopted segmentation as a selective search strategy for object recognition. For this purpose we prefer to generate many approximate locations over few and precise object delineations, as objects whose locations are not generated can never be recognised and appearance and immediate nearby context are effective for object recognition. Therefore our selective search uses locations at all scales. Furthermore, rather than using a single best segmentation algorithm, we have shown that for recognition it is prudent to use a set of complementary segmentations. In particular this paper accounts for different scene conditions such as shadows, shading, and highlights by employing a variety of invariant colour spaces. This results in a powerful selective search strategy that generates only 1,536 classindependent locations per image to capture 96.7% of all the objects in the Pascal VOC 2007 test set. This is the highest recall reported to date.
We show that segmentation as a selective search strategy is highly effective for object recognition: For the part-based system of the number of considered windows can be reduced by 20 times at a loss of 3% MAP overall. More importantly, by capitalizing on the reduced number of locations we can do object recognition using a powerful yet expensive bag-of-words implementation and improve the state-of-the-art for 8 out of 20 classes for up to 8.5% in terms of Average Precision.
References
 B. Alexe, T. Deselaers, and V. Ferrari. What is an object?
In
CVPR, 2010. 2, 4, 6, 7
 P. Arbel´aez, M. Maire, C. Fowlkes, and J. Malik. Contour detection and hierarchical image segmentation. TPAMI, 2011. 1, 3
 J. Carreira and C. Sminchisescu. Constrained parametric min-cuts for automatic object segmentation. In CVPR, 2010. 1, 2, 3, 4, 6
 O. Chum and A. Zisserman. An exemplar model for learning object classes. In CVPR, 2007. 2
 G. Csurka, C. R. Dance, L. Fan, J. Willamowski, and C. Bray.
Visual categorization with bags of keypoints. In ECCV Statistical
Learning in Computer Vision, 2004. 1, 4
 N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In CVPR, 2005. 1, 2, 4
 I. Endres and D. Hoiem. Category independent object proposals.
In ECCV, 2010. 1, 2, 3, 4, 6
 M. Everingham, L. van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The pascal visual object classes (voc) challenge.
IJCV, 88:303–338, 2010. 1, 3, 4, 5
 P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan. Object detection with discriminatively trained part based models. TPAMI, 32:1627–1645, 2010. 1, 2, 4, 7, 8
 P. F. Felzenszwalb and D. P. Huttenlocher. Efficient Graph-Based
Image Segmentation. IJCV, 59:167–181, 2004. 3, 4, 5
 T. Gevers and H. M. G. Stokman. Classification of color edges in video into shadow-geometry, highlight, or material transitions.
TMM, 5(2):237–243, 2003. 4
 C. Gu, J. J. Lim, P. Arbel´aez, and J. Malik. Recognition using regions. In CVPR, 2009. 3
 H. Harzallah, F. Jurie, and C. Schmid. Combining efficient object localization and image classification. In ICCV, 2009. 1, 2, 4, 5, 6
 D. Hoiem, A. A. Efros, and M. Hebert. Recovering surface layout from an image. IJCV, 2007. 2
 C. H. Lampert, M. B. Blaschko, and T. Hofmann. Efficient subwindow search: A branch and bound framework for object localization. TPAMI, 31:2129–2142, 2009. 2, 4
 S. Lazebnik, C. Schmid, and J. Ponce. Beyond bags of features:
Spatial pyramid matching for recognizing natural scene categories.
In CVPR, 2006. 4
 F. Li, J. Carreira, and C. Sminchisescu. Object recognition as ranking holistic figure-ground hypotheses. In CVPR, 2010. 3
 D. G. Lowe. Distinctive image features from scale-invariant keypoints. IJCV, 60:91–110, 2004. 4
 S. Maji, A. C. Berg, and J. Malik. Classification using intersection kernel support vector machines is efficient. In CVPR, 2008. 4
 S. Maji and J. Malik. Object detection using a max-margin hough transform. In CVPR, 2009. 2
 T. Malisiewicz and A. A. Efros.
Improving spatial support for objects via multiple segmentations. In BMVC, 2007. 2
 B. Russell, A. Efros, J. Sivic, W. Freeman, and A. Zisserman. Using multiple segmentations to discover objects and their extent in image collections. In CVPR, 2006. 2
 J. Sivic and A. Zisserman. Video google: A text retrieval approach to object matching in videos. In ICCV, 2003. 1, 4
 S. Sonnenburg, G. Raetsch, S. Henschel, C. Widmer, J. Behr, A. Zien, F. de Bona, A. Binder, C. Gehl, and V. Franc. The shogun machine learning toolbox. JMLR, 11:1799–1802, 2010. 4
 J. R. R. Uijlings, A. W. M. Smeulders, and R. J. H. Scha. What is the spatial extent of an object? In CVPR, 2009. 1
 K. E. A. van de Sande, T. Gevers, and C. G. M. Snoek. Evaluating color descriptors for object and scene recognition. TPAMI, 32:1582–1596, 2010. 1, 4
 A. Vedaldi, V. Gulshan, M. Varma, and A. Zisserman. Multiple kernels for object detection. In ICCV, 2009. 1, 2, 4, 5, 6
 P. Viola and M. J. Jones. Robust real-time face detection. IJCV, 57:137–154, 2004. 1, 2
 L. Zhu, Y. Chen, A. Yuille, and W. Freeman. Latent hierarchical structural learning for object detection. In CVPR, 2010. 1, 4, 8Simultaneous Detection and Segmentation
Bharath Hariharan1, Pablo Arbel´aez1,2, Ross Girshick1, and Jitendra Malik1
1 University of California, Berkeley
2 Universidad de los Andes, Colombia
{bharath2,arbelaez,rbg,malik}@eecs.berkeley.edu
Abstract. We aim to detect all instances of a category in an image and, for each instance, mark the pixels that belong to it. We call this task Simultaneous Detection and Segmentation (SDS). Unlike classical bounding box detection, SDS requires a segmentation and not just a box. Unlike classical semantic segmentation, we require individual object instances.
We build on recent work that uses convolutional neural networks to classify category-independent region proposals (R-CNN ), introducing a novel architecture tailored for SDS. We then use category-specific, topdown figure-ground predictions to refine our bottom-up proposals. We show a 7 point boost (16% relative) over our baselines on SDS, a 5 point boost (10% relative) over state-of-the-art on semantic segmentation, and state-of-the-art performance in object detection. Finally, we provide diagnostic tools that unpack performance and provide directions for future work.
Keywords: detection, segmentation, convolutional networks.
Introduction
Object recognition comes in many flavors, two of the most popular being object detection and semantic segmentation. Starting with face detection, the task in object detection is to mark out bounding boxes around each object of a particular category in an image. In this task, a predicted bounding box is considered a true positive if it overlaps by more than 50% with a ground truth box, and different algorithms are compared based on their precision and recall. Object detection systems strive to find every instance of the category and estimate the spatial extent of each. However, the detected objects are very coarsely localized using just bounding boxes.
In contrast, semantic segmentation requires one to assign a category label to all pixels in an image. The MSRC dataset was one of the first publicly available benchmarks geared towards this task. Later, the standard metric used to evaluate algorithms in this task converged on pixel IU (intersection over union): for each category, this metric computes the intersection over union of the predicted pixels and ground truth pixels over the entire dataset. This task deals with "stuff" categories (such as grass, sky, road) and "thing" categories (such as cow, person, car) interchangeably. For things, this means that there is no notion
D. Fleet et al. (Eds.): ECCV 2014, Part VII, LNCS 8695, pp. 297–312, 2014. c
⃝ Springer International Publishing Switzerland 2014
B. Hariharan et al. of object instances. A typical semantic segmentation algorithm might accurately mark out the dog pixels in the image, but would provide no indication of how many dogs there are, or of the precise spatial extent of any one particular dog.
These two tasks have continued to this day and were part of the PASCAL
VOC challenge. Although often treated as separate problems, we believe the distinction between them is artificial. For the "thing" categories, we can think of a unified task: detect all instances of a category in an image and, for each instance, correctly mark the pixels that belong to it. Compared to the bounding boxes output by an object detection system or the pixel-level category labels output by a semantic segmentation system, this task demands a richer, and potentially more useful, output. Our aim in this paper is to improve performance on this task, which we call Simultaneous Detection and Segmentation(SDS).
The SDS algorithm we propose has the following steps (Figure 1):
1. Proposal Generation: We start with category-independent bottom-up object proposals. Because we are interested in producing segmentations and not just bounding boxes, we need region proposals. We use MCG to generate
2000 region candidates per image. We consider each region candidate as a putative object hypothesis.
2. Feature Extraction: We use a convolutional neural network to extract features on each region. We extract features from both the bounding box of the region as well as from the region foreground. This follows work by Girshick et al. (R-CNN) who achieved competitive semantic segmentation results and dramatically improved the state-of-the-art in object detection by using
CNNs to classify region proposals. We consider several ways of training the CNNs. We find that, compared to using the same CNN for both inputs (image windows and region masks), using separate networks where each network is finetuned for its respective role dramatically improves performance. We improve performance further by training both networks jointly, resulting in a feature extractor that is trained end-to-end for the SDS task.
3. Region Classification: We train an SVM on top of the CNN features to assign a score for each category to each candidate.
4. Region Refinement: We do non-maximum suppression (NMS) on the scored candidates. Then we use the features from the CNN to produce category-specific coarse mask predictions to refine the surviving candidates.
Combining this mask with the original region candidates provides a further boost.
Since this task is not a standard one, we need to decide on evaluation metrics.
The metric we suggest in this paper is an extension to the bounding box detection metric. It has been proposed earlier. Given an image, we expect the algorithm to produce a set of object hypotheses, where each hypothesis comes with a predicted segmentation and a score. A hypothesis is correct if its segmentation overlaps with the segmentation of a ground truth instance by more than
50%. As in the classical bounding box task, we penalize duplicates. With this labeling, we compute a precision recall (PR) curve, and the average precision
Simultaneous Detection and Segmentation(AP), which is the area under the curve. We call the AP computed in this way
APr, to distinguish it from the traditional bounding box AP, which we call APb(the superscripts r and b correspond to region and bounding box respectively).
APr measures the accuracy of segmentation, and also requires the algorithm to get each instance separately and completely. Our pipeline achieves an APr of 49.5% while at the same time improving APb from 51.0% (R-CNN) to 53.0%.
One can argue that the 50% threshold is itself artificial. For instance if we want to count the number of people in a crowd, we do not need to know their accurate segmentations. On the contrary, in a graphics application that seeks to matte an object into a scene, we might want extremely accurate segmentations.
Thus the threshold at which we regard a detection as a true positive depends on the application. In general, we want algorithms that do well under a variety of thresholds. As the threshold varies, the PR curve traces out a PR surface. We can use the volume under this PR surface as a metric. We call this metric APr vol and APb vol respectively. APr vol has the attractive property that an APr vol of 1 implies we can perfectly detect and precisely segment all objects. Our pipeline gets an APr vol of 41.4%. We improve APb vol from 41.9% (R-CNN) to 44.2%.
We also find that our pipeline furthers the state-of-the-art in the classic
PASCAL VOC semantic segmentation task, from 47.9% to 52.6%. Last but not the least, following work in object detection, we also provide a set of diagnostic tools for analyzing common error modes in the SDS task. Our algorithm, the benchmark and all diagnostic tools are publicly available at http://www.eecs.berkeley.edu/Research/Projects/CS/vision/shape/sds.
����
���
�������
���
��������
����
���������
����������
��������
����������
�������
��������������
�������
����������
Fig. 1. Overview of our pipeline. Our algorithm is based on classifying region proposals using features extracted from both the bounding box of the region and the region foreground with a jointly trained CNN. A final refinement step improves segmentation.
Related Work
For semantic segmentation, several researchers have tried to use activations from off-the-shelf object detectors to guide the segmentation process. Yang et al. use object detections from the deformable parts model to segment the image, pasting figure-ground masks and reasoning about their relative depth ordering.
B. Hariharan et al.
Arbel´aez et al. use poselet detections as features to score region candidates, in addition to appearance-based cues. Ladicky et al. use object detections as higher order potentials in a CRF-based segmentation system: all pixels in the foreground of a detected object are encouraged to share the category label of the detection. In addition, their system is allowed to switch off these potentials by assigning a true/false label to each detection. This system was extended by Boix et al. who added a global, image-level node in the CRF to reason about the categories present in the image, and by Kim et al. who added relationships between objects. In more recent work, Tighe et al. use exemplar object detectors to segment out the scene as well as individual instances.
There has also been work on localizing detections better using segmentation.
Parkhi et al. use color models from predefined rectangles on cat and dog faces to do GrabCut and improve the predicted bounding box. Dai and Hoiem generalize this to all categories and use instance and category appearance models to improve detection. These approaches do well when the objects are coherent in color or texture. This is not true of many categories such as people, where each object can be made of multiple regions of different appearance. An alternative to doing segmentation post facto is to use segmentation to generate object proposals which are then classified. The proposals may be used as just bounding boxes or as region proposals. These proposals incorporate both the consistency of appearance in an object as well as the possibility of having multiple disparate regions for each object. State-of-the-art detection systems and segmentation systems are now based on these methods.
In many of these approaches, segmentation is used only to localize the detections better. Other authors have explored using segmentation as a stronger cue. Fidler et al. use the output of a state-of-the-art semantic segmentation approach to score detections better. Mottaghi uses detectors based on non-rectangular patches to both detect and segment objects.
The approaches above were typically built on features such as SIFT or HOG. Recently the computer vision community has shifted towards using convolutional neural networks (CNNs). CNNs have their roots in the Neocognitron proposed by Fukushima. Trained with the back-propagation algorithm, LeCun showed that they could be used for handwritten zip code recognition. They have since been used in a variety of tasks, including detection and semantic segmentation. Krizhevsky et al. showed a large increase in performance by using CNNs for classification in the ILSVRC challenge.
Donahue et al. showed that Krizhevsky's architecture could be used as a generic feature extractor that did well across a wide variety of tasks. Girshick et al. build on this and finetune Krizhevsky's architecture for detection to nearly double the state-of-the-art performance. They use a simple pipeline, using
CNNs to classify bounding box proposals from. Our algorithm builds on this system, and on high quality region proposals from.
Simultaneous Detection and Segmentation
Our Approach
Proposal Generation
A large number of methods to generate proposals have been proposed in the literature. The methods differ on the type of outputs they produce (boxes vs segments) and the metrics they do well on. Since we are interested in the APr metric, we care about segments, and not just boxes. Keeping our task in mind, we use candidates from MCG for this paper. This approach significantly outperforms all competing approaches on the object level Jaccard index metric, which measures the average best overlap achieved by a candidate for a ground truth object. In our experiments we find that simply switching to MCG from
Selective Search improves APb slightly (by 0.7 points), justifying this choice.
We use the proposals from MCG as is. MCG starts by computing a segmentation hierarchy at multiple image resolutions, which are then fused into a single multiscale hierarchy at the finest scale. Then candidates are produced by combinatorially grouping regions from all the single scale hierarchies and from the multiscale hierarchy. The candidates are ranked based on simple features such as size and location, shape and contour strength.
Feature Extraction
We start from the R-CNN object detector proposed by Girshick et al. and adapt it to the SDS task. Girshick et al. train a CNN on ImageNet Classification and then finetune the network on the PASCAL detection set. For finetuning they took bounding boxes from Selective Search, padded them, cropped them and warped them to a square and fed them to the network. Bounding boxes that overlap with the ground truth by more than 50% were taken as positives and other boxes as negatives. The class label for each positive box was taken to be the class of the ground truth box that overlaps the most with the box.
The network thus learned to predict if the bounding box overlaps highly with a ground truth bounding box. We are working with MCG instead of Selective
Search, so we train a similar object detection network, finetuned using bounding boxes of MCG regions instead of Selective Search boxes.
At test time, to extract features from a bounding box, Girshick et al. pad and crop the box, warp it to a square and pass it through the network, and extract features from one of the later layers, which is then fed into an SVM. In this paper we will use the penultimate fully connected layer.
For the SDS task, we can now use this network finetuned for detection to extract feature vectors from MCG bounding boxes. However these feature vectors do not contain any information about the actual region foreground, and so will be ill-equipped to decide if the region overlaps highly with a ground truth segmentation or not. To get around this, we start with the idea used by Girshick et al. for their experiment on semantic segmentation: we extract a second set of features from the region by feeding it the cropped, warped box, but with
B. Hariharan et al. the background of the region masked out (with the mean image.) Concatenating these two feature vectors together gives us the feature vector we use. (In their experiments Girshick et al. found both sets of features to be useful.) This method of extracting features out of the region is the simplest way of extending the object detection system to the SDS task and forms our baseline. We call this feature extractor A.
The network we are using above has been finetuned to classify bounding boxes, so its use in extracting features from the region foreground is suboptimal. Several neurons in the network may be focussing on context in the background, which will be unavailable when the network is fed the region foreground. This suggests that we should use a different network to extract the second set of features: one that is finetuned on the kinds of inputs that it is going to see. We therefore finetune another network (starting again from the net trained on ImageNet) which is fed as input cropped, padded bounding boxes of MCG regions with the background masked out. Because this region sees the actual foreground, we can actually train it to predict region overlap instead, which is what we care about. Therefore we change the labeling of the MCG regions to be based on segmentation overlap of the region with a ground truth region (instead of overlap with bounding box). We call this feature extractor B.
The previous strategy is still suboptimal, because the two networks have been trained in isolation, while at test time the two feature sets are going to be combined and fed to the classifier. This suggests that one should train the networks jointly. We formalize this intuition as follows. We create a neural network with the architecture shown in Figure 2. This architecture is a single network with two pathways. The first pathway operates on the cropped bounding box of the region (the "box" pathway) while the second pathway operates on the cropped bounding box with the background masked (the "region" pathway). The two pathways are disjoint except at the very final classifier layer, which concatenates the features from both pathways. Both these pathways individually have the same architecture as that of Krizhevsky et al. Note that both A and B can be seen as instantiations of this architecture, but with different sets of weights.
A uses the same network parameters for both pathways. For B, the box pathway gets its weights from a network finetuned separately using bounding box overlap, while the region pathway gets its parameters from a network finetuned separately using region overlap.
Instead of using the same network in both pathways or training the two pathways in isolation, we now propose to train it as a whole directly. We use segmentation overlap as above. We initialize the box pathway with the network finetuned on boxes and the region pathway with the network finetuned on regions, and then finetune the entire network. At test time, we discard the final classification layer and use the output of the penultimate layer, which concatenates the features from the two pathways. We call this feature extractor C.
Simultaneous Detection and Segmentation
�������
�������
�������
�������
�������
�����
�����
�������
�������
�������
�������
�������
�����
�����
�������
����������
Fig. 2. Left: The region with its bounding box. Right: The architecture that we train for C. The top pathway operates on cropped boxes and the bottom pathway operates on region foregrounds.
Region Classification
We use the features from the previous step to train a linear SVM. We first train an initial SVM using ground truth as positives and regions overlapping ground truth by less than 20% as negative. Then we re-estimate the positive set: for each ground truth we pick the highest scoring MCG candidate that overlaps by more than 50%. Ground truth regions for which no such candidate exists (very few in number) are discarded. We then retrain the classifier using this new positive set. This training procedure corresponds to a multiple instance learning problem where each ground truth defines a positive bag of regions that overlap with it by more than 50%, and each negative region is its own bag. We found this training to work better than using just the ground truth as positives.
At test time we use the region classifiers to score each region. Because there may be multiple overlapping regions, we do a strict non-max suppression using a region overlap threshold of 0. This is because while the bounding box of two objects can in fact overlap, their pixel support in the image typically shouldn't.
Post NMS, we work with only the top 20,000 detections for each category (over the whole dataset) and discard the rest for computational reasons. We confirmed that this reduction in detections has no effect on the APr metric.
Region Refinement
We take each of the remaining regions and refine its support. This is necessary because our region candidates have been created by a purely bottom-up, class agnostic process. Since the candidate generation has not made use of categoryspecific shape information, it is prone to both undershooting (i.e. missing some part of the object) and overshooting (i.e. including extraneous stuff).
We first learn to predict a coarse, top-down figure-ground mask for each region. To do this, we take the bounding box of each predicted region, pad it as for feature extraction, and then discretize the resulting box into a 10 × 10 grid. For each grid cell we train a logistic regression classifier to predict the probability that the grid cell belongs to the foreground. The features we use are the features extracted from the CNN, together with the figure-ground mask of the region
B. Hariharan et al.
Fig. 3. Some examples of region refinement. We show in order the image, the original region, the coarse 10 × 10 mask, the coarse mask projected to superpixels, the output of the final classifier on superpixels and the final region after thresholding. Refinement uses top-down category specific information to fill in the body of the train and the cat and remove the road from the car. discretized to the same 10 × 10 grid. The classifiers are trained on regions from the training set that overlap by more than 70% with a ground truth region.
This coarse figure-ground mask makes a top-down prediction about the shape of the object but does not necessarily respect the bottom-up contours. In addition, because of its coarse nature it cannot do a good job of modeling thin structures like aircraft wings or structures that move around. This information needs to come from the bottom-up region candidate. Hence we train a second stage to combine this coarse mask with the region candidate. We project the coarse mask to superpixels by assigning to each superpixel the average value of the coarse mask in the superpixel. Then we classify each superpixel, using as features this projected value in the superpixel and a 0 or 1 encoding if the superpixel belongs to the original region candidate. Figure 3 illustrates this refinement.
Experiments and Results
We use the segmentation annotations from SBD to train and evaluate. We train all systems on PASCAL VOC 2012 train. For all training and finetuning of the network we use the recently released Caffe framework.
Results on APr and APr vol
Table 1 and Table 2 show results on the APr and the APr vol metrics respectively on PASCAL VOC 2012 val (ground truth segmentations are not available for test). We compute APr vol by averaging the APr obtained for 9 thresholds.
1. O2P uses features and regions from Carreira et al., which is the state-ofthe-art in semantic segmentation. We train region classifiers on these features and do NMS to get detections. This baseline gets a mean APr of 25.2% and a mean APr vol of 23.4%.
Simultaneous Detection and Segmentation
2. A is our most naive feature extractor. It uses MCG candidates and features from the bounding box and region foreground, using a single CNN finetuned using box overlaps. It achieves a mean APr of 42.9% and a mean APr vol of 37.0%, a large jump over O2P. This mirrors gains in object detection observed by Girshick et al., although since O2P is not designed for this task the comparison is somewhat unfair.
3. B is the result of finetuning a separate network exclusively on region foregrounds with labels defined by region overlap. This gives a large jump of the APr metric (of about 4 percentage points) and a smaller but significant jump on the APr vol metric of about 2.5 percentage points.
4. C is the result of training a single large network with two pathways. There is a clear gain over using two isolated networks: on both metrics we gain about 0.7 percentage points.
5. C+ref is the result of refining the masks of the regions obtained from C.
We again gain 2 points in the APr metric and 1.2 percentage points in the APr vol metric. This large jump indicates that while MCG candidates we start from are very high quality, there is still a lot to be gained from refining the regions in a category specific manner.
A paired sample t-test indicates that each of the above improvements are statistically significant at the 0.05 significance level.
The left part of Figure 5 plots the improvement in mean APr over A as we vary the threshold at which a detection is considered correct. Each of our improvements increases APr across all thresholds, indicating that we haven't overfit to a particular regime.
Clearly we get significant gains over both our naive baseline as well as O2P.
However, prior approaches that reason about segmentation together with detection might do better on the APr metric. To see if this is the case, we compare to the SegDPM work of Fidler et al.. SegDPM combined DPMs with
O2P and achieved a 9 point boost over DPMs in classical object detection.
For this method, only the bounding boxes are available publicly, and for some boxes the algorithm may choose not to have associated segments. We therefore compute an upper bound of its performance by taking each detection, considering all MCG regions whose bounding box overlaps with the detection by more than 70%, and selecting the region which best overlaps a ground truth.
Since SegDPM detections are only available on PASCAL VOC2010 val, we restrict our evaluations only to this set. Our upper bound on SegDPM has a mean APr of 31.3, whereas C+ref achieves a mean APr of 50.3.
Producing Diagnostic Information
Inspired by, we created tools for figuring out error modes and avenues for improvement for the SDS task. As in, we evaluate the impact of error modes by measuring the improvement in APr if the error mode was corrected. For localization, we assign labels to detections under two thresholds: the usual strict
B. Hariharan et al.
Table 1. Results on APr on VOC2012 val. All numbers are %.
O2P
A
B
C
C+ref aeroplane
68.4 bicycle
49.4 bird
52.1 boat
32.8 bottle
33.0 bus
67.8 car
53.6 cat
73.9 chair
19.9 cow
43.7 diningtable
25.7 dog
60.6 horse
55.9 motorbike
58.9 person
56.7 pottedplant
28.5 sheep
55.6 sofa
32.1 train
64.7 tvmonitor
Mean
Table 2. Results on APr vol on VOC2012 val. All numbers are %.
O2P
A
B
C
C+ref aeroplane
52.3 bicycle
42.6 bird
42.2 boat
28.6 bottle
28.6 bus
58.0 car
45.4 cat
58.9 chair
19.7 cow
37.1 diningtable
22.8 dog
49.5 horse
42.9 motorbike
45.9 person
48.5 pottedplant
25.5 sheep
44.5 sofa
30.2 train
52.6 tvmonitor
Mean
Simultaneous Detection and Segmentation
307 threshold of 0.5 and a more lenient threshold of 0.1 (note that this is a threshold on region overlap). Detections that count as true positives under the lenient threshold but as false positives under the strict threshold are considered mislocalizations. Duplicate detections are also considered mislocalizations. We then consider the performance if either a) all mislocalized instances were removed, or b) all mislocalized instances were correctly localized and duplicates removed.
Figure 4 shows how the PR curve for the APr benchmark changes if mislocalizations are corrected or removed for two categories. For the person category, removing mislocalizations brings precision up to essentially 100%, indicating that mislocalization is the predominant source of false positives. Correcting the mislocalizations provides a huge jump in recall. For the cat category the improvement provided by better localization is much less, indicating that there are still some false positives arising from misclassifications.
We can do this analysis for all categories. The average improvement in APr by fixing mislocalization is a measure of the impact of mislocalization on performance. We can also measure impact in this way for other error modes: for instance, false positives on objects of other similar categories, or on background.(For defining similar and non-similar categories, we divide object categories into
"animals", "transport" and "indoor" groups.) The left subfigure in Figure 6 shows the result of such an analysis on our best system (C+ref). The dark blue bar shows the APr improvement if we remove mislocalized detections and the light blue bar shows the improvement if we correct them. The other two bars show the improvement from removing confusion with similar categories and background. Mislocalization has a huge impact: it sets us back by about
16 percentage points. Compared to that confusion with similar categories or background is virtually non-existent.
We can measure the impact of mislocalization on the other algorithms in Table 1 as well, as shown in Table 3. It also shows the upper bound APr achievable when all mislocalization is fixed. Improvements in the feature extractor improve the upper bound (indicating fewer misclassifications) but also reduce the gap due to mislocalization (indicating better localization). Refinement doesn't change the upper bound and only improves localization, as expected.
To get a better handle on what one needs to do to improve localization, we considered two statistics. For each detection and a ground truth, instead of just taking the overlap (i.e. intersection over union), we can compute the pixel precision (fraction of the region that lies inside the ground truth) and pixel recall(fraction of the ground truth that lies inside the region). It can be shown that having both a pixel precision > 67% and a pixel recall > 67% is guaranteed to give an overlap of greater than 50%. We assign detection labels using pixel precision or pixel recall using a threshold of 67% and compute the respective
AP. Comparing these two numbers then gives us a window into the kind of localization errors: a low pixel precision AP indicates that the error mode is overshooting the region and predicting extraneous background pixels, while a low pixel recall AP indicates that the error mode is undershooting the region and missing out some ground truth pixels.
B. Hariharan et al.
The second half of Figure 6 shows the difference between pixel precision AP(APpp) and pixel recall AP (APpr). Bars to the left indicate higher pixel recall
AP, while bars to the right indicate higher pixel precision AP. For some categories such as person and bird we tend to miss ground truth pixels, whereas for others such as bicycle we tend to leak into the background.
Recall
Precision
C+ref
No misloc
Corr misloc
Recall
Precision
C+ref
No misloc
Corr misloc
Fig. 4. PR on person(left) and cat(right). Blue is C+ref. Green is if an oracle removes mislocalized predictions, and red is if the oracle corrects our mislocalizations.
−1
Overlap Threshold
Change in APr (percentage points)
B
C
C+ref
−4
−2
Overlap Threshold
Change in APb (percentage points)
R−CNN−MCG
A
B
C
Fig. 5. Left: Improvement in mean APr over A due to our 3 variants for a variety of overlap thresholds. We get improvements for all overlap thresholds. Right: A similar plot for APb. Improvements are relative to R-CNN with Selective Search proposals.
As the threshold becomes stricter, the better localization of our approach is apparent.
Results on APb and APb vol
Comparison with prior work is easier on the classical bounding box and segmentation metrics. It also helps us evaluate if handling the SDS task also improves performance on the individual tasks. To compare on APb, we retrain our final region classifiers for the bounding box detection task. This is because the ranking of regions based on bounding box overlap is different from that based on
Simultaneous Detection and Segmentation
B
S
L
Improvement in APr (percentage points)
−0.4
−0.2
0.4 aeroplane bicycle bird boat bottle bus car cat chair cow diningtable dog horse motorbike person pottedplant sheep sofa train tvmonitor
APpp−APpr
Fig. 6. Left: Impact of the three kinds of false positives on mean APr. L : mislocalization, B : detection on background, and S : misfirings on similar categories. Right:
Disambiguating between two kinds of mislocalizations. Bars to the left mean that we frequently overshoot the ground truth, while bars to the right mean that we undershoot.
Table 3. Maximum achievable APr (assuming perfect localization) and loss in APr due to mislocalization for all systems
A
B
C
C+ref
AP Upper bound
Loss due to mislocalization
15.8 segmentation overlap. As in, we use ground truth boxes as positive, and MCG boxes overlapping by less than 50% as negative. At test time we do not do any region refinement.
We add two baselines: R-CNN is the system of Girshick et al. taken as is, and R-CNN-MCG is R-CNN on boxes from MCG instead of Selective Search. Note that neither of these baselines uses features from the region foreground.
Table 4 shows the mean APb and APb vol. We get improvements over R-CNN on both APb and APb vol, with improvements on the latter metric being somewhat larger. The right half of Figure 5 shows the variation in APb as we vary the overlap threshold for counting something as correct. We plot the improvement in APb over vanilla R-CNN. We do worse than R-CNN for low thresholds, but are much better for higher thresholds. This is also true to some extent for RCNN-MCG, so this is partly a property of MCG, and partly a consequence of our algorithm's improved localization. Interestingly, C does worse than B. We posit that this is because now the entire network has been finetuned for SDS.
Finally we evaluated C on PASCAL VOC 2012 test. Our mean APb of 50.7 is an improvement over the R-CNN mean APb of 49.6 (both without bounding box regression), and much better than other systems, such as SegDPM (40.7).
B. Hariharan et al.
Table 4. Results on APb and APb vol on VOC12 val. All numbers are %.
R-CNN R-CNN-MCG
A
B
C
Mean APb
Mean APb vol
Results on Pixel IU
For the semantic segmentation task, we convert the output of our final system(C+ref) into a pixel-level category labeling using the simple pasting scheme proposed by Carreira et al.. We cross validate the hyperparameters of this pasting step on the VOC11 segmentation Val set. The results are in Table 5. We compare to O2P and R-CNN which are the current state-of-the-art on this task. We advance the state-of-the-art by about 5 points, or 10% relative.
To conclude, our pipeline achieves good results on the SDS task while improving state-of-the-art in object detection and semantic segmentation. Figure 7 shows examples of the output of our system.
Table 5. Results on Pixel IU. All numbers are %.
O2P R-CNN C+ref
Mean Pixel IU (VOC2011 Test)
Mean Pixel IU (VOC2012 Test)Fig. 7. Top detections: 3 persons, 2 bikes, diningtable, sheep, chair, cat. We can handle uncommon pose and clutter and are able to resolve individual instances.
Acknowledgments. This work was supported by ONR MURI N000141010933, a Google Research Grant and a Microsoft Research fellowship. We thank the NVIDIA Corporation for providing GPUs through their academic program.
Simultaneous Detection and Segmentation
References
1. Arbel´aez, P., Pont-Tuset, J., Barron, J., Marques, F., Malik, J.: Multiscale combinatorial grouping. In: CVPR (2014)
2. Arbel´aez, P., Hariharan, B., Gu, C., Gupta, S., Malik, J.: Semantic segmentation using regions and parts. In: CVPR (2012)
3. Boix, X., Gonfaus, J.M., van de Weijer, J., Bagdanov, A.D., Serrat, J., Gonz`alez, J.: Harmony potentials. IJCV 96(1) (2012)
4. Bourdev, L., Maji, S., Brox, T., Malik, J.: Detecting people using mutually consistent poselet activations. In: Daniilidis, K., Maragos, P., Paragios, N. (eds.) ECCV 2010, Part VI. LNCS, vol. 6316, pp. 168–181. Springer, Heidelberg (2010)
5. Carreira, J., Caseiro, R., Batista, J., Sminchisescu, C.: Semantic segmentation with second-order pooling. In: Fitzgibbon, A., Lazebnik, S., Perona, P., Sato, Y., Schmid, C. (eds.) ECCV 2012, Part VII. LNCS, vol. 7578, pp. 430–443. Springer, Heidelberg (2012)
6. Carreira, J., Sminchisescu, C.: Constrained parametric min-cuts for automatic object segmentation. In: CVPR (2010)
7. Dai, Q., Hoiem, D.: Learning to localize detected objects. In: CVPR (2012)
8. Dalal, N., Triggs, B.: Histograms of oriented gradients for human detection. In:
CVPR (2005)
9. Deng, J., Berg, A., Satheesh, S., Su, H., Khosla, A., Fei-Fei, L.: ImageNet Large
Scale Visual Recognition Competition 2012 (ILSVRC 2012) (2012), http://www.image-net.org/challenges/LSVRC/2012/
10. Donahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E., Darrell, T.:
Decaf: A deep convolutional activation feature for generic visual recognition. arXiv preprint arXiv:1310.1531 (2013)
11. Everingham, M., Van Gool, L., Williams, C.K.I., Winn, J., Zisserman, A.: The Pascal Visual Object Classes (VOC) Challenge. IJCV 88(2) (2010)
12. Farabet, C., Couprie, C., Najman, L., LeCun, Y.: Learning hierarchical features for scene labeling. TPAMI 35(8) (2013)
13. Felzenszwalb, P.F., Girshick, R.B., McAllester, D., Ramanan, D.: Object detection with discriminatively trained part-based models. TPAMI 32(9) (2010)
14. Fidler, S., Mottaghi, R., Yuille, A., Urtasun, R.: Bottom-up segmentation for topdown detection. In: CVPR (2013)
15. Fukushima, K.: Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. Biological Cybernetics
16. Girshick, R., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for accurate object detection and semantic segmentation. In: CVPR (2014)
17. Hariharan, B., Arbelaez, P., Bourdev, L., Maji, S., Malik, J.: Semantic contours from inverse detectors. In: ICCV (2011)
18. Hoiem, D., Chodpathumwan, Y., Dai, Q.: Diagnosing error in object detectors. In:
Fitzgibbon, A., Lazebnik, S., Perona, P., Sato, Y., Schmid, C. (eds.) ECCV 2012, Part III. LNCS, vol. 7574, pp. 340–353. Springer, Heidelberg (2012)
19. Jia, Y.: Caffe: An open source convolutional architecture for fast feature embedding(2013), http://caffe.berkeleyvision.org/
20. Kim, B.-S., Sun, M., Kohli, P., Savarese, S.: Relating things and stuff by high-order potential modeling. In: Fusiello, A., Murino, V., Cucchiara, R. (eds.) ECCV 2012
Ws/Demos, Part III. LNCS, vol. 7585, pp. 293–304. Springer, Heidelberg (2012)
B. Hariharan et al.
21. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep convolutional neural networks. In: NIPS (2012)
22. Ladick´y, L., Sturgess, P., Alahari, K., Russell, C., Torr, P.H.S.: What, where and how many? Combining object detectors and CRFs. In: Daniilidis, K., Maragos, P., Paragios, N. (eds.) ECCV 2010, Part IV. LNCS, vol. 6314, pp. 424–437. Springer, Heidelberg (2010)
23. LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W., Jackel, L.D.: Backpropagation applied to handwritten zip code recognition. Neural
Computation 1(4) (1989)
24. Lowe, D.G.: Distinctive image features from scale-invariant keypoints. IJCV 60(2)
25. Mottaghi, R.: Augmenting deformable part models with irregular-shaped object patches. In: CVPR (2012)
26. Parkhi, O.M., Vedaldi, A., Jawahar, C., Zisserman, A.: The truth about cats and dogs. In: ICCV (2011)
27. van de Sande, K.E., Uijlings, J.R., Gevers, T., Smeulders, A.W.: Segmentation as selective search for object recognition. In: ICCV (2011)
28. Sermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., LeCun, Y.: Overfeat:
Integrated recognition, localization and detection using convolutional networks. In:
ICLR (2014)
29. Sermanet, P., Kavukcuoglu, K., Chintala, S., LeCun, Y.: Pedestrian detection with unsupervised multi-stage feature learning. In: CVPR (2013)
30. Shotton, J., Winn, J.M., Rother, C., Criminisi, A.: TextonBoost: Joint appearance, shape and context modeling for multi-class object recognition and segmentation.
In: Leonardis, A., Bischof, H., Pinz, A. (eds.) ECCV 2006, Part I. LNCS, vol. 3951, pp. 1–15. Springer, Heidelberg (2006)
31. Tighe, J., Niethammer, M., Lazebnik, S.: Scene parsing with object instances and occlusion handling. In: ECCV (2010)
32. Yang, Y., Hallman, S., Ramanan, D., Fowlkes, C.C.: Layered object models for image segmentation. TPAMI 34(9) (2012)