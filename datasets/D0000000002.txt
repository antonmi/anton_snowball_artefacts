Representation Learning: A Review and New
Perspectives
Yoshua Bengio, Aaron Courville, and Pascal Vincent
Department of computer science and operations research, U. Montreal
!
Abstract—
The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and joint training of deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep architectures. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.
Index Terms—Deep learning, representation learning, feature learning, unsupervised learning, Boltzmann Machine, RBM, auto-encoder, neural network
INTRODUCTION
The performance of machine learning methods is heavily dependent on the choice of data representation (or features) on which they are applied. For that reason, much of the actual effort in deploying machine learning algorithms goes into the design of preprocessing pipelines and data transformations that result in a representation of the data that can support effective machine learning. Such feature engineering is important but labor-intensive and highlights the weakness of current learning algorithms: their inability to extract and organize the discriminative information from the data. Feature engineering is a way to take advantage of human ingenuity and prior knowledge to compensate for that weakness. In order to expand the scope and ease of applicability of machine learning, it would be highly desirable to make learning algorithms less dependent on feature engineering, so that novel applications could be constructed faster, and more importantly, to make progress towards Artificial Intelligence (AI). An AI must fundamentally understand the world around us, and we argue that this can only be achieved if it can learn to identify and disentangle the underlying explanatory factors hidden in the observed milieu of low-level sensory data.
This paper is about feature learning, or representation learning, i.e., learning transformations of the data that make it easier to extract useful information when building classifiers or other predictors. In the case of probabilistic models, a good representation is often one that captures the posterior distribution of the underlying explanatory factors for the observed input.
Among the various ways of learning representations, this paper focuses on deep learning methods: those that are formed by the composition of multiple non-linear transformations of the data, with the goal of yielding more abstract – and ultimately more useful – representations. Here we survey this rapidly developing area with special emphasis on recent progress. We consider some of the fundamental questions that have been driving research in this area. Specifically, what makes one representation better than another? Given an example, how should we compute its representation, i.e. perform feature extraction? Also, what are appropriate objectives for learning good representations? In the course of dealing with these issues we review some of the most popular models in the field and place them in a context of the field as a whole.
WHY SHOULD WE CARE ABOUT LEARNING
REPRESENTATIONS?
Representation learning has become a field in itself in the machine learning community, with regular workshops at the leading conferences such as NIPS and ICML, sometimes under the header of Deep Learning or Feature Learning. Although depth is an important part of the story, many other priors are interesting and can be conveniently captured by a learner when the learning problem is cast as one of learning a representation, as discussed in the next section. The rapid increase in scientific activity on representation learning has been accompanied and nourished (in a virtuous circle) by a remarkable string of empirical successes both in academia and in industry. In this section, we briefly highlight some of these high points.
Speech Recognition and Signal Processing
Speech was one of the early applications of neural networks, in particular convolutional (or time-delay) neural networks 1.
The recent revival of interest in neural networks, deep learning, and representation learning has had a strong impact in the area of speech recognition, with breakthrough results (Dahl et al., 2010; Seide et al., 2011; Mohamed et al., 2012; Dahl et al., 2012) obtained by several academics as well as researchers at industrial labs taking over the task of bringing these algorithms to a larger scale and into products. For example, Microsoft has released in 2012 a new version of their MAVIS (Microsoft
Audio Video Indexing Service) speech system based on deep learning (Seide et al., 2011). These authors managed to reduce
1. See Bengio (1993) for a review of early work in this area. arXiv:1206.5538v2 [cs.LG] 18 Oct 2012
2 the word error rate on four major benchmarks by about 30%(e.g. from 27.4% to 18.5% on RT03S) compared to state-ofthe-art models based on Gaussian mixtures for the acoustic modeling and trained on the same amount of data (309 hours of speech). The relative improvement in error rate obtained by Dahl et al. (2012) on a smaller large-vocabulary speech recognition benchmark (Bing mobile business search dataset, with 40 hours of speech) is between 16% and 23%.
Representation-learning algorithms (based on recurrent neural networks) have also been applied to music, substantially beating the state-of-the-art in polyphonic transcription (Boulanger-Lewandowski et al., 2012), with a relative error improvement of between 5% and 30% on a standard benchmark of four different datasets.
Object Recognition
The beginnings of deep learning in 2006 have focused on the MNIST digit image classification problem (Hinton et al., 2006a; Bengio et al., 2007), breaking the supremacy of SVMs(1.4% error) on this dataset2. The latest records are still held by deep networks: Ciresan et al. (2012) currently claims the title of state-of-the-art for the unconstrained version of the task(e.g., using a convolutional architecture), with 0.27% error, and Rifai et al. (2011c) is state-of-the-art for the knowledgefree version of MNIST, with 0.81% error.
In the last few years, deep learning has moved from digits to object recognition in natural images, and the latest breakthrough has been achieved on the ImageNet dataset3 bringing down the state-of-the-art error rate from 26.1% to
15.3% (Krizhevsky et al., 2012).
Natural Language Processing
Besides speech recognition, there are many other Natural
Language Processing applications of representation learning algorithms. The idea of distributed representation for symbolic data was introduced by
Hinton (1986), and first developed in the context of statistical language modeling by Bengio et al. (2003)4. They are all based on learning a distributed representation for each word, also called a word embedding.
Combining this idea with a convolutional architecture, Collobert et al. (2011) developed the SENNA system5 that shares representations across the tasks of language modeling, part-ofspeech tagging, chunking, named entity recognition, semantic role labeling and syntactic parsing. SENNA approaches or surpasses the state-of-the-art on these tasks but is much faster than traditional predictors and requires only 3500 lines of C code to perform its predictions.
The neural net language model was also improved by adding recurrence to the hidden layers (Mikolov et al., 2011), allowing it to beat the state-of-the-art (smoothed n-gram models) not only in terms of perplexity (exponential of the average negative log-likelihood of predicting the right next word, going down from 140 to 102) but also in terms of 2. for the knowledge-free version of the task, where no image-specific prior is used, such as image deformations or convolutions
3. The 1000-class ImageNet benchmark, whose results are detailed here: http://www.image-net.org/challenges/LSVRC/2012/results.html
4. See this review of neural net language models (Bengio, 2008).
5. downloadable from http://ml.nec-labs.com/senna/ word error rate in speech recognition (since the language model is an important component of a speech recognition system), decreasing it from 17.2% (KN5 baseline) or 16.9%(discriminative language model) to 14.4% on the Wall Street
Journal benchmark task. Similar models have been applied in statistical machine translation (Schwenk et al., 2012), improving the BLEU score by almost 2 points. Recursive autoencoders (which generalize recurrent networks) have also been used to beat the state-of-the-art in full sentence paraphrase detection (Socher et al., 2011a) almost doubling the F1 score for paraphrase detection. Representation learning can also be used to perform word sense disambiguation (Bordes et al., 2012), bringing up the accuracy from 67.8% to 70.2% on the subset of Senseval-3 where the system could be applied(with subject-verb-object sentences). Finally, it has also been successfully used to surpass the state-of-the-art in sentiment analysis (Glorot et al., 2011b; Socher et al., 2011b).
Multi-Task and Transfer Learning, Domain Adaptation
Transfer learning is the ability of a learning algorithm to exploit commonalities between different learning tasks in order to share statistical strength, and transfer knowledge across tasks. As discussed below, we hypothesize that representation learning algorithms have an advantage for such tasks because they learn representations that capture underlying factors, a subset of which may be relevant for each particular task, as illustrated in Figure 1. This hypothesis seems confirmed by a number of empirical results showing the strengths of representation learning algorithms in transfer learning scenarios. raw input x task 1 output y1 task 3 output y3 task 2 output y2
Task%A%
Task%B%
Task%C%
%output%
%input%
%shared% subsets%of% factors%
Fig. 1.
Illustration of a representation-learning model which discovers explanatory factors (middle hidden layer, in red), some of which explain the input (semi-supervised setting), and some of which explain the target for each task. Because these subsets overlap, sharing of statistical strength allows gains in generalization.
Most impressive are the two transfer learning challenges held in 2011 and won by representation learning algorithms.
First, the Transfer Learning Challenge, presented at an ICML
2011 workshop of the same name, was won using unsupervised layer-wise pre-training (Bengio, 2011; Mesnil et al., 2011). A second Transfer Learning Challenge was held the same year and won by Goodfellow et al. (2011). Results were presented at NIPS 2011's Challenges in Learning Hierarchical
Models Workshop. Other examples of the successful application of representation learning in fields related to transfer
3 learning include domain adaptation, where the target remains the same but the input distribution changes (Glorot et al., 2011b; Chen et al., 2012). Of course, the case of jointly predicting outputs for many tasks or classes, i.e., performing multi-task learning also enhances the advantage of representation learning algorithms, e.g. as in Krizhevsky et al. (2012);
Collobert et al. (2011).
WHAT MAKES A REPRESENTATION GOOD?
Priors for Representation Learning in AI
In Bengio and LeCun (2007), one of us introduced the notion of AI-tasks, which are challenging for current machine learning algorithms, and involve complex but highly structured dependencies. One reason why explicitly dealing with representations is interesting is because they can be convenient to express many general priors about the world around us, i.e., priors that are not task-specific but would be likely to be useful for a learning machine to solve AI-tasks. Examples of such general-purpose priors are the following:
• Smoothness: we want to learn functions f s.t. x ≈ y generally implies f(x) ≈ f(y). This is the most basic prior and is present in most machine learning, but is insufficient to get around the curse of dimensionality, as discussed in Section 3.2 below.
• Multiple explanatory factors: the data generating distribution is generated by different underlying factors, and for the most part what one learns about one factor generalizes in many configurations of the other factors.
The objective to recover or at least disentangle these underlying factors of variation is discussed in Section 3.5.
This assumption is behind the idea of distributed representations, discussed in Section 3.3 below.
• A hierarchical organization of explanatory factors: the concepts that are useful at describing the world around us can be defined in terms of other concepts, in a hierarchy, with more abstract concepts higher in the hierarchy, being defined in terms of less abstract ones. This is the assumption exploited by having deep representations, elaborated in Section 3.4 below.
• Semi-supervised learning: in the context where we have input variables X and target variables Y we may want to predict, a subset of the factors that explain X's distribution explain a great deal of Y, given X. Hence representations that are useful for P(X) tend to be useful when learning P(Y |X), allowing sharing of statistical strength between the unsupervised and supervised learning tasks, as discussed in Section 4.
• Shared factors across tasks: in the context where we have many Y 's of interest or many learning tasks in general, tasks (e.g., the corresponding P(Y |X, task)) are explained by factors that are shared with other tasks, allowing sharing of statistical strengths across tasks, as discussed in the previous section (Multi-Task and Transfer Learning, Domain Adaptation).
• Manifolds: probability mass concentrates near regions that have a much smaller dimensionality than the original space where the data lives. This is explicitly exploited in some of the auto-encoder algorithms and other manifoldinspired algorithms described respectively in Sections 7.2 and 8.
• Natural clustering: different values of categorical variables such as object classes6 are associated with separate manifolds. More precisely, the local variations on the manifold tend to preserve the value of a category, and a linear interpolation between examples of different classes in general involves going through a low density region, i.e., P(X|Y = i) for different i tend to be well separated and not overlap much. For example, this is exploited in the Manifold Tangent Classifier discussed in Section 8.3. This hypothesis is consistent with the idea that humans have named categories and classes because of such statistical structure (discovered by their brain and propagated by their culture), and machine learning tasks often involves predicting such categorical variables.
• Temporal and spatial coherence: this is similar to the cluster assumption but concerns sequences of observations; consecutive or spatially nearby observations tend to be associated with the same value of relevant categorical concepts, or result in a small move on the surface of the high-density manifold. More generally, different factors change at different temporal and spatial scales, and many categorical concepts of interest change slowly. When attempting to capture such categorical variables, this prior can be enforced by making the associated representations slowly changing, i.e., penalizing changes in values over time or space. This prior was introduced in Becker and Hinton (1992) and is discussed in Section 11.3.
• Sparsity: for any given observation x, only a small fraction of the possible factors are relevant. In terms of representation, this could be represented by features that are often zero (as initially proposed by Olshausen and Field (1996)), or by the fact that most of the extracted features are insensitive to small variations of x. This can be achieved with certain forms of priors on latent variables (peaked at 0), or by using a non-linearity whose value is often flat at 0 (i.e., 0 and with a 0 derivative), or simply by penalizing the magnitude of the Jacobian matrix (of derivatives) of the function mapping input to representation. This is discussed in Sections 6.1.3 and 7.2.
We can view many of the above priors as ways to help the learner discover and disentangle some of the underlying (and a priori unknown) factors of variation that the data may reveal.
This idea is pursued further in Sections 3.5 and 11.4.
Smoothness and the Curse of Dimensionality
For AI-tasks, such as computer vision and natural language understanding, it seems hopeless to rely only on simple parametric models (such as linear models) because they cannot capture enough of the complexity of interest. Conversely, machine learning researchers have sought flexibility in local7 non-parametric learners such as kernel machines with
6. it is often the case that the Y of interest is a category
7. local in the sense that the value of the learned function at x depends mostly on training examples x(t)'s close to x
4 a fixed generic local-response kernel (such as the Gaussian kernel). Unfortunately, as argued at length by Bengio and Monperrus (2005); Bengio et al. (2006a); Bengio and LeCun(2007); Bengio (2009); Bengio et al. (2010), most of these algorithms only exploit the principle of local generalization, i.e., the assumption that the target function (to be learned) is smooth enough, so they rely on examples to explicitly map out the wrinkles of the target function. Generalization is mostly achieved by a form of local interpolation between neighboring training examples. Although smoothness can be a useful assumption, it is insufficient to deal with the curse of dimensionality, because the number of such wrinkles (ups and downs of the target function) may grow exponentially with the number of relevant interacting factors, when the data are represented in raw input space. We advocate learning algorithms that are flexible and non-parametric8 but do not rely exclusively on the smoothness assumption. Instead, we propose to incorporate generic priors such as those enumerated above into representation-learning algorithms. Smoothnessbased learners (such as kernel machines) and linear models can still be useful on top of such learned representations. In fact, the combination of learning a representation and kernel machine is equivalent to learning the kernel, i.e., the feature space. Kernel machines are useful, but they depend on a prior definition of a suitable similarity metric, or a feature space in which naive similarity metrics suffice. We would like to use the data, along with very generic priors, to discover those features, or equivalently, a similarity function.
Distributed representations
Good representations are expressive, meaning that a reasonably-sized learned representation can capture a huge number of possible input configurations. A simple counting argument helps us to assess the expressiveness of a model producing a representation: how many parameters does it require compared to the number of input regions (or configurations) it can distinguish? A one-hot representations, such as the result of traditional clustering algorithms, a Gaussian mixture model, a nearest-neighbor algorithm, a decision tree, or a Gaussian SVM all require O(N) parameters (and/or
O(N) examples) to distinguish O(N) input regions. One could naively believe that in order to define O(N) input regions one cannot do better. However, RBMs, sparse coding, autoencoders or multi-layer neural networks can all represent up to
O(2k) input regions using only O(N) parameters (with k the number of non-zero elements in a sparse representation, and k = N in non-sparse RBMs and other dense representations).
These are all distributed representations (where k elements can independently be varied, e.g., they are not mutually exclusive) or sparse (distributed representations where only a few of the elements can be varied at a time). The generalization of clustering to distributed representations is multi-clustering, where either several clusterings take place in parallel or the 8. We understand non-parametric as including all learning algorithms whose capacity can be increased appropriately as the amount of data and its complexity demands it, e.g. including mixture models and neural networks where the number of parameters is a data-selected hyper-parameter. same clustering is applied on different parts of the input, such as in the very popular hierarchical feature extraction for object recognition based on a histogram of cluster categories detected in different patches of an image (Lazebnik et al., 2006; Coates and Ng, 2011a). The exponential gain from distributed or sparse representations is discussed further in section 3.2 (and Figure 3.2) of Bengio (2009). It comes about because each parameter (e.g. the parameters of one of the units in a sparse code, or one of the units in a Restricted
Boltzmann Machine) can be re-used in many examples that are not simply near neighbors of each other, whereas with local generalization, different regions in input space are basically associated with their own private set of parameters, e.g., as in decision trees, nearest-neighbors, Gaussian SVMs, etc. In a distributed representation, an exponentially large number of possible subsets of features or hidden units can be activated in response to a given input. In a single-layer model, each feature is typically associated with a preferred input direction, corresponding to a hyperplane in input space, and the code or representation associated with that input is precisely the pattern of activation (which features respond to the input, and how much). This is in contrast with a non-distributed representation such as the one learned by most clustering algorithms, e.g., k-means, in which the representation of a given input vector is a one-hot code identifying which one of a small number of cluster centroids best represents the input 9.
Depth and abstraction
Depth is a key aspect to representation learning strategies we consider in this paper. As we will discuss, deep architectures are often challenging to train effectively and this has been the subject of much recent research and progress. However, despite these challenges, they carry two significant advantages that motivate our long-term interest in discovering successful training strategies for deep architectures. These advantages are: (1) deep architectures promote the re-use of features, and(2) deep architectures can potentially lead to progressively more abstract features at higher layers of representations(more removed from the data).
Feature re-use. The notion of re-use, which explains the power of distributed representations, is also at the heart of the theoretical advantages behind deep learning, i.e., constructing multiple levels of representation or learning a hierarchy of features. The depth of a circuit is the length of the longest path from an input node of the circuit to an output node of the circuit. The crucial property of a deep circuit is that its number of paths, i.e., ways to re-use different parts, can grow exponentially with its depth. Formally, one can change the depth of a given circuit by changing the definition of what
9. As discussed in (Bengio, 2009), things are only slightly better when allowing continuous-valued membership values, e.g., in ordinary mixture models (with separate parameters for each mixture component), but the difference in representational power is still exponential (Montufar and Morton, 2012). The situation may also seem better with a decision tree, where each given input is associated with a one-hot code over the tree leaves, which deterministically selects associated ancestors (the path from root to node).
Unfortunately, the number of different regions represented (equal to the number of leaves of the tree) still only grows linearly with the number of parameters used to specify it (Bengio and Delalleau, 2011).
5 each node can compute, but only by a constant factor. The typical computations we allow in each node include: weighted sum, product, artificial neuron model (such as a monotone nonlinearity on top of an affine transformation), computation of a kernel, or logic gates. Theoretical results clearly show families of functions where a deep representation can be exponentially more efficient than one that is insufficiently deep (H˚astad, 1986; H˚astad and Goldmann, 1991; Bengio et al., 2006a;
Bengio and LeCun, 2007; Bengio and Delalleau, 2011). If the same family of functions can be represented with fewer parameters (or more precisely with a smaller VC-dimension, learning theory would suggest that it can be learned with fewer examples, yielding improvements in both computational efficiency (less nodes to visit) and statistical efficiency (less parameters to learn, and re-use of these parameters over many different kinds of inputs).
Abstraction and invariance. Deep architectures can lead to abstract representations because more abstract concepts can often be constructed in terms of less abstract ones. In some cases, such as in the convolutional neural network (LeCun et al., 1998b), we build this abstraction in explicitly via a pooling mechanism (see section 11.2). More abstract concepts are generally invariant to most local changes of the input. That makes the representations that capture these concepts generally highly non-linear functions of the raw input. This is obviously true of categorical concepts, where more abstract representations detect categories that cover more varied phenomena (e.g. larger manifolds with more wrinkles) and thus they potentially have greater predictive power. Abstraction can also appear in high-level continuous-valued attributes that are only sensitive to some very specific types of changes in the input. Learning these sorts of invariant features has been a long-standing goal in pattern recognition.
Disentangling Factors of Variation
Beyond being distributed and invariant, we would like our representations to disentangle the factors of variation. Different explanatory factors of the data tend to change independently of each other in the input distribution, and only a few at a time tend to change when one considers a sequence of consecutive real-world inputs.
Complex data arise from the rich interaction of many sources. These factors interact in a complex web that can complicate AI-related tasks such as object classification. For example, an image is composed of the interaction between one or more light sources, the object shapes and the material properties of the various surfaces present in the image. Shadows from objects in the scene can fall on each other in complex patterns, creating the illusion of object boundaries where there are none and dramatically effect the perceived object shape.
How can we cope with these complex interactions? How can we disentangle the objects and their shadows? Ultimately, we believe the approach we adopt for overcoming these challenges must leverage the data itself, using vast quantities of unlabeled examples, to learn representations that separate the various explanatory sources. Doing so should give rise to a representation significantly more robust to the complex and richly structured variations extant in natural data sources for
AI-related tasks.
It is important to distinguish between the related but distinct goals of learning invariant features and learning to disentangle explanatory factors. The central difference is the preservation of information. Invariant features, by definition, have reduced sensitivity in the direction of invariance. This is the goal of building features that are insensitive to variation in the data that are uninformative to the task at hand. Unfortunately, it is often difficult to determine a priori which set of features will ultimately be relevant to the task at hand. Further, as is often the case in the context of deep learning methods, the feature set being trained may be destined to be used in multiple tasks that may have distinct subsets of relevant features. Considerations such as these lead us to the conclusion that the most robust approach to feature learning is to disentangle as many factors as possible, discarding as little information about the data as is practical. If some form of dimensionality reduction is desirable, then we hypothesize that the local directions of variation least represented in the training data should be first to be pruned out (as in PCA, for example, which does it globally instead of around each example).
What are good criteria for learning representations?
One of the challenges of representation learning that distinguishes it from other machine learning tasks such as classification is the difficulty in establishing a clear objective, or target for training. In the case of classification, the objective is (at least conceptually) obvious, we want to minimize the number of misclassifications on the training dataset. In the case of representation learning, our objective is far-removed from the ultimate objective, which is typically learning a classifier or some other predictor. Our problem is reminiscent of the credit assignment problem encountered in reinforcement learning. We have proposed that a good representation is one that disentangles the underlying factors of variation, but how do we translate that into appropriate training criteria? Is it even necessary to do anything but maximize likelihood under a good model or can we introduce priors such as those enumerated above (possibly data-dependent ones) that help the representation better do this disentangling? This question remains clearly open but is discussed in more detail in Sections 3.5 and 11.4.
BUILDING DEEP REPRESENTATIONS
In 2006, a breakthrough in feature learning and deep learning was initiated by Geoff Hinton and quickly followed up in the same year (Hinton et al., 2006a; Bengio et al., 2007;
Ranzato et al., 2007). It has been extensively reviewed and discussed in Bengio (2009). A central idea, referred to as greedy layerwise unsupervised pre-training, was to learn a hierarchy of features one level at a time, using unsupervised feature learning to learn a new transformation at each level to be composed with the previously learned transformations; essentially, each iteration of unsupervised feature learning adds one layer of weights to a deep neural network. Finally, the set
6 of layers could be combined to initialize a deep supervised predictor, such as a neural network classifier, or a deep generative model, such as a Deep Boltzmann Machine (Salakhutdinov and Hinton, 2009).
This paper is mostly about feature learning algorithms that can be used to form deep architectures. In particular, it was empirically observed that layerwise stacking of feature extraction often yielded better representations, e.g., in terms of classification error (Larochelle et al., 2009; Erhan et al., 2010b), quality of the samples generated by a probabilistic model (Salakhutdinov and Hinton, 2009) or in terms of the invariance properties of the learned features (Goodfellow et al., 2009). Whereas this section focuses on the idea of stacking single-layer models, Section 10 follows up with a discussion on joint training of all the layers.
The greedy layerwise unsupervised pre-training procedure (Hinton et al., 2006a; Bengio et al., 2007; Bengio, 2009) is based on training each layer with an unsupervised representation learning algorithm, taking the features produced at the previous level as input for the next level. It is then straightforward to use the resulting deep feature extraction either as input to a standard supervised machine learning predictor (such as an SVM) or as initialization for a deep supervised neural network (e.g., by appending a logistic regression layer or purely supervised layers of a multi-layer neural network). The layerwise procedure can also be applied in a purely supervised setting, called the greedy layerwise supervised pre-training (Bengio et al., 2007). For example, after the first one-hidden-layer MLP is trained, its output layer is discarded and another one-hidden-layer MLP can be stacked on top of it, etc. Although results reported in Bengio et al.(2007) were not as good as for unsupervised pre-training, they were nonetheless better than without pre-training at all.
Alternatively, the outputs of the previous layer can be fed as extra inputs for the next layer, as successfully done in Yu et al.
Whereas combining single layers into a supervised model is straightforward, it is less clear how layers pre-trained by unsupervised learning should be combined to form a better unsupervised model. We cover here some of the approaches to do so, but no clear winner emerges and much work has to be done to validate existing proposals or improve them.
The first proposal was to stack pre-trained RBMs into a Deep Belief Network (Hinton et al., 2006a) or DBN, where the top layer is interpreted as an RBM and the lower layers as a directed sigmoid belief network. However, it is not clear how to approximate maximum likelihood training to further optimize this generative model. One option is the wake-sleep algorithm (Hinton et al., 2006a) but more work should be done to assess the efficiency of this procedure in terms of improving the generative model.
The second approach that has been put forward is to combine the RBM parameters into a Deep Boltzmann Machine(DBM), by basically halving the RBM weights to obtain the DBM weights (Salakhutdinov and Hinton, 2009). The DBM can then be trained by approximate maximum likelihood as discussed in more details later (Section 10.2). This joint training has brought substantial improvements, both in terms of likelihood and in terms of classification performance of the resulting deep feature learner (Salakhutdinov and Hinton, Another early approach was to stack RBMs or autoencoders into a deep auto-encoder (Hinton and Salakhutdinov, 2006). If we have a series of encoder-decoder pairs(f (i)(·), g(i)(·)), then the overall encoder is the composition of the encoders, f (N)(... f (2)(f (1)(·))), and the overall decoder is its "transpose" (often with transposed weight matrices as well), g(1)(g(2)(... f (N)(·))). The deep auto-encoder (or its regularized version, as discussed in Section 7.2) can then be jointly trained, with all the parameters optimized with respect to a common training criterion. More work on this avenue clearly needs to be done, and it was probably avoided by fear of the challenges in training deep feedforward networks, discussed in the Section 10 along with very encouraging recent results.
Yet another recently proposed approach to training deep architectures(Ngiam et al., 2011) is to consider the iterative construction of a free energy function (i.e., with no explicit latent variables, except possibly for a top-level layer of hidden units) for a deep architecture as the composition of transformations associated with lower layers, followed by toplevel hidden units. The question is then how to train a model defined by an arbitrary parametrized (free) energy function.
Ngiam et al. (2011) have used Hybrid Monte Carlo (Neal, 1993), but other options include contrastive divergence (Hinton et al., 2006b), score matching (Hyv¨arinen, 2005a; Hyv¨arinen, 2008), denoising score matching (Kingma and LeCun, 2010;
Vincent, 2011), and noise-contrastive estimation (Gutmann and Hyvarinen, 2010).
SINGLE-LAYER LEARNING MODULES
Within the community of researchers interested in representation learning, there has developed two broad parallel lines of inquiry: one rooted in probabilistic graphical models and one rooted in neural networks. Fundamentally, the difference between these two paradigms is whether the layered architecture of a deep learning model is to be interpreted as describing a probabilistic graphical model or as describing a computation graph. In short, are hidden units considered latent random variables or as computational nodes?
To date, the dichotomy between these two paradigms has remained in the background, perhaps because they appear to have more characteristics in common than separating them.
We suggest that this is likely a function of the fact that much recent progress in both of these areas has focused on singlelayer greedy learning modules and the similarities between the types of single-layer models that have been explored: mainly, the restricted Boltzmann machine (RBM) on the probabilistic side, and the auto-encoder variants on the neural network side. Indeed, as shown by one of us (Vincent, 2011) and others (Swersky et al., 2011), in the case of the restricted
Boltzmann machine, training the model via an inductive principle known as score matching (Hyv¨arinen, 2005b) (to be discussed in sec. 6.4.3) is essentially identical to a regularized reconstruction objective of an auto-encoder. Another strong
7 link between pairs of models on both sides of this divide is when the computational graph for computing representation in the neural network model corresponds exactly to the computational graph that corresponds to inference in the probabilistic model, and this happens to also correspond to the structure of graphical model itself.
The connection between these two paradigms becomes more tenuous when we consider deeper models where, in the case of a probabilistic model, exact inference typically becomes intractable. In the case of deep models, the computational graph diverges from the structure of the model. For example, in the case of a deep Boltzmann machine, unrolling variational(approximate) inference into a computational graph results in a recurrent graph structure. We have performed preliminary exploration (Savard, 2011) of deterministic variants of deep auto-encoders whose computational graph is similar to that of a deep Boltzmann machine (in fact very close to the meanfield variational approximations associated with the Boltzmann machine), and that is one interesting intermediate point to explore (between the deterministic approaches and the graphical model approaches).
In the next few sections we will review the major developments in single-layer training modules used to support feature learning and particularly deep learning. We divide these sections between (Section 6) the probabilistic models, with inference and training schemes that directly parametrize the generative – or decoding – pathway and (Section 7) the typically neural network-based models that directly parametrize the encoding pathway. Interestingly, some models, like Predictive Sparse Decomposition (PSD) (Kavukcuoglu et al., 2008) inherit both properties, and will also be discussed (Section 7.2.4). We then present a different view of representation learning, based on the associated geometry and the manifold assumption, in Section 8.
Before we do this, we consider an unsupervised single-layer representation learning algorithm that spans all three views(probabilistic, auto-encoder, and manifold learning) discussed here.
Principal Components Analysis
We will use probably the oldest feature extraction algorithm, principal components analysis (PCA) (Pearson, 1901;
Hotelling, 1933), to illustrate the probabilistic, auto-encoder and manifold views of representation-learning. PCA learns a linear transformation h = f(x) = W T x + b of input x ∈ Rdx, where the columns of dx × dh matrix W form an orthogonal basis for the dh orthogonal directions of greatest variance in the training data. The result is dh features (the components of representation h) that are decorrelated. The three interpretations of PCA are the following: a) it is related to probabilistic models (Section 6) such as probabilistic PCA, factor analysis and the traditional multivariate Gaussian distribution (the leading eigenvectors of the covariance matrix are the principal components); b) the representation it learns is essentially the same as that learned by a basic linear auto-encoder (Section 7.2); and c) it can be viewed as a simple linear form of linear manifold learning (Section 8), i.e., characterizing a lower-dimensional region in input space near which the data density is peaked. Thus, PCA may be in the back of the reader's mind as a common thread relating these various viewpoints. Unfortunately the expressive power of linear features is very limited: they cannot be stacked to form deeper, more abstract representations since the composition of linear operations yields another linear operation. Here, we focus on recent algorithms that have been developed to extract non-linear features, which can be stacked in the construction of deep networks, although some authors simply insert a nonlinearity between learned single-layer linear projections (Le et al., 2011c; Chen et al., 2012).
Another rich family of feature extraction techniques that this review does not cover in any detail due to space constraints is Independent Component Analysis or ICA (Jutten and Herault, 1991; Comon, 1994; Bell and Sejnowski, 1997). Instead, we refer the reader to Hyv¨arinen et al. (2001a); Hyv¨arinen et al.(2009). Note that, while in the simplest case (complete, noisefree) ICA yields linear features, in the more general case it can be equated with a linear generative model with nonGaussian independent latent variables, similar to sparse coding(section 6.1.3), which result in non-linear features. Therefore, ICA and its variants like Independent and Topographic
ICA (Hyv¨arinen et al., 2001b) can and have been used to build deep networks (Le et al., 2010, 2011c): see section 11.2. The notion of obtaining independent components also appears similar to our stated goal of disentangling underlying explanatory factors through deep networks. However, for complex realworld distributions, it is doubtful that the relationship between truly independent underlying factors and the observed highdimensional data can be adequately characterized by a linear transformation.
PROBABILISTIC MODELS
From the probabilistic modeling perspective, the question of feature learning can be interpreted as an attempt to recover a parsimonious set of latent random variables that describe a distribution over the observed data. We can express any probabilistic model over the joint space of the latent variables, h, and observed or visible variables x, (associated with the data) as p(x, h). Feature values are conceived as the result of an inference process to determine the probability distribution of the latent variables given the data, i.e. p(h | x), often referred to as the posterior probability. Learning is conceived in term of estimating a set of model parameters that (locally) maximizes the likelihood of the training data with respect to the distribution over these latent variables. The probabilistic graphical model formalism gives us two possible modeling paradigms in which we can consider the question of inferring latent variables: directed and undirected graphical models. The key distinguishing factor between these paradigms is the nature of their parametrization of the joint distribution p(x, h). The choice of directed versus undirected model has a major impact on the nature and computational costs of the algorithmic approach to both inference and learning.
Directed Graphical Models
Directed latent factor models are parametrized through a decomposition of the joint distribution, p(x, h) = p(x | h)p(h), involving a prior p(h), and a likelihood p(x
| h) that
8 describes the observed data x in terms of the latent factors h. Unsupervised feature learning models that can be interpreted with this decomposition include: Principal Components
Analysis (PCA) (Roweis, 1997; Tipping and Bishop, 1999), sparse coding (Olshausen and Field, 1996), sigmoid belief networks (Neal, 1992) and the newly introduced spike-andslab sparse coding model (Goodfellow et al., 2011).
Explaining Away
In the context of latent factor models, the form of the directed model often leads to one important property, namely explaining away: a priori independent causes of an event can become non-independent given the observation of the event.
Latent factor models can generally be interpreted as latent cause models, where the h activations cause the observed x.
This renders the a priori independent h to be non-independent.
As a consequence, recovering the posterior distribution of h, p(h | x) (which we use as a basis for feature representation), is often computationally challenging and can be entirely intractable, especially when h is discrete.
A classic example that illustrates the phenomenon is to imagine you are on vacation away from home and you receive a phone call from the company that installed the security system at your house. They tell you that the alarm has been activated. You begin worrying your home has been burglarized, but then you hear on the radio that a minor earthquake has been reported in the area of your home. If you happen to know from prior experience that earthquakes sometimes cause your home alarm system to activate, then suddenly you relax, confident that your home has very likely not been burglarized.
The example illustrates how the observation, alarm activation, rendered two otherwise entirely independent causes, burglarized and earthquake, to become dependent – in this case, the dependency is one of mutual exclusivity. Since both burglarized and earthquake are very rare events and both can cause alarm activation, the observation of one explains away the other. The example demonstrates not only how observations can render causes to be statistically dependent, but also the utility of explaining away. It gives rise to a parsimonious prediction of the unseen or latent events from the observations.
Returning to latent factor models, despite the computational obstacles we face when attempting to recover the posterior over h, explaining away promises to provide a parsimonious p(h | x), which can be an extremely useful characteristic of a feature encoding scheme. If one thinks of a representation as being composed of various feature detectors and estimated attributes of the observed input, it is useful to allow the different features to compete and collaborate with each other to explain the input. This is naturally achieved with directed graphical models, but can also be achieved with undirected models (see Section 6.2) such as Boltzmann machines if there are lateral connections between the corresponding units or corresponding interaction terms in the energy function that defines the probability model.
Probabilistic Interpretation of PCA
While PCA was not originally cast as probabilistic model, it possesses a natural probabilistic interpretation (Roweis, 1997;
Tipping and Bishop, 1999) that casts PCA as factor analysis: p(h)
=
N(h; 0, σ2 hI) p(x | h)
=
N(x; Wh + µx, σ2 xI), (1) where x ∈ Rdx, h ∈ Rdh, N(v; µ, Σ) is the multivariate normal density of v with mean µ and covariance Σ, and columns of W span the same space as leading dh principal components, but are not constrained to be orthonormal.
Sparse Coding
As in the case of PCA, sparse coding has both a probabilistic and non-probabilistic interpretation. Sparse coding also relates a latent representation h (either a vector of random variables or a feature vector, depending on the interpretation) to the data x through a linear mapping W, which we refer to as the dictionary. The difference between sparse coding and PCA is that sparse coding includes a penalty to ensure a sparse activation of h is used to encode each input x.
Specifically, from a non-probabilistic perspective, sparse coding can be seen as recovering the code or feature vector associated with a new input x via: h∗ = f(x) = argmin h
∥x − Wh∥2
2 + λ∥h∥1, Learning the dictionary W can be accomplished by optimizing the following training criterion with respect to W:
JSC =
� t
∥x(t) − Wh∗(t)∥2
2, (3) where the x(t) is the input vector for example t and h∗(t) are the corresponding sparse codes determined by Eq. 2. W is usually constrained to have unit-norm columns (because one can arbitrarily exchange scaling of column i with scaling of h(t) i, such a constraint is necessary for the L1 penalty to have any effect).
The probabilistic interpretation of sparse coding differs from that of PCA, in that instead of a Gaussian prior on the latent random variable h, we use a sparsity inducing Laplace prior(corresponding to an L1 penalty): p(h)
= dh
� i λ exp(−λ|hi|) p(x | h)
=
N(x; Wh + µx, σ2 xI).
In the case of sparse coding, because we will seek a sparse representation (i.e., one with many features set to exactly zero), we will be interested in recovering the MAP (maximum a posteriori value of h: i.e. h∗ = argmaxh p(h | x) rather than its expected value E[ [h] |x]. Under this interpretation, dictionary learning proceeds as maximizing the likelihood of the data given these MAP values of h∗: argmaxW
� t p(x(t) | h∗(t)) subject to the norm constraint on W. Note that this parameter learning scheme, subject to the MAP values of the latent h, is not standard practice in the probabilistic graphical model literature. Typically the likelihood of the data p(x) =
� h p(x | h)p(h) is maximized directly. In the presence of latent variables, expectation maximization (Dempster et al., 1977) is employed where the parameters are optimized with respect to the marginal likelihood, i.e., summing or integrating the joint log-likelihood over the values of the latent variables
9 under their posterior P(h | x), rather than considering only the MAP values of h. The theoretical properties of this form of parameter learning are not yet well understood but seem to work well in practice (e.g. k-Means vs Gaussian mixture models and Viterbi training for HMMs). Note also that the interpretation of sparse coding as a MAP estimation can be questioned (Gribonval, 2011), because even though the interpretation of the L1 penalty as a log-prior is a possible interpretation, there can be other Bayesian interpretations compatible with the training criterion.
Sparse coding is an excellent example of the power of explaining away. The Laplace distribution (equivalently, the L1 penalty) over the latent h acts to resolve a sparse and parsimonious representation of the input. Even with a very overcomplete dictionary with many redundant bases, the MAP inference process used in sparse coding to find h∗ can pick out the most appropriate bases and zero the others, despite them having a high degree of correlation with the input. This property arises naturally in directed graphical models such as sparse coding and is entirely owing to the explaining away effect. It is not seen in commonly used undirected probabilistic models such as the RBM, nor is it seen in parametric feature encoding methods such as auto-encoders. The trade-off is that, compared to methods such as RBMs and auto-encoders, inference in sparse coding involves an extra inner-loop of optimization to find h∗ with a corresponding increase in the computational cost of feature extraction. Compared to autoencoders and RBMs, the code in sparse coding is a free variable for each example, and in that sense the implicit encoder is non-parametric.
One might expect that the parsimony of the sparse coding representation and its explaining away effect would be advantageous and indeed it seems to be the case. Coates and Ng (2011a) demonstrated with the CIFAR-10 object classification task (Krizhevsky and Hinton, 2009) with a patchbase feature extraction pipeline, that in the regime with few(< 1000) labeled training examples per class, the sparse coding representation significantly outperformed other highly competitive encoding schemes. Possibly because of these properties, and because of the very computationally efficient algorithms that have been proposed for it (in comparison with the general case of inference in the presence of explaining away), sparse coding enjoys considerable popularity as a feature learning and encoding paradigm. There are numerous examples of its successful application as a feature representation scheme, including natural image modeling (Raina et al., 2007; Kavukcuoglu et al., 2008; Coates and Ng, 2011a;
Yu et al., 2011), audio classification (Grosse et al., 2007), natural language processing (Bagnell and Bradley, 2009), as well as being a very successful model of the early visual cortex (Olshausen and Field, 1997). Sparsity criteria can also be generalized successfully to yield groups of features that prefer to all be zero, but if one or a few of them are active then the penalty for activating others in the group is small. Different group sparsity patterns can incorporate different forms of prior knowledge (Kavukcuoglu et al., 2009; Jenatton et al., 2009;
Bach et al., 2011; Gregor et al., 2011).
Spike-and-Slab Sparse Coding. Spike-and-slab sparse coding (S3C) is one example of a promising variation on sparse coding for feature learning (Goodfellow et al., 2012). The S3C model possesses a set of latent binary spike variables together with a a set of latent real-valued slab variables. The activation of the spike variables dictate the sparsity pattern.
S3C has been applied to the CIFAR-10 and CIFAR-100 object classification tasks (Krizhevsky and Hinton, 2009), and shows the same pattern as sparse coding of superior performance in the regime of relatively few (< 1000) labeled examples per class (Goodfellow et al., 2012). In fact, in both the CIFAR100 dataset (with 500 examples per class) and the CIFAR10 dataset (when the number of examples is reduced to a similar range), the S3C representation actually outperforms sparse coding representations. This advantage was revealed clearly with S3C winning the NIPS'2011 Transfer Learning
Challenge (Goodfellow et al., 2011).
Undirected Graphical Models
Undirected graphical models, also called Markov random fields (MRFs), parametrize the joint p(x, h) through a factorization in terms of unnormalized non-negative clique potentials: p(x, h) = 1
Zθ
� i ψi(x)
� j ηj(h)
� k νk(x, h)(5) where ψi(x), ηj(h) and νk(x, h) are the clique potentials describing the interactions between the visible elements, between the hidden variables, and those interaction between the visible and hidden variables respectively. The partition function Zθ ensures that the distribution is normalized. Within the context of unsupervised feature learning, we generally see a particular form of Markov random field called a Boltzmann distribution with clique potentials constrained to be positive: p(x, h) = 1
Zθ exp (−Eθ(x, h)), (6) where Eθ(x, h) is the energy function and contains the interactions described by the MRF clique potentials and θ are the model parameters that characterize these interactions.
A
Boltzmann machine is defined as a network of symmetrically-coupled binary random variables or units.
These stochastic units can be divided into two groups: (1) the visible units x ∈ {0, 1}dx that represent the data, and (2) the hidden or latent units h ∈ {0, 1}dh that mediate dependencies between the visible units through their mutual interactions. The pattern of interaction is specified through the energy function:
EBM θ(x, h) = −1
2xT Ux − 1
2hT V h − xT Wh − bT x − dT h, (7) where θ
=
{U, V, W, b, d} are the model parameters which respectively encode the visible-to-visible interactions, the hidden-to-hidden interactions, the visible-to-hidden interactions, the visible self-connections, and the hidden self-connections (also known as biases). To avoid overparametrization, the diagonals of U and V are set to zero.
The Boltzmann machine energy function specifies the probability distribution over the joint space [x, h], via the Boltzmann distribution, Eq. 6, with the partition function Zθ given by:
Zθ = x1=1
� x1=0
· · · xdx =1
� xdx =0 h1=1
� h1=0
· · · hdh =1
� hdh =0 exp
�
−EBM θ(x, h; θ)
�
This joint probability distribution gives rise to the set of conditional distributions of the form:
P(hi | x, h\i) = sigmoid
�
�� j
Wjixj +
� i′̸=i
Vii′hi′ + di
�
�
P(xj | h, x\j) = sigmoid
�
�� i
Wjixj +
� j′̸=j
Ujj′xj′ + bj
�
�.
In general, inference in the Boltzmann machine is intractable.
For example, computing the conditional probability of hi given the visibles, P(hi | x), requires marginalizing over the rest of the hiddens, which implies evaluating a sum with 2dh−1 terms:
P(hi | x) = h1=1
� h1=0
· · · hi−1=1
� hi−1=0 hi+1=1
� hi+1=0
· · · hdh =1
� hdh =0
P(h | x)
However with some judicious choices in the pattern of interactions between the visible and hidden units, more tractable subsets of the model family are possible, as we discuss next.
Restricted Boltzmann Machines
The restricted Boltzmann machine (RBM) is likely the most popular subclass of Boltzmann machine (Smolensky, 1986).
It is defined by restricting the interactions in the Boltzmann energy function, in Eq. 7, to only those between h and x, i.e.
ERBM θ is EBM θ with U = 0 and V = 0. As such, the RBM can be said to form a bipartite graph with the visibles and the hiddens forming two layers of vertices in the graph (and no connection between units of the same layer). With this restriction, the RBM possesses the useful property that the conditional distribution over the hidden units factorizes given the visibles:
P(h | x) =
� i
P(hi | x)
P(hi = 1 | x) = sigmoid
�� j
Wjixj + di
�
Likewise, the conditional distribution over the visible units given the hiddens also factorizes:
P(x | h) =
� j
P(xj | h)
P(xj = 1 | h) = sigmoid
�� i
Wjihi + bj
�
This conditional factorization property of the RBM immediately implies that most inferences we would like to make are readily tractable. For example, the RBM feature representation is taken to be the set of posterior marginals P(hi | x), which, given the conditional independence described in Eq. 12, are immediately available. Note that this is in stark contrast to the situation with popular directed graphical models for unsupervised feature extraction, where computing the posterior probability is intractable.
Importantly, the tractability of the RBM does not extend to its partition function, which still involves summing an exponential number of terms. It does imply however that we can limit the number of terms to min{2dx, 2dh}. Usually this is still an unmanageable number of terms and therefore we must resort to approximate methods to deal with its estimation.
It is difficult to overstate the impact the RBM has had to the fields of unsupervised feature learning and deep learning.
It has been used in a truly impressive variety of applications, including fMRI image classification (Schmah et al., 2009), motion and spatial transformations (Taylor and Hinton, 2009; Memisevic and Hinton, 2010), collaborative filtering(Salakhutdinov et al., 2007) and natural image modeling(Ranzato and Hinton, 2010; Courville et al., 2011b).
Generalizations of the RBM to Real-valued data
Important progress has been made in the last few years in defining generalizations of the RBM that better capture realvalued data, in particular real-valued image data, by better modeling the conditional covariance of the input pixels. The standard RBM, as discussed above, is defined with both binary visible variables v ∈ {0, 1} and binary latent variables h ∈
{0, 1}. The tractability of inference and learning in the RBM has inspired many authors to extend it, via modifications of its energy function, to model other kinds of data distributions. In particular, there has been multiple attempts to develop RBMtype models of real-valued data, where x ∈ Rdx. The most straightforward approach to modeling real-valued observations within the RBM framework is the so-called Gaussian RBM(GRBM) where the only change in the RBM energy function is to the visible units biases, by adding a bias term that is quadratic in the visible units x. While it probably remains the most popular way to model real-valued data within the RBM framework, Ranzato and Hinton (2010) suggest that the GRBM has proved to be a somewhat unsatisfactory model of natural images. The trained features typically do not represent sharp edges that occur at object boundaries and lead to latent representations that are not particularly useful features for classification tasks. Ranzato and Hinton (2010) argue that the failure of the GRBM to adequately capture the statistical structure of natural images stems from the exclusive use of the model capacity to capture the conditional mean at the expense of the conditional covariance. Natural images, they argue, are chiefly characterized by the covariance of the pixel values, not by their absolute values. This point is supported by the common use of preprocessing methods that standardize the global scaling of the pixel values across images in a dataset or across the pixel values within each image.
These kinds of concerns about the ability of the GRBM to model natural image data has lead to the development of alternative RBM-based models that each attempt to take on this objective of better modeling non-diagonal conditional covariances. (Ranzato and Hinton, 2010) introduced the mean and covariance RBM (mcRBM). Like the GRBM, the mcRBM is a 2-layer Boltzmann machine that explicitly models the visible units as Gaussian distributed quantities. However unlike the GRBM, the mcRBM uses its hidden layer to independently parametrize both the mean and covariance of the data through two sets of hidden units. The mcRBM is a combination of the covariance RBM (cRBM) (Ranzato et al., 2010a), that models the conditional covariance, with the GRBM that captures the 11 conditional mean. While the GRBM has shown considerable potential as the basis of a highly successful phoneme recognition system (Dahl et al., 2010), it seems that due to difficulties in training the mcRBM, the model has been largely superseded by the mPoT model. The mPoT model (mean-product of Student's T-distributions model)(Ranzato et al., 2010b) is a combination of the GRBM and the product of Student's Tdistributions model (Welling et al., 2003). It is an energy-based model where the conditional distribution over the visible units conditioned on the hidden variables is a multivariate Gaussian(non-diagonal covariance) and the complementary conditional distribution over the hidden variables given the visibles are a set of independent Gamma distributions.
The PoT model has recently been generalized to the mPoT model (Ranzato et al., 2010b) to include nonzero Gaussian means by the addition of GRBM-like hidden units, similarly to how the mcRBM generalizes the cRBM. The mPoT model has been used to synthesize large-scale natural images (Ranzato et al., 2010b) that show large-scale features and shadowing structure. It has been used to model natural textures (Kivinen and Williams, 2012) in a tiled-convolution configuration (see section 11.2).
Another recently introduced RBM-based model with the objective of having the hidden units encode both the mean and covariance information is the spike-and-slab Restricted
Boltzmann Machine (ssRBM) (Courville et al., 2011a,b).
The ssRBM is defined as having both a real-valued "slab" variable and a binary "spike" variable associated with each unit in the hidden layer. The ssRBM has been demonstrated as a feature learning and extraction scheme in the context of CIFAR-10 object classification (Krizhevsky and Hinton, 2009) from natural images and has performed well in the role (Courville et al., 2011a,b). When trained convolutionally(see Section 11.2) on full CIFAR-10 natural images, the model demonstrated the ability to generate natural image samples that seem to capture the broad statistical structure of natural images better than previous parametric generative models, as illustrated with the samples of Figure 2.
The mcRBM, mPoT and ssRBM each set out to model real-valued data such that the hidden units encode not only the conditional mean of the data but also its conditional covariance. Other than differences in the training schemes, the most significant difference between these models is how they encode their conditional covariance. While the mcRBM and the mPoT use the activation of the hidden units to enforce constraints on the covariance of x, the ssRBM uses the hidden unit to pinch the precision matrix along the direction specified by the corresponding weight vector. These two ways of modeling conditional covariance diverge when the dimensionality of the hidden layer is significantly different from that of the input. In the over-complete setting, sparse activation with the ssRBM parametrization permits variance only in the select directions of the sparsely activated hidden units. This is a property the ssRBM shares with sparse coding models (Olshausen and Field, 1997; Grosse et al., 2007). On the other hand, in the case of the mPoT or mcRBM, an over-complete set of constraints on the covariance implies that capturing arbitrary covariance along a particular direction of the input requires
Fig. 2. (Top) Samples from a convolutionally trained µ-ssRBM, see details in Courville et al. (2011b). (Bottom) The images in the CIFAR-10 training set closest (L2 distance with contrast normalized training images) to the corresponding model samples.
The model does not appear to be capturing the natural image statistical structure by overfitting particular examples from the dataset. decreasing potentially all constraints with positive projection in that direction. This perspective would suggest that the mPoT and mcRBM do not appear to be well suited to provide a sparse representation in the overcomplete setting.
RBM parameter estimation
In this section we discuss several algorithms for training the restricted Boltzmann machine. Many of the methods we discuss are applicable to more general undirected graphical models, but are particularly practical in the RBM setting.
Freund and Haussler (1994) proposed a learning algorithm for harmoniums (RBMs) based on projection pursuit (Friedman and Stuetzle, 1981). Contrastive Divergence (Hinton, 1999;
Hinton et al., 2006a) has been used most often to train
RBMs, and many recent papers use Stochastic Maximum
Likelihood (Younes, 1999; Tieleman, 2008).
As discussed in Sec. 6.1, in training probabilistic models parameters are typically adapted in order to maximize the likelihood of the training data (or equivalently the log-likelihood, or its penalized version, which adds a regularization term).
With T training examples, the log likelihood is given by:
T
� t=1 log P(x(t); θ) =
T
� t=1 log
� h∈{0,1}dh
P(x(t), h; θ).
One straightforward way we can consider maximizing this quantity is to take small steps uphill, following the loglikelihood gradient, to find a local maximum of the likelihood.
For any Boltzmann machine, the gradient of the log-likelihood
12 of the data is given by:
∂
∂θi
T
� t=1 log p(x(t))
=
−
T
� t=1
Ep(h|x(t))
� ∂
∂θi EBM θ(x(t), h)
�
T
� t=1
Ep(x,h)
� ∂
∂θi EBM θ(x, h)
�, (15) where we have the expectations with respect to p(h(t) | x(t)) in the "clamped" condition (also called the positive phase), and over the full joint p(x, h) in the "unclamped" condition(also called the negative phase). Intuitively, the gradient acts to locally move the model distribution (the negative phase distribution) toward the data distribution (positive phase distribution), by pushing down the energy of (h, x(t)) pairs (for h ∼ P(h|x(t))) while pushing up the energy of (h, x) pairs(for (h, x) ∼ P(h, x)) until the two forces are in equilibrium, at which point the sufficient statistics (gradient of the energy function) have equal expectations with x sampled from the training distribution or with x sampled from the model.
The RBM conditional independence properties imply that the expectation in the positive phase of Eq. 15 is readily tractable. The negative phase term – arising from the partition function's contribution to the log-likelihood gradient – is more problematic because the computation of the expectation over the joint is not tractable. The various ways of dealing with the partition function's contribution to the gradient have brought about a number of different training algorithms, many trying to approximate the log-likelihood gradient.
To approximate the expectation of the joint distribution in the negative phase contribution to the gradient, it is natural to again consider exploiting the conditional independence of the RBM in order to specify a Monte Carlo approximation of the expectation over the joint:
Ep(x,h)
� ∂
∂θi ERBM θ(x, h)
�
≈ 1
L
L
� l=1
∂
∂θi ERBM θ(˜x(l), ˜h(l)), (16) with the samples (˜x(l), ˜h(l)) drawn by a block Gibbs MCMC(Markov chain Monte Carlo) sampling scheme from the model distribution:
˜x(l)
∼
P(x | ˜h(l−1))
˜h(l)
∼
P(h | ˜x(l)).
Naively, for each gradient update step, one would start a Gibbs sampling chain, wait until the chain converges to the equilibrium distribution and then draw a sufficient number of samples to approximate the expected gradient with respect to the model (joint) distribution in Eq. 16. Then restart the process for the next step of approximate gradient ascent on the log-likelihood. This procedure has the obvious flaw that waiting for the Gibbs chain to "burn-in" and reach equilibrium anew for each gradient update cannot form the basis of a practical training algorithm. Contrastive Divergence (Hinton, 1999; Hinton et al., 2006a), Stochastic Maximum Likelihood (Younes, 1999; Tieleman, 2008) and fast-weights persistent contrastive divergence or FPCD (Tieleman and Hinton, 2009) are all examples of algorithms that attempt sidestep the need to burn-in the negative phase Markov chain.
Contrastive Divergence:
Contrastive divergence (CD) estimation (Hinton, 1999; Hinton et al., 2006a) uses a biased estimate of the gradient in Eq. 15 by approximating the negative phase expectation with a very short Gibbs chain (often just one step) initialized at the training data used in the positive phase. This initialization is chosen to reduce the variance of the negative expectation based on samples from the short running Gibbs sampler. The intuition is that, while the samples drawn from very short
Gibbs chains may be a heavily biased (and poor) representation of the model distribution, they are at least moving in the direction of the model distribution relative to the data distribution represented by the positive phase training data.
Consequently, they may combine to produce a good estimate of the gradient, or direction of progress. Much has been written about the properties and alternative interpretations of CD, e.g.
Carreira-Perpi˜nan and Hinton (2005); Yuille (2005); Bengio and Delalleau (2009); Sutskever and Tieleman (2010).
Stochastic Maximum Likelihood:
The Stochastic Maximum Likelihood (SML) algorithm (also known as persistent contrastive divergence or PCD) (Younes, 1999; Tieleman, 2008) is an alternative way to sidestep an extended burn-in of the negative phase Gibbs sampler. At each gradient update, rather than initializing the Gibbs chain at the positive phase sample as in CD, SML initializes the chain at the last state of the chain used for the previous update. In other words, SML uses a continually running Gibbs chain (or often a number of Gibbs chains run in parallel) from which samples are drawn to estimate the negative phase expectation.
Despite the model parameters changing between updates, these changes should be small enough that only a few steps of Gibbs(in practice, often one step is used) are required to maintain samples from the equilibrium distribution of the Gibbs chain, i.e. the model distribution.
One aspect of SML that has received considerable recent attention is that it relies on the Gibbs chain to have reasonably good mixing properties for learning to succeed. Typically, as learning progresses and the weights of the RBM grow, the ergodicity of the Gibbs sample begins to break down10. If the learning rate ϵ associated with gradient ascent θ ← θ + ϵˆg(with E[ˆg] ≈ ∂ log pθ(x)
∂θ
) is not reduced to compensate, then the Gibbs sampler will diverge from the model distribution and learning will fail. There have been a number of attempts made to address the failure of Gibbs chain mixing in the context of SML. Desjardins et al. (2010); Cho et al. (2010);
Salakhutdinov (2010b,a) have all considered various forms of tempered transitions to improve the mixing rate of the negative phase Gibbs chain.
Tieleman and Hinton (2009) have proposed quite a different approach to addressing potential mixing problems of SML with their fast-weights persistent contrastive divergence
10. When weights become large, the estimated distribution is more peaky, and the chain takes very long time to mix, to move from mode to mode, so that practically the gradient estimator can be very poor. This is a serious chicken-and-egg problem because if sampling is not effective, nor is the training procedure, which may seem to stall.(FPCD), and it has also been exploited to train Deep Boltzmann Machines (Salakhutdinov, 2010a) and construct a pure sampling algorithm for RBMs (Breuleux et al., 2011). FPCD builds on the surprising but robust tendency of Gibbs chains to mix better during SML learning than when the model parameters are fixed. The phenomenon is rooted in the form of the likelihood gradient itself (Eq. 15). The samples drawn from the SML Gibbs chain are used in the negative phase of the gradient, which implies that the learning update will slightly increase the energy (decrease the probability) of those samples, making the region in the neighborhood of those samples less likely to be resampled and therefore making it more likely that the samples will move somewhere else (typically going near another mode). Rather than drawing samples from the distribution of the current model (with parameters θ), FPCD exaggerates this effect by drawing samples from a local perturbation of the model with parameters θ∗ and an update specified by: θ∗ t+1 = (1 − η)θt+1 + ηθ∗ t + ϵ∗ ∂
∂θi
� T
� t=1 log p(x(t))
�, (17) where ϵ∗ is the relatively large fast-weight learning rate(ϵ∗ > ϵ) and 0 < η < 1 (but near 1) is a forgetting factor that keeps the perturbed model close to the current model.
Unlike tempering, FPCD does not converge to the model distribution as ϵ and ϵ∗ go to 0, and further work is necessary to characterize the nature of its approximation to the model distribution. Nevertheless, FPCD is a popular and apparently effective means of drawing approximate samples from the model distribution that faithfully represent its diversity, at the price of sometimes generating spurious samples in between two modes (because the fast weights roughly correspond to a smoothed view of the current model's energy function). It has been applied in a variety of applications (Tieleman and Hinton, 2009; Ranzato et al., 2011; Kivinen and Williams, 2012) and it has been transformed into a sampling algorithm (Breuleux et al., 2011) that also shares this fast mixing property with herding (Welling, 2009), for the same reason, i.e., introducing negative correlations between consecutive samples of the chain in order to promote faster mixing.
Pseudolikelihood, Ratio-matching and other Inductive Principles
While CD, SML and FPCD are by far the most popular methods for training RBMs and RBM-based models, all of these methods are perhaps most naturally described as offering different approximations to maximum likelihood training. There exist other inductive principles that are alternatives to maximum likelihood that can also be used to train RBMs. In particular, these include pseudo-likelihood (Besag, 1975) and ratiomatching (Hyv¨arinen, 2007). Both of these inductive principles attempt to avoid explicitly dealing with the partition function, and their asymptotic efficiency has been analyzed (Marlin and de Freitas, 2011). Pseudo-likelihood seeks to maximize the product of all one-dimensional conditional distributions of the form P(xd|x\d), while ratio-matching can be interpreted as an extension of score matching (Hyv¨arinen, 2005a) to discrete data types. Both methods amount to weighted differences of the gradient of the RBM free energy11 evaluated at a data point and at all neighboring points within a hamming ball of radius
1. One drawback of these methods is that the computation of the statistics for all neighbors of each training data point require a significant computational overhead, scaling linearly with the dimensionality of the input, nd. CD, SML and FPCD have no such issue. Marlin et al. (2010) provides an excellent survey of these methods and their relation to CD and SML.
They also empirically compared all of these methods on a range of classification, reconstruction and density modeling tasks and found that, in general, SML provided the best combination of overall performance and computational tractability.
However, in a later study, the same authors (Swersky et al., 2011) found denoising score matching (Kingma and LeCun, 2010; Vincent, 2011) to be a competitive inductive principle both in terms of classification performance (with respect to
SML) and in terms of computational efficiency (with respect to analytically obtained score matching). Note that denoising score matching is a special case of the denoising auto-encoder training criterion (Section 7.2.2) when the reconstruction error residual equals a gradient, i.e., the score function associated with an energy function, as shown in (Vincent, 2011).
In the spirit of the Boltzmann machine update rule (Eq. 15) several other principles have been proposed to train energybased models. One approach is noise-contrastive estimation (Gutmann and Hyvarinen, 2010), in which the training criterion is transformed into a probabilistic classification problem: distinguish between (positive) training examples and(negative) noise samples generated by a broad distribution(such as the Gaussian). Another family of approaches, more in the spirit of Contrastive Divergence, relies on distinguishing positive examples (of the training distribution) and negative examples obtained by slight perturbations of the positive examples (Collobert and Weston, 2008; Bordes et al., 2012;
Weston et al., 2010). This apparently simple principle has been used successfully to train a model on huge quantities of data to map images and queries in the same space for Google's image search (Weston et al., 2010).
DIRECT ENCODING: LEARNING A PARAMETRIC MAP FROM INPUT TO REPRESENTATION
Within the framework of probabilistic models adopted in Section 6, the learned representation is always associated with latent variables, specifically with their posterior distribution given an observed input x. Unfortunately, the posterior distribution of latent variables given inputs tends to become very complicated and intractable if the model has more than a couple of interconnected layers, whether in the directed or undirected graphical model frameworks. It then becomes necessary to resort to sampling or approximate inference techniques, and to pay the associated computational and approximation error price. This is in addition to the difficulties raised by the intractable partition function in undirected graphical
11. The free energy F(x; θ) is defined in relation to the marginal likelihood of the data: F(x; θ) = − log P(x) − log Zθ and in the case of the RBM is tractable.
14 models. Moreover a posterior distribution over latent variables is not yet a simple usable feature vector that can for example be fed to a classifier. So actual feature values are typically derived from that distribution, taking the latent variable's expectation (as is typically done with RBMs), their marginal probability, or finding their most likely value (as in sparse coding). If we are to extract stable deterministic numerical feature values in the end anyway, an alternative (apparently) non-probabilistic feature learning paradigm that focuses on carrying out this part of the computation, very efficiently, is that of auto-encoders and other directly parametrized feature or representation functions. The commonality between these methods is that they learn a direct encoding, i.e., parametric map from inputs to their representation, The regularized auto-encoders are described in the next section, and are concerned with the case where the encoding function that computes the representation is associated with a decoding function that maps back to input space. In sections 8.1 and 11.3, we consider some direct encoding methods that do not require a decoder and a reconstruction error, such as semi-supervised embedding (Weston et al., 2008) and slow feature analysis (Wiskott and Sejnowski, 2002).
Auto-Encoders
Whereas probabilistic models sometimes define intermediate variables whose posterior can then be interpreted as a representation, in the auto-encoder framework (LeCun, 1987; Bourlard and Kamp, 1988; Hinton and Zemel, 1994), one starts by explicitly defining a feature-extracting function in a specific parametrized closed form. This function, that we will denote fθ, is called the encoder and will allow the straightforward and efficient computation of a feature vector h = fθ(x) from an input x. For each example x(t) from a data set
{x(1),..., x(T )}, we define h(t) = fθ(x(t))(18) where h(t) is the feature-vector or representation or code computed from x(t). Another closed form parametrized function gθ, called the decoder, maps from feature space back into input space, producing a reconstruction r = gθ(h). Whereas probabilistic models are defined from an explicit probability function and are trained to maximize (often approximately) the data likelihood (or a proxy), auto-encoders are parametrized through their encoder and decoder and are trained using a different training principle. The set of parameters θ of the encoder and decoder are learned simultaneously on the task of reconstructing as well as possible the original input, i.e. attempting to incur the lowest possible reconstruction error
L(x, r) – a measure of the discrepancy between x and its reconstruction – on average over a training set. Note how the main objective is to make reconstruction error low on the training examples, and by generalization, where the probability is high under the unknown data-generating distribution. For the minimization of reconstruction error to capture the structure of the data-generating distribution, it is therefore important that something in the training criterion or the parametrization prevents the auto-encoder from learning the identity function, which would yield zero reconstruction error everywhere. This is achieved through various means in the different forms of auto-encoders, as described below in more detail, and we call these regularized auto-encoders. A particular form of regularization consists in constraining the code to have a low dimension, and this is what the classical auto-encoder or PCA do.
In summary, basic auto-encoder training consists in finding a value of parameter vector θ minimizing reconstruction error
JDAE(θ)
=
� t
L(x(t), gθ(fθ(x(t))))(19) where x(t) is a training example. This minimization is usually carried out by stochastic gradient descent as in the training of Multi-Layer-Perceptrons (MLPs). Since auto-encoders were primarily developed as MLPs predicting their input, the most commonly used forms for the encoder and decoder are affine mappings, optionally followed by a non-linearity: fθ(x)
= sf(b + Wx)(20) gθ(h)
= sg(d + W ′h)(21) where sf and sg are the encoder and decoder activation functions (typically the element-wise sigmoid or hyperbolic tangent non-linearity, or the identity function if staying linear).
The set of parameters of such a model is θ = {W, b, W ′, d} where b and d are called encoder and decoder bias vectors, and W and W ′ are the encoder and decoder weight matrices.
The choice of sg and L depends largely on the input domain range. and nature, and are usually chosen so that L returns a negative log-likelihood for the observed value of x. A natural choice for an unbounded domain is a linear decoder with a squared reconstruction error, i.e. sg(a) = a and L(x, r) =
∥x − r∥2. If inputs are bounded between 0 and 1 however, ensuring a similarly-bounded reconstruction can be achieved by using sg = sigmoid. In addition if the inputs are of a binary nature, a binary cross-entropy loss12 is sometimes used.
In the case of a linear auto-encoder (linear encoder and decoder) with squared reconstruction error, the basic autoencoder objective in Equation 19 is known to learn the same subspace13 as PCA. This is also true when using a sigmoid nonlinearity in the encoder (Bourlard and Kamp, 1988), but not if the weights W and W ′ are tied (W ′ = W T ).
Similarly, Le et al. (2011b) recently showed that adding a regularization term of the form � i
� j s3(Wjxi) to a linear auto-encoder with tied weights, where s3 is a nonlinear convex function, yields an efficient algorithm for learning linear ICA.
If both encoder and decoder use a sigmoid non-linearity, then fθ(x) and gθ(h) have the exact same form as the conditionals P(h | v) and P(v | h) of binary RBMs (see Section
6.2.1). This similarity motivated an initial study (Bengio et al., 2007) of the possibility of replacing RBMs with auto-encoders as the basic pre-training strategy for building deep networks, as well as the comparative analysis of auto-encoder reconstruction error gradient and contrastive divergence updates (Bengio and Delalleau, 2009).
12. L(x, r) = − �dx i=1 xi log(ri) + (1 − ri) log(1 − ri)
13. Contrary to traditional PCA loading factors, but similarly to the parameters learned by probabilistic PCA, the weight vectors learned by such an auto-encoder are not constrained to form an orthonormal basis, nor to have a meaningful ordering. They will however span the same subspace.
One notable difference in the parametrization is that RBMs use a single weight matrix, which follows naturally from their energy function, whereas the auto-encoder framework allows for a different matrix in the encoder and decoder. In practice however, weight-tying in which one defines W ′
= W T may be (and is most often) used, rendering the parametrizations identical. The usual training procedures however differ greatly between the two approaches. A practical advantage of training auto-encoder variants is that they define a simple tractable optimization objective that can be used to monitor progress.
Regularized Auto-Encoders
Traditionally, auto-encoders, like PCA, were primarily seen as a dimensionality reduction technique and thus used a bottleneck, i.e. dh < dx. But successful uses of sparse coding and RBM approaches tend to favour learning over-complete representations, i.e. dh > dx. This can render the autoencoding problem too simple (e.g. simply duplicating the input in the features may allow perfect reconstruction without having extracted more meaningful features). Thus alternative ways to "constrain" the representation, other than constraining its dimensionality, have been investigated. We broadly refer to these alternatives as "regularized" auto-encoders. The effect of a bottleneck or of these regularization terms is that the auto-encoder cannot reconstruct well everything, it is trained to reconstruct well the training examples and generalization means that reconstruction error is also small on test examples.
An interesting justification (Ranzato et al., 2008) for the sparsity penalty (or any penalty that restricts in a soft way the volume of hidden configurations easily accessible by the learner) is that it acts in spirit like the partition function of RBMs, by making sure that only few input configurations can have a low reconstruction error.
Alternatively, one can view the objective of the regularization applied to an auto-encoder is to make the representation as "constant" (insensitive) as possible with respect to changes in input. This view immediately justifies two variants of regularized auto-encoders described below: contractive autoencoders reduce the number of effective degrees of freedom of the representation (around each point) by making the encoder contractive, i.e., making the derivative of the encoder small(thus making the hidden units saturate), while the denoising auto-encoder makes the whole mapping "robust", i.e., insensitive to small random perturbations, or contractive, making sure that the reconstruction cannot be good when moving in most directions around a training example.
Sparse Auto-Encoders
The earliest use of single-layer auto-encoders for building deep architectures by stacking them (Bengio et al., 2007) considered the idea of tying the encoder weights and decoder weights to restrict capacity as well as the idea of introducing a form of sparsity regularization (Ranzato et al., 2007).
Several ways of introducing sparsity in the representation learned by auto-encoders have then been proposed, some by penalizing the hidden unit biases (making these additive offset parameters more negative) (Ranzato et al., 2007; Lee et al., 2008; Goodfellow et al., 2009; Larochelle and Bengio, 2008) and some by directly penalizing the output of the hidden unit activations (making them closer to their saturating value at
0) (Ranzato et al., 2008; Le et al., 2011a; Zou et al., 2011).
Note that penalizing the bias runs the danger that the weights could compensate for the bias, which could hurt the numerical optimization of parameters. When directly penalizing the hidden unit outputs, several variants can be found in the literature, but no clear comparative analysis has been published to evaluate which one works better. Although the L1 penalty(i.e., simply the sum of output elements hj in the case of sigmoid non-linearity) would seem the most natural (because of its use in sparse coding), it is used in few papers involving sparse auto-encoders. A close cousin of the L1 penalty is the Student-t penalty (log(1+h2 j)), originally proposed for sparse coding (Olshausen and Field, 1997). Several papers penalize the average output ¯hj (e.g. over a minibatch), and instead of pushing it to 0, encourage it to approach a fixed target, either through a mean-square error penalty, or maybe more sensibly (because hj behaves like a probability), a KullbackLiebler divergence with respect to the binomial distribution with probability ρ: −ρ log ¯hj − (1 − ρ) log(1 − ¯hj)+constant, e.g., with ρ = 0.05.
Denoising Auto-Encoders
Vincent et al. (2008, 2010) proposed altering the training objective in Equation 19 from mere reconstruction to that of denoising an artificially corrupted input, i.e. learning to reconstruct the clean input from a corrupted version. Learning the identity is no longer enough: the learner must capture the structure of the input distribution in order to optimally undo the effect of the corruption process, with the reconstruction essentially being a nearby but higher density point than the corrupted input. Figure 3 illustrates that the denoising autoencoder is learning a reconstruction function that corresponds to a vector field pointing towards high-density regions (the manifold where examples concentrate).
Corrupted input
Corrupted input prior:&examples&concentrate& near&a&lower&dimensional&
"manifold"&& original input
Fig. 3.
When the data concentrate near a lower-dimensional manifold, the corruption vector is most of the time almost orthogonal to the manifold, and the reconstruction function learns to denoise, map from low-probability configurations (corrupted inputs) to high-probability ones (original inputs), creating a kind of vector field aligned with the score (derivative of the estimated density).
Formally, the objective optimized by such a Denoising
Auto-Encoder (DAE) is:
JDAE
=
� t
Eq(˜x|x(t))
�
L(x(t), gθ(fθ(˜x)))
�(22) where Eq(˜x|x(t)) [·] denotes the expectation over corrupted examples ˜x drawn from corruption process q(˜x|x(t)). In practice this is optimized by stochastic gradient descent, where the stochastic gradient is estimated by drawing one or a few corrupted versions of x(t) each time x(t) is considered. Corruptions considered in Vincent et al. (2010) include additive isotropic Gaussian noise, salt and pepper noise for gray-scale images, and masking noise (salt or pepper only). Qualitatively better features are reported, resulting in improved classification performance, compared to basic auto-encoders, and similar or better than that obtained with RBMs. Chen et al. (2012) show that a simpler alternative with a closed form solution can be obtained when restricting to a linear auto-encoder and have successfully applied it to domain adaptation.
The analysis in Vincent (2011) relates the denoising autoencoder criterion to energy-based probabilistic models: denoising auto-encoders basically learn in r(˜x) − ˜x a vector pointing in the direction of the estimated score i.e., ∂ log p(˜x)
∂˜x, as illustrated in Figure 3. In the special case of linear reconstruction and squared error, Vincent (2011) shows that
DAE training amounts to learning an energy-based model, whose energy function is very close to that of a GRBM, using a regularized variant of the score matching parameter estimation technique (Hyv¨arinen, 2005a; Hyv¨arinen, 2008;
Kingma and LeCun, 2010) termed denoising score matching (Vincent, 2011). Previously, Swersky (2010) had shown that training GRBMs with score matching was equivalent to training a regular (non-denoising) auto-encoder with an additional regularization term, while, following up on the theoretical results in Vincent (2011), Swersky et al. (2011) showed the practical advantage of the denoising criterion to implement score matching efficiently.
Contractive Auto-Encoders
Contractive Auto-Encoders (CAE) proposed by Rifai et al.(2011a) follow up on Denoising Auto-Encoders (DAE) and share a similar motivation of learning robust representations.
CAEs achieve this by adding an analytic contractive penalty term to the basic auto-encoder of Equation 19. This term is the Frobenius norm of the encoder's Jacobian, and results in penalizing the sensitivity of learned features to infinitesimal changes of the input.
Let J(x) =
∂fθ
∂x (x) the Jacobian matrix of the encoder evaluated at x. The CAE's training objective is the following:
JCAE
=
� t
L(x(t), gθ(fθ(x(t)))) + λ
���J(x(t))
���
F(23) where λ is a hyper-parameter controlling the strength of the regularization.
For an affine sigmoid encoder, the contractive penalty term is easy to compute:
Jj(x)
= fθ(x)j(1 − fθ(x)j)Wj
���J(x(t))
���
=
� j(fθ(x)j(1 − fθ(x)j))2∥Wj∥2
There are at least three notable differences with DAEs, which may be partly responsible for the better performance that
CAE features seem to empirically demonstrate: a) the sensitivity of the features is penalized14 directly rather than the sensitivity of the reconstruction; b) penalty is analytic rather than stochastic: an efficiently computable expression replaces what might otherwise require dx corrupted samples to size up(i.e. the sensitivity in dx directions); c) a hyper-parameter λ allows a fine control of the trade-off between reconstruction and robustness (while the two are mingled in a DAE). Note however that there is a tight connection between the DAE and the CAE: as shown in (Bengio et al., 2012b) a DAE with small corruption noise can be seen (through a Taylor expansion) as a type of contractive auto-encoder where the contractive penalty is on the whole reconstruction function rather than just on the encoder15.
A potential disadvantage of the CAE's analytic penalty is that it amounts to only encouraging robustness to infinitesimal changes of the input. This is remedied by a further extension proposed in Rifai et al. (2011b) and termed CAE+H, that penalizes all higher order derivatives, in an efficient stochastic manner, by adding a third term that encourages J(x) and J(x + ϵ) to be close:
JCAE+H
=
� t
L(x(t), gθ(x(t))) + λ
���J(x(t))
���
F
+γEϵ
�
∥J(x) − J(x + ϵ)∥2
F
�(25) where ϵ ∼ N(0, σ2I), and γ is the associated regularization strength hyper-parameter. As for the DAE, the training criterion is optimized by stochastic gradient descent, whereby the expectation is approximated by drawing several corrupted versions of x(t).
Note that the DAE and CAE have been successfully used to win the final phase of the Unsupervised and Transfer
Learning Challenge (Mesnil et al., 2011). Note also that the representation learned by the CAE tends to be saturated rather than sparse, i.e., most of the hidden units are near the extremes of their range (e.g. 0 or 1), and their derivative
∂hi(x)
∂x is tiny. The non-saturated units are few and sensitive to the inputs, with their associated filters (hidden unit weight vector) together forming a basis explaining the local changes around x, as discussed in Section 8.2. Another way to get saturated (i.e. nearly binary) units (for the purpose of hashing) is semantic hashing (Salakhutdinov and Hinton, 2007).
Predictive Sparse Decomposition
Sparse coding (Olshausen and Field, 1997) may be viewed as a kind of auto-encoder that uses a linear decoder with a squared reconstruction error, but whose non-parametric encoder fθ performs the comparatively non-trivial and relatively costly minimization of Equation. 2, which entails an iterative optimization.
A practically successful variant of sparse coding and auto-encoders, named Predictive Sparse Decomposition or 14. i.e., the robustness of the representation is encouraged.
15. but note that in the CAE, the decoder weights are tied to the encoder weights, to avoid degenerate solutions, and this should also make the decoder contractive.
PSD (Kavukcuoglu et al., 2008) replaces that costly and highly non-linear encoding step by a fast non-iterative approximation during recognition (computing the learned features).
PSD has been applied to object recognition in images and video (Kavukcuoglu et al., 2009, 2010; Jarrett et al., 2009;
Farabet et al., 2011), but also to audio (Henaff et al., 2011), mostly within the framework of multi-stage convolutional and hierarchical architectures (see Section 11.2). The main idea can be summarized by the following equation for the training criterion, which is simultaneously optimized with respect to the hidden codes (representation) h(t) and with respect to the parameters (W, α):
JPSD =
� t λ∥h(t)∥1 + ∥x(t) − Wh(t)∥2
2 + ∥h(t) − fα(x(t))∥2
2 (26) where x(t) is the input vector for example t, h(t) is the optimized hidden code for that example, and fα(·) is the encoding function, the simplest variant being fα(x(t)) = tanh(b + W T x(t))(27) where the encoding weights are the transpose of the decoding weights, but many other variants have been proposed, including the use of a shrinkage operation instead of the hyperbolic tangent (Kavukcuoglu et al., 2010). Note how the L1 penalty on h tends to make them sparse, and notice that it is the same criterion as sparse coding with dictionary learning(Eq. 3) except for the additional constraint that one should be able to approximate the sparse codes h with a parametrized encoder fα(x). One can thus view PSD as an approximation to sparse coding, where we obtain a fast approximate encoding process as a side effect of training. In practice, once PSD is trained, object representations used to feed a classifier are computed from fα(x), which is very fast, and can then be further optimized (since the encoder can be viewed as one stage or one layer of a trainable multi-stage system such as a feedforward neural network).
PSD can also be seen as a kind of auto-encoder (there is an encoder fα(·) and a decoder W) where, instead of being tied to the output of the encoder, the codes h are given some freedom that can help to further improve reconstruction. One can also view the encoding penalty added on top of sparse coding as a kind of regularizer that forces the sparse codes to be nearly computable by a smooth and efficient encoder. This is in contrast with the codes obtained by complete optimization of the sparse coding criterion, which are highly non-smooth or even non-differentiable, a problem that motivated other approaches to smooth the inferred codes of sparse coding (Bagnell and Bradley, 2009), so a sparse coding stage could be jointly optimized along with following stages of a deep architecture.
REPRESENTATION
LEARNING
AS
MANIFOLD LEARNING
Another important perspective on representation learning is based on the geometric notion of manifold. Its premise is the manifold hypothesis (Cayton, 2005; Narayanan and Mitter, 2010), according to which real-world data presented in high dimensional spaces are expected to concentrate in the vicinity of a manifold M of much lower dimensionality dM, embedded in high dimensional input space Rdx. This can be a potentially powerful prior for representation learning for AI tasks. As soon as there is a notion of "representation" then one can think of a manifold by considering the variations in input space, which are captured by or reflected (by corresponding changes) in the learned representation. To first approximation, some directions are well preserved (they are the tangent directions of the manifold) while others aren't (they are directions orthogonal to the manifolds). With this perspective, the primary unsupervised learning task is then seen as modeling the structure of the datasupporting manifold16. The associated representation being learned corresponds to an intrinsic coordinate system on the embedded manifold. The archetypal manifold modeling algorithm is, not surprisingly, also the archetypal low dimensional representation learning algorithm: Principal Component Analysis. PCA models a linear manifold. It was initially devised by
Pearson (1901) precisely with the objective of finding the closest linear manifold (specifically a line or a plane) to a cloud of data points. The principal components, i.e. the representation fθ(x) that PCA yields for an input point x, uniquely locates its projection on that manifold: it corresponds to intrinsic coordinates on the manifold. Data manifold for complex real world domains are however expected to be strongly nonlinear. Their modeling is sometimes approached as patchworks of locally linear tangent spaces (Vincent and Bengio, 2003;
Brand, 2003). The large majority of algorithms built on this geometric perspective adopt a non-parametric approach, based on a training set nearest neighbor graph (Sch¨olkopf et al., 1998; Roweis and Saul, 2000; Tenenbaum et al., 2000;
Brand, 2003; Belkin and Niyogi, 2003; Donoho and Grimes, 2003; Weinberger and Saul, 2004; Hinton and Roweis, 2003; van der Maaten and Hinton, 2008). In these non-parametric approaches, each high-dimensional training point has its own set of free low-dimensional embedding coordinates, which are optimized so that certain properties of the neighborhood graph computed in original high dimensional input space are best preserved. These methods however do not directly learn a parametrized feature extraction function fθ(x) applicable to new test points17, which seriously limits their use as feature extractors, except in a transductive setting. Comparatively few non-linear manifold learning methods have been proposed, that learn a parametric map that can directly compute a representation for new points; we will focus on these.
Learning a parametric mapping based on a neighborhood graph
The non-parametric manifold learning algorithms we just mentioned are all based on a training set neighborhood graph, typically derived from pairwise Euclidean distances between training points. Some of them are not too difficult to modify from non-parametric to instead learn a parametric mapping fθ, 16. What is meant by data manifold is actually a loosely defined notion: data points need not strictly lie on it, but the probability density is expected to fall off sharply as one moves away from the "manifold" (which may actually be constituted of several possibly disconnected manifolds with different intrinsic dimensionality).
17. For several of these techniques, representations for new points can be computed using the Nystr¨om approximation as has been proposed as an extension in (Bengio et al., 2004), but this remains cumbersome and computationally expensive.
18 that will be applicable to new points. The principle is simple: instead of having free low-dimensional embedding coordinate
"parameters" for each training point, these coordinates are now obtained through an explicitly parametrized function on inputspace coordinates, whose parameters are to be learned. The same optimization objective as in the non-parametric version can be minimized, through gradient descent: now instead of gradient descent updates on the embedding coordinates, gradients are backpropagated further to the parameters of that mapping function. Thus a parametric version of the very successful non-parametric manifold embedding algorithm tSNE (van der Maaten and Hinton, 2008) has been proposed in (van der Maaten, 2009), and could be directly applied to learning a direct parametric encoding.
Another interesting approach, that learns a direct encoding while taking into account the manifold hypothesis through a neighborhood graph is Semi-Supervised Embedding (Weston et al., 2008). Here a deep parametrized neural network architecture simultaneously learns a manifold embedding and a classifier. While optimizing the supervised classification cost, the training criterion also uses training set neighbors of each training example to encourage intermediate layers of representation to be invariant when changing the training example for a neighbor.
The more reduced and tightly controlled number of free parameters in such methods, compared to their pure nonparametric counterparts, forces the models to generalize the manifold shape non-locally (Bengio et al., 2006b), which, provided that generalization is valid, can translate into better features and final performance (van der Maaten and Hinton, Yet basing the modeling of manifolds on training set neighborhood relationships might be risky statistically in high dimensional spaces (sparsely populated due to the curse of dimensionality) as e.g. most Euclidean nearest neighbors risk having too little in common semantically. The neareset neighbor graph is simply not enough densely populated to map out satisfyingly the wrinkles of the target manifold. It can also become problematic computationally to consider all pairs of data points18, which scales quadratically with training set size.
Learning a non-linear manifold through a coding scheme
We now turn to manifold interpretations of learning techniques that are not based on training set neighbor searches. Let us begin with PCA, seen as an encoding scheme. In PCA, the same basis vectors are used to project any input point x. The sensitivity of the extracted components (the code) to input changes in the direction of these vectors is the same regardless of position x. The tangent space is the same everywhere along the linear manifold. By contrast, for a non-linear manifold, the tangent space is expected to change directions as we move, as illustrated by the tangent plane in Figure 6. In non-linear representation-learning algorithms it is convenient to think about the local variations in the representation as the input
18. Even if pairs are picked stochastically, many must be considered before obtaining one that weighs significantly on the optimization objective. x is varied on the manifold, i.e., as we move from a highprobability example to a very close one in input space. As we will discuss below, the first derivative of the mapping from input to representation (the encoder) therefore specifies the shape of the manifold (its tangent plane) around an example x lying on it. If the density was really concentrated on the manifold, and the encoder had captured that, we would find the derivatives to be non-zero only in the directions spanned by the tangent plane.
Let us consider sparse-coding in this light: parameter matrix
W may be interpreted as a dictionary of input directions from which a different subset will be picked to model the local tangent space at an x on the manifold. That subset corresponds to the active, i.e. non-zero, features for input x. Note that nonzero component hi will be sensitive to small changes of the input in the direction of the associated weight vector W:,i, whereas inactive features are more likely to be stuck at 0 until a significant displacement has taken place in input space.
The Local Coordinate Coding (LCC) algorithm (Yu et al., 2009) is very similar to sparse coding, but is explicitly derived from a manifold perspective. Using the same notation as that of sparse-coding in Equation 2, LCC replaces regularization term ∥h(t)∥1 = � j |h(t) j | yielding objective
JLCC =
� t
�
∥x(t) − Wh(t)∥2
2 + λ
� j
|h(t) j |∥W:,j − x(t)∥1+p
�
This is identical to sparse-coding when p = −1, but with larger p it encourages the active anchor points for x(t) (i.e. the codebook vectors W:,j with non-negligible |h(t) j | that are combined to reconstruct x(t)) to be not too far from x(t), hence the local aspect of the algorithm. An important theoretical contribution of Yu et al. (2009) is to show that that any Lipschitz-smooth function φ : M → R defined on a smooth nonlinear manifold M embedded in Rdx, can be well approximated by a globally linear function with respect to the resulting coding scheme (i.e. linear in h), where the accuracy of the approximation and required number dh of anchor points depend on dM rather than dx. This result has been further extended with the use of local tangent directions (Yu and Zhang, 2010), as well as to multiple layers (Lin et al., 2010).
Let us now consider the efficient non-iterative "feedforward" encoders fθ, used by PSD and the auto-encoders reviewed in Section 7.2, that are in the form of Equation
20 or 27.The computed representation for x will be only significantly sensitive to input space directions associated with non-saturated hidden units (see e.g. Eq. 24 for the Jacobian of a sigmoid layer). These directions to which the representation is significantly sensitive, like in the case of PCA or sparse coding, may be viewed as spanning the tangent space of the manifold at training point x.
Rifai et al. (2011a) empirically analyze in this light the singular value spectrum of the Jacobian (derivatives of representation vector with respect to input vector) of a trained
CAE. Here the SVD provides an ordered orthonormal basis of most sensitive directions. The spectrum is sharply decreasing, indicating a relatively small number of significantly sensitive directions. This is taken as empirical evidence that the 1"
MNIST"
Input"Point"
Tangents"
Fig. 4.
The tangent vectors to the high-density manifold as estimated by a Contractive Auto-Encoder (Rifai et al., 2011a).
The original input is shown on the top left. Each tangent vector(images on right side of first row) corresponds to a plausible additive deformation of the original input, as illustrated on the second row, where a bit of the 3rd singular vector is added to the original, to form a translated and deformed image. Unlike in PCA, the tangent vectors are different for different inputs, because the estimated manifold is highly non-linear.
CAE indeed modeled the tangent space of a low-dimensional manifold. The leading singular vectors form a basis for the tangent plane of the estimated manifold, as illustrated in Figure 4. The CAE criterion is believed to achieve this thanks to its two opposing terms: the isotropic contractive penalty, that encourages the representation to be equally insensitive to changes in any input directions, and the reconstruction term, that pushes different training points (in particular neighbors) to have a different representation (so they may be reconstructed accurately), thus counteracting the isotropic contractive pressure only in directions tangent to the manifold.
Note that analyzing learned representations through the lens of the spectrum of the Jacobian and relating it to the notion of tangent space of a manifold is feasible, whenever the mapping is differentiable, and regardless of how it was learned, whether as direct encoding (as in auto-encoder variants), or derived from latent variable inference (as in sparse coding or RBMs). Exact low dimensional manifold models (like PCA) would yield non-zero singular values associated to directions along the manifold, and exact zeros for directions orthogonal to the manifold. But in smooth models like the contractive auto-encoder or the RBM we will instead have large versus relatively small singular values (as opposed to non-zero versus exactly zero).
Leveraging the modeled tangent spaces
The local tangent space, at a point along the manifold, can be thought of capturing locally valid transformations that were prominent in the training data. For example Rifai et al.(2011c) examine the tangent directions extracted with an SVD of the Jacobian of CAEs trained on digits, images, or textdocument data: they appear to correspond to small translations or rotations for images or digits, and to substitutions of words within a same theme for documents. Such very local transformations along a data manifold are not expected to change class identity. To build their Manifold Tangent
Classifier (MTC), Rifai et al. (2011c) then apply techniques such as tangent distance (Simard et al., 1993) and tangent propagation (Simard et al., 1992), that were initially developed to build classifiers that are insensitive to input deformations provided as prior domain knowledge. Now these techniques are applied using the local leading tangent directions extracted by a CAE, i.e. not using any prior domain knowledge (except the broad prior about the existence of a manifold). This approach set a new record for MNIST digit classification among prior-knowledge free approaches19.
CONNECTIONS
BETWEEN PROBABILISTIC
AND DIRECT ENCODING MODELS
The standard likelihood framework for probabilistic models decomposes the training criterion for models with parameters θ in two parts: the log-likelihood log P(x|θ) (or log P(x|h, θ) with latent variables h), and the prior log P(θ)(or log P(h|θ) + log P(θ) with latent variables).
PSD: a probabilistic interpretation
In the case of the PSD algorithm, a connection can be made between the above standard probabilistic view and the direct encoding computation graph. In this view, the probabilistic model of PSD is the same directed generative model P(x|h) of sparse coding (Section 6.1.3), which only accounts for the decoder. The encoder is viewed as an approximate inference mechanism used to guess P(h|x) and initialize a MAP iterative inference (where the sparse prior P(h) is taken into account). However, note that in PSD, the encoder is trained jointly with the decoder, rather than simply taking the end result of iterative inference as a target to approximate. An interesting view20 to integrate this fact is that the encoder is a parametric approximation for the MAP solution of a variational lower bound on the joint log-likelihood. When
MAP learning is viewed as a special case of variational learning (where the approximation of the joint log-likelihood is with a dirac distribution located at the MAP solution), the variational recipe tells us to simultaneously improve the likelihood (reduce reconstruction error) and improve the variational approximation (reduce the discrepancy between the encoder output and the latent variable value). Hence PSD is an interesting case of representation learning algorithm that sits at the intersection of probabilistic models (with latent variables) and direct encoding methods (which directly parametrize the mapping from input to representation). RBMs also sit at the intersection because their particular parametrization includes an explicit mapping from input to representation, thanks to the restricted connectivity between hidden units. However, this nice property does not extend to their natural deep generalizations, i.e., Deep Boltzmann Machines, discussed in Section 10.2.
Regularized
Auto-Encoders
Capture
Local
Statistics of the Density
Can we also say something about the probabilistic interpretation of regularized auto-encoders, including sparse
19. It yielded 0.81% error rate using the full MNIST training set, with no prior deformations, and no convolution.
20. suggested by Ian Goodfellow, personal communication
20 auto-encoders, denoising auto-encoders, and contractive autoencoders? Their training criterion does not fit the standard likelihood framework because this would require a kind of prior(e.g. we want a sparse or contractive or robust representation) that is data-dependent.
An interesting hypothesis emerges to answer that question, out of recent theoretical results (Vincent, 2011; Bengio et al., 2012b): their training criterion, instead of being a form of maximum likelihood, corresponds to a different inductive principle, such as score matching. The score matching connection is discussed in Section 7.2.2 and has been shown for a particular parametrization of Denoising Auto-Encoder and equivalent Gaussian RBM (Vincent, 2011). The work in Bengio et al. (2012b) generalizes this idea to a broader class of parametrizations (arbitrary encoders and decoders), and shows that by regularizing the auto-encoder so that it be contractive (which is the case not only of contractive autoencoders but also of denoising and sparse ones), one obtains that the reconstruction function and its derivative estimate local statistics of the underlying data-generative distribution, such as the local mean (the mean in a small ball around each point), the local covariance, and the first and second derivatives of the estimated density. This view can actually be exploited to successfully sample from auto-encoders, as shown in Rifai et al. (2012); Bengio et al. (2012b). The proposed sampling algorithms are MCMCs similar to Langevin MCMC, using not just the estimated first derivative of the density but also the estimated second derivative, so as to stay close to manifolds near which the density concentrates. x" r(x)" x1" x2" x3"
Fig. 5. The reconstruction function r(x) (in green) learned by an autoencoder on a 1-dimensional input with high capacity, minimizing reconstruction error at the training examples xi (with in r(xi) in red) while trying to be as constant as possible otherwise. The dotted line is the identity reconstruction (which might be obtained without the regularizer). The blue arrows shows the vector field of r(x) − x pointing towards high density peaks as estimated by the model, and estimating the score (logdensity derivative).
This interpretation connects well with the geometric perspective introduced in Section 8. The regularization effects(e.g., due to a sparsity regularizer, a contractive regularizer, or the denoising criterion) asks the learned representation to be as insensitive as possible to the input, while minimizing reconstruction error on the training examples forces the representation to contain just enough information to distinguish them. The solution is that variations along the high-density manifolds are preserved while other variations are compressed.
It means that the reconstruction function should be as constant as possible while reproducing training examples, i.e., that points near a training example should be mapped to that training example, as illustrated in Figure 5. This also means that the reconstruction function should map an input towards the nearest point manifold, i.e., the difference between reconstruction and input is a vector aligned with the estimated score (the derivative of the log-density with respect to the input). When the score is zero (on the manifold), we have to look towards the second derivative of the log-density or of the energy (and the first derivative of the reconstruction function).
The directions of smallest second derivatives of the log-density are those where the density remains high (where the first derivative remains close to 0) and correspond to moving on the manifold.
Fig. 6.
Illustration of the sampling procedure for regularized auto-encoders (Rifai et al., 2012; Bengio et al., 2012b): Each
MCMC step consists in adding noise δ mostly in the directions of the estimated manifold tangent plane and projecting back towards the manifold (high-density regions) by performing a reconstruction step.
As illustrated in Figure 6, the basic idea of the autoencoder sampling algorithms in Rifai et al. (2012); Bengio et al. (2012b) is to make MCMC moves where one (a) moves toward the manifold by following the density gradient(i.e., applying a reconstruction) and (b) adds noise in the directions of the leading singular vectors of the reconstruction(or encoder) Jacobian, corresponding to those associated with smallest second derivative of the log-density.
Learning Approximate Inference
Let us now consider from closer how a representation is computed in probabilistic models with latent variables, when iterative inference is required. There is a computation graph(possibly with random number generation in some of the nodes, in the case of MCMC) that maps inputs to representation, and in the case of deterministic inference (e.g., MAP inference or variational inference), that function could be optimized directly. This is a way to generalize PSD that has been explored in recent work on probabilistic models at the intersection of inference and learning (Bagnell and Bradley, 2009; Gregor and LeCun, 2010b; Grubb and Bagnell, 2010;
Salakhutdinov and Larochelle, 2010; Stoyanov et al., 2011;
Eisner, 2012), where a central idea is that instead of using a generic inference mechanism, one can use one that is learned and is more efficient, taking advantage of the specifics of the type of data on which it is applied.
Sampling Challenges
A troubling challenge with many probabilistic models with latent variables like most Boltzmann machine variants is that good MCMC sampling is required as part of the learning procedure, but that sampling becomes extremely inefficient (or unreliable) as training progresses because the modes of the learned distribution become sharper, making mixing between modes very slow. Whereas initially during training a learner assigns mass almost uniformly, as training progresses, its entropy decreases, approaching the entropy of the target distribution as more examples and more computation are provided. According to our Manifold and Natural Clustering priors of Section 3.1, the target distribution has sharp modes (manifolds) separated by extremely low density areas. Mixing then becomes more difficult because MCMC methods, by their very nature, tend to make small steps to nearby high-probability configurations.
This is illustrated in Figure 7.
1"
Fig. 7. Top: early during training, MCMC mixes easily because the estimated distribution has high entropy and puts enough mass everywhere for small-steps movements (MCMC) to go from mode to mode. Bottom: later on, training relying on good mixing can stall because estimated modes are separated by long low-density deserts.
Bengio et al. (2012a) suggest that deep representations could help mixing between such well separated modes, based on both theoretical arguments and on empirical evidence. The idea is that if higher-level representations disentangle better the underlying abstract factors, then small steps in this abstract space (e.g., swapping from one category to another) can easily be done by MCMC. The high-level representations can then be mapped back to the input space in order to obtain inputlevel samples, as in the Deep Belief Networks (DBN) sampling algorithm (Hinton et al., 2006a). This has been demonstrated both with DBNs and with the newly proposed algorithm for sampling from contracting and denoising auto-encoders (Rifai et al., 2012; Bengio et al., 2012b). This observation alone does not suffice to solve the problem of training a DBN or a DBM, but it may provide a crucial ingredient, and it makes it possible to consider successfully sampling from deep models trained by procedures that do not require an MCMC, like the stacked regularized auto-encoders used in Rifai et al. (2012).
Evaluating and Monitoring Performance
It is always possible to evaluate a feature learning algorithm in terms of its usefulness with respect to a particular task (e.g. object classification), with a predictor that is fed or initialized with the learned features. In practice, we do this by saving the features learned (e.g. at regular intervals during training, to perform early stopping) and training a cheap classifier on top (such as a linear classifier). However, training the final classifier can be a substantial computational overhead (e.g., supervised fine-tuning a deep neural network takes usually more training iterations than the feature learning itself), so we may want to avoid having to train a classifier for every training iteration of the unsupervised learner and every hyper-parameter setting. More importantly this may give an incomplete evaluation of the features (what would happen for other tasks?). All these issues motivate the use of methods to monitor and evaluate purely unsupervised performance. This is rather easy with all the auto-encoder variants (with some caution outlined below) and rather difficult with the undirected graphical models such as the RBM and Boltzmann machines.
For auto-encoder and sparse coding variants, test set reconstruction error can readily be computed, but by itself may be misleading because larger capacity (e.g., more features, more training time) tends to systematically lead to lower reconstruction error, even on the test set. Hence it cannot be used reliably for selecting most hyper-parameters. On the other hand, denoising reconstruction error is clearly immune to this problem, so that solves the problem for denoising auto-encoders. It is not clear and remains to be demonstrated empirically or theoretically if this may also be true of the training criteria for sparse auto-encoders and contractive autoencoders.
For RBMs and some (not too deep) Boltzmann machines, one option is the use of Annealed Importance Sampling (Murray and Salakhutdinov, 2009) in order to estimate the partition function (and thus the test log-likelihood). Note that this estimator can have high variance and that it becomes less reliable(variance becomes too large) as the model becomes more interesting, with larger weights, more non-linearity, sharper modes and a sharper probability density function (see our previous discussion in Section 9.4). Another interesting and recently proposed option for RBMs is to track the partition function during training (Desjardins et al., 2011), which could be useful for early stopping and reducing the cost of ordinary
AIS. For toy RBMs (e.g., 25 hidden units or less, or 25 inputs or less), the exact log-likelihood can also be computed analytically, and this can be a good way to debug and verify some properties of interest.
GLOBAL TRAINING OF DEEP MODELS
One of the most interesting challenges raised by deep architectures is: how should we jointly train all the levels? In the previous section we have only discussed how single-layer models could be combined to form a deep model with a joint training criterion. Here we consider joint training of all the levels and the difficulties that may arise. This follows up on
Section 4, where we focused on how to combine single-layer learners into deep architectures.
On the Challenge of Training Deep Architectures
Higher-level abstraction means more non-linearity. It means that two nearby configurations of the input may have to be interpreted very differently because a few surface details change the underlying semantics, whereas most other changes in the surface details would not change the underlying semantics. The representations associated with input manifolds may be complex because these functions may have to unfold and distort input manifolds that generally have complicated shapes into spaces where distributions are much simpler, where relations between factors are simpler, maybe even linear or involving many (conditional) independencies. Our expectation is that modeling the joint distribution between high-level abstractions and concepts should be much easier in the sense of requiring much less data to learn. The hard part is learning the representation. The price to pay may be that these more abstract representation functions (mapping input to representation) involve a more challenging training task, because of the added non-linearity.
It is only since 2006 that researchers have seriously investigated ways to train deep architectures, to the exception of the convolutional networks (LeCun et al., 1998b). The first realization was that unsupervised or supervised layer-wise training was easier, and that this could be taken advantage of by stacking single-layer models into deeper ones, as discussed at length in Section 4.
It is interesting to ask why does the layerwise unsupervised pre-training procedure sometimes help a supervised learner (Erhan et al., 2010b). There seems to be a more general principle at play 21 of guiding the training of intermediate representations, which may be easier than trying to learn it all in one go. This is nicely related with the curriculum learning idea (Bengio et al., 2009) that it may be much easier to learn simpler concepts first and then build higher-level ones on top of simpler ones. This is also coherent with the success of several deep learning algorithms that provide some such guidance for intermediate representations, like SemiSupervised Embedding (Weston et al., 2008).
The question of why unsupervised pre-training could be helpful was extensively studied (Erhan et al., 2010b), trying to dissect the answer into a regularization effect and an optimization effect. The regularization effect is clear from the experiments where the stacked RBMs or denoising autoencoders are used to initialize a supervised classification neural network (Erhan et al., 2010b). It may simply come from the use of unsupervised learning to bias the learning dynamics and initialize it in the basin of attraction of a "good" local minimum (of the training criterion), where "good" is in terms of generalization error. The underlying hypothesis exploited by this procedure is that some of the features or latent factors that are good at capturing the leading variations in the input distribution are also good at capturing the variations in the target output random variables of interest (e.g., classes). The optimization effect is more difficult to tease out because the top two layers of a deep neural net can just overfit the training
21. First communicated to us by Leon Bottou set whether the lower layers compute useful features or not, but there are several indications that optimizing the lower levels with respect to a supervised training criterion is challenging.
One such indication is that changing the numerical conditions of the optimization procedure can have a profound impact on the joint training of a deep architecture, for example changing the initialization range and changing the type of non-linearity used (Glorot and Bengio, 2010), much more so than with shallow architectures. One hypothesis to explain some of the difficulty in the optimization of deep architectures is centered on the singular values of the Jacobian matrix associated with the transformation from the features at one level into the features at the next level (Glorot and Bengio, 2010). If these singular values are all small (less than 1), then the mapping is contractive in every direction and gradients would vanish when propagated backwards through many layers. This is a problem already discussed for recurrent neural networks (Bengio et al., 1994), which can be seen as very deep networks with shared parameters at each layer, when unfolded in time. This optimization difficulty has motivated the exploration of second-order methods for deep architectures and recurrent networks, in particular Hessian-free secondorder methods (Martens, 2010; Martens and Sutskever, 2011).
Unsupervised pre-training has also been proposed to help training recurrent networks and temporal RBMs (Sutskever et al., 2009), i.e., at each time step there is a local signal to guide the discovery of good features to capture in the state variables: model with the current state (as hidden units) the joint distribution of the previous state and the current input. Natural gradient (Amari, 1998) methods that can be applied to networks with millions of parameters (i.e. with good scaling properties) have also been proposed (Le Roux et al., 2008b). Cho et al. (2011) proposes to use adaptive learning rates for RBM training, along with a novel and interesting idea for a gradient estimator that takes into account the invariance of the model to flipping hidden unit bits and inverting signs of corresponding weight vectors. At least one study indicates that the choice of initialization (to make the Jacobian of each layer closer to 1 across all its singular values) could substantially reduce the training difficulty of deep networks (Glorot and Bengio, 2010) and this is coherent with the success of the initialization procedure of Echo State
Networks (Jaeger, 2007), as recently studied by Sutskever(2012). There are also several experimental results (Glorot and Bengio, 2010; Glorot et al., 2011a; Nair and Hinton, 2010) showing that the choice of hidden units non-linearity could influence both training and generalization performance, with particularly interesting results obtained with sparse rectifying units (Jarrett et al., 2009; Nair and Hinton, 2010; Glorot et al., 2011a; Krizhevsky et al., 2012). Another promising idea to improve the conditioning of neural network training is to nullify the average value and slope of each hidden unit output (Raiko et al., 2012), and possibly locally normalize magnitude as well (Jarrett et al., 2009). The debate still rages between using online methods such as stochastic gradient descent and using second-order methods on large minibatches(of several thousand examples) (Martens, 2010; Le et al., 2011a), with a variant of stochastic gradient descent recently
23 winning an optimization challenge 22.
Finally, several recent results exploiting large quantities of labeled data suggest that with proper initialization and choice of non-linearity, very deep purely supervised networks can be trained successfully without any layerwise pre-training (Ciresan et al., 2010; Glorot et al., 2011a;
Krizhevsky et al., 2012). Researchers report than in such conditions, layerwise unsupervised pre-training brought little or no improvement over pure supervised learning from scratch when training for long enough. This reinforces the hypothesis that unsupervised pre-training acts as a prior, which may be less necessary when very large quantities of labeled data are available, but begs the question of why this had not been discovered earlier. The latest results reported in this respect (Krizhevsky et al., 2012) are particularly interesting because they allowed to drastically reduce the error rate of object recognition on a benchmark (the 1000-class ImageNet task) where many more traditional computer vision approaches had been evaluated(http://www.image-net.org/challenges/LSVRC/2012/results.html).
The main techniques that allowed this success include the following: efficient GPU training allowing one to train longer (more than 100 million visits of examples), an aspect first reported by Lee et al. (2009a); Ciresan et al. (2010), large number of labeled examples, artificially transformed examples (see Section 11.1), a large number of tasks (1000 or 10000 classes for ImageNet), convolutional architecture with max-pooling (see section 11 for these latter two techniques), rectifying non-linearities(discussed above), careful initialization (discussed above), careful parameter update and adaptive learning rate heuristics, layerwise feature normalization (across features), and a new dropout trick based on injecting strong binary multiplicative noise on hidden units. This trick is similar to the binary noise injection used at each layer of a stack of denoising auto-encoder.
Future work is hopefully going to help identify which of these elements matter most, how to generalize them across a large variety of tasks and architectures, and in particular contexts where most examples are unlabeled, i.e., including with an unsupervised component in the training criterion.
Joint Training of Deep Boltzmann Machines
We now consider the problem of joint training of all layers of a specific unsupervised model, the Deep Boltzmann Machine(DBM). Whereas much progress (albeit with many unanswered questions) has been made on jointly training all the layers of deep architectures using back-propagated gradients(i.e., mostly in the supervised setting), much less work has been done on their purely unsupervised counterpart, e.g. with
DBMs23. Note however that one could hope that the successful techniques described in the previous section could be applied to unsupervised learning algorithms.
Like the RBM, the DBM is another particular subset of the Boltzmann machine family of models where the units
22. https://sites.google.com/site/nips2011workshop/optimization-challenges
23. Joint training of all the layers of a Deep Belief Net is much more challenging because of the much harder inference problem involved. are again arranged in layers. However unlike the RBM, the DBM possesses multiple layers of hidden units, with units in odd-numbered layers being conditionally independent given even-numbered layers, and vice-versa. With respect to the Boltzmann energy function of Eq. 7, the DBM corresponds to setting U = 0 and a sparse connectivity structure in both V and W. We can make the structure of the DBM more explicit by specifying its energy function. For the model with two hidden layers it is given as:
EDBM θ(v, h(1), h(2); θ) = − vT Wh(1) − h(1) T V h(2)− d(1) T h(1) − d(2) T h(2) − bT v, (29) with θ = {W, V, d(1), d(2), b}. The DBM can also be characterized as a bipartite graph between two sets of vertices, formed by the units in odd and even-numbered layers (with v := h(0)).
Mean-field approximate inference
A key point of departure from the RBM is that the posterior distribution over the hidden units (given the visibles) is no longer tractable, due to the interactions between the hidden units. Salakhutdinov and Hinton (2009) resort to a mean-field approximation to the posterior. Specifically, in the case of a model with two hidden layers, we wish to approximate P
� h(1), h(2) | v
� with the factored distribution
Qv(h(1), h(2)) = �N1 j=1 Qv
� h(1) j
� �N2 i=1 Qv
� h(2) i
�, such that the KL divergence KL
�
P
� h(1), h(2) | v
�
∥Qv(h1, h2)
� is minimized or equivalently, that a lower bound to the log likelihood is maximized: log P(v) > L(Qv) ≡
� h(1)
� h(2)
Qv(h(1), h(2)) log
�P(v, h(1), h(2))
Qv(h(1), h(2))
�
Maximizing this lower-bound with respect to the mean-field distribution Qv(h1, h2) (by setting derivatives to zero) yields the following mean field update equations:
ˆh(1) i
← sigmoid
�� j
Wjivj +
� k
Vikˆh(2) k
+ d(1) i
�
ˆh(2) k
← sigmoid
�� i
Vikˆh(1) i
+ d(2) k
�
Note how the above equations ostensibly look like a fixed point recurrent neural network, i.e., with constant input. In the same way that an RBM can be associated with a simple auto-encoder, the above mean-field update equations for the DBM can be associated with a recurrent auto-encoder. In that case the training criterion involves the reconstruction error at the last or at consecutive time steps. This type of model has been explored by Savard (2011) and Seung (1998) and shown to do a better job at denoising than ordinary auto-encoders.
Iterating Eq. (31-32) until convergence yields the Q parameters used to estimate the "variational positive phase" of Eq. 33:
L(Qv) =EQv
� log P(v, h(1), h(2)) − log Qv(h(1), h(2))
�
=EQv
�
−EDBM θ(v, h(1), h(2)) − log Qv(h(1), h(2))
�
− log Zθ
∂L(Qv)
∂θ
= −EQv
�∂EDBM θ(v, h(1), h(2))
∂θ
�
+ EP
�∂EDBM θ(v, h(1), h(2))
∂θ
�
Note that this variational learning procedure leaves the "negative phase" untouched. It can thus be estimated through SML or Contrastive Divergence (Hinton, 2000) as in the RBM case.
Training Deep Boltzmann Machines
Despite the intractability of inference in the DBM, its training should not, in theory, be much more complicated than that of the RBM. The major difference being that instead of maximizing the likelihood directly, we instead choose parameters to maximize the lower-bound on the likelihood given in Eq. 30.
The SML-based algorithm for maximizing this lower-bound is as follows:
1) Clamp the visible units to a training example.
2) Iterate over Eq. (31-32) until convergence.
3) Generate negative phase samples v−, h(1)− and h(2)− through SML.
4) Compute ∂L(Qv) /∂θ using the values obtained in steps
2-3.
5) Finally, update the model parameters with a step of approximate stochastic gradient ascent.
While the above procedure appears to be a simple extension of the highly effective SML scheme for training RBMs, as we demonstrate in Desjardins et al. (2012), this procedure seems vulnerable to falling in poor local minima which leave many hidden units effectively dead (not significantly different from its random initialization with small norm).
The failure of the SML joint training strategy was noted by Salakhutdinov and Hinton (2009). As an alternative, they proposed a greedy layer-wise training strategy. This procedure consists in pre-training the layers of the DBM, in much the same way as the Deep Belief Network: i.e. by stacking RBMs and training each layer to independently model the output of the previous layer. A final joint "fine-tuning" is done following the above SML-based procedure.
BUILDING-IN INVARIANCE
It is well understood that incorporating prior domain knowledge is an almost sure way to boost performance of any machine-learning-based prediction system. Exploring good strategies for doing so is a very important research avenue.
However, if we are to advance our understanding of core machine learning principles, it is important that we keep comparisons between predictors fair and maintain a clear awareness of the prior domain knowledge used by different learning algorithms, especially when comparing their performance on benchmark problems. We have so far tried to present feature learning and deep learning approaches without enlisting specific domain knowledge, only generic inductive biases for high dimensional problems. The approaches as we presented them should thus be potentially applicable to any high dimensional problem. In this section, we briefly review how basic domain knowledge can be leveraged to learn yet better features.
The most prevalent approach to incorporating prior knowledge is to hand-design better features to feed a generic classifier, and has been used extensively in computer vision(e.g. (Lowe, 1999)). Here, we rather focus on how basic domain knowledge of the input, in particular its topological structure (e.g. bitmap images having a 2D structure), may be used to learn better features.
Augmenting the dataset with known input deformations
Since machine learning approaches learn from data, it is usually possible to improve results by feeding them a larger quantity of representative data. Thus, a straightforward and generic way to leverage prior domain knowledge of input deformations that are known not to change its class (e.g. small affine transformations of images such as translations, rotations, scaling, shearing), is to use them to generate more training data. While being an old approach (Baird, 1990; Poggio and Vetter, 1992), it has been recently applied with great success in the work of Ciresan et al. (2010) who used an efficient GPU implementation (40× speedup) to train a standard but large deep multilayer Perceptron on deformed MNIST digits. Using both affine and elastic deformations (Simard et al., 2003), with plain old stochastic gradient descent, they reach a record
0.32% classification error rate.
Convolution and pooling
Another powerful approach is based on even more basic knowledge of merely the topological structure of the input dimensions. By this we mean e.g., the 2D layout of pixels in images or audio spectrograms, the 3D structure of videos, the 1D sequential structure of text or of temporal sequences in general. Based on such structure, one can define local receptive fields (Hubel and Wiesel, 1959), so that each lowlevel feature will be computed from only a subset of the input: a neighborhood in the topology (e.g. a sub-image at a given position). This topological locality constraint corresponds to a layer having a very sparse weight matrix with non-zeros only allowed for topologically local connections. Computing the associated matrix products can of course be made much more efficient than having to handle a dense matrix, in addition to the statistical gain from a much smaller number of free parameters. In domains with such topological structure, similar input patterns are likely to appear at different positions, and nearby values (e.g. consecutive frames or nearby pixels) are likely to have stronger dependencies that are also important to model the data. In fact these dependencies can be exploited to discover the topology (Le Roux et al., 2008a), i.e. recover a regular grid of pixels out of a set of vectors without any order information, e.g. after the elements have been arbitrarily shuffled in the same way for all examples. Thus a same local feature computation is likely to be relevant at all translated positions of the receptive field. Hence the idea of sweeping such
25 a local feature extractor over the topology: this corresponds to a convolution, and transforms an input into a similarly shaped feature map. Equivalently to sweeping, this may be seen as static but differently positioned replicated feature extractors that all share the same parameters. This is at the heart of convolutional networks (LeCun et al., 1989, 1998b) which have been applied both to object recognition and to image segmentation (Turaga et al., 2010). Another hallmark of the convolutional architecture is that values computed by the same feature detector applied at several neighboring input locations are then summarized through a pooling operation, typically taking their max or their sum. This confers the resulting pooled feature layer some degree of invariance to input translations, and this style of architecture (alternating selective feature extraction and invariance-creating pooling) has been the basis of convolutional networks, the Neocognitron (Fukushima and Miyake, 1982) and HMAX (Riesenhuber and Poggio, 1999) models, and argued to be the architecture used by mammalian brains for object recognition (Riesenhuber and Poggio, 1999;
Serre et al., 2007; DiCarlo et al., 2012). The output of a pooling unit will be the same irrespective of where a specific feature is located inside its pooling region. Empirically the use of pooling seems to contribute significantly to improved classification accuracy in object classification tasks (LeCun et al., 1998b; Boureau et al., 2010, 2011). A successful variant of pooling connected to sparse coding is L2 pooling (Hyv¨arinen et al., 2009; Kavukcuoglu et al., 2009; Le et al., 2010), for which the pool output is the square root of the possibly weighted sum of squares of filter outputs. Ideally, we would like to generalize feature-pooling so as to learn what features should be pooled together, e.g. as successfully done in several papers (Hyv¨arinen and Hoyer, 2000; Kavukcuoglu et al., 2009; Le et al., 2010; Ranzato and Hinton, 2010;
Courville et al., 2011b; Coates and Ng, 2011b; Gregor et al., 2011). In this way, the features learned are becoming invariant to the variations captured by the span of the features pooled.
Patch-based training
The simplest approach for learning a convolutional layer in an unsupervised fashion is patch-based training: simply feeding a generic unsupervised feature learning algorithm with local patches extracted at random positions of the inputs. The resulting feature extractor can then be swiped over the input to produce the convolutional feature maps. That map may be used as a new input for the next layer, and the operation repeated to thus learn and stack several layers. Such an approach was recently used with Independent Subspace Analysis (Le et al., 2011c) on 3D video blocks, reaching the state-of-the-art on Hollywood2, UCF, KTH and YouTube action recognition datasets. Similarly (Coates and Ng, 2011a) compared several feature learners with patch-based training and reached stateof-the-art results on several classification benchmarks. Interestingly, in this work performance was almost as good with very simple k-means clustering as with more sophisticated feature learners. We however conjecture that this is the case only because patches are rather low dimensional (compared to the dimension of a whole image). A large dataset might provide sufficient coverage of the space of e.g. edges prevalent in 6 × 6 patches, so that a distributed representation is not absolutely necessary. Another plausible explanation for this success is that the clusters identified in each image patch are then pooled into a histogram of cluster counts associated with a larger sub-image. Whereas the output of a regular clustering is a one-hot non-distributed code, this histogram is itself a distributed representation, and the "soft" k-means (Coates and Ng, 2011a) representation allows not only the nearest filter but also its neighbors to be active.
Convolutional and tiled-convolutional training
It is also possible to directly train large convolutional layers using an unsupervised criterion. An early approach is that of Jain and Seung (2008) who trained a standard but deep convolutional MLP on the task of denoising images, i.e. as a deep, convolutional, denoising auto-encoder. Convolutional versions of the RBM or its extensions have also been developed (Desjardins and Bengio, 2008; Lee et al., 2009a;
Taylor et al., 2010) as well as a probabilistic max-pooling operation built into Convolutional Deep Networks (Lee et al., 2009a,b; Krizhevsky, 2010). Other unsupervised feature learning approaches that were adapted to the convolutional setting include PSD (Kavukcuoglu et al., 2009, 2010; Jarrett et al., 2009; Farabet et al., 2011; Henaff et al., 2011), a convolutional version of sparse coding called deconvolutional networks (Zeiler et al., 2010), Topographic ICA (Le et al., 2010), and mPoT that Kivinen and Williams (2012) applied to modeling natural textures. Gregor and LeCun (2010a);
Le et al. (2010) also demonstrated the technique of tiledconvolution, where parameters are shared only between feature extractors whose receptive fields are k steps away (so the ones looking at immediate neighbor locations are not shared).
This allows pooling units to be invariant to more than just translations, and is a kind of hybrid between convolutional networks and earlier neural networks with local connections but no weight sharing (LeCun, 1986, 1989).
Alternatives to pooling
Alternatively, one can also use explicit knowledge of the expected invariants expressed mathematically to define transformations that are robust to a known family of input deformations, using so-called scattering operators (Mallat, 2011;
Bruna and Mallat, 2011), which can be computed in a way interestingly analogous to deep convolutional networks and wavelets. Like convolutional networks, the scattering operators alternate two types of operations: convolving with a filter followed by pooling (as a norm). Unlike convolutional networks, the proposed approach keeps at each level all of the information about the input (in a way that can be inverted), and automatically yields a very sparse (but also very highdimensional) representation. Another difference is that the filters are not learned but instead set so as to guarantee that a priori specified invariances are robustly achieved. Just a few levels were sufficient to achieve impressive results on several benchmark datasets.
Temporal coherence and slow features
The principle of identifying slowly moving/changing factors in temporal/spatial data has been investigated by many (Becker
26 and Hinton, 1993; Wiskott and Sejnowski, 2002; Hurri and Hyv¨arinen, 2003; K¨ording et al., 2004; Cadieu and Olshausen, 2009) as a principle for finding useful representations. In particular this idea has been applied to image sequences and as an explanation for why V1 simple and complex cells behave the way they do. A good overview can be found in Hurri and Hyv¨arinen (2003); Berkes and Wiskott (2005).
More recently, temporal coherence has been successfully exploited in deep architectures to model video (Mobahi et al., 2009). It was also found that temporal coherence discovered visual features similar to those obtained by ordinary unsupervised feature learning (Bergstra and Bengio, 2009), and a temporal coherence penalty has been combined with a training criterion for unsupervised feature learning (Zou et al., 2011), sparse auto-encoders with L1 regularization, in this case, yielding improved classification performance.
The temporal coherence prior can be expressed in several ways. The simplest and most commonly used is just the squared difference between feature values at times t and t + 1. Other plausible temporal coherence priors include the following. First, instead of penalizing the squared change, penalizing the absolute value (or a similar sparsity penalty) would state that most of the time the change should be exactly
0, which would intuitively make sense for the real-life factors that surround us. Second, one would expect that instead of just being slowly changing, different factors could be associated with their own different time scale. The specificity of their time scale could thus become a hint to disentangle explanatory factors. Third, one would expect that some factors should really be represented by a group of numbers (such as x, y, and z position of some object in space and the pose parameters of Hinton et al. (2011)) rather than by a single scalar, and that these groups tend to move together. Structured sparsity penalties (Kavukcuoglu et al., 2009; Jenatton et al., 2009;
Bach et al., 2011; Gregor et al., 2011) could be used for this purpose.
Algorithms to Disentangle Factors of Variation
The goal of building invariant features is to remove sensitivity of the representation to directions of variance in the data that are uninformative to the task at hand. However it is often the case that the goal of feature extraction is the disentangling or separation of many distinct but informative factors in the data, e.g., in a video of people: subject identity, action performed, subject pose relative to the camera, etc. In this situation, the methods of generating invariant features, such as featurepooling, may be inadequate.
Roughly speaking, the process of building invariant features can be seen as consisting of two steps (often performed together). First, a set of low-level features are recovered that account for the data. Second, subsets of these low level features are pooled together to form higher-level invariant features, exemplified by the pooling and subsampling layers of convolutional neural networks (Fukushima, 1980; LeCun et al., 1989, 1998c). With this arrangement, the invariant representation formed by the pooling features offers a somewhat incomplete window on the data as the detailed representation of the lower-level features is abstracted away in the pooling procedure. While we would like higher-level features to be more abstract and exhibit greater invariance, we have little control over what information is lost through feature pooling.
For example, consider a higher-level feature made invariant to the color of its target stimulus by forming a subspace of low-level features that represent the target stimulus in various colors (forming a basis for the subspace). If this is the only higher-level feature that is associated with these low-level colored features then the color information of the stimulus is lost to the higher-level pooling feature and every layer above. This loss of information becomes a problem when the information that is lost is necessary to successfully complete the task at hand such as object classification. In the above example, color is often a very discriminative feature in object classification tasks. Losing color information through feature-pooling would result in significantly poorer classification performance.
Obviously, what we really would like is for a particular feature set to be invariant to the irrelevant features and disentangle the relevant features. Unfortunately, it is often difficult to determine a priori which set of features will ultimately be relevant to the task at hand.
An interesting approach to taking advantage of some of the factors of variation known to exist in the data is the transforming auto-encoder (Hinton et al., 2011): instead of a scalar pattern detector (e.g,. corresponding to the probability of presence of a particular form in the input) one can think of the features as organized in groups that include both a pattern detector and pose parameters that specify attributes of the detected pattern. In (Hinton et al., 2011), what is assumed a priori is that pairs of inputs (or consecutive inputs) are observed with an associated value for the corresponding change in the pose parameters. For example, an animal that controls its eyes knows what changes to its ocular motor system were applied when going from one image on its retina to the next image associated with the following saccade and controlled head motion. In that work, it is also assumed that the pose changes are the same for all the pattern detectors, and this makes sense for global changes such as image translation and camera geometry changes. Instead, we would like to discover the pose parameters and attributes that should be associated with each feature detector, without having to specify ahead of time what they should be, force them to be the same for all features, and having to necessarily observe the changes in all of the pose parameters or attributes.
The approach taken recently in the Manifold Tangent Classifier, discussed in section 8.3, is interesting in this respect.
Without using any supervision or prior knowledge, it finds prominent local factors of variation (the tangent vectors to the manifold, extracted from a CAE, interpreted as locally valid input "deformations"). Higher-level features are subsequently encouraged to be invariant to these factors of variation, so that they must depend on other characteristics. In a sense this approach is disentangling valid local deformations along the data manifold from other, more drastic changes, associated to other factors of variation such as those that affect class
27 identity.24
One solution to the problem of information loss that would fit within the feature-pooling paradigm, is to consider many overlapping pools of features based on the same low-level feature set. Such a structure would have the potential to learn a redundant set of invariant features that may not cause significant loss of information. However it is not obvious what learning principle could be applied that can ensure that the features are invariant while maintaining as much information as possible. While a Deep Belief Network or a Deep Boltzmann Machine (as discussed in sections 4 and 10.2 respectively) with two hidden layers would, in principle, be able to preserve information into the "pooling" second hidden layer, there is no guarantee that the second layer features are more invariant than the "low-level" first layer features.
However, there is some empirical evidence that the second layer of the DBN tends to display more invariance than the first layer (Erhan et al., 2010a). A second issue with this approach is that it could nullify one of the major motivations for pooling features: to reduce the size of the representation. A pooling arrangement with a large number of overlapping pools could lead to as many pooling features as low-level features
– a situation that is both computationally and statistically undesirable.
A more principled approach, from the perspective of ensuring a more robust compact feature representation, can be conceived by reconsidering the disentangling of features through the lens of its generative equivalent – feature composition. Since many unsupervised learning algorithms have a generative interpretation (or a way to reconstruct inputs from their high-level representation), the generative perspective can provide insight into how to think about disentangling factors. The majority of the models currently used to construct invariant features have the interpretation that their low-level features linearly combine to construct the data.25 This is a fairly rudimentary form of feature composition with significant limitations. For example, it is not possible to linearly combine a feature with a generic transformation (such as translation) to generate a transformed version of the feature. Nor can we even consider a generic color feature being linearly combined with a gray-scale stimulus pattern to generate a colored pattern. It would seem that if we are to take the notion of disentangling seriously we require a richer interaction of features than that offered by simple linear combinations.
CONCLUSION
This review of representation learning and deep learning has covered three major and apparently disconnected approaches: the probabilistic models (both the directed kind such as sparse coding and the undirected kind such as Boltzmann
24. The changes that affect class identity might, in input space, actually be of similar magnitude to local deformations, but not follow along the manifold, i.e. cross zones of low density.
25. As an aside, if we are given only the values of the higher-level pooling features, we cannot accurately recover the data because we do not know how to apportion credit for the pooling feature values to the lower-level features. This is simply the generative version of the consequences of the loss of information caused by pooling. machines), the reconstruction-based algorithms related to autoencoders, and the geometrically motivated manifold-learning approaches. Drawing connections between these approaches is currently a very active area of research and is likely to continue to produce models and methods that take advantage of the relative strengths of each paradigm.
Practical Concerns and Guidelines. One of the criticisms addressed to artificial neural networks and deep learning algorithms is that they have many hyper-parameters and variants and that exploring their configurations and architectures is an art. This has motivated an earlier book on the "Tricks of the Trade" (Orr and Muller, 1998) of which LeCun et al. (1998a) is still relevant for training deep architectures, in particular what concerns initialization, ill-conditioning and stochastic gradient descent. A good and more modern compendium of good training practice, particularly adapted to training RBMs, is provided in Hinton (2010), while a similar guide oriented more towards deep neural networks can be found in Bengio(2013), both of which are part of a novel version of the above book, entitled "Neural Networks: Tricks of the Trade, Reloaded".
Incorporating Generic AI-level Priors. We have covered many high-level generic priors that we believe could be very useful to bring machine learning closer to AI, and that can be incorporated into representation-learning algorithms. Many of these relate to the assumed existence of multiple underlying factors of variation, whose variations are in some sense orthogonal to each other in input space. They are expected to be organized at multiple levels of abstraction, hence the need for deep architectures, which also have statistical advantages because they allow to re-use parameters in a combinatorially efficient way. Only a few of these factors are relevant for any particular example, justifying the sparsity of representation prior. Subsets of these factors explain different random variables of interest(inputs, tasks) and they vary in structured ways in time and space (temporal and spatial coherence). We expect future successful applications of representation learning to refine and increase that list of priors, and to incorporate most of them instead of focusing on only one. Research in training criteria that better take these priors into account are likely to move us closer to the long-term objective of discovering learning algorithms that can disentangle the underlying explanatory factors for AI tasks.
Inference. We anticipate that methods based on directly parametrizing a representation function will incorporate more and more of the iterative type of computation one finds in the inference procedures of probabilistic latent-variable models.
There is already movement in the other direction, with probabilistic latent-variable models exploiting approximate inference mechanisms that are themselves learned (i.e., producing a parametric description of the representation function). A major appeal of probabilistic models is that the semantics of the latent variables are clear and this allows a clean separation of the problems of modeling (choose the energy function), inference (estimating P(h|x)), and learning (optimizing the parameters), using generic tools in each case. On the other hand, doing approximate inference and not taking that approximation into account explicitly in the approximate optimization
28 for learning could have detrimental effects, hence the appeal of learning approximate inference. More fundamentally, there is the question of the multimodality of the posterior P(h|x). If there are exponentially many probable configurations of values of the factors hi that can explain x, then we seem to be stuck with very poor inference, either focusing on a single mode(MAP inference), assuming some kind of strong factorization(as in variational inference) or using an MCMC that cannot visit enough modes of P(h|x) to possibly do a good job of inference. What we propose as food for thought is the idea of dropping the requirement of an explicit representation of the posterior and settle for an implicit representation that exploits potential structure in P(h|x) in order to represent it compactly: even though P(h|x) may have an exponential number of modes, it may be possible to represent it with a small set of numbers. For example, consider computing a deterministic feature representation f(x) that implicitly captures the information about a highly multi-modal P(h|x), in the sense that all the questions (e.g. making some prediction about some target concept) that can be asked from P(h|x) can also be answered from f(x).
Optimization. Much remains to be done to better understand the successes and failures of training deep architectures, both in the supervised case (with many recent successes) and the unsupervised case (where much more work needs to be done). Although regularization effects can be important on small datasets, the effects that persist on very large datasets suggest some optimization issues are involved. Are they more due to local minima (we now know there are huge numbers of them) and the dynamics of the training procedure? Or are they due mostly to ill-conditioning and may be handled by approximate second-order methods? These basic questions remain unanswered and deserve much more study.
Acknowledgments
The author would like to thank David Warde-Farley, Razvan
Pascanu and Ian Goodfellow for useful feedback, as well as
NSERC, CIFAR and the Canada Research Chairs for funding.
REFERENCES
Amari, S. (1998).
Natural gradient works efficiently in learning. Neural Computation, 10(2), 251–276.
Bach, F., Jenatton, R., Mairal, J., and Obozinski, G. (2011).
Structured sparsity through convex optimization.
CoRR, abs/1109.2397.
Bagnell, J. A. and Bradley, D. M. (2009). Differentiable sparse coding. In NIPS'2009, pages 113–120.
Baird, H. (1990). Document image defect models. In IAPR
Workshop, Syntactic & Structural Patt. Rec., pages 38–46.
Becker, S. and Hinton, G. (1992). A self-organizing neural network that discovers surfaces in random-dot stereograms.
Nature, 355, 161–163.
Becker, S. and Hinton, G. E. (1993). Learning mixture models of spatial coherence. Neural Computation, 5, 267–277.
Belkin, M. and Niyogi, P. (2003). Laplacian eigenmaps for dimensionality reduction and data representation.
Neural
Computation, 15(6), 1373–1396.
Bell, A. and Sejnowski, T. J. (1997). The independent components of natural scenes are edge filters. Vision Research, 37, 3327–3338.
Bengio, Y. (1993).
A connectionist approach to speech recognition. International Journal on Pattern Recognition and Artificial Intelligence, 7(4), 647–668.
Bengio, Y. (2008). Neural net language models. Scholarpedia, Bengio, Y. (2009).
Learning deep architectures for AI.
Foundations and Trends in Machine Learning, 2(1), 1–127.
Also published as a book. Now Publishers, 2009.
Bengio, Y. (2011).
Deep learning of representations for unsupervised and transfer learning. In JMLR W&CP: Proc.
Unsupervised and Transfer Learning.
Bengio, Y. (2013). Practical recommendations for gradientbased training of deep architectures.
In K.-R. M¨uller, G. Montavon, and G. B. Orr, editors, Neural Networks:
Tricks of the Trade, Reloaded. Springer.
Bengio, Y. and Delalleau, O. (2009). Justifying and generalizing contrastive divergence. Neural Computation, 21(6), 1601–1621.
Bengio, Y. and Delalleau, O. (2011). On the expressive power of deep architectures. In ALT'2011.
Bengio, Y. and LeCun, Y. (2007). Scaling learning algorithms towards AI. In L. Bottou, O. Chapelle, D. DeCoste, and J. Weston, editors, Large Scale Kernel Machines. MIT Press.
Bengio, Y. and Monperrus, M. (2005). Non-local manifold tangent learning. In NIPS'2004, pages 129–136. MIT Press.
Bengio, Y., Simard, P., and Frasconi, P. (1994). Learning longterm dependencies with gradient descent is difficult. IEEE
Transactions on Neural Networks, 5(2), 157–166.
Bengio, Y., Ducharme, R., Vincent, P., and Jauvin, C. (2003).
A neural probabilistic language model.
JMLR, 3, 1137–
Bengio, Y., Paiement, J.-F., Vincent, P., Delalleau, O., Le
Roux, N., and Ouimet, M. (2004). Out-of-sample extensions for LLE, Isomap, MDS, Eigenmaps, and Spectral Clustering. In NIPS'2003.
Bengio, Y., Delalleau, O., and Le Roux, N. (2006a). The curse of highly variable functions for local kernel machines. In
NIPS'2005, pages 107–114.
Bengio, Y., Larochelle, H., and Vincent, P. (2006b). Non-local manifold Parzen windows. In NIPS'2005, pages 115–122.
MIT Press.
Bengio, Y., Lamblin, P., Popovici, D., and Larochelle, H.(2007). Greedy layer-wise training of deep networks. In
NIPS'2006.
Bengio, Y., Louradour, J., Collobert, R., and Weston, J. (2009).
Curriculum learning. In ICML'09.
Bengio, Y., Delalleau, O., and Simard, C. (2010). Decision trees do not generalize to new variations. Computational
Intelligence, 26(4), 449–467.
Bengio, Y., Mesnil, G., Dauphin, Y., and Rifai, S. (2012a).
Better mixing via deep representations. Technical Report arXiv:1207.4404, Universite de Montreal.
Bengio, Y., Alain, G., and Rifai, S. (2012b). Implicit density estimation by local moment matching to sample from autoencoders. Technical report, arXiv:1207.0057.
Bergstra, J. and Bengio, Y. (2009). Slow, decorrelated features for pretraining complex cell-like networks. In NIPS'2009, pages 99–107.
Berkes, P. and Wiskott, L. (2005). Slow feature analysis yields a rich repertoire of complex cell properties.
Journal of Vision, 5(6), 579–602.
Besag, J. (1975). Statistical analysis of non-lattice data. The Statistician, 24(3), 179–195.
Bordes, A., Glorot, X., Weston, J., and Bengio, Y. (2012). Joint learning of words and meaning representations for open-text semantic parsing. AISTATS'2012.
Boulanger-Lewandowski, N., Bengio, Y., and Vincent, P.
Modeling temporal dependencies in highdimensional sequences: Application to polyphonic music generation and transcription. In ICML'2012.
Boureau, Y., Ponce, J., and LeCun, Y. (2010).
A theoretical analysis of feature pooling in vision algorithms.
In
ICML'10.
Boureau, Y., Le Roux, N., Bach, F., Ponce, J., and LeCun, Y.(2011). Ask the locals: multi-way local pooling for image recognition. In ICCV'11.
Bourlard, H. and Kamp, Y. (1988).
Auto-association by multilayer perceptrons and singular value decomposition.
Biological Cybernetics, 59, 291–294.
Brand, M. (2003). Charting a manifold. In NIPS'2002, pages
961–968. MIT Press.
Breuleux, O., Bengio, Y., and Vincent, P. (2011). Quickly generating representative samples from an rbm-derived process.
Neural Computation, 23(8), 2053–2073.
Bruna, J. and Mallat, S. (2011). Classification with scattering operators. In ICPR'2011.
Cadieu, C. and Olshausen, B. (2009). Learning transformational invariants from natural movies. In NIPS'2009, pages
209–216. MIT Press.
Carreira-Perpi˜nan, M. A. and Hinton, G. E. (2005).
On contrastive divergence learning.
In AISTATS'2005, pages
33–40.
Cayton, L. (2005). Algorithms for manifold learning. Technical Report CS2008-0923, UCSD.
Chen, M., Xu, Z., Winberger, K. Q., and Sha, F. (2012).
Marginalized denoising autoencoders for domain adaptation. In ICML'2012.
Cho, K., Raiko, T., and Ilin, A. (2010). Parallel tempering is efficient for learning restricted Boltzmann machines. In
IJCNN'2010.
Cho, K., Raiko, T., and Ilin, A. (2011). Enhanced gradient and adaptive learning rate for training restricted Boltzmann machines. In ICML'2011, pages 105–112.
Ciresan, D., Meier, U., and Schmidhuber, J. (2012). Multicolumn deep neural networks for image classification. Technical report, arXiv:1202.2745.
Ciresan, D. C., Meier, U., Gambardella, L. M., and Schmidhuber, J. (2010). Deep big simple neural nets for handwritten digit recognition. Neural Computation, 22, 1–14.
Coates, A. and Ng, A. Y. (2011a). The importance of encoding versus training with sparse coding and vector quantization.
In ICML'2011.
Coates, A. and Ng, A. Y. (2011b). Selecting receptive fields in deep networks. In NIPS'2011.
Collobert, R. and Weston, J. (2008). A unified architecture for natural language processing: Deep neural networks with multitask learning. In ICML'2008.
Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., and Kuksa, P. (2011). Natural language processing (almost) from scratch.
Journal of Machine
Learning Research, 12, 2493–2537.
Comon, P. (1994). Independent component analysis - a new concept? Signal Processing, 36, 287–314.
Courville, A., Bergstra, J., and Bengio, Y. (2011a). A spike and slab restricted Boltzmann machine. In AISTATS'2011.
Courville, A., Bergstra, J., and Bengio, Y. (2011b).
Unsupervised models of images by spike-and-slab RBMs.
In
ICML'2011.
Dahl, G. E., Ranzato, M., Mohamed, A., and Hinton, G. E.
Phone recognition with the mean-covariance restricted Boltzmann machine. In NIPS'2010.
Dahl, G. E., Yu, D., Deng, L., and Acero, A. (2012). Contextdependent pre-trained deep neural networks for large vocabulary speech recognition. IEEE Transactions on Audio, Speech, and Language Processing, 20(1), 33–42.
Dempster, A. P., Laird, N. M., and Rubin, D. B. (1977).
Maximum-likelihood from incomplete data via the EM algorithm. J. Royal Statistical Society B, 39, 1–38.
Desjardins, G. and Bengio, Y. (2008). Empirical evaluation of convolutional RBMs for vision. Technical Report 1327, Dept. IRO, U. Montr´eal.
Desjardins, G., Courville, A., Bengio, Y., Vincent, P., and Delalleau, O. (2010). Tempered Markov chain monte carlo for training of restricted Boltzmann machine.
In JMLR
W&CP: Proc. AISTATS'2010, volume 9, pages 145–152.
Desjardins, G., Courville, A., and Bengio, Y. (2011).
On tracking the partition function. In NIPS'2011.
Desjardins, G., Courville, A., and Bengio, Y. (2012).
On training deep Boltzmann machines.
Technical Report arXiv:1203.4416v1, Universit´e de Montr´eal.
DiCarlo, J., Zoccolan, D., and Rust, N. (2012). How does the brain solve visual object recognition? Neuron.
Donoho, D. L. and Grimes, C. (2003).
Hessian eigenmaps: new locally linear embedding techniques for highdimensional data. Technical Report 2003-08, Dept. Statistics, Stanford University.
Eisner, J. (2012).
Learning approximate inference policies for fast prediction.
Keynote talk at ICML Workshop on
Inferning: Interactions Between Search and Learning.
Erhan, D., Courville, A., and Bengio, Y. (2010a). Understanding representations learned in deep architectures. Technical
Report 1355, Universit´e de Montr´eal/DIRO.
Erhan, D., Bengio, Y., Courville, A., Manzagol, P.-A., Vincent, P., and Bengio, S. (2010b). Why does unsupervised pretraining help deep learning? Journal of Machine Learning
Research, 11, 625–660.
Farabet, C., LeCun, Y., Kavukcuoglu, K., Culurciello, E., Martini, B., Akselrod, P., and Talay, S. (2011).
Largescale fpga-based convolutional networks. In R. Bekkerman, M. Bilenko, and J. Langford, editors, Scaling up Machine
Learning: Parallel and Distributed Approaches. Cambridge
University Press.
Freund, Y. and Haussler, D. (1994). Unsupervised learning of distributions on binary vectors using two layer networks.
Technical Report UCSC-CRL-94-25, University of California, Santa Cruz.
Friedman, J. H. and Stuetzle, W. (1981). Projection pursuit regression.
J. American Statistical Association, 76(376), 817–823.
Fukushima, K. (1980). Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. Biological Cybernetics, 36, 193–202.
Fukushima, K. and Miyake, S. (1982). Neocognitron: A new algorithm for pattern recognition tolerant of deformations and shifts in position. Pattern Recognition, 15, 455–469.
Glorot, X. and Bengio, Y. (2010).
Understanding the difficulty of training deep feedforward neural networks.
In
AISTATS'2010, pages 249–256.
Glorot, X., Bordes, A., and Bengio, Y. (2011a). Deep sparse rectifier neural networks. In AISTATS'2011.
Glorot, X., Bordes, A., and Bengio, Y. (2011b).
Domain adaptation for large-scale sentiment classification: A deep learning approach. In ICML'2011.
Goodfellow, I., Le, Q., Saxe, A., and Ng, A. (2009). Measuring invariances in deep networks. In NIPS'2009, pages 646–
Goodfellow, I., Courville, A., and Bengio, Y. (2011). Spikeand-slab sparse coding for unsupervised feature discovery.
In NIPS Workshop on Challenges in Learning Hierarchical
Models.
Goodfellow, I. J., Courville, A., and Bengio, Y. (2012). Spikeand-slab sparse coding for unsupervised feature discovery. arXiv:1201.3382.
Gregor, K. and LeCun, Y. (2010a). Emergence of complexlike cells in a temporal product network with local receptive fields. Technical report, arXiv:1006.0448.
Gregor, K. and LeCun, Y. (2010b). Learning fast approximations of sparse coding. In ICML'2010.
Gregor, K., Szlam, A., and LeCun, Y. (2011). Structured sparse coding via lateral inhibition. In NIPS'2011.
Gribonval, R. (2011). Should penalized least squares regression be interpreted as Maximum A Posteriori estimation?
IEEE Transactions on Signal Processing, 59(5), 2405–2410.
Grosse, R., Raina, R., Kwong, H., and Ng, A. Y. (2007).
Shift-invariant sparse coding for audio classification.
In
UAI'2007.
Grubb, A. and Bagnell, J. A. D. (2010). Boosted backpropagation learning for training deep modular networks.
In
ICML'2010.
Gutmann, M. and Hyvarinen, A. (2010).
Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In AISTATS'2010.
H˚astad, J. (1986).
Almost optimal lower bounds for small depth circuits. In STOC'86, pages 6–20.
H˚astad, J. and Goldmann, M. (1991). On the power of smalldepth threshold circuits. Computational Complexity, 1, 113–
Henaff, M., Jarrett, K., Kavukcuoglu, K., and LeCun, Y.(2011). Unsupervised learning of sparse features for scalable audio classification. In ISMIR'11.
Hinton, G., Krizhevsky, A., and Wang, S. (2011). Transforming auto-encoders. In ICANN'2011.
Hinton, G. E. (1986). Learning distributed representations of concepts. In Proceedings of the Eighth Annual Conference of the Cognitive Science Society, pages 1–12, Amherst 1986.
Lawrence Erlbaum, Hillsdale.
Hinton, G. E. (1999). Products of experts. In ICANN'1999.
Hinton, G. E. (2000).
Training products of experts by minimizing contrastive divergence. Technical Report GCNU
TR 2000-004, Gatsby Unit, University College London.
Hinton, G. E. (2010).
A practical guide to training restricted Boltzmann machines. Technical Report UTML TR
2010-003, Department of Computer Science, University of Toronto.
Hinton, G. E. and Roweis, S. (2003).
Stochastic neighbor embedding. In NIPS'2002.
Hinton, G. E. and Salakhutdinov, R. (2006).
Reducing the dimensionality of data with neural networks.
Science, 313(5786), 504–507.
Hinton, G. E. and Zemel, R. S. (1994).
Autoencoders, minimum description length, and helmholtz free energy. In
NIPS'1993.
Hinton, G. E., Osindero, S., and Teh, Y. (2006a). A fast learning algorithm for deep belief nets. Neural Computation, 18, 1527–1554.
Hinton, G. E., Osindero, S., Welling, M., and Teh, Y. (2006b).
Unsupervised discovery of non-linear structure using contrastive backpropagation.
Cognitive Science, 30(4), 725–
Hotelling, H. (1933).
Analysis of a complex of statistical variables into principal components. Journal of Educational
Psychology, 24, 417–441, 498–520.
Hubel, D. H. and Wiesel, T. N. (1959).
Receptive fields of single neurons in the cat's striate cortex.
Journal of Physiology, 148, 574–591.
Hurri, J. and Hyv¨arinen, A. (2003). Temporal coherence, natural image sequences, and the visual cortex. In NIPS'2002.
Hyv¨arinen, A. (2005a). Estimation of non-normalized statistical models using score matching. J. Machine Learning
Res., 6.
Hyv¨arinen, A. (2005b).
Estimation of non-normalized statistical models using score matching. Journal of Machine
Learning Research, 6, 695–709.
Hyv¨arinen, A. (2007). Some extensions of score matching.
Computational Statistics and Data Analysis, 51, 2499–2512.
Hyv¨arinen, A. (2008). Optimal approximation of signal priors.
Neural Computation, 20(12), 3087–3110.
Hyv¨arinen, A. and Hoyer, P. (2000). Emergence of phase and shift invariant features by decomposition of natural images into independent feature subspaces. Neural Computation, 12(7), 1705–1720.
Hyv¨arinen, A., Karhunen, J., and Oja, E. (2001a). Independent
Component Analysis. Wiley-Interscience.
Hyv¨arinen, A., Hoyer, P. O., and Inki, M. (2001b).
Topographic independent component analysis. Neural Computation, 13(7), 1527–1558.
Hyv¨arinen, A., Hurri, J., and Hoyer, P. O. (2009). Natural
Image Statistics: A probabilistic approach to early computational vision. Springer-Verlag.
Jaeger, H. (2007). Echo state network. Scholarpedia, 2(9), Jain, V. and Seung, S. H. (2008). Natural image denoising with convolutional networks. In NIPS'2008.
Jarrett, K., Kavukcuoglu, K., Ranzato, M., and LeCun, Y.(2009). What is the best multi-stage architecture for object recognition? In ICCV'09.
Jenatton, R., Audibert, J.-Y., and Bach, F. (2009). Structured variable selection with sparsity-inducing norms. Technical report, arXiv:0904.3523.
Jutten, C. and Herault, J. (1991). Blind separation of sources, part I: an adaptive algorithm based on neuromimetic architecture. Signal Processing, 24, 1–10.
Kavukcuoglu, K., Ranzato, M., and LeCun, Y. (2008). Fast inference in sparse coding algorithms with applications to object recognition. CBLL-TR-2008-12-01, NYU.
Kavukcuoglu, K., Ranzato, M.-A., Fergus, R., and LeCun, Y. (2009). Learning invariant features through topographic filter maps. In CVPR'2009.
Kavukcuoglu, K., Sermanet, P., Boureau, Y.-L., Gregor, K., Mathieu, M., and LeCun, Y. (2010). Learning convolutional feature hierarchies for visual recognition. In NIPS'2010.
Kingma, D. and LeCun, Y. (2010). Regularized estimation of image statistics by score matching. In NIPS'2010.
Kivinen, J. J. and Williams, C. K. I. (2012). Multiple texture
Boltzmann machines. In AISTATS'2012.
K¨ording, K. P., Kayser, C., Einh¨auser, W., and K¨onig, P.
How are complex cell properties adapted to the statistics of natural stimuli? J. Neurophysiology, 91.
Krizhevsky, A. (2010). Convolutional deep belief networks on cifar-10. Technical report, U. Toronto.
Krizhevsky, A. and Hinton, G. (2009).
Learning multiple layers of features from tiny images. Technical report, U.
Toronto.
Krizhevsky, A., Sutskever, I., and Hinton, G. (2012). ImageNet classification with deep convolutional neural networks. In
Advances in Neural Information Processing Systems 25(NIPS'2012).
Larochelle, H. and Bengio, Y. (2008). Classification using discriminative restricted Boltzmann machines. In ICML'2008.
Larochelle, H., Bengio, Y., Louradour, J., and Lamblin, P.(2009). Exploring strategies for training deep neural networks. Journal of Machine Learning Research, 10, 1–40.
Lazebnik, S., Schmid, C., and Ponce, J. (2006).
Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories. In CVPR'2006.
Le, Q., Ngiam, J., Chen, Z., hao Chia, D. J., Koh, P. W., and Ng, A. (2010).
Tiled convolutional neural networks.
In
NIPS'2010.
Le, Q., Ngiam, J., Coates, A., Lahiri, A., Prochnow, B., and Ng, A. (2011a). On optimization methods for deep learning.
In ICML'2011.
Le, Q. V., Karpenko, A., Ngiam, J., and Ng, A. Y. (2011b).
ICA with reconstruction cost for efficient overcomplete feature learning. In NIPS'2011.
Le, Q. V., Zou, W. Y., Yeung, S. Y., and Ng, A. Y.(2011c). Learning hierarchical spatio-temporal features for action recognition with independent subspace analysis. In
CVPR'2011.
Le Roux, N., Bengio, Y., Lamblin, P., Joliveau, M., and Kegl, B. (2008a).
Learning the 2-D topology of images.
In
NIPS'07.
Le Roux, N., Manzagol, P.-A., and Bengio, Y. (2008b). Topmoumoute online natural gradient algorithm. In NIPS'07.
LeCun, Y. (1986).
Learning processes in an asymmetric threshold network. In F. Fogelman-Souli´e, E. Bienenstock, and G. Weisbuch, editors, Disordered Systems and Biological Organization, pages 233–240. Springer-Verlag, Les
Houches, France.
LeCun, Y. (1987). Mod`eles connexionistes de l'apprentissage.
Ph.D. thesis, Universit´e de Paris VI.
LeCun, Y. (1989). Generalization and network design strategies. In Connectionism in Perspective. Elsevier Publishers.
LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., and Jackel, L. D. (1989). Backpropagation applied to handwritten zip code recognition. Neural
Computation, 1(4), 541–551.
LeCun, Y., Bottou, L., Orr, G. B., and M¨uller, K.-R. (1998a).
Efficient backprop. In Neural Networks, Tricks of the Trade, Lecture Notes in Computer Science LNCS 1524. Springer
Verlag.
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998b).
Gradient-based learning applied to document recognition.
Proceedings of the IEEE, 86(11), 2278–2324.
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998c).
Gradient based learning applied to document recognition.
IEEE, 86(11), 2278–2324.
Lee, H., Ekanadham, C., and Ng, A. (2008).
Sparse deep belief net model for visual area V2. In NIPS'07.
Lee, H., Grosse, R., Ranganath, R., and Ng, A. Y. (2009a).
Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations.
In
ICML'2009.
Lee, H., Pham, P., Largman, Y., and Ng, A. (2009b).
Unsupervised feature learning for audio classification using convolutional deep belief networks. In NIPS'2009.
Lin, Y., Tong, Z., Zhu, S., and Yu, K. (2010). Deep coding network. In NIPS'2010.
Lowe, D. (1999). Object recognition from local scale invariant features. In ICCV'99.
Mallat, S. (2011). Group invariant scattering. Communications in Pure and Applied Mathematics. to appear.
Marlin, B. and de Freitas, N. (2011). Asymptotic efficiency of deterministic estimators for discrete energy-based models:
Ratio matching and pseudolikelihood. In UAI'2011.
Marlin, B., Swersky, K., Chen, B., and de Freitas, N. (2010).
Inductive principles for restricted Boltzmann machine learning. In AISTATS'2010, pages 509–516.
Martens, J. (2010). Deep learning via Hessian-free optimization. In ICML'2010, pages 735–742.
Martens, J. and Sutskever, I. (2011). Learning recurrent neural networks with Hessian-free optimization. In ICML'2011.
Memisevic, R. and Hinton, G. E. (2010). Learning to represent
32 spatial transformations with factored higher-order Boltzmann machines. Neural Comp., 22(6).
Mesnil, G., Dauphin, Y., Glorot, X., Rifai, S., Bengio, Y., Goodfellow, I., Lavoie, E., Muller, X., Desjardins, G., Warde-Farley, D., Vincent, P., Courville, A., and Bergstra, J. (2011). Unsupervised and transfer learning challenge: a deep learning approach. In JMLR W&CP: Proc. Unsupervised and Transfer Learning, volume 7.
Mikolov, T., Deoras, A., Kombrink, S., Burget, L., and Cernocky, J. (2011).
Empirical evaluation and combination of advanced language modeling techniques.
In INTERSPEECH'2011.
Mobahi, H., Collobert, R., and Weston, J. (2009).
Deep learning from temporal coherence in video. In ICML'2009.
Mohamed, A., Dahl, G., and Hinton, G. (2012).
Acoustic modeling using deep belief networks. IEEE Trans. on Audio, Speech and Language Processing, 20(1), 14–22.
Montufar, G. F. and Morton, J. (2012). When does a mixture of products contain a product of mixtures? Technical report, arXiv:1206.0387.
Murray, I. and Salakhutdinov, R. (2009). Evaluating probabilities under high-dimensional latent variable models. In
NIPS'2008, pages 1137–1144.
Nair, V. and Hinton, G. E. (2010).
Rectified linear units improve restricted Boltzmann machines. In ICML'10.
Narayanan, H. and Mitter, S. (2010). Sample complexity of testing the manifold hypothesis. In NIPS'2010.
Neal, R. M. (1992). Connectionist learning of belief networks.
Artificial Intelligence, 56, 71–113.
Neal, R. M. (1993).
Probabilistic inference using Markov chain Monte-Carlo methods.
Technical Report CRG-TR93-1, Dept. of Computer Science, University of Toronto.
Ngiam, J., Chen, Z., Koh, P., and Ng, A. (2011). Learning deep energy models. In Proc. ICML'2011. ACM.
Olshausen, B. A. and Field, D. J. (1996).
Emergence of simple-cell receptive field properties by learning a sparse code for natural images. Nature, 381, 607–609.
Olshausen, B. A. and Field, D. J. (1997).
Sparse coding with an overcomplete basis set: a strategy employed by V1?
Vision Research, 37, 3311–3325.
Orr, G. and Muller, K.-R., editors (1998). Neural networks: tricks of the trade. Lect. Notes Comp. Sc.,. Springer-Verlag.
Pearson, K. (1901).
On lines and planes of closest fit to systems of points in space. Philosophical Magazine, 2(6).
Poggio, T. and Vetter, T. (1992). Recognition and structure from one 2d model view: Observations on prototypes, object classes and symmetries. AI Lab Memo No. 1347, MIT.
Raiko, T., Valpola, H., and LeCun, Y. (2012). Deep learning made easier by linear transformations in perceptrons.
In
AISTATS'2012.
Raina, R., Battle, A., Lee, H., Packer, B., and Ng, A. Y. (2007).
Self-taught learning: transfer learning from unlabeled data.
In ICML'2007.
Ranzato, M. and Hinton, G. H. (2010). Modeling pixel means and covariances using factorized third-order Boltzmann machines. In CVPR'2010, pages 2551–2558.
Ranzato, M., Poultney, C., Chopra, S., and LeCun, Y. (2007).
Efficient learning of sparse representations with an energybased model. In NIPS'06.
Ranzato, M., Boureau, Y., and LeCun, Y. (2008).
Sparse feature learning for deep belief networks. In NIPS'2007.
Ranzato, M., Krizhevsky, A., and Hinton, G. (2010a). Factored
3-way restricted Boltzmann machines for modeling natural images. In AISTATS'2010, pages 621–628.
Ranzato, M., Mnih, V., and Hinton, G. (2010b). Generating more realistic images using gated MRF's. In NIPS'2010.
Ranzato, M., Susskind, J., Mnih, V., and Hinton, G. (2011).
On deep generative models with applications to recognition.
In CVPR'2011.
Riesenhuber, M. and Poggio, T. (1999). Hierarchical models of object recognition in cortex. Nature Neuroscience.
Rifai, S., Vincent, P., Muller, X., Glorot, X., and Bengio, Y. (2011a). Contractive auto-encoders: Explicit invariance during feature extraction. In ICML'2011.
Rifai, S., Mesnil, G., Vincent, P., Muller, X., Bengio, Y., Dauphin, Y., and Glorot, X. (2011b).
Higher order contractive auto-encoder. In ECML PKDD.
Rifai, S., Dauphin, Y., Vincent, P., Bengio, Y., and Muller, X.(2011c). The manifold tangent classifier. In NIPS'2011.
Rifai, S., Bengio, Y., Dauphin, Y., and Vincent, P. (2012). A generative process for sampling contractive auto-encoders.
In ICML'2012.
Roweis, S. (1997). EM algorithms for PCA and sensible PCA.
CNS Technical Report CNS-TR-97-02, Caltech.
Roweis, S. and Saul, L. K. (2000). Nonlinear dimensionality reduction by locally linear embedding. Science, 290(5500), 2323–2326.
Salakhutdinov, R. (2010a).
Learning deep Boltzmann machines using adaptive MCMC. In ICML'2010.
Salakhutdinov, R. (2010b). Learning in Markov random fields using tempered transitions. In NIPS'2010.
Salakhutdinov, R. and Hinton, G. E. (2007). Semantic hashing.
In SIGIR'2007.
Salakhutdinov, R. and Hinton, G. E. (2009). Deep Boltzmann machines. In AISTATS'2009, pages 448–455.
Salakhutdinov, R. and Larochelle, H. (2010). Efficient learning of deep Boltzmann machines. In AISTATS'2010.
Salakhutdinov, R., Mnih, A., and Hinton, G. (2007).
Restricted Boltzmann machines for collaborative filtering. In
ICML'2007, pages 791–798.
Savard, F. (2011). R´eseaux de neurones `a relaxation entraˆın´es par crit`ere d'autoencodeur d´ebruitant.
Master's thesis, Universit´e de Montr´eal.
Schmah, T., Hinton, G. E., Zemel, R., Small, S. L., and Strother, S. (2009). Generative versus discriminative training of RBMs for classification of fMRI images.
In
NIPS'2008, pages 1409–1416.
Sch¨olkopf, B., Smola, A., and M¨uller, K.-R. (1998). Nonlinear component analysis as a kernel eigenvalue problem. Neural
Computation, 10, 1299–1319.
Schwenk, H., Rousseau, A., and Attik, M. (2012).
Large, pruned or continuous space language models on a gpu for statistical machine translation. In Workshop on the future of language modeling for HLT.
Seide, F., Li, G., and Yu, D. (2011). Conversational speech transcription using context-dependent deep neural networks.
In Interspeech 2011, pages 437–440.
Serre, T., Wolf, L., Bileschi, S., and Riesenhuber, M. (2007).
Robust object recognition with cortex-like mechanisms.
IEEE Trans. Pattern Anal. Mach. Intell., 29(3), 411–426.
Seung, S. H. (1998). Learning continuous attractors in recurrent networks. In NIPS'1997.
Simard, D., Steinkraus, P. Y., and Platt, J. C. (2003). Best practices for convolutional neural networks. In ICDAR'2003.
Simard, P., Victorri, B., LeCun, Y., and Denker, J. (1992). Tangent prop - A formalism for specifying selected invariances in an adaptive network. In NIPS'1991.
Simard, P. Y., LeCun, Y., and Denker, J. (1993).
Efficient pattern recognition using a new transformation distance. In
NIPS'92, pages 50–58.
Smolensky, P. (1986). Information processing in dynamical systems: Foundations of harmony theory. In D. E. Rumelhart and J. L. McClelland, editors, Parallel Distributed
Processing, volume 1, chapter 6, pages 194–281. MIT Press, Cambridge.
Socher, R., Huang, E. H., Pennington, J., Ng, A. Y., and Manning, C. D. (2011a).
Dynamic pooling and unfolding recursive autoencoders for paraphrase detection.
In
NIPS'2011.
Socher, R., Pennington, J., Huang, E. H., Ng, A. Y., and Manning, C. D. (2011b).
Semi-supervised recursive autoencoders for predicting sentiment distributions.
In
EMNLP'2011.
Stoyanov, V., Ropson, A., and Eisner, J. (2011). Empirical risk minimization of graphical model parameters given approximate inference, decoding, and model structure. In
AISTATS'2011.
Sutskever, I. (2012).
Training Recurrent Neural Networks.
Ph.D. thesis, Departement of computer science, University of Toronto.
Sutskever, I. and Tieleman, T. (2010). On the Convergence
Properties of Contrastive Divergence. In AISTATS'2010.
Sutskever, I., Hinton, G. E., and Taylor, G. (2009). The recurrent temporal restricted Boltzmann machine. In NIPS'2008.
Swersky, K. (2010).
Inductive Principles for Learning Restricted Boltzmann Machines.
Master's thesis, University of British Columbia.
Swersky, K., Ranzato, M., Buchman, D., Marlin, B., and de Freitas, N. (2011). On score matching for energy based models: Generalizing autoencoders and simplifying deep learning. In Proc. ICML'2011. ACM.
Taylor, G. and Hinton, G. (2009).
Factored conditional restricted Boltzmann machines for modeling motion style.
In ICML'2009, pages 1025–1032.
Taylor, G., Fergus, R., LeCun, Y., and Bregler, C. (2010).
Convolutional learning of spatio-temporal features.
In
ECCV'10, pages 140–153.
Tenenbaum, J., de Silva, V., and Langford, J. C. (2000). A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500), 2319–2323.
Tieleman, T. (2008).
Training restricted Boltzmann machines using approximations to the likelihood gradient. In
ICML'2008, pages 1064–1071.
Tieleman, T. and Hinton, G. (2009). Using fast weights to improve persistent contrastive divergence. In ICML'2009.
Tipping, M. E. and Bishop, C. M. (1999). Probabilistic principal components analysis. Journal of the Royal Statistical
Society B, 61(3), 611–622.
Turaga, S. C., Murray, J. F., Jain, V., Roth, F., Helmstaedter, M., Briggman, K., Denk, W., and Seung, H. S. (2010).
Convolutional networks can learn to generate affinity graphs for image segmentation. Neural Computation, 22, 511–538. van der Maaten, L. (2009). Learning a parametric embedding by preserving local structure. In AISTATS'2009. van der Maaten, L. and Hinton, G. E. (2008). Visualizing data using t-sne. J. Machine Learning Res., 9.
Vincent, P. (2011). A connection between score matching and denoising autoencoders. Neural Computation, 23(7).
Vincent, P. and Bengio, Y. (2003). Manifold Parzen windows.
In NIPS'2002. MIT Press.
Vincent, P., Larochelle, H., Bengio, Y., and Manzagol, P.A. (2008). Extracting and composing robust features with denoising autoencoders. In ICML 2008.
Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., and Manzagol, P.-A. (2010). Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. J. Machine Learning Res., 11.
Weinberger, K. Q. and Saul, L. K. (2004).
Unsupervised learning of image manifolds by semidefinite programming.
In CVPR'2004, pages 988–995.
Welling, M. (2009). Herding dynamic weights for partially observed random field models. In UAI'2009.
Welling, M., Hinton, G. E., and Osindero, S. (2003). Learning sparse topographic representations with products of Studentt distributions. In NIPS'2002.
Weston, J., Ratle, F., and Collobert, R. (2008). Deep learning via semi-supervised embedding. In ICML 2008.
Weston, J., Bengio, S., and Usunier, N. (2010). Large scale image annotation: learning to rank with joint word-image embeddings. Machine Learning, 81(1), 21–35.
Wiskott, L. and Sejnowski, T. (2002). Slow feature analysis:
Unsupervised learning of invariances. Neural Computation, 14(4), 715–770.
Younes, L. (1999). On the convergence of markovian stochastic algorithms with rapidly decreasing ergodicity rates.
Stochastics and Stochastic Reports, 65(3), 177–228.
Yu, D., Wang, S., and Deng, L. (2010). Sequential labeling using deep-structured conditional random fields.
IEEE
Journal of Selected Topics in Signal Processing.
Yu, K. and Zhang, T. (2010). Improved local coordinate coding using local tangents. In ICML'2010.
Yu, K., Zhang, T., and Gong, Y. (2009). Nonlinear learning using local coordinate coding. In NIPS'2009.
Yu, K., Lin, Y., and Lafferty, J. (2011).
Learning image representations from the pixel level via hierarchical sparse coding. In CVPR.
Yuille, A. L. (2005). The convergence of contrastive divergences. In NIPS'2004, pages 1593–1600.
Zeiler, M., Krishnan, D., Taylor, G., and Fergus, R. (2010).
Deconvolutional networks. In CVPR'2010.
Zou, W. Y., Ng, A. Y., and Yu, K. (2011).
Unsupervised learning of visual invariance with temporal coherence. In
NIPS 2011 Workshop on Deep Learning and Unsupervised
Feature Learning.Journal of Machine Learning Research 11 (2010) 625-660
Submitted 8/09; Published 2/10
Why Does Unsupervised Pre-training Help Deep Learning?
Dumitru Erhan∗
DUMITRU.ERHAN@UMONTREAL.CA
Yoshua Bengio
YOSHUA.BENGIO@UMONTREAL.CA
Aaron Courville
AARON.COURVILLE@UMONTREAL.CA
Pierre-Antoine Manzagol
PIERRE-ANTOINE.MANZAGOL@UMONTREAL.CA
Pascal Vincent
PASCAL.VINCENT@UMONTREAL.CA
D´epartement d'informatique et de recherche op´erationnelle
Universit´e de Montr´eal
2920, chemin de la Tour
Montr´eal, Qu´ebec, H3T 1J8, Canada
Samy Bengio
BENGIO@GOOGLE.COM
Google Research
1600 Amphitheatre Parkway
Mountain View, CA, 94043, USA
Editor: L´eon Bottou
Abstract
Much recent research has been devoted to learning algorithms for deep architectures such as Deep
Belief Networks and stacks of auto-encoder variants, with impressive results obtained in several areas, mostly on vision and language data sets. The best results obtained on supervised learning tasks involve an unsupervised learning component, usually in an unsupervised pre-training phase.
Even though these new algorithms have enabled training deep models, many questions remain as to the nature of this difficult learning problem. The main question investigated here is the following: how does unsupervised pre-training work? Answering this questions is important if learning in deep architectures is to be further improved. We propose several explanatory hypotheses and test them through extensive simulations. We empirically show the influence of pre-training with respect to architecture depth, model capacity, and number of training examples. The experiments confirm and clarify the advantage of unsupervised pre-training. The results suggest that unsupervised pretraining guides the learning towards basins of attraction of minima that support better generalization from the training data set; the evidence from these results supports a regularization explanation for the effect of pre-training.
Keywords: deep architectures, unsupervised pre-training, deep belief networks, stacked denoising auto-encoders, non-convex optimization
1. Introduction
Deep learning methods aim at learning feature hierarchies with features from higher levels of the hierarchy formed by the composition of lower level features. They include learning methods for a wide array of deep architectures (Bengio, 2009 provides a survey), including neural networks with many hidden layers (Bengio et al., 2007; Ranzato et al., 2007; Vincent et al., 2008; Collobert and Weston, 2008) and graphical models with many levels of hidden variables (Hinton et al., 2006), ∗. Part of this work was done while Dumitru Erhan was at Google Research. c⃝2010 Dumitru Erhan, Yoshua Bengio, Aaron Courville, Pierre-Antoine Manzagol, Pascal Vincent and Samy Bengio.
ERHAN, BENGIO, COURVILLE, MANZAGOL, VINCENT AND BENGIO among others (Zhu et al., 2009; Weston et al., 2008). Theoretical results (Yao, 1985; H˚astad, 1986;
H˚astad and Goldmann, 1991; Bengio et al., 2006), reviewed and discussed by Bengio and LeCun(2007), suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g., in vision, language, and other AI-level tasks), one may need deep architectures.
The recent surge in experimental work in the field seems to support this notion, accumulating evidence that in challenging AI-related tasks—such as computer vision (Bengio et al., 2007; Ranzato et al., 2007; Larochelle et al., 2007; Ranzato et al., 2008; Lee et al., 2009; Mobahi et al., 2009; Osindero and Hinton, 2008), natural language processing (NLP) (Collobert and Weston, 2008; Weston et al., 2008), robotics (Hadsell et al., 2008), or information retrieval (Salakhutdinov and Hinton, 2007; Salakhutdinov et al., 2007)—deep learning methods significantly out-perform comparable but shallow competitors, and often match or beat the state-of-the-art.
These recent demonstrations of the potential of deep learning algorithms were achieved despite the serious challenge of training models with many layers of adaptive parameters. In virtually all instances of deep learning, the objective function is a highly non-convex function of the parameters, with the potential for many distinct local minima in the model parameter space. The principal difficulty is that not all of these minima provide equivalent generalization errors and, we suggest, that for deep architectures, the standard training schemes (based on random initialization) tend to place the parameters in regions of the parameters space that generalize poorly—as was frequently observed empirically but rarely reported (Bengio and LeCun, 2007).
The breakthrough to effective training strategies for deep architectures came in 2006 with the algorithms for training deep belief networks (DBN) (Hinton et al., 2006) and stacked autoencoders (Ranzato et al., 2007; Bengio et al., 2007), which are all based on a similar approach: greedy layer-wise unsupervised pre-training followed by supervised fine-tuning. Each layer is pretrained with an unsupervised learning algorithm, learning a nonlinear transformation of its input(the output of the previous layer) that captures the main variations in its input. This unsupervised pre-training sets the stage for a final training phase where the deep architecture is fine-tuned with respect to a supervised training criterion with gradient-based optimization. While the improvement in performance of trained deep models offered by the pre-training strategy is impressive, little is understood about the mechanisms underlying this success.
The objective of this paper is to explore, through extensive experimentation, how unsupervised pre-training works to render learning deep architectures more effective and why they appear to work so much better than traditional neural network training methods. There are a few reasonable hypotheses why unsupervised pre-training might work. One possibility is that unsupervised pretraining acts as a kind of network pre-conditioner, putting the parameter values in the appropriate range for further supervised training. Another possibility, suggested by Bengio et al. (2007), is that unsupervised pre-training initializes the model to a point in parameter space that somehow renders the optimization process more effective, in the sense of achieving a lower minimum of the empirical cost function.
Here, we argue that our experiments support a view of unsupervised pre-training as an unusual form of regularization: minimizing variance and introducing bias towards configurations of the parameter space that are useful for unsupervised learning. This perspective places unsupervised pretraining well within the family of recently developed semi-supervised methods. The unsupervised pre-training approach is, however, unique among semi-supervised training strategies in that it acts by defining a particular initialization point for standard supervised training rather than either modifying the supervised objective function (Barron, 1991) or explicitly imposing constraints on the parameWHY DOES UNSUPERVISED PRE-TRAINING HELP DEEP LEARNING? ters throughout training (Lasserre et al., 2006). This type of initialization-as-regularization strategy has precedence in the neural networks literature, in the shape of the early stopping idea (Sj¨oberg and Ljung, 1995; Amari et al., 1997), and in the Hidden Markov Models (HMM) community (Bahl et al., 1986; Povey and Woodland, 2002) where it was found that first training an HMM as a generative model was essential (as an initialization step) before fine-tuning it discriminatively. We suggest that, in the highly non-convex situation of training a deep architecture, defining a particular initialization point implicitly imposes constraints on the parameters in that it specifies which minima (out of a very large number of possible minima) of the cost function are allowed. In this way, it may be possible to think of unsupervised pre-training as being related to the approach of Lasserre et al.
Another important and distinct property of the unsupervised pre-training strategy is that in the standard situation of training using stochastic gradient descent, the beneficial generalization effects due to pre-training do not appear to diminish as the number of labeled examples grows very large.
We argue that this is a consequence of the combination of the non-convexity (multi-modality) of the objective function and the dependency of the stochastic gradient descent method on example ordering. We find that early changes in the parameters have a greater impact on the final region (basin of attraction of the descent procedure) in which the learner ends up. In particular, unsupervised pre-training sets the parameter in a region from which better basins of attraction can be reached, in terms of generalization. Hence, although unsupervised pre-training is a regularizer, it can have a positive effect on the training objective when the number of training examples is large.
As previously stated, this paper is concerned with an experimental assessment of the various competing hypotheses regarding the role of unsupervised pre-training in the recent success of deep learning methods. To this end, we present a series of experiments design to pit these hypotheses against one another in an attempt to resolve some of the mystery surrounding the effectiveness of unsupervised pre-training.
In the first set of experiments (in Section 6), we establish the effect of unsupervised pre-training on improving the generalization error of trained deep architectures. In this section we also exploit dimensionality reduction techniques to illustrate how unsupervised pre-training affects the location of minima in parameter space.
In the second set of experiments (in Section 7), we directly compare the two alternative hypotheses (pre-training as a pre-conditioner; and pre-training as an optimization scheme) against the hypothesis that unsupervised pre-training is a regularization strategy. In the final set of experiments, (in Section 8), we explore the role of unsupervised pre-training in the online learning setting, where the number of available training examples grows very large. In these experiments, we test key aspects of our hypothesis relating to the topology of the cost function and the role of unsupervised pre-training in manipulating the region of parameter space from which supervised training is initiated.
Before delving into the experiments, we begin with a more in-depth view of the challenges in training deep architectures and how we believe unsupervised pre-training works towards overcoming these challenges.
ERHAN, BENGIO, COURVILLE, MANZAGOL, VINCENT AND BENGIO
2. The Challenges of Deep Learning
In this section, we present a perspective on why standard training of deep models through gradient backpropagation appears to be so difficult. First, it is important to establish what we mean in stating that training is difficult.
We believe the central challenge in training deep architectures is dealing with the strong dependencies that exist during training between the parameters across layers. One way to conceive the difficulty of the problem is that we must simultaneously:
1. adapt the lower layers in order to provide adequate input to the final (end of training) setting of the upper layers
2. adapt the upper layers to make good use of the final (end of training) setting of the lower layers.
The second problem is easy on its own (i.e., when the final setting of the other layers is known). It is not clear how difficult is the first one, and we conjecture that a particular difficulty arises when both sets of layers must be learned jointly, as the gradient of the objective function is limited to a local measure given the current setting of other parameters. Furthermore, because with enough capacity the top two layers can easily overfit the training set, training error does not necessarily reveal the difficulty in optimizing the lower layers. As shown in our experiments here, the standard training schemes tend to place the parameters in regions of the parameters space that generalize poorly.
A separate but related issue appears if we focus our consideration of traditional training methods for deep architectures on stochastic gradient descent. A sequence of examples along with an online gradient descent procedure defines a trajectory in parameter space, which converges in some sense(the error does not improve anymore, maybe because we are near a local minimum). The hypothesis is that small perturbations of that trajectory (either by initialization or by changes in which examples are seen when) have more effect early on. Early in the process of following the stochastic gradient, changes in the weights tend to increase their magnitude and, consequently, the amount of nonlinearity of the network increases. As this happens, the set of regions accessible by stochastic gradient descent on samples of the training distribution becomes smaller. Early on in training small perturbations allow the model parameters to switch from one basin to a nearby one, whereas later on (typically with larger parameter values), it is unlikely to "escape" from such a basin of attraction.
Hence the early examples can have a larger influence and, in practice, trap the model parameters in particular regions of parameter space that correspond to the specific and arbitrary ordering of the training examples.1 An important consequence of this phenomenon is that even in the presence of a very large (effectively infinite) amounts of supervised data, stochastic gradient descent is subject to a degree of overfitting to the training data presented early in the training process. In that sense, unsupervised pre-training interacts intimately with the optimization process, and when the number of training examples becomes large, its positive effect is seen not only on generalization error but also on training error.
1. This process seems similar to the "critical period" phenomena observed in neuroscience and psychology (Bornstein, WHY DOES UNSUPERVISED PRE-TRAINING HELP DEEP LEARNING?
3. Unsupervised Pre-training Acts as a Regularizer
As stated in the introduction, we believe that greedy layer-wise unsupervised pre-training overcomes the challenges of deep learning by introducing a useful prior to the supervised fine-tuning training procedure. We claim that the regularization effect is a consequence of the pre-training procedure establishing an initialization point of the fine-tuning procedure inside a region of parameter space in which the parameters are henceforth restricted. The parameters are restricted to a relatively small volume of parameter space that is delineated by the boundary of the local basin of attraction of the supervised fine-tuning cost function.
The pre-training procedure increases the magnitude of the weights and in standard deep models, with a sigmoidal nonlinearity, this has the effect of rendering both the function more nonlinear and the cost function locally more complicated with more topological features such as peaks, troughs and plateaus. The existence of these topological features renders the parameter space locally more difficult to travel significant distances via a gradient descent procedure. This is the core of the restrictive property imposed by the pre-training procedure and hence the basis of its regularizing properties.
But unsupervised pre-training restricts the parameters to particular regions: those that correspond to capturing structure in the input distribution P(X). To simply state that unsupervised pretraining is a regularization strategy somewhat undermines the significance of its effectiveness. Not all regularizers are created equal and, in comparison to standard regularization schemes such as
L1 and L2 parameter penalization, unsupervised pre-training is dramatically effective. We believe the credit for its success can be attributed to the unsupervised training criteria optimized during unsupervised pre-training.
During each phase of the greedy unsupervised training strategy, layers are trained to represent the dominant factors of variation extant in the data. This has the effect of leveraging knowledge of X to form, at each layer, a representation of X consisting of statistically reliable features of X that can then be used to predict the output (usually a class label) Y. This perspective places unsupervised pre-training well within the family of learning strategies collectively know as semisupervised methods. As with other recent work demonstrating the effectiveness of semi-supervised methods in regularizing model parameters, we claim that the effectiveness of the unsupervised pretraining strategy is limited to the extent that learning P(X) is helpful in learning P(Y|X). Here, we find transformations of X—learned features—that are predictive of the main factors of variation in P(X), and when the pre-training strategy is effective,2 some of these learned features of X are also predictive of Y. In the context of deep learning, the greedy unsupervised strategy may also have a special function. To some degree it resolves the problem of simultaneously learning the parameters at all layers (mentioned in Section 2) by introducing a proxy criterion. This proxy criterion encourages significant factors of variation, present in the input data, to be represented in intermediate layers.
To clarify this line of reasoning, we can formalize the effect of unsupervised pre-training in inducing a prior distribution over the parameters. Let us assume that parameters are forced to be chosen in a bounded region S ⊂ Rd. Let S be split in regions {Rk} that are the basins of attraction of descent procedures in the training error (note that {Rk} depends on the training set, but the dependency decreases as the number of examples increases). We have ∪kRk = S and Ri ∩ R j = /0 for i ̸= j. Let vk =
R 1θ∈Rkdθ be the volume associated with region Rk (where θ are our model's
2. Acting as a form of (data-dependent) "prior" on the parameters, as we are about to formalize.
ERHAN, BENGIO, COURVILLE, MANZAGOL, VINCENT AND BENGIO parameters). Let rk be the probability that a purely random initialization (according to our initialization procedure, which factorizes across parameters) lands in Rk, and let πk be the probability that pre-training (following a random initialization) lands in Rk, that is, ∑k rk = ∑k πk = 1. We can now take into account the initialization procedure as a regularization term: regularizer = −logP(θ).
For pre-trained models, the prior is Ppre−training(θ) = ∑ k
1θ∈Rkπk/vk.
For the models without unsupervised pre-training, the prior is Pno−pre−training(θ) = ∑ k
1θ∈Rkrk/vk.
One can verify that Ppre−training(θ ∈ Rk) = πk and Pno−pre−training(θ ∈ Rk) = rk. When πk is tiny, the penalty is high when θ ∈ Rk, with unsupervised pre-training. The derivative of this regularizer is zero almost everywhere because we have chosen a uniform prior inside each region Rk. Hence, to take the regularizer into account, and having a generative model Ppre−training(θ) for θ (i.e., this is the unsupervised pre-training procedure), it is reasonable to sample an initial θ from it (knowing that from this point on the penalty will not increase during the iterative minimization of the training criterion), and this is exactly how the pre-trained models are obtained in our experiments.
Note that this formalization is just an illustration: it is there to simply show how one could conceptually think of an initialization point as a regularizer and should not be taken as a literal interpretation of how regularization is explicitly achieved, since we do not have an analytic formula for computing the πk's and vk's. Instead these are implicitly defined by the whole unsupervised pre-training procedure.
4. Previous Relevant Work
We start with an overview of the literature on semi-supervised learning (SSL), since the SSL framework is essentially the one in which we operate as well.
4.1 Related Semi-Supervised Methods
It has been recognized for some time that generative models are less prone to overfitting than discriminant ones (Ng and Jordan, 2002). Consider input variable X and target variable Y. Whereas a discriminant model focuses on P(Y|X), a generative model focuses on P(X,Y) (often parametrized as P(X|Y)P(Y)), that is, it also cares about getting P(X) right, which can reduce the freedom of fitting the data when the ultimate goal is only to predict Y given X.
Exploiting information about P(X) to improve generalization of a classifier has been the driving idea behind semi-supervised learning (Chapelle et al., 2006). For example, one can use unsupervised learning to map X into a representation (also called embedding) such that two examples x1 and x2 that belong to the same cluster (or are reachable through a short path going through neighboring examples in the training set) end up having nearby embeddings. One can then use supervised learning(e.g., a linear classifier) in that new space and achieve better generalization in many cases (Belkin
WHY DOES UNSUPERVISED PRE-TRAINING HELP DEEP LEARNING? and Niyogi, 2002; Chapelle et al., 2003). A long-standing variant of this approach is the application of Principal Components Analysis as a pre-processing step before applying a classifier (on the projected data). In these models the data is first transformed in a new representation using unsupervised learning, and a supervised classifier is stacked on top, learning to map the data in this new representation into class predictions.
Instead of having separate unsupervised and supervised components in the model, one can consider models in which P(X) (or P(X,Y)) and P(Y|X) share parameters (or whose parameters are connected in some way), and one can trade-off the supervised criterion −logP(Y|X) with the unsupervised or generative one (−logP(X) or −logP(X,Y)). It can then be seen that the generative criterion corresponds to a particular form of prior (Lasserre et al., 2006), namely that the structure of P(X) is connected to the structure of P(Y|X) in a way that is captured by the shared parametrization.
By controlling how much of the generative criterion is included in the total criterion, one can find a better trade-off than with a purely generative or a purely discriminative training criterion (Lasserre et al., 2006; Larochelle and Bengio, 2008).
In the context of deep architectures, a very interesting application of these ideas involves adding an unsupervised embedding criterion at each layer (or only one intermediate layer) to a traditional supervised criterion (Weston et al., 2008). This has been shown to be a powerful semi-supervised learning strategy, and is an alternative to the kind of algorithms described and evaluated in this paper, which also combine unsupervised learning with supervised learning.
In the context of scarcity of labelled data (and abundance of unlabelled data), deep architectures have shown promise as well. Salakhutdinov and Hinton (2008) describe a method for learning the covariance matrix of a Gaussian Process, in which the usage of unlabelled examples for modeling
P(X) improves P(Y|X) quite significantly. Note that such a result is to be expected: with few labelled samples, modeling P(X) usually helps. Our results show that even in the context of abundant labelled data, unsupervised pre-training still has a pronounced positive effect on generalization: a somewhat surprising conclusion.
4.2 Early Stopping as a Form of Regularization
We stated that pre-training as initialization can be seen as restricting the optimization procedure to a relatively small volume of parameter space that corresponds to a local basin of attraction of the supervised cost function. Early stopping can be seen as having a similar effect, by constraining the optimization procedure to a region of the parameter space that is close to the initial configuration of parameters. With τ the number of training iterations and η the learning rate used in the update procedure, τη can be seen as the reciprocal of a regularization parameter. Indeed, restricting either quantity restricts the area of parameter space reachable from the starting point. In the case of the optimization of a simple linear model (initialized at the origin) using a quadratic error function and simple gradient descent, early stopping will have a similar effect to traditional regularization.
Thus, in both pre-training and early stopping, the parameters of the supervised cost function are constrained to be close to their initial values.3 A more formal treatment of early stopping as regularization is given by Sj¨oberg and Ljung (1995) and Amari et al. (1997). There is no equivalent treatment of pre-training, but this paper sheds some light on the effects of such initialization in the case of deep architectures.
3. In the case of pre-training the "initial values" of the parameters for the supervised phase are those that were obtained at the end of pre-training.
ERHAN, BENGIO, COURVILLE, MANZAGOL, VINCENT AND BENGIO
5. Experimental Setup and Methodology
In this section, we describe the setting in which we test the hypothesis introduced in Section 3 and previously proposed hypotheses. The section includes a description of the deep architectures used, the data sets and the details necessary to reproduce our results.
5.1 Models
All of the successful methods (Hinton et al., 2006; Hinton and Salakhutdinov, 2006; Bengio et al., 2007; Ranzato et al., 2007; Vincent et al., 2008; Weston et al., 2008; Ranzato et al., 2008; Lee et al., 2008) in the literature for training deep architectures have something in common: they rely on an unsupervised learning algorithm that provides a training signal at the level of a single layer.
Most work in two main phases. In a first phase, unsupervised pre-training, all layers are initialized using this layer-wise unsupervised learning signal. In a second phase, fine-tuning, a global training criterion (a prediction error, using labels in the case of a supervised task) is minimized. In the algorithms initially proposed (Hinton et al., 2006; Bengio et al., 2007; Ranzato et al., 2007), the unsupervised pre-training is done in a greedy layer-wise fashion: at stage k, the k-th layer is trained(with respect to an unsupervised criterion) using as input the output of the previous layer, and while the previous layers are kept fixed.
We shall consider two deep architectures as representatives of two families of models encountered in the deep learning literature.
5.1.1 DEEP BELIEF NETWORKS
The first model is the Deep Belief Net (DBN) by Hinton et al. (2006), obtained by training and stacking several layers of Restricted Boltzmann Machines (RBM) in a greedy manner. Once this stack of RBMs is trained, it can be used to initialize a multi-layer neural network for classification.
An RBM with n hidden units is a Markov Random Field (MRF) for the joint distribution between hidden variables hi and observed variables xj such that P(h|x) and P(x|h) factorize, that is, P(h|x) = ∏i P(hi|x) and P(x|h) = ∏ j P(xj|h). The sufficient statistics of the MRF are typically hi, xj and hixj, which gives rise to the following joint distribution:
P(x,h) ∝ eh′Wx+b′x+c′h with corresponding parameters θ = (W,b,c) (with ′ denoting transpose, ci associated with hi, b j with xj, and Wij with hixj). If we restrict hi and xj to be binary units, it is straightforward to show that
P(x|h)
= ∏ j
P(xj|h) with
P(xj = 1|h)
= sigmoid(b j +∑ i
Wijhi). where sigmoid(a) = 1/(1+exp(−a)) (applied element-wise on a vector a), and P(h|x) also has a similar form:
P(h|x)
= ∏ i
P(hi|x) with
P(hi = 1|x)
= sigmoid(ci +∑ j
Wijxj).
WHY DOES UNSUPERVISED PRE-TRAINING HELP DEEP LEARNING?
The RBM form can be generalized to other conditional distributions besides the binomial, including continuous variables. Welling et al. (2005) describe a generalization of RBM models to conditional distributions from the exponential family.
RBM models can be trained by approximate stochastic gradient descent. Although P(x) is not tractable in an RBM, the Contrastive Divergence estimator (Hinton, 2002) is a good stochastic approximation of ∂logP(x)
∂θ, in that it very often has the same sign (Bengio and Delalleau, 2009).
A DBN is a multi-layer generative model with layer variables h0 (the input or visible layer), h1, h2, etc. The top two layers have a joint distribution which is an RBM, and P(hk|hk+1) are parametrized in the same way as for an RBM. Hence a 2-layer DBN is an RBM, and a stack of RBMs share parametrization with a corresponding DBN. The contrastive divergence update direction can be used to initialize each layer of a DBN as an RBM, as follows. Consider the first layer of the DBN trained as an RBM P1 with hidden layer h1 and visible layer v1. We can train a second RBM P2 that models (in its visible layer) the samples h1 from P1(h1|v1) when v1 is sampled from the training data set. It can be shown that this maximizes a lower bound on the log-likelihood of the DBN. The number of layers can be increased greedily, with the newly added top layer trained as an RBM to model the samples produced by chaining the posteriors P(hk|hk−1) of the lower layers (starting from h0 from the training data set).
The parameters of a DBN or of a stack of RBMs also correspond to the parameters of a deterministic feed-forward multi-layer neural network. The i-th unit of the k-th layer of the neural network outputs ˆhki = sigmoid(cki +∑jWkijˆhk−1,j), using the parameters ck and Wk of the k-th layer of the DBN. Hence, once the stack of RBMs or the DBN is trained, one can use those parameters to initialize the first layers of a corresponding multi-layer neural network. One or more additional layers can be added to map the top-level features ˆhk to the predictions associated with a target variable(here the probabilities associated with each class in a classification task). Bengio (2009) provides more details on RBMs and DBNs, and a survey of related models and deep architectures.
5.1.2 STACKED DENOISING AUTO-ENCODERS
The second model, by Vincent et al. (2008), is the so-called Stacked Denoising Auto-Encoder(SDAE). It borrows the greedy principle from DBNs, but uses denoising auto-encoders as a building block for unsupervised modeling. An auto-encoder learns an encoder h(·) and a decoder g(·) whose composition approaches the identity for examples in the training set, that is, g(h(x)) ≈ x for x in the training set.
Assuming that some constraint prevents g(h(·)) from being the identity for arbitrary arguments, the auto-encoder has to capture statistical structure in the training set in order to minimize reconstruction error. However, with a high capacity code (h(x) has too many dimensions), a regular auto-encoder could potentially learn a trivial encoding. Note that there is an intimate connection between minimizing reconstruction error for auto-encoders and contrastive divergence training for
RBMs, as both can be shown to approximate a log-likelihood gradient (Bengio and Delalleau, 2009).
The denoising auto-encoder (Vincent et al., 2008; Seung, 1998; LeCun, 1987; Gallinari et al., 1987) is a stochastic variant of the ordinary auto-encoder with the distinctive property that even with a high capacity model, it cannot learn the identity mapping. A denoising autoencoder is explicitly trained to denoise a corrupted version of its input. Its training criterion can also be viewed as a variational lower bound on the likelihood of a specific generative model. It has been shown on an array of data sets to perform significantly better than ordinary auto-encoders and similarly or better
ERHAN, BENGIO, COURVILLE, MANZAGOL, VINCENT AND BENGIO than RBMs when stacked into a deep supervised architecture (Vincent et al., 2008). Another way to prevent regular auto-encoders with more code units than inputs to learn the identity is to restrict the capacity of the representation by imposing sparsity on the code (Ranzato et al., 2007, 2008).
We now summarize the training algorithm of the Stacked Denoising Auto-Encoders. More details are given by Vincent et al. (2008). Each denoising auto-encoder operates on its inputs x, either the raw inputs or the outputs of the previous layer. The denoising auto-encoder is trained to reconstruct x from a stochastically corrupted (noisy) transformation of it. The output of each denoising auto-encoder is the "code vector" h(x), not to confuse with the reconstruction obtained by applying the decoder to that code vector. In our experiments h(x) = sigmoid(b +Wx) is an ordinary neural network layer, with hidden unit biases b, and weight matrix W. Let C(x) represent a stochastic corruption of x. As done by Vincent et al. (2008), we setCi(x) = xi or 0, with a random subset (of a fixed size) selected for zeroing. We have also considered a salt and pepper noise, where we select a random subset of a fixed size and set Ci(x) = Bernoulli(0.5). The denoised "reconstruction" is obtained from the noisy input with ˆx = sigmoid(c+W Th(C(x))), using biases c and the transpose of the feedforward weights W. In the experiments on images, both the raw input xi and its reconstruction ˆxi for a particular pixel i can be interpreted as a Bernoulli probability for that pixel: the probability of painting the pixel as black at that location. We denote CE(x||ˆx) = ∑i CE(xi|| ˆxi) the sum of the component-wise cross-entropy between the Bernoulli probability distributions associated with each element of x and its reconstruction probabilities ˆx: CE(x||ˆx) = −∑i (xilog ˆxi +(1−xi)log(1− ˆxi)).
The Bernoulli model only makes sense when the input components and their reconstruction are in ; another option is to use a Gaussian model, which corresponds to a Mean Squared Error (MSE) criterion.
With either DBN or SDAE, an output logistic regression layer is added after unsupervised training. This layer uses softmax (multinomial logistic regression) units to estimate P(class|x) = softmaxclass(a), where ai is a linear combination of outputs from the top hidden layer. The whole network is then trained as usual for multi-layer perceptrons, to minimize the output (negative loglikelihood) prediction error.
5.2 Data Sets
We experimented on three data sets, with the motivation that our experiments would help understand previously presented results with deep architectures, which were mostly with the MNIST data set and variations (Hinton et al., 2006; Bengio et al., 2007; Ranzato et al., 2007; Larochelle et al., 2007;
Vincent et al., 2008):
MNIST the digit classification data set by LeCun et al. (1998), containing 60,000 training and 10,000 testing examples of 28x28 handwritten digits in gray-scale.
InfiniteMNIST a data set by Loosli et al. (2007), which is an extension of MNIST from which one can obtain a quasi-infinite number of examples. The samples are obtained by performing random elastic deformations of the original MNIST digits. In this data set, there is only one set of examples, and the models will be compared by their (online) performance on it.
Shapeset is a synthetic data set with a controlled range of geometric invariances. The underlying task is binary classification of 10 × 10 images of triangles and squares. The examples show
WHY DOES UNSUPERVISED PRE-TRAINING HELP DEEP LEARNING? images of shapes with many variations, such as size, orientation and gray-level. The data set is composed of 50000 training, 10000 validation and 10000 test images.4
5.3 Setup
The models used are
1. Deep Belief Networks containing Bernoulli RBM layers, 2. Stacked Denoising Auto-Encoders with Bernoulli input units, and 3. standard feed-forward multi-layer neural networks, each with 1–5 hidden layers. Each hidden layer contains the same number of hidden units, which is a hyperparameter. The other hyperparameters are the unsupervised and supervised learning rates, the L2 penalty / weight decay,5 and the fraction of stochastically corrupted inputs (for the SDAE).
For MNIST, the number of supervised and unsupervised passes through the data (epochs) is 50 and 50 per layer, respectively. With InfiniteMNIST, we perform 2.5 million unsupervised updates followed by 7.5 million supervised updates.6 The standard feed-forward networks are trained using
10 million supervised updates. For MNIST, model selection is done by choosing the hyperparameters that optimize the supervised (classification) error on the validation set. For InfiniteMNIST, we use the average online error over the last million examples for hyperparameter selection. In all cases, purely stochastic gradient updates are applied.
The experiments involve the training of deep architectures with a variable number of layers with and without unsupervised pre-training. For a given layer, weights are initialized using random samples from uniform[−1/
√ k,1/
√ k], where k is the number of connections that a unit receives from the previous layer (the fan-in). Either supervised gradient descent or unsupervised pre-training follows.
In most cases (for MNIST), we first launched a number of experiments using a cross-product of hyperparameter values7 applied to 10 different random initialization seeds. We then selected the hyperparameter sets giving the best validation error for each combination of model (with or without pre-training), number of layers, and number of training iterations. Using these hyper-parameters, we launched experiments using an additional 400 initialization seeds. For InfiniteMNIST, only one seed is considered (an arbitrarily chosen value).
In the discussions below we sometimes use the word apparent local minimum to mean the solution obtained after training, when no further noticeable progress seems achievable by stochastic gradient descent. It is possible that these are not really near a true local minimum (there could be a tiny ravine towards significant improvement, not accessible by gradient descent), but it is clear that these end-points represent regions where gradient descent is stuck. Note also that when we write of number of layers it is to be understood as the number of hidden layers in the network.
4. The data set can be downloaded from http://www.iro.umontreal.ca/˜lisa/twiki/bin/view.cgi/Public/
ShapesetDataForJMLR.
5. A penalizing term λ||θ||2
2 is added to the supervised objective, where θ are the weights of the network, and λ is a hyper-parameter modulating the strength of the penalty.
6. The number of examples was chosen to be as large as possible, while still allowing for the exploration a variety of hyper-parameters.
7. Number of hidden units ∈ {400,800,1200}; learning rate ∈ {0.1,0.05,0.02,0.01,0.005}; ℓ2 penalty coefficient λ ∈ {10−4,10−5,10−6,0}; pre-training learning rate ∈ {0.01,0.005,0.002,0.001,0.0005}; corruption probability
∈ {0.0,0.1,0.25,0.4}; tied weights ∈ {yes,no}.
ERHAN, BENGIO, COURVILLE, MANZAGOL, VINCENT AND BENGIO
6. The Effect of Unsupervised Pre-training
We start by a presentation of large-scale simulations that were intended to confirm some of the previously published results about deep architectures. In the process of analyzing them, we start making connections to our hypotheses and motivate the experiments that follow.
6.1 Better Generalization
When choosing the number of units per layer, the learning rate and the number of training iterations to optimize classification error on the validation set, unsupervised pre-training gives substantially lower test classification error than no pre-training, for the same depth or for smaller depth on various vision data sets (Ranzato et al., 2007; Bengio et al., 2007; Larochelle et al., 2009, 2007; Vincent et al., 2008) no larger than the MNIST digit data set (experiments reported from 10,000 to 50,000 training examples).
Such work was performed with only one or a handful of different random initialization seeds, so one of the goals of this study was to ascertain the effect of the random seed used when initializing ordinary neural networks (deep or shallow) and the pre-training procedure. For this purpose, between 50 and 400 different seeds were used to obtain the graphics on MNIST.
Figure 1: Effect of depth on performance for a model trained (left) without unsupervised pretraining and (right) with unsupervised pre-training, for 1 to 5 hidden layers (networks with 5 layers failed to converge to a solution, without the use of unsupervised pretraining). Experiments on MNIST. Box plots show the distribution of errors associated with 400 different initialization seeds (top and bottom quartiles in box, plus outliers beyond top and bottom quartiles). Other hyperparameters are optimized away (on the validation set). Increasing depth seems to increase the probability of finding poor apparent local minima.
Figure 1 shows the resulting distribution of test classification error, obtained with and without pre-training, as we increase the depth of the network. Figure 2 shows these distributions as histograms in the case of 1 and 4 layers. As can be seen in Figure 1, unsupervised pre-training allows
WHY DOES UNSUPERVISED PRE-TRAINING HELP DEEP LEARNING? classification error to go down steadily as we move from 1 to 4 hidden layers, whereas without pre-training the error goes up after 2 hidden layers. It should also be noted that we were unable to effectively train 5-layer models without use of unsupervised pre-training. Not only is the error obtained on average with unsupervised pre-training systematically lower than without the pre-training, it appears also more robust to the random initialization. With unsupervised pre-training the variance stays at about the same level up to 4 hidden layers, with the number of bad outliers growing slowly.
Contrast this with the case without pre-training: the variance and number of bad outliers grows sharply as we increase the number of layers beyond 2. The gain obtained with unsupervised pretraining is more pronounced as we increase the number of layers, as is the gain in robustness to random initialization. This can be seen in Figure 2. The increase in error variance and mean for deeper architectures without pre-training suggests that increasing depth increases the probability of finding poor apparent local minima when starting from random initialization. It is also interesting to note the low variance and small spread of errors obtained with 400 seeds with unsupervised pre-training: it suggests that unsupervised pre-training is robust with respect to the random initialization seed (the one used to initialize parameters before pre-training).
Figure 2: Histograms presenting the test errors obtained on MNIST using models trained with or without pre-training (400 different initializations each). Left: 1 hidden layer. Right: 4 hidden layers.
These experiments show that the variance of final test error with respect to initialization random seed is larger without pre-training, and this effect is magnified for deeper architectures. It should however be noted that there is a limit to the success of this technique: performance degrades for 5 layers on this problem.
6.2 Visualization of Features
Figure 3 shows the weights (called filters) of the first layer of the DBN before and after supervised fine-tuning. For visualizing what units do on the 2nd and 3rd layer, we used the activation maximization technique described by Erhan et al. (2009): to visualize what a unit responds most to, the method looks for the bounded input pattern that maximizes the activation of a given unit. This is an
ERHAN, BENGIO, COURVILLE, MANZAGOL, VINCENT AND BENGIO optimization problem which is solved by performing gradient ascent in the space of the inputs, to find a local maximum of the activation function. Interestingly, nearly the same maximal activation input pattern is recovered from most random initializations of the input pattern.
Figure 3: Visualization of filters learned by a DBN trained on InfiniteMNIST. The top figures contain a visualization of filters after pre-training, while the bottoms ones picture the same units after supervised fine-tuning; from left to right: units from the 1st, 2nd and 3rd layers, respectively.
For comparison, we have also visualized the filters of a network for 1–3 layers in which no pretraining was performed (Figure 4). While the first layer filters do seem to correspond to localized features, 2nd and 3rd layers are not as interpretable anymore. Qualitatively speaking, filters from the bottom row of Figure 3 and those from Figure 4 have little in common, which is an interesting conclusion in itself. In addition, there seems to be more interesting visual structures in the features learned in networks with unsupervised pre-training.
Several interesting conclusions can be drawn from Figure 3. First, supervised fine-tuning (after unsupervised pre-training), even with 7.5 million updates, does not change the weights in a significant way (at least visually): they seem stuck in a certain region of weight space, and the sign of weights does not change after fine-tuning (hence the same pattern is seen visually). Second, different layers change differently: the first layer changes least, while supervised training has more effect when performed on the 3rd layer. Such observations are consistent with the predictions made by our hypothesis: namely that the early dynamics of stochastic gradient descent, the dynamics induced by unsupervised pre-training, can "lock" the training in a region of the parameter space that is essentially inaccessible for models that are trained in a purely supervised way.
Finally, the features increase in complexity as we add more layers. First layer weights seem to encode basic stroke-like detectors, second layer weights seem to detect digit parts, while top layer weights detect entire digits. The features are more complicated as we add more layers, and displaying only one image for each "feature" does not do justice to the non-linear nature of that
WHY DOES UNSUPERVISED PRE-TRAINING HELP DEEP LEARNING? feature. For example, it does not show the set of patterns on which the feature is highly active (or highly inactive).
While Figures 3–4 show only the filters obtained on InfiniteMNIST, the visualizations are similar when applied on MNIST. Likewise, the features obtained with SDAE result in qualitatively similar conclusions; Erhan et al. (2009) gives more details.
Figure 4: Visualization of filters learned by a network without pre-training, trained on
InfiniteMNIST. The filters are shown after supervised training; from left to right: units from the 1st, 2nd and 3rd layers, respectively.
6.3 Visualization of Model Trajectories During Learning
Visualizing the learned features allows for a qualitative comparison of the training strategies for deep architectures. However it is not useful for investigating how these strategies are influenced by random initialization, as the features learned from multiple initializations look similar. If it was possible for us to visualize a variety of models at the same time, it would allow us to explore our hypothesis, and ascertain to what degree and how the set of pre-trained models (for different random seeds) is far from the set of models without pre-training. Do these two sets cover very different regions in parameter space? Are parameter trajectories getting stuck in many different apparent local minima?
Unfortunately, it is not possible to directly compare parameter values of two architectures, because many permutations of the same parameters give rise to the same model. However, one can take a functional approximation approach in which we compare the function (from input to output) represented by each network, rather than comparing the parameters. The function is the infinite ordered set of output values associated with all possible inputs, and it can be approximated with a finite number of inputs (preferably plausible ones). To visualize the trajectories followed during training, we use the following procedure. For a given model, we compute and concatenate all its outputs on the test set examples as one long vector summarizing where it stands in "function space".
We get one such vector for each partially trained model (at each training iteration). This allows us to plot many learning trajectories, one for each initialization seed, with or without pre-training. Using a dimensionality reduction algorithm we then map these vectors to a two-dimensional space for visualization.8 Figures 5 and 6 present the results using dimensionality reduction techniques that
8. Note that we can and do project the models with and without pre-training at the same time, so as to visualize them in the same space.
ERHAN, BENGIO, COURVILLE, MANZAGOL, VINCENT AND BENGIO focus respectively on local9 and global structure.10 Each point is colored according to the training iteration, to help follow the trajectory movement.
−100
−80
−60
−40
−20
−100
−80
−60
−40
−20
2 layers without pre−training
2 layers with pre−training
Figure 5: 2D visualizations with tSNE of the functions represented by 50 networks with and 50 networks without pre-training, as supervised training proceeds over MNIST. See Section 6.3 for an explanation. Color from dark blue to cyan and red indicates a progression in training iterations (training is longer without pre-training). The plot shows models with 2 hidden layers but results are similar with other depths.
What seems to come out of these visualizations is the following:
1. The pre-trained and not pre-trained models start and stay in different regions of function space.
2. From the visualization focusing on local structure (Figure 5) we see that all trajectories of a given type (with pre-training or without) initially move together. However, at some point(after about 7 epochs) the different trajectories (corresponding to different random seeds) diverge (slowing down into elongated jets) and never get back close to each other (this is more true for trajectories of networks without pre-training). This suggests that each trajectory moves into a different apparent local minimum.11
9. t-Distributed Stochastic Neighbor Embedding, or tSNE, by van der Maaten and Hinton (2008), with the default parameters available in the public implementation: http://ict.ewi.tudelft.nl/˜lvandermaaten/t-SNE.html.
10. Isomap by Tenenbaum et al. (2000), with one connected component.
11. One may wonder if the divergence points correspond to a turning point in terms of overfitting. As shall be seen in Figure 8, the test error does not improve much after the 7th epoch, which reinforces this hypothesis.
WHY DOES UNSUPERVISED PRE-TRAINING HELP DEEP LEARNING?
−4000
−3000
−2000
−1000
−1500
−1000
−500
Without pre−training
With pre−training
Figure 6: 2D visualization with ISOMAP of the functions represented by 50 networks with and 50 networks without pre-training, as supervised training proceeds over MNIST. See Section 6.3 for an explanation. Color from dark blue to cyan indicates a progression in training iterations (training is longer without pre-training). The plot shows models with
2 hidden layers but results are similar with other depths.
3. From the visualization focusing on global structure (Figure 6), we see the pre-trained models live in a disjoint and much smaller region of space than the not pre-trained models. In fact, from the standpoint of the functions found without pre-training, the pre-trained solutions look all the same, and their self-similarity increases (variance across seeds decreases) during training, while the opposite is observed without pre-training. This is consistent with the formalization of pre-training from Section 3, in which we described a theoretical justification for viewing unsupervised pre-training as a regularizer; there, the probabilities of pre-traininig parameters landing in a basin of attraction is small.
The visualizations of the training trajectories do seem to confirm our suspicions. It is difficult to guarantee that each trajectory actually does end up in a different local minimum (corresponding to a different function and not only to different parameters). However, all tests performed (visual inspection of trajectories in function space, but also estimation of second derivatives in the directions of all the estimated eigenvectors of the Jacobian not reported in details here) were consistent with that interpretation.
We have also analyzed models obtained at the end of training, to visualize the training criterion in the neighborhood of the parameter vector θ∗ obtained. This is achieved by randomly sampling a direction v (from the stochastic gradient directions) and by plotting the training criterion around
ERHAN, BENGIO, COURVILLE, MANZAGOL, VINCENT AND BENGIO θ∗ in that direction, that is, at θ = θ∗ + αv, for α ∈ {−2.5,−2.4,...,−0.1,0,0.1,...2.4,2.5}, and v normalized (||v|| = 1). This analysis is visualized in Figure 7. The error curves look close to quadratic and we seem to be near a local minimum in all directions investigated, as opposed to a saddle point or a plateau. A more definite answer could be given by computing the full Hessian eigenspectrum, which would be expensive. Figure 7 also suggests that the error landscape is a bit flatter in the case of unsupervised pre-training, and flatter for deeper architectures.
Figure 7: Training errors obtained on Shapeset when stepping in parameter space around a converged model in 7 random gradient directions (stepsize of 0.1). Top: no pre-training.
Bottom: with unsupervised pre-training. Left: 1 hidden layer. Middle: 2 hidden layers. Right: 3 hidden layers. Compare also with Figure 8, where 1-layer networks with unsupervised pre-training obtain higher training errors.
6.4 Implications
The series of results presented so far show a picture that is consistent with our hypothesis. Better generalization that seems to be robust to random initializations is indeed achieved by pre-trained models, which indicates that unsupervised learning of P(X) is helpful in learning P(Y|X). The function space landscapes that we visualized point to the fact that there are many apparent local minima. The pre-trained models seem to end up in distinct regions of these error landscapes (and, implicitly, in different parts of the parameter space). This is both seen from the function space trajectories and from the fact that the visualizations of the learned features are qualitatively very different from those obtained by models without pre-training.
7. The Role of Unsupervised Pre-training
The observations so far in this paper confirm that starting the supervised optimization from pretrained weights rather than from randomly initialized weights consistently yields better performing
WHY DOES UNSUPERVISED PRE-TRAINING HELP DEEP LEARNING? classifiers on MNIST. To better understand where this advantage came from, it is important to realize that the supervised objective being optimized is exactly the same in both cases. The gradient-based optimization procedure is also the same. The only thing that differs is the starting point in parameter space: either picked at random or obtained after unsupervised pre-training (which also starts from a random initialization).
Deep architectures, since they are built from the composition of several layers of non-linearities, yield an error surface that is non-convex and hard to optimize, with the suspected presence of many local minima (as also shown by the above visualizations). A gradient-based optimization should thus end in the apparent local minimum of whatever basin of attraction we started from. From this perspective, the advantage of unsupervised pre-training could be that it puts us in a region of parameter space where basins of attraction run deeper than when picking starting parameters at random. The advantage would be due to a better optimization.
Now it might also be the case that unsupervised pre-training puts us in a region of parameter space in which training error is not necessarily better than when starting at random (or possibly worse), but which systematically yields better generalization (test error). Such behavior would be indicative of a regularization effect. Note that the two forms of explanation are not necessarily mutually exclusive.
Finally, a very simple explanation could be the most obvious one: namely the disparity in the magnitude of the weights (or more generally, the marginal distribution of the weights) at the start of the supervised training phase. We shall analyze (and rule out) this hypothesis first.
7.1 Experiment 1: Does Pre-training Provide a Better Conditioning Process for Supervised
Learning?
Typically gradient descent training of the deep model is initialized with randomly assigned weights, small enough to be in the linear region of the parameter space (close to zero for most neural network and DBN models). It is reasonable to ask if the advantage imparted by having an initial unsupervised pre-training phase is simply due to the weights being larger and therefore somehow providing a better "conditioning" of the initial values for the optimization process; we wanted to rule out this possibility.
By conditioning, we mean the range and marginal distribution from which we draw initial weights. In other words, could we get the same performance advantage as unsupervised pre-training if we were still drawing the initial weights independently, but from a more suitable distribution than the uniform[−1/
√ k,1/
√ k]? To verify this, we performed unsupervised pre-training, and computed marginal histograms for each layer's pre-trained weights and biases (one histogram per each layer's weights and biases). We then resampled new "initial" random weights and biases according to these histograms (independently for each parameter), and performed fine-tuning from there. The resulting parameters have the same marginal statistics as those obtained after unsupervised pre-training, but not the same joint distribution.
Two scenarios can be imagined. In the first, the initialization from marginals would lead to significantly better performance than the standard initialization (when no pre-training is used).
This would mean that unsupervised pre-training does provide a better marginal conditioning of ERHAN, BENGIO, COURVILLE, MANZAGOL, VINCENT AND BENGIO the weights. In the second scenario, the marginals would lead to performance similar to or worse than that without pre-training.12 initialization.
Uniform
Histogram
Unsup.pre-tr.
1 layer
1.81±0.07
1.94±0.09
1.41±0.07
2 layers
1.77±0.10
1.69±0.11
1.37±0.09
Table 1: Effect of various initialization strategies on 1 and 2-layer architectures: independent uniform densities (one per parameter), independent densities from the marginals after unsupervised pre-training, or unsupervised pre-training (which samples the parameters in a highly dependent way so that they collaborate to make up good denoising auto-encoders.)
Experiments on MNIST, numbers are mean and standard deviation of test errors (across different initialization seeds).
What we observe in Table 1 seems to fall within the first scenario. However, while initializing the weights to match the marginal distributions at the end of pre-training appears to slightly improve the generalization error on MNIST for 2 hidden layers, the difference is not significant and it is far from fully accounting for the discrepancy between the pre-trained and non-pre-trained results.
This experiment constitutes evidence against the preconditioning hypothesis, but does not exclude either the optimization hypothesis or the regularization hypothesis.
7.2 Experiment 2: The Effect of Pre-training on Training Error
The optimization and regularization hypotheses diverge on their prediction on how unsupervised pre-training should affect the training error: the former predicts that unsupervised pre-training should result in a lower training error, while the latter predicts the opposite. To ascertain the influence of these two possible explanatory factors, we looked at the test cost (Negative Log Likelihood on test data) obtained as a function of the training cost, along the trajectory followed in parameter space by the optimization procedure. Figure 8 shows 400 of these curves started from a point in parameter space obtained from random initialization, that is, without pre-training (blue), and 400 started from pre-trained parameters (red).
The experiments were performed for networks with 1, 2 and 3 hidden layers. As can be seen in Figure 8, while for 1 hidden layer, unsupervised pre-training reaches lower training cost than no pre-training, hinting towards a better optimization, this is not necessarily the case for the deeper networks. The remarkable observation is rather that, at a same training cost level, the pre-trained models systematically yield a lower test cost than the randomly initialized ones. The advantage appears to be one of better generalization rather than merely a better optimization procedure.
This brings us to the following result: unsupervised pre-training appears to have a similar effect to that of a good regularizer or a good "prior" on the parameters, even though no explicit regularization term is apparent in the cost being optimized. As we stated in the hypothesis, it might be reasoned that restricting the possible starting points in parameter space to those that minimize the unsupervised pre-training criterion (as with the SDAE), does in effect restrict the set of possible
12. We observed that the distribution of weights after unsupervised pre-training is fat-tailed. It is conceivable that sampling from such a distribution in order to initialize a deep architecture might actually hurt the performance of a deep architecture (compared to random initialization from a uniform distribution).
WHY DOES UNSUPERVISED PRE-TRAINING HELP DEEP LEARNING?
Figure 8: Evolution without pre-training (blue) and with pre-training (red) on MNIST of the log of the test NLL plotted against the log of the train NLL as training proceeds. Each of the 2 × 400 curves represents a different initialization. The errors are measured after each pass over the data. The rightmost points were measured after the first pass of gradient updates. Since training error tends to decrease during training, the trajectories run from right (high training error) to left (low training error). Trajectories moving up (as we go leftward) indicate a form of overfitting. All trajectories are plotted on top of each other. final configurations for parameter values. Like regularizers in general, unsupervised pre-training (in this case, with denoising auto-encoders) might thus be seen as decreasing the variance and introducing a bias (towards parameter configurations suitable for performing denoising). Unlike ordinary regularizers, unsupervised pre-training does so in a data-dependent manner.
7.3 Experiment 3: The Influence of the Layer Size
Another signature characteristic of regularization is that the effectiveness of regularization increases as capacity (e.g., the number of hidden units) increases, effectively trading off one constraint on the model complexity for another. In this experiment we explore the relationship between the number of units per layer and the effectiveness of unsupervised pre-training. The hypothesis that unsupervised pre-training acts as a regularizer would suggest that we should see a trend of increasing effectiveness of unsupervised pre-training as the number of units per layer are increased.
We trained models on MNIST with and without pre-training using increasing layer sizes: 25, 50, 100, 200, 400, 800 units per layer. Results are shown in Figure 9. Qualitatively similar results were obtained on Shapeset. In the case of SDAE, we were expecting the denoising pre-training procedure to help classification performance most for large layers; this is because the denoising pre-training allows useful representations to be learned in the over-complete case, in which a layer is larger than its input (Vincent et al., 2008). What we observe is a more systematic effect: while unsupervised pre-training helps for larger layers and deeper networks, it also appears to hurt for too small networks.
Figure 9 also shows that DBNs behave qualitatively like SDAEs, in the sense that unsupervised pre-training architectures with smaller layers hurts performance. Experiments on InfiniteMNIST reveal results that are qualitatively the same. Such an experiment seemingly points to a re-verification of the regularization hypothesis. In this case, it would seem that unsupervised pre-training acts as an additional regularizer for both DBN and SDAE models—on top of the regularization provided by
ERHAN, BENGIO, COURVILLE, MANZAGOL, VINCENT AND BENGIO
Figure 9: Effect of layer size on the changes brought by unsupervised pre-training, for networks with 1, 2 or 3 hidden layers. Experiments on MNIST. Error bars have a height of two standard deviations (over initialization seed). Pre-training hurts for smaller layer sizes and shallower networks, but it helps for all depths for larger networks. the small size of the hidden layers. As the model size decreases from 800 hidden units, the generalization error increases, and it increases more with unsupervised pre-training presumably because of the extra regularization effect: small networks have a limited capacity already so further restricting it (or introducing an additional bias) can harm generalization. Such a result seems incompatible with a pure optimization effect. We also obtain the result that DBNs and SDAEs seem to have qualitatively similar effects as pre-training strategies.
The effect can be explained in terms of the role of unsupervised pre-training as promoting input transformations (in the hidden layers) that are useful at capturing the main variations in the input distribution P(X). It may be that only a small subset of these variations are relevant for predicting the class label Y. When the hidden layers are small it is less likely that the transformations for predicting Y are included in the lot learned by unsupervised pre-training.
7.4 Experiment 4: Challenging the Optimization Hypothesis
Experiments 1–3 results are consistent with the regularization hypothesis and Experiments 2–3 would appear to directly support the regularization hypothesis over the alternative—that unsupervised pre-training aids in optimizing the deep model objective function.
In the literature there is some support for the optimization hypothesis. Bengio et al. (2007) constrained the top layer of a deep network to have 20 units and measured the training error of networks with and without pre-training. The idea was to prevent the networks from overfitting the training error simply with the top hidden layer, thus to make it clearer whether some optimization
WHY DOES UNSUPERVISED PRE-TRAINING HELP DEEP LEARNING? effect (of the lower layers) was going on. The reported training and test errors were lower for pretrained networks. One problem with the experimental paradigm used by Bengio et al. (2007) is their use of early stopping. This is problematic because, as previously mentioned, early stopping is itself a regularizer, and it can influence greatly the training error that is obtained. It is conceivable that if Bengio et al. (2007) had run the models to convergence, the results could have been different. We needed to verify this.
Figure 10 shows what happens without early stopping. The training error is still higher for pre-trained networks even though the generalization error is lower. This result now favors the regularization hypothesis against the optimization story. What may have happened is that early stopping prevented the networks without pre-training from moving too much towards their apparent local minimum.
Figure 10: For MNIST, a plot of the log(train NLL) vs. log(test NLL) at each epoch of training. The top layer is constrained to 20 units.
7.5 Experiment 5: Comparing pre-training to L1 and L2 regularization
An alternative hypothesis would be that classical ways of regularizing could perhaps achieve the same effect as unsupervised pre-training. We investigated the effect of L1 and L2 regularization(i.e., adding a ||θ||1 or ||θ||2
2 term to the supervised objective function) in a network without pretraining. We found that while in the case of MNIST a small penalty can in principle help, the gain is nowhere near as large as it is with pre-training. For InfiniteMNIST, the optimal amount of L1 and L2 regularization is zero.13
13. Which is consistent with the classical view of regularization, in which its effect should diminish as we add more and more data.
ERHAN, BENGIO, COURVILLE, MANZAGOL, VINCENT AND BENGIO
This is not an entirely surprising finding: not all regularizers are created equal and these results are consistent with the literature on semi-supervised training that shows that unsupervised learning can be exploited as a particularly effective form of regularization.
7.6 Summary of Findings: Experiments 1-5
So far, the results obtained from the previous experiments point towards a pretty clear explanation of the effect of unsupervised pre-training: namely, that its effect is a regularization effect. We have seen that it is not simply sufficient to sample random weights with the same magnitude: the (datadependent) unsupervised initialization is crucial. We have also observed that canonical regularizers(L1/L2 penalties on the weights) do not achieve the same level of performance.
The most compelling pieces of evidence in support of the regularization hypothesis are Figures
8 and 9. The alternative explanation—that unsupervised pre-training has an optimization effect— suggested by Bengio et al. (2007) doesn't seem to be supported by our experimental setup.
8. The Online Learning Setting
Our hypothesis included not only the statistical/phenomenological hypothesis that unsupervised pre-training acted as a regularizer, but also contains a mechanism for how such behavior arises both as a consequence of the dynamic nature of training—following a stochastic gradient through two phases of training and as a consequence of the non-convexity of the supervised objective function.
In our hypothesis, we posited that early examples induce changes in the magnitude of the weights that increase the amount of non-linearity of the network, which in turn decreases the number of regions accessible to the stochastic gradient descent procedure. This means that the early examples (be they pre-training examples or otherwise) determine the basin of attraction for the remainder of training; this also means that the early examples have a disproportionate influence on the configuration of parameters of the trained models.
One consequence to the hypothesized mechanism is that we would predict that in the online learning setting with unbounded or very large data sets, the behavior of unsupervised pre-training would diverge from the behavior of a canonical regularizer (L1/L2). This is because the effectiveness of a canonical regularizer decreases as the data set grows, whereas the effectiveness of unsupervised pre-training as a regularizer is maintained as the data set grows.
Note that stochastic gradient descent in online learning is a stochastic gradient descent optimization of the generalization error, so good online error in principle implies that we are optimizing well the generalization error. Indeed, each gradient ∂L(x,y)
∂θ for example (x,y) (with L(x,y) the supervised loss with input x and label y) sampled from the true generating distribution P(x,y) is an unbiased
Monte-Carlo estimator of the true gradient of generalization error, that is, ∑y
R x
∂L(x,y)
∂θ
P(x,y)dx.
In this section we empirically challenge this aspect of the hypothesis and show that the evidence does indeed support our hypothesis over what is more typically expected from a regularizer.
8.1 Experiment 6: Effect of Pre-training with Very Large Data Sets
The results presented here are perhaps the most surprising findings of this paper. Figure 11 shows the online classification error (on the next block of examples, as a moving average) for 6 architectures that are trained on InfiniteMNIST: 1 and 3-layer DBNs, 1 and 3-layer SDAE, as well as 1 and 3-layer networks without pre-training.
WHY DOES UNSUPERVISED PRE-TRAINING HELP DEEP LEARNING?
Figure 11: Comparison between 1 and 3-layer networks trained on InfiniteMNIST. Online classification error, computed as an average over a block of last 100,000 errors.
We can draw several observations from these experiments. First, 3-layer networks without pre-training are worse at generalization, compared to the 1-layer equivalent. This confirms the hypothesis that even in an online setting, optimization of deep networks is harder than shallow ones. Second, 3-layer SDAE models seem to generalize better than 3-layer DBNs. Finally and most importantly, the pre-training advantage does not vanish as the number of training examples increases, on the contrary.
Note that the number of hidden units of each model is a hyperparameter.14 So theoretical results suggest that 1-layer networks without pre-training should in principle be able to represent the input distribution as capacity and data grow. Instead, without pre-training, the networks are not able to take advantage of the additional capacity, which again points towards the optimization explanation.
It is clear, however, that the starting point of the non-convex optimization matters, even for networks that are seemingly "easier" to optimize (1-layer ones), which supports our hypothesis.
Another experiment that shows the effects of large-scale online stochastic non-convex optimization is shown in Figure 12. In the setting of InfiniteMNIST, we compute the error on the training set, in the same order that we presented the examples to the models. We observe several interesting results: first, note that both models are better at classifying more recently seen examples. This is a natural effect of stochastic gradient descent with a constant learning rate (which gives exponentially more weight to recent examples). Note also that examples at the beginning of training are essentially like test examples for both models, in terms of error. Finally, we observe that the pre-trained
14. This number was chosen individually for each model s.t. the error on the last 1 million examples is minimized. In practice, this meant 2000 units for 1-layer networks and 1000 units/layer for 3-layer networks.
ERHAN, BENGIO, COURVILLE, MANZAGOL, VINCENT AND BENGIO
Figure 12: Error of 1-layer network with RBM pre-training and without, on the 10 million examples used for training it. The errors are calculated in the same order (from left to right, above) as the examples were presented during training. Each error bar corresponds to a block of consecutive training examples. model is better across the board on the training set. This fits well with the optimization hypothesis, since it shows that unsupervised pre-training has an optimization effect.
What happens in this setting is that the training and generalization errors converge as the empirical distribution (defined by the training set) converges to the true data distribution. These results show that the effectiveness of unsupervised pre-training does not diminish with increasing data set sizes. This would be unexpected from a superficial understanding of unsupervised pre-training as a regularization method. However it is entirely consistent with our interpretation, stated in our hypothesis, of the role of unsupervised pre-training in the online setting with stochastic gradient descent training on a non-convex objective function.
8.2 Experiment 7: The Effect of Example Ordering
The hypothesized mechanism implies, due to the dynamics of learning—the increase in weight magnitude and non-linearity as training proceeds, as well as the dependence of the basin of attraction on early data—that, when training with stochastic gradient descent, we should see increased sensitivity to early examples. In the case of InfiniteMNIST we operate in an online stochastic optimization regime, where we try to find a local minimum of a highly non-convex objective function. It is then interesting to study to what extent the outcome of this optimization is influenced by the examples seen at different points during training, and whether the early examples have a stronger influence(which would not be the case with a convex objective).
To quantify the variance of the outcome with respect to training samples at different points during training, and to compare these variances for models with and without pre-training, we proceeded with the following experiment. Given a data set with 10 million examples, we vary (by resampling)
WHY DOES UNSUPERVISED PRE-TRAINING HELP DEEP LEARNING? the first million examples (across 10 different random draws, sampling a different set of 1 million examples each time) and keep the other ones fixed. After training the (10) models, we measure the variance (across the 10 draws) of the output of the networks on a fixed test set (i.e., we measure the variance in function space). We then vary the next million examples in the same fashion, and so on, to see how much each of the ten parts of the training set influenced the final function.
Figure 13: Variance of the output of a trained network with 1 layer. The variance is computed as a function of the point at which we vary the training samples. Note that the 0.25 mark corresponds to the start of pre-training.
Figure 13 shows the outcome of such an analysis. The samples at the beginning15 do seem to influence the output of the networks more than the ones at the end. However, this variance is lower for the networks that have been pre-trained. In addition to that, one should note that the variance of pre-trained network at 0.25 (i.e., the variance of the output as a function of the first samples used for supervised training) is lower than the variance of the supervised network at 0.0. Such results imply that unsupervised pre-training can be seen as a sort of variance reduction technique, consistent with a regularization hypothesis. Finally, both networks are more influenced by the last examples used for optimization, which is simply due to the fact that we use stochastic gradient with a constant learning rate, where the most recent examples' gradient has a greater influence.
These results are consistent with what our hypothesis predicts: both the fact that early examples have greater influence (i.e., the variance is higher) and that pre-trained models seem to reduce this variance are in agreement with what we would have expected.
15. Which are unsupervised examples, for the red curve, until the 0.25 mark in Figure 13.
ERHAN, BENGIO, COURVILLE, MANZAGOL, VINCENT AND BENGIO
8.3 Experiment 8: Pre-training only k layers
From Figure 11 we can see that unsupervised pre-training makes quite a difference for 3 layers, on
InfiniteMNIST. In Figure 14 we explore the link between depth and unsupervised pre-training in more detail. The setup is as follows: for both MNIST and InfiniteMNIST we pre-train only the bottom k layers and randomly initialize the top n − k layers in the usual way. In this experiment, n = 3 and we vary k from 0 (which corresponds to a network with no pre-training) to k = n (which corresponds to the normal pre-trained case).
For MNIST, we plot the log(train NLL) vs. log(test NLL) trajectories, where each point corresponds to a measurement after a certain number of epochs. The trajectories go roughly from the right to left and from top to bottom, corresponding to the lowering of the training and test errors.
We can also see that models overfit from a certain point onwards.
Figure 14: On the left: for MNIST, a plot of the log(train NLL) vs. log(test NLL) at each epoch of training. We pre-train the first layer, the first two layers and all three layers using RBMs and randomly initialize the other layers; we also compare with the network whose layers are all randomly initialized. On the right: InfiniteMNIST, the online classification error. We pre-train the first layer, the first two layers or all three layers using denoising auto-encoders and leave the rest of the network randomly initialized.
For InfiniteMNIST, we simply show the online error. The results are ambiguous w.r.t the difficulty of optimizing the lower layers versus the higher ones. We would have expected that the largest incremental benefit came from pre-training the first layer or first two layers. It is true for the first two layers, but not the first. As we pre-train more layers, the models become better at generalization. In the case of the finite MNIST, note how the final training error (after the same number of epochs) becomes worse with pre-training of more layers. This clearly brings additional support to the regularization explanation.
WHY DOES UNSUPERVISED PRE-TRAINING HELP DEEP LEARNING?
9. Discussion and Conclusions
We have shown that unsupervised pre-training adds robustness to a deep architecture. The same set of results also suggests that increasing the depth of an architecture that is not pre-trained increases the probability of finding poor apparent local minima. Pre-trained networks give consistently better generalization. Our visualizations point to the observations that pre-trained networks learn qualitatively different features (if networks are visualized in the weight space) compared to networks without pre-training. Moreover, the trajectories of networks with different initialization seeds seem to fall into many distinct apparent local minima, which are again different (and seemingly far apart) depending on whether we use pre-training or not.
We have shown that unsupervised pre-training is not simply a way of getting a good initial marginal distribution, and that it captures more intricate dependencies between parameters. One of our findings is that deep networks with unsupervised pre-training seem to exhibit some properties of a regularizer: with small enough layers, pre-trained deep architectures are systematically worse than randomly initialized deep architectures. Moreover, when the layers are big enough, the pre-trained models obtain worse training errors, but better generalization performance. Additionally, we have re-done an experiment which purportedly showed that unsupervised pre-training can be explained with an optimization hypothesis and observed a regularization effect instead. We also showed that classical regularization techniques (such as L1/L2 penalties on the network weights) cannot achieve the same performance as unsupervised pre-training, and that the effect of unsupervised pre-training does not go away with more training data, so if unsupervised pre-training is a regularizer, it is certainly of a rather different kind.
The two unsupervised pre-training strategies considered—denoising auto-encoders and Restricted
Boltzmann Machines—seem to produce qualitatively similar observations. We have observed that, surprisingly, the pre-training advantage is present even in the case of really large training sets, pointing towards the conclusion that the starting point in the non-convex optimization problem is indeed quite important; a fact confirmed by our visualizations of filters at various levels in the network.
Finally, the other important set of results show that unsupervised pre-training acts like a variance reduction technique, yet a network with pre-training has a lower training error on a very large data set, which supports an optimization interpretation of the effect of pre-training.
How do we make sense of all these results? The contradiction between what looks like regularization effects and what looks like optimization effects appears, on the surface, unresolved. Instead of sticking to these labels, we attempted to draw a hypothesis, described in Section 3 about the dynamics of learning in an architecture that is trained using two phases (unsupervised pre-training and supervised fine-tuning), which we believe to be consistent with all the above results.
This hypothesis suggests that there are consequences of the non-convexity of the supervised objective function, which we observed in various ways throughout our experiments. One of these consequences is that early examples have a big influence on the outcome of training and this is one of the reasons why in a large-scale setting the influence of unsupervised pre-training is still present.
Throughout this paper, we have delved on the idea that the basin of attraction induced by the early examples (in conjunction with unsupervised pre-training) is, for all practical purposes, a basin from which supervised training does not escape.
This effect can be observed from the various visualizations and performance evaluations that we made. Unsupervised pre-training, as a regularizer that only influences the starting point of supervised training, has an effect that, contrary to classical regularizers, does not disappear with
ERHAN, BENGIO, COURVILLE, MANZAGOL, VINCENT AND BENGIO more data (at least as far as we can see from our results). Basically, unsupervised pre-training favors hidden units that compute features of the input X that correspond to major factors of variation in the true P(X). Assuming that some of these are near features useful at predicting variations in Y, unsupervised pre-training sets up the parameters near a solution of low predictive generalization error.
One of the main messages that our results imply is that the optimization of a non-convex objective function with stochastic gradient descent presents challenges for analysis, especially in a regime with large amounts of data. Our analysis so far shows that it is possible for networks that are trained in such a regime to be influenced more by early examples. This can pose problems in scenarios where we would like our networks to be able to capture more of the information in later examples, that is, when training from very large data sets and trying to capture a lot of information from them.
One interesting realization is that with a small training set, we do not usually put a lot of importance on minimizing the training error, because overfitting is a major issue; the training error is not a good way to distinguish between the generalization performance of two models. In that setting, unsupervised pre-training helps to find apparent local minima that have better generalization error.
With a large training set, as we saw in Figure 12, the empirical and true distributions converge. In such a scenario, finding a better apparent local minimum will matter and stronger (better) optimization strategies should have a significant impact on generalization when the training set is very large. Note also that it would be interesting to extend our experimental techniques to the problem of training deep auto-encoders (with a bottleneck), where previous results (Hinton and Salakhutdinov, 2006) show that not only test error but also training error is greatly reduced by unsupervised pre-training, which is a strong indicator of an optimization effect. We hypothesize that the presence of the bottleneck is a crucial element that distinguishes the deep auto-encoders from the deep classifiers studied here.
In spite of months of CPU time on a cluster devoted to the experiments described here (which is orders of magnitude more than most previous work in this area), more could certainly be done to better understand these effects. Our original goal was to have well-controlled experiments with well understood data sets. It was not to advance a particular algorithm but rather to try to better understand a phenomenon that has been well documented elsewhere. Nonetheless, our results are limited by the data sets used and it is plausible that different conclusions could be drawn, should the same experiments be carried out on other data.
Our results suggest that optimization in deep networks is a complicated problem that is influenced in great part by the early examples during training. Future work should clarify this hypothesis.
If it is true and we want our learners to capture really complicated distributions from very large training sets, it may mean that we should consider learning algorithms that reduce the effect of the early examples, allowing parameters to escape from the attractors in which current learning dynamics get stuck.
The observations reported here suggest more detailed explanations than those already discussed, which could be tested in future work. We hypothesize that the factors of variation present in the input distribution are disentangled more and more as we go from the input layer to higher-levels of the feature hierarchy. This is coherent with observations of increasing invariance to geometric transformations in DBNs trained on images (Goodfellow et al., 2009), as well as by visualizing the variations in input images generated by sampling from the model (Hinton, 2007; Susskind et al., 2008), or when considering the preferred input associated with different units at different depths (Lee et al., WHY DOES UNSUPERVISED PRE-TRAINING HELP DEEP LEARNING?
2009; Erhan et al., 2009). As a result, during early stages of learning, the upper layers (those that typically learn quickly) would have access to a more robust representation of the input and are less likely to be hindered by the entangling of factors variations present in the input. If this disentangling hypothesis is correct, it would help to explain how unsupervised pre-training can address the chicken-and-egg issue explained in Section 2: the lower layers of a supervised deep architecture need the upper layers to define what they should extract, and vice-versa. Instead, the lower layers can extract robust and disentangled representations of the factors of variation and the upper layers select and combine the appropriate factors (sometimes not all at the top hidden layer). Note that as factors of variation are disentangled, it could also happen that some of them are not propagated upward (before fine-tuning), because RBMs do not try to represent in their hidden layer input bits that are independent.
To further explain why smaller hidden layers yield worse performance with pre-training than without (Figure 9), one may hypothesize further that, for some data sets, the leading factors of variation present in P(X) (presumably the only ones captured in a smaller layer) are less predictive of Y than random projections16 can be, precisely because of the hypothesized disentangling effect.
With enough hidden units, unsupervised pre-training may extract among the larger set of learned features some that are highly predictive of Y (more so than random projections). This additional hypothesis could be tested by measuring the mutual information between each hidden unit and the object categories (as done by Lee et al., 2009), as the number of hidden units is varied (like in Figure 9). It is expected that the unit with the most mutual information will be less informative with pre-training when the number of hidden units is too small, and more informative with pre-training when the number of hidden units is large enough.
Under the hypothesis that we have proposed in Section 3, the following result is unaccounted for: in Figure 8(a), training error is lower with pre-training when there is only one hidden layer, but worse with more layers. This may be explained by the following additional hypothesis. Although each layer extracts information about Y in some of its features, it is not guaranteed that all of that information is preserved when moving to higher layers. One may suspect this in particular for RBMs, which would not encode in their hidden layer any input bits that would be marginally independent of the others, because these bits would be explained by the visible biases: perfect disentangling of Y from the other factors of variation in X may yield marginally independent bits about
Y. Although supervised fine-tuning should help to bubble up that information towards the output layer, it might be more difficult to do so for deeper networks, explaining the above-stated feature of Figure 8. Instead, in the case of a single hidden layer, less information about Y would have been dropped (if at all), making the job of the supervised output layer easier. This is consistent with earlier results (Larochelle et al., 2009) showing that for several data sets supervised fine-tuning significantly improves classification error, when the output layer only takes input from the top hidden layer. This hypothesis is also consistent with the observation made here (Figure 1) that unsupervised pre-training actually does not help (and can hurt) for too deep networks.
In addition to exploring the above hypotheses, future work should include an investigation of the connection between the results presented in this paper and by Hinton and Salakhutdinov (2006), where it seems to be hard to obtain a good training reconstruction error with deep auto-encoders (in an unsupervised setting) without performing pre-training. Other avenues for future work include the analysis and understanding of deep semi-supervised techniques where one does not separate
16. Meaning the random initialization of hidden layers.
ERHAN, BENGIO, COURVILLE, MANZAGOL, VINCENT AND BENGIO between the pre-training phase and the supervised phase, such as work by Weston et al. (2008) and Larochelle and Bengio (2008). Such algorithms fall more squarely into the realm of semi-supervised methods. We expect that analyses similar to the ones we performed would be potentially harder, but perhaps revealing as well.
Many open questions remain towards understanding and improving deep architectures. Our conviction is that devising improved strategies for learning in deep architectures requires a more profound understanding of the difficulties that we face with them. This work helps with such understanding via extensive simulations and puts forward a hypothesis explaining the mechanisms behind unsupervised pre-training, which is well supported by our results.
Acknowledgments
This research was supported by funding from NSERC, MITACS, FQRNT, and the Canada Research
Chairs. The authors also would like to thank the editor and reviewers, as well as Fernando Pereira for their helpful comments and suggestions.
References
Shun-ichi Amari, Noboru Murata, Klaus-Robert M¨uller, Michael Finke, and Howard Hua Yang.
Asymptotic statistical theory of overtraining and cross-validation. IEEE Transactions on Neural
Networks, 8(5):985–996, 1997.
Lalit Bahl, Peter Brown, Peter deSouza, and Robert Mercer. Maximum mutual information estimation of hidden markov parameters for speech recognition. In International Conference on
Acoustics, Speech and Signal Processing (ICASSP), pages 49–52, Tokyo, Japan, 1986.
Andrew E. Barron. Complexity regularization with application to artificial neural networks. In
G. Roussas, editor, Nonparametric Functional Estimation and Related Topics, pages 561–576.
Kluwer Academic Publishers, 1991.
Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps and spectral techniques for embedding and clustering. In T.G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural
Information Processing Systems 14 (NIPS'01), Cambridge, MA, 2002. MIT Press.
Yoshua Bengio. Learning deep architectures for AI. Foundations and Trends in Machine Learning, 2(1):1–127, 2009. Also published as a book. Now Publishers, 2009.
Yoshua Bengio and Olivier Delalleau. Justifying and generalizing contrastive divergence. Neural
Computation, 21(6):1601–1621, June 2009.
Yoshua Bengio and Yann LeCun. Scaling learning algorithms towards AI. In L. Bottou, O. Chapelle, D. DeCoste, and J. Weston, editors, Large Scale Kernel Machines, pages 321–360. MIT Press, Yoshua Bengio, Olivier Delalleau, and Nicolas Le Roux. The curse of highly variable functions for local kernel machines. In Y. Weiss, B. Sch¨olkopf, and J. Platt, editors, Advances in Neural Information Processing Systems 18 (NIPS'05), pages 107–114. MIT Press, Cambridge, MA, WHY DOES UNSUPERVISED PRE-TRAINING HELP DEEP LEARNING?
Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle. Greedy layer-wise training of deep networks. In Bernhard Sch¨olkopf, John Platt, and Thomas Hoffman, editors, Advances in Neural Information Processing Systems 19 (NIPS'06), pages 153–160. MIT Press, 2007.
Marc H. Bornstein. Sensitive periods in development : interdisciplinary perspectives / edited by
Marc H. Bornstein. Lawrence Erlbaum Associates, Hillsdale, N.J. :, 1987.
Olivier Chapelle, Jason Weston, and Bernhard Sch¨olkopf. Cluster kernels for semi-supervised learning. In S. Becker, S. Thrun, and K. Obermayer, editors, Advances in Neural Information Processing Systems 15 (NIPS'02), pages 585–592, Cambridge, MA, 2003. MIT Press.
Olivier Chapelle, Bernhard Sch¨olkopf, and Alexander Zien. Semi-Supervised Learning. MIT Press, Ronan Collobert and Jason Weston. A unified architecture for natural language processing: Deep neural networks with multitask learning. In William W. Cohen, Andrew McCallum, and Sam T.
Roweis, editors, Proceedings of the Twenty-fifth International Conference on Machine Learning(ICML'08), pages 160–167. ACM, 2008.
Dumitru Erhan, Yoshua Bengio, Aaron Courville, and Pascal Vincent. Visualizing higher-layer features of a deep network. Technical Report 1341, Universit´e de Montr´eal, 2009.
Patrick Gallinari, Yann LeCun, Sylvie Thiria, and Francoise Fogelman-Soulie. Memoires associatives distribuees. In Proceedings of COGNITIVA 87, Paris, La Villette, 1987.
Ian Goodfellow, Quoc Le, Andrew Saxe, and Andrew Ng. Measuring invariances in deep networks.
In Y. Bengio, D. Schuurmans, J. Lafferty, C. K. I. Williams, and A. Culotta, editors, Advances in Neural Information Processing Systems 22, pages 646–654. 2009.
Raia Hadsell, Ayse Erkan, Pierre Sermanet, Marco Scoffier, Urs Muller, and Yann LeCun. Deep belief net learning in a long-range vision system for autonomous off-road driving.
In Proc.
Intelligent Robots and Systems (IROS'08), pages 628–633, 2008.
Johan H˚astad. Almost optimal lower bounds for small depth circuits. In Proceedings of the 18th annual ACM Symposium on Theory of Computing, pages 6–20, Berkeley, California, 1986. ACM
Press.
Johan H˚astad and Mikael Goldmann. On the power of small-depth threshold circuits. Computational Complexity, 1:113–129, 1991.
Geoffrey E. Hinton. Training products of experts by minimizing contrastive divergence. Neural
Computation, 14:1771–1800, 2002.
Geoffrey E. Hinton. To recognize shapes, first learn to generate images. In Paul Cisek, Trevor
Drew, and John Kalaska, editors, Computational Neuroscience: Theoretical Insights into Brain
Function. Elsevier, 2007.
Geoffrey E. Hinton and Ruslan Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504–507, July 2006.
ERHAN, BENGIO, COURVILLE, MANZAGOL, VINCENT AND BENGIO
Goeffrey E. Hinton, Simon Osindero, and Yee Whye Teh. A fast learning algorithm for deep belief nets. Neural Computation, 18:1527–1554, 2006.
Hugo Larochelle and Yoshua Bengio. Classification using discriminative restricted Boltzmann machines. In William W. Cohen, Andrew McCallum, and Sam T. Roweis, editors, Proceedings of the Twenty-fifth International Conference on Machine Learning (ICML'08), pages 536–543.
ACM, 2008.
Hugo Larochelle, Dumitru Erhan, Aaron Courville, James Bergstra, and Yoshua Bengio. An empirical evaluation of deep architectures on problems with many factors of variation. In Int. Conf.
Mach. Learn., pages 473–480, 2007.
Hugo Larochelle, Yoshua Bengio, Jerome Louradour, and Pascal Lamblin. Exploring strategies for training deep neural networks. The Journal of Machine Learning Research, 10:1–40, January
Julia A. Lasserre, Christopher M. Bishop, and Thomas P. Minka. Principled hybrids of generative and discriminative models.
In Proceedings of the Computer Vision and Pattern Recognition
Conference (CVPR'06), pages 87–94, Washington, DC, USA, 2006. IEEE Computer Society.
Yann LeCun. Mod`eles connexionistes de l'apprentissage. PhD thesis, Universit´e de Paris VI, 1987.
Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.
Honglak Lee, Chaitanya Ekanadham, and Andrew Ng. Sparse deep belief net model for visual area
V2. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information
Processing Systems 20 (NIPS'07), pages 873–880. MIT Press, Cambridge, MA, 2008.
Honglak Lee, Roger Grosse, Rajesh Ranganath, and Andrew Y. Ng. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. In L´eon Bottou and Michael Littman, editors, Proceedings of the Twenty-sixth International Conference on Machine
Learning (ICML'09). ACM, Montreal (Qc), Canada, 2009.
Ga¨elle Loosli, St´ephane Canu, and L´eon Bottou. Training invariant support vector machines using selective sampling. In L´eon Bottou, Olivier Chapelle, Dennis DeCoste, and Jason Weston, editors, Large Scale Kernel Machines, pages 301–320. MIT Press, Cambridge, MA., 2007.
Hossein Mobahi, Ronan Collobert, and Jason Weston. Deep learning from temporal coherence in video. In L´eon Bottou and Michael Littman, editors, Proceedings of the 26th International
Conference on Machine Learning, pages 737–744, Montreal, June 2009. Omnipress.
Andrew Y. Ng and Michael I. Jordan. On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes. In T.G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing Systems 14 (NIPS'01), pages 841–848, 2002.
Simon Osindero and Geoffrey E. Hinton. Modeling image patches with a directed hierarchy of markov random field. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20 (NIPS'07), pages 1121–1128, Cambridge, MA, 2008.
MIT Press.
WHY DOES UNSUPERVISED PRE-TRAINING HELP DEEP LEARNING?
Dan Povey and Philip C. Woodland. Minimum phone error and i-smoothing for improved discriminative training. In Acoustics, Speech, and Signal Processing, 2002. Proceedings. (ICASSP '02).
IEEE International Conference on, volume 1, pages I–105–I–108 vol.1, 2002.
Marc'Aurelio Ranzato, Christopher Poultney, Sumit Chopra, and Yann LeCun. Efficient learning of sparse representations with an energy-based model. In B. Sch¨olkopf, J. Platt, and T. Hoffman, editors, Advances in Neural Information Processing Systems 19 (NIPS'06), pages 1137–1144.
MIT Press, 2007.
Marc'Aurelio Ranzato, Y-Lan Boureau, and Yann LeCun. Sparse feature learning for deep belief networks. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20 (NIPS'07), pages 1185–1192, Cambridge, MA, 2008. MIT Press.
Ruslan Salakhutdinov and Geoffrey E. Hinton. Using deep belief nets to learn covariance kernels for Gaussian processes. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20 (NIPS'07), pages 1249–1256, Cambridge, MA, 2008.
MIT Press.
Ruslan Salakhutdinov and Geoffrey E. Hinton. Semantic hashing. In Proceedings of the 2007 Workshop on Information Retrieval and applications of Graphical Models (SIGIR 2007), Amsterdam, 2007. Elsevier.
Ruslan Salakhutdinov, Andriy Mnih, and Geoffrey E. Hinton. Restricted Boltzmann machines for collaborative filtering. In Zoubin Ghahramani, editor, Proceedings of the Twenty-fourth International Conference on Machine Learning (ICML'07), pages 791–798, New York, NY, USA, 2007.
ACM.
Sebastian H. Seung.
Learning continuous attractors in recurrent networks.
In M.I. Jordan, M.J. Kearns, and S.A. Solla, editors, Advances in Neural Information Processing Systems 10(NIPS'97), pages 654–660. MIT Press, 1998.
Jonas Sj¨oberg and Lennart Ljung. Overtraining, regularization and searching for a minimum, with application to neural networks. International Journal of Control, 62(6):1391–1407, 1995.
Joshua M. Susskind, Geoffrey E., Javier R. Movellan, and Adam K. Anderson. Generating facial expressions with deep belief nets. In V. Kordic, editor, Affective Computing, Emotion Modelling, Synthesis and Recognition, pages 421–440. ARS Publishers, 2008.
Joshua Tenenbaum, Vin de Silva, and John C. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500):2319–2323, December 2000.
Laurens van der Maaten and Geoffrey E. Hinton. Visualizing data using t-sne. Journal of Machine
Learning Research, 9:2579–2605, November 2008.
Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and composing robust features with denoising autoencoders. In Andrew McCallum and Sam Roweis, editors, Proceedings of the 25th Annual International Conference on Machine Learning (ICML
2008), pages 1096–1103. Omnipress, 2008.
ERHAN, BENGIO, COURVILLE, MANZAGOL, VINCENT AND BENGIO
Max Welling, Michal Rosen-Zvi, and Geoffrey E. Hinton. Exponential family harmoniums with an application to information retrieval. In L.K. Saul, Y. Weiss, and L. Bottou, editors, Advances in Neural Information Processing Systems 17 (NIPS'04), pages 1481–1488, Cambridge, MA, 2005.
MIT Press.
Jason Weston, Fr´ed´eric Ratle, and Ronan Collobert. Deep learning via semi-supervised embedding. In William W. Cohen, Andrew McCallum, and Sam T. Roweis, editors, Proceedings of the Twenty-fifth International Conference on Machine Learning (ICML'08), pages 1168–1175, New
York, NY, USA, 2008. ACM.
Andrew Yao. Separating the polynomial-time hierarchy by oracles. In Proceedings of the 26th
Annual IEEE Symposium on Foundations of Computer Science, pages 1–10, 1985.
Long Zhu, Yuanhao Chen, and Alan Yuille. Unsupervised learning of probabilistic grammar-markov models for object categories. IEEE Transactions on Pattern Analysis and Machine Intelligence, 31(1):114–128, 2009.Practical Bayesian Optimization of Machine
Learning Algorithms
Jasper Snoek
Department of Computer Science
University of Toronto jasper@cs.toronto.edu
Hugo Larochelle
Department of Computer Science
University of Sherbrooke hugo.larochelle@usherbrooke.edu
Ryan P. Adams
School of Engineering and Applied Sciences
Harvard University rpa@seas.harvard.edu
Abstract
The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a "black art" requiring expert experience, rules of thumb, or sometimes bruteforce search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand.
In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieve expertlevel performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.
Introduction
Machine learning algorithms are rarely parameter-free: parameters controlling the rate of learning or the capacity of the underlying model must often be specified. These parameters are often considered nuisances, making it appealing to develop machine learning algorithms with fewer of them.
Another, more flexible take on this issue is to view the optimization of such parameters as a procedure to be automated. Specifically, we could view such tuning as the optimization of an unknown black-box function and invoke algorithms developed for such problems. A good choice is Bayesian optimization, which has been shown to outperform other state of the art global optimization algorithms on a number of challenging optimization benchmark functions. For continuous functions, Bayesian optimization typically works by assuming the unknown function was sampled from a Gaussian process and maintains a posterior distribution for this function as observations are made or, in our case, as the results of running learning algorithm experiments with different hyperparameters are observed. To pick the hyperparameters of the next experiment, one can optimize the expected improvement (EI) over the current best result or the Gaussian process upper confidence bound (UCB). EI and UCB have been shown to be efficient in the number of function evaluations required to find the global optimum of many multimodal black-box functions.
Machine learning algorithms, however, have certain characteristics that distinguish them from other black-box optimization problems. First, each function evaluation can require a variable amount of time: training a small neural network with 10 hidden units will take less time than a bigger network with 1000 hidden units. Even without considering duration, the advent of cloud computing makes it possible to quantify economically the cost of requiring large-memory machines for learning, changing the actual cost in dollars of an experiment with a different number of hidden units.
Second, machine learning experiments are often run in parallel, on multiple cores or machines. In both situations, the standard sequential approach of GP optimization can be suboptimal.
In this work, we identify good practices for Bayesian optimization of machine learning algorithms.
We argue that a fully Bayesian treatment of the underlying GP kernel is preferred to the approach based on optimization of the GP hyperparameters, as previously proposed. Our second contribution is the description of new algorithms for taking into account the variable and unknown cost of experiments or the availability of multiple cores to run experiments in parallel.
Gaussian processes have proven to be useful surrogate models for computer experiments and good practices have been established in this context for sensitivity analysis, calibration and prediction.
While these strategies are not considered in the context of optimization, they can be useful to researchers in machine learning who wish to understand better the sensitivity of their models to various hyperparameters. Hutter et al. have developed sequential model-based optimization strategies for the configuration of satisfiability and mixed integer programming solvers using random forests. The machine learning algorithms we consider, however, warrant a fully Bayesian treatment as their expensive nature necessitates minimizing the number of evaluations. Bayesian optimization strategies have also been used to tune the parameters of Markov chain Monte Carlo algorithms. Recently, Bergstra et al. have explored various strategies for optimizing the hyperparameters of machine learning algorithms. They demonstrated that grid search strategies are inferior to random search, and suggested the use of Gaussian process Bayesian optimization, optimizing the hyperparameters of a squared-exponential covariance, and proposed the Tree Parzen Algorithm.
Bayesian Optimization with Gaussian Process Priors
As in other kinds of optimization, in Bayesian optimization we are interested in finding the minimum of a function f(x) on some bounded set X, which we will take to be a subset of RD. What makes Bayesian optimization different from other procedures is that it constructs a probabilistic model for f(x) and then exploits this model to make decisions about where in X to next evaluate the function, while integrating out uncertainty. The essential philosophy is to use all of the information available from previous evaluations of f(x) and not simply rely on local gradient and Hessian approximations. This results in a procedure that can find the minimum of difficult non-convex functions with relatively few evaluations, at the cost of performing more computation to determine the next point to try. When evaluations of f(x) are expensive to perform — as is the case when it requires training a machine learning algorithm — then it is easy to justify some extra computation to make better decisions. For an overview of the Bayesian optimization formalism and a review of previous work, see, e.g., Brochu et al.. In this section we briefly review the general Bayesian optimization approach, before discussing our novel contributions in Section 3.
There are two major choices that must be made when performing Bayesian optimization. First, one must select a prior over functions that will express assumptions about the function being optimized.
For this we choose the Gaussian process prior, due to its flexibility and tractability. Second, we must choose an acquisition function, which is used to construct a utility function from the model posterior, allowing us to determine the next point to evaluate.
Gaussian Processes
The Gaussian process (GP) is a convenient and powerful prior distribution on functions, which we will take here to be of the form f : X ! R. The GP is defined by the property that any finite set of N points {xn 2 X}N n=1 induces a multivariate Gaussian distribution on RN. The nth of these points is taken to be the function value f(xn), and the elegant marginalization properties of the Gaussian distribution allow us to compute marginals and conditionals in closed form. The support and properties of the resulting distribution on functions are determined by a mean function m : X ! R and a positive definite covariance function K : X ⇥ X ! R. We will discuss the impact of covariance functions in Section 3.1. For an overview of Gaussian processes, see Rasmussen and Williams.
Acquisition Functions for Bayesian Optimization
We assume that the function f(x) is drawn from a Gaussian process prior and that our observations are of the form {xn, yn}N n=1, where yn ⇠ N(f(xn), ⌫) and ⌫ is the variance of noise introduced into the function observations. This prior and these data induce a posterior over functions; the acquisition function, which we denote by a : X ! R+, determines what point in X should be evaluated next via a proxy optimization xnext = argmaxx a(x), where several different functions have been proposed. In general, these acquisition functions depend on the previous observations, as well as the GP hyperparameters; we denote this dependence as a(x ; {xn, yn}, ✓). There are several popular choices of acquisition function. Under the Gaussian process prior, these functions depend on the model solely through its predictive mean function µ(x ; {xn, yn}, ✓) and predictive variance function �2(x ; {xn, yn}, ✓). In the proceeding, we will denote the best current value as xbest = argminxn f(xn), �(·) will denote the cumulative distribution function of the standard normal, and �(·) will denote the standard normal density function.
Probability of Improvement
One intuitive strategy is to maximize the probability of improving over the best current value. Under the GP this can be computed analytically as aPI(x ; {xn, yn}, ✓) = �(�(x)), �(x) = f(xbest) � µ(x ; {xn, yn}, ✓)
�(x ; {xn, yn}, ✓)
Expected Improvement
Alternatively, one could choose to maximize the expected improvement(EI) over the current best. This also has closed form under the Gaussian process: aEI(x ; {xn, yn}, ✓) = �(x ; {xn, yn}, ✓) (�(x) �(�(x)) + N(�(x) ; 0, 1))
GP Upper Confidence Bound
A more recent development is the idea of exploiting lower confidence bounds (upper, when considering maximization) to construct acquisition functions that minimize regret over the course of their optimization. These acquisition functions have the form aLCB(x ; {xn, yn}, ✓) = µ(x ; {xn, yn}, ✓) �  �(x ; {xn, yn}, ✓), (3) with a tunable  to balance exploitation against exploration.
In this work we will focus on the EI criterion, as it has been shown to be better-behaved than probability of improvement, but unlike the method of GP upper confidence bounds (GP-UCB), it does not require its own tuning parameter. Although the EI algorithm performs well in minimization problems, we wish to note that the regret formalization may be more appropriate in some settings.
We perform a direct comparison between our EI-based approach and GP-UCB in Section 4.1.
Practical Considerations for Bayesian Optimization of Hyperparameters
Although an elegant framework for optimizing expensive functions, there are several limitations that have prevented it from becoming a widely-used technique for optimizing hyperparameters in machine learning problems. First, it is unclear for practical problems what an appropriate choice is for the covariance function and its associated hyperparameters. Second, as the function evaluation itself may involve a time-consuming optimization procedure, problems may vary significantly in duration and this should be taken into account. Third, optimization algorithms should take advantage of multi-core parallelism in order to map well onto modern computational environments. In this section, we propose solutions to each of these issues.
Covariance Functions and Treatment of Covariance Hyperparameters
The power of the Gaussian process to express a rich distribution on functions rests solely on the shoulders of the covariance function. While non-degenerate covariance functions correspond to infinite bases, they nevertheless can correspond to strong assumptions regarding likely functions. In particular, the automatic relevance determination (ARD) squared exponential kernel
KSE(x, x0) = ✓0 exp
⇢
�1
2r2(x, x0)
� r2(x, x0) =
D
X d=1(xd � x0 d)2/✓2 d.(4) is often a default choice for Gaussian process regression. However, sample functions with this covariance function are unrealistically smooth for practical optimization problems. We instead propose
3 the use of the ARD Mat´ern 5/2 kernel:
KM52(x, x0) = ✓0
✓
1 + p
5r2(x, x0) + 5
3r2(x, x0)
◆ exp n
� p
5r2(x, x0) o
This covariance function results in sample functions which are twice-differentiable, an assumption that corresponds to those made by, e.g., quasi-Newton methods, but without requiring the smoothness of the squared exponential.
After choosing the form of the covariance, we must also manage the hyperparameters that govern its behavior (Note that these "hyperparameters" are distinct from those being subjected to the overall
Bayesian optimization.), as well as that of the mean function. For our problems of interest, typically we would have D + 3 Gaussian process hyperparameters: D length scales ✓1:D, the covariance amplitude ✓0, the observation noise ⌫, and a constant mean m. The most commonly advocated approach is to use a point estimate of these parameters by optimizing the marginal likelihood under the Gaussian process, p(y | {xn}N n=1, ✓, ⌫, m) = N(y | m1, ⌃✓ + ⌫I), where y = [y1, y2, · · ·, yN]T, and ⌃✓ is the covariance matrix resulting from the N input points under the hyperparameters ✓.
However, for a fully-Bayesian treatment of hyperparameters (summarized here by ✓ alone), it is desirable to marginalize over hyperparameters and compute the integrated acquisition function:
ˆa(x ; {xn, yn}) =
Z a(x ; {xn, yn}, ✓) p(✓ | {xn, yn}N n=1) d✓, (6) where a(x) depends on ✓ and all of the observations. For probability of improvement and EI, this expectation is the correct generalization to account for uncertainty in hyperparameters. We can therefore blend acquisition functions arising from samples from the posterior over GP hyperparameters and have a Monte Carlo estimate of the integrated expected improvement. These samples can be acquired efficiently using slice sampling, as described in Murray and Adams. As both optimization and Markov chain Monte Carlo are computationally dominated by the cubic cost of solving an N-dimensional linear system (and our function evaluations are assumed to be much more expensive anyway), the fully-Bayesian treatment is sensible and our empirical evaluations bear this out.
Figure 1 shows how the integrated expected improvement changes the acquistion function.
Modeling Costs
Ultimately, the objective of Bayesian optimization is to find a good setting of our hyperparameters as quickly as possible. Greedy acquisition procedures such as expected improvement try to make the best progress possible in the next function evaluation. From a practial point of view, however, we are not so concerned with function evaluations as with wallclock time. Different regions of the parameter space may result in vastly different execution times, due to varying regularization, learning rates, etc. To improve our performance in terms of wallclock time, we propose optimizing with the expected improvement per second, which prefers to acquire points that are not only likely to be good, but that are also likely to be evaluated quickly. This notion of cost can be naturally generalized to other budgeted resources, such as reagents or money.
Just as we do not know the true objective function f(x), we also do not know the duration function c(x) : X ! R+. We can nevertheless employ our Gaussian process machinery to model ln c(x) alongside f(x). In this work, we assume that these functions are independent of each other, although their coupling may be usefully captured using GP variants of multi-task learning (e.g., ).
Under the independence assumption, we can easily compute the predicted expected inverse duration and use it to compute the expected improvement per second as a function of x.
Monte Carlo Acquisition for Parallelizing Bayesian Optimization
With the advent of multi-core computing, it is natural to ask how we can parallelize our Bayesian optimization procedures. More generally than simply batch parallelism, however, we would like to be able to decide what x should be evaluated next, even while a set of points are being evaluated.
Clearly, we cannot use the same acquisition function again, or we will repeat one of the pending experiments. Ideally, we could perform a roll-out of our acquisition policy, to choose a point that appropriately balanced information gain and exploitation. However, such roll-outs are generally intractable. Instead we propose a sequential strategy that takes advantage of the tractable inference properties of the Gaussian process to compute Monte Carlo estimates of the acquisiton function under different possible results from pending function evaluations.(a) Posterior samples under varying hyperparameters(b) Expected improvement under varying hyperparameters(c) Integrated expected improvement
Figure 1: Illustration of integrated expected improvement. (a) Three posterior samples are shown, each with different length scales, after the same five observations. (b) Three expected improvement acquisition functions, with the same data and hyperparameters.
The maximum of each is shown. (c) The integrated expected improvement, with its maximum shown.(a) Posterior samples after three data(b) Expected improvement under three fantasies(c) Expected improvement across fantasies
Figure 2: Illustration of the acquisition with pending evaluations. (a) Three data have been observed and three posterior functions are shown, with "fantasies" for three pending evaluations. (b) Expected improvement, conditioned on the each joint fantasy of the pending outcome. (c) Expected improvement after integrating over the fantasy outcomes.
Consider the situation in which N evaluations have completed, yielding data {xn, yn}N n=1, and in which J evaluations are pending at locations {xj}J j=1. Ideally, we would choose a new point based on the expected acquisition function under all possible outcomes of these pending evaluations:
ˆa(x ; {xn, yn}, ✓, {xj}) =
Z
RJ a(x ; {xn, yn}, ✓, {xj, yj}) p({yj}J j=1 | {xj}J j=1, {xn, yn}N n=1) dy1 · · · dyJ.
This is simply the expectation of a(x) under a J-dimensional Gaussian distribution, whose mean and covariance can easily be computed. As in the covariance hyperparameter case, it is straightforward to use samples from this distribution to compute the expected acquisition and use this to select the next point. Figure 2 shows how this procedure would operate with queued evaluations. We note that a similar approach is touched upon briefly by Ginsbourger and Riche, but they view it as too intractable to warrant attention. We have found our Monte Carlo estimation procedure to be highly effective in practice, however, as will be discussed in Section 4.
Empirical Analyses
In this section, we empirically analyse1 the algorithms introduced in this paper and compare to existing strategies and human performance on a number of challenging machine learning problems.
We refer to our method of expected improvement while marginalizing GP hyperparameters as "GP
EI MCMC", optimizing hyperparameters as "GP EI Opt", EI per second as "GP EI per Second", and N times parallelized GP EI MCMC as "Nx GP EI MCMC". Each results figure plots the progression of minxn f(xn) over the number of function evaluations or time, averaged over multiple runs of each algorithm. If not specified otherwise, xnext = argmaxx a(x) is computed using gradientbased search with multiple restarts (see supplementary material for details). The code used is made publicly available at http://www.cs.toronto.edu/˜jasper/software.html.
1All experiments were conducted on identical machines using the Amazon EC2 service.
Min Function Value
Function evaluations
GP EI Opt
GP EI MCMC
GP−UCB
TPA(a)
Min Function Value
Function Evaluations
GP EI MCMC
GP EI Opt
GP EI per Sec
Tree Parzen Algorithm(b)
Min Function Value
Minutes
GP EI MCMC
GP EI per Second(c)
Figure 3: Comparisons on the Branin-Hoo function (3a) and training logistic regression on MNIST (3b). (3c) shows GP EI MCMC and GP EI per Second from (3b), but in terms of time elapsed.
Min Function Value
Function evaluations
GP EI MCMC
GP EI per second
GP EI Opt
Random Grid Search
3x GP EI MCMC
5x GP EI MCMC
10x GP EI MCMC(a)
Min function value
Time (Days)
GP EI MCMC
GP EI per second
GP EI Opt
3x GP EI MCMC
5x GP EI MCMC
10x GP EI MCMC(b)
Min Function Value
Function evaluations
3x GP EI MCMC (On grid)
5x GP EI MCMC (On grid)
3x GP EI MCMC (Off grid)
5x GP EI MCMC (Off grid)(c)
Figure 4: Different strategies of optimization on the Online LDA problem compared in terms of function evaluations (4a), walltime (4b) and constrained to a grid or not (4c).
Branin-Hoo and Logistic Regression
We first compare to standard approaches and the recent Tree Parzen Algorithm2 (TPA) of Bergstra et al. on two standard problems. The Branin-Hoo function is a common benchmark for Bayesian optimization techniques that is defined over x 2 R2 where 0  x1  15 and �5  x2  15. We also compare to TPA on a logistic regression classification task on the popular MNIST data. The algorithm requires choosing four hyperparameters, the learning rate for stochastic gradient descent, on a log scale from 0 to 1, the `2 regularization parameter, between 0 and 1, the mini batch size, from 20 to 2000 and the number of learning epochs, from 5 to 2000. Each algorithm was run on the Branin-Hoo and logistic regression problems 100 and 10 times respectively and mean and standard error are reported. The results of these analyses are presented in Figures 3a and 3b in terms of the number of times the function is evaluated. On Branin-Hoo, integrating over hyperparameters is superior to using a point estimate and the GP EI significantly outperforms TPA, finding the minimum in less than half as many evaluations, in both cases. For logistic regression, 3b and 3c show that although EI per second is less efficient in function evaluations it outperforms standard EI in time.
Online LDA
Latent Dirichlet Allocation (LDA) is a directed graphical model for documents in which words are generated from a mixture of multinomial "topic" distributions. Variational Bayes is a popular paradigm for learning and, recently, Hoffman et al. proposed an online learning approach in that context. Online LDA requires 2 learning parameters, ⌧0 and , that control the learning rate
⇢t = (⌧0 + t)� used to update the variational parameters of LDA based on the tth minibatch of document word count vectors. The size of the minibatch is also a third parameter that must be chosen. Hoffman et al. relied on an exhaustive grid search of size 6 ⇥ 6 ⇥ 8, for a total of 288 hyperparameter configurations.
We used the code made publically available by Hoffman et al. to run experiments with online
LDA on a collection of Wikipedia articles. We downloaded a random set of 249 560 articles, split into training, validation and test sets of size 200 000, 24 560 and 25 000 respectively. The documents are represented as vectors of word counts from a vocabulary of 7702 words. As reported in Hoffman et al., we used a lower bound on the per word perplexity of the validation set documents as the performance measure. One must also specify the number of topics and the hyperparameters ⌘ for the symmetric Dirichlet prior over the topic distributions and ↵ for the symmetric Dirichlet prior over the per document topic mixing weights. We followed Hoffman et al. and used 100 topics and ⌘ = ↵ = 0.01 in our experiments in order to emulate their analysis and repeated exactly the grid
2Using the publicly available code from https://github.com/jaberg/hyperopt/wiki
Time (hours)
Min function value
GP EI MCMC
GP EI per Second
3x GP EI MCMC
3x GP EI per Second
Random Grid Search(a)
Min Function Value
Function evaluations
GP EI MCMC
GP EI per Second
3x GP EI MCMC
3x GP EI per Second(b)
Min Function Value
Function evaluations
Matern 52 ARD
SqExp
SqExp ARD
Matern 32 ARD(c)
Figure 5: A comparison of various strategies for optimizing the hyperparameters of M3E models on the protein motif finding task in terms of walltime (5a), function evaluations (5b) and different covariance functions(5c). search reported in the paper3. Each online LDA evaluation generally took between five to ten hours to converge, thus the grid search requires approximately 60 to 120 processor days to complete.
In Figures 4a and 4b we compare our various strategies of optimization over the same grid on this expensive problem. That is, the algorithms were restricted to only the exact parameter settings as evaluated by the grid search. Each optimization was then repeated 100 times (each time picking two different random experiments to initialize the optimization with) and the mean and standard error are reported4. Figure 4c also presents a 5 run average of optimization with 3 and 5 times parallelized
GP EI MCMC, but without restricting the new parameter setting to be on the pre-specified grid (see supplementary material for details). A comparison with their "on grid" versions is illustrated.
Clearly integrating over hyperparameters is superior to using a point estimate in this case. While
GP EI MCMC is the most efficient in terms of function evaluations, we see that parallelized GP EI
MCMC finds the best parameters in significantly less time. Finally, in Figure 4c we see that the parallelized GP EI MCMC algorithms find a significantly better minimum value than was found in the grid search used by Hoffman et al. while running a fraction of the number of experiments.
Motif Finding with Structured Support Vector Machines
In this example, we consider optimizing the learning parameters of Max-Margin Min-Entropy(M3E) Models, which include Latent Structured Support Vector Machines as a special case. Latent structured SVMs outperform SVMs on problems where they can explicitly model problem-dependent hidden variables. A popular example task is the binary classification of protein DNA sequences. The hidden variable to be modeled is the unknown location of particular subsequences, or motifs, that are indicators of positive sequences.
Setting the hyperparameters, such as the regularisation term, C, of structured SVMs remains a challenge and these are typically set through a time consuming grid search procedure as is done in. Indeed, Kumar et al. avoided hyperparameter selection for this task as it was too computationally expensive. However, Miller et al. demonstrate that results depend highly on the setting of the parameters, which differ for each protein. M3E models introduce an entropy term, parameterized by ↵, which enables the model to outperform latent structured SVMs. This additional performance, however, comes at the expense of an additional problem-dependent hyperparameter.
We emulate the experiments of Miller et al. for one protein with approximately 40 000 sequences. We explore 25 settings of the parameter C, on a log scale from 10�1 to 106, 14 settings of ↵, on a log scale from 0.1 to 5 and the model convergence tolerance, ✏ 2 {10�4,10�3,10�2,10�1}.
We ran a grid search over the 1400 possible combinations of these parameters, evaluating each over 5 random 50-50 training and test splits.
In Figures 5a and 5b, we compare the randomized grid search to GP EI MCMC, GP EI per Second and their 3x parallelized versions, all constrained to the same points on the grid. Each algorithm was repeated 100 times and the mean and standard error are shown. We observe that the Bayesian optimization strategies are considerably more efficient than grid search which is the status quo. In this case, GP EI MCMC is superior to GP EI per Second in terms of function evaluations but GP
EI per Second finds better parameters faster than GP EI MCMC as it learns to use a less strict
3i.e. the only difference was the randomly sampled collection of articles in the data set and the choice of the vocabulary. We ran each evaluation for 10 hours or until convergence.
4The restriction of the search to the same grid was chosen for efficiency reasons: it allowed us to repeat the experiments several times efficiently, by first computing all function evaluations over the whole grid and reusing these values within each repeated experiment.
Min Function Value
Function evaluations
GP EI MCMC
GP EI Opt
GP EI per Second
GP EI MCMC 3x Parallel
Human Expert
Min function value
Time (Hours)
GP EI MCMC
GP EI Opt
GP EI per Second
GP EI MCMC 3x Parallel
Figure 6: Validation error on the CIFAR-10 data for different optimization strategies. convergence tolerance early on while exploring the other parameters. Indeed, 3x GP EI per second, is the least efficient in terms of function evaluations but finds better parameters faster than all the other algorithms. Figure 5c compares the use of various covariance functions in GP EI MCMC optimization on this problem, again repeating the optimization 100 times. It is clear that the selection of an appropriate covariance significantly affects performance and the estimation of length scale parameters is critical. The assumption of the infinite differentiability as imposed by the commonly used squared exponential is too restrictive for this problem.
Convolutional Networks on CIFAR-10
Neural networks and deep learning methods notoriously require careful tuning of numerous hyperparameters. Multi-layer convolutional neural networks are an example of such a model for which a thorough exploration of architechtures and hyperparameters is beneficial, as demonstrated in Saxe et al., but often computationally prohibitive. While Saxe et al. demonstrate a methodology for efficiently exploring model architechtures, numerous hyperparameters, such as regularisation parameters, remain. In this empirical analysis, we tune nine hyperparameters of a three-layer convolutional network on the CIFAR-10 benchmark dataset using the code provided 5. This model has been carefully tuned by a human expert to achieve a highly competitive result of 18% test error on the unaugmented data, which matches the published state of the art result on CIFAR10. The parameters we explore include the number of epochs to run the model, the learning rate, four weight costs (one for each layer and the softmax output weights), and the width, scale and power of the response normalization on the pooling layers of the network.
We optimize over the nine parameters for each strategy on a withheld validation set and report the mean validation error and standard error over five separate randomly initialized runs. Results are presented in Figure 6 and contrasted with the average results achieved using the best parameters found by the expert. The best hyperparameters found by the GP EI MCMC approach achieve an error on the test set of 14.98%, which is over 3% better than the expert and the state of the art on
CIFAR-10. The same procedure was repeated on the CIFAR-10 data augmented with horizontal reflections and translations, similarly improving on the expert from 11% to 9.5% test error and achieving to our knowledge the lowest error reported on the competitive CIFAR-10 benchmark.
Conclusion
We presented methods for performing Bayesian optimization for hyperparameter selection of general machine learning algorithms. We introduced a fully Bayesian treatment for EI, and algorithms for dealing with variable time regimes and running experiments in parallel. The effectiveness of our approaches were demonstrated on three challenging recently published problems spanning different areas of machine learning. The resulting Bayesian optimization finds better hyperparameters significantly faster than the approaches used by the authors and surpasses a human expert at selecting hyperparameters on the competitive CIFAR-10 dataset, beating the state of the art by over 3%.
Acknowledgements
The authors thank Alex Krizhevsky, Hoffman et al. and Miller et al. for making their code and data available, and George Dahl for valuable feedback. This work was funded by DARPA Young
Faculty Award N66001-12-1-4219, NSERC and an Amazon AWS in Research grant.
5Available at: http://code.google.com/p/cuda-convnet/
References
 Jonas Mockus, Vytautas Tiesis, and Antanas Zilinskas. The application of Bayesian methods for seeking the extremum. Towards Global Optimization, 2:117–129, 1978.
 D.R. Jones. A taxonomy of global optimization methods based on response surfaces. Journal of Global Optimization, 21(4):345–383, 2001.
 Niranjan Srinivas, Andreas Krause, Sham Kakade, and Matthias Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. In Proceedings of the 27th International Conference on Machine Learning, 2010.
 Adam D. Bull. Convergence rates of efficient global optimization algorithms. Journal of Machine Learning Research, (3-4):2879–2904, 2011.
 James S. Bergstra, R´emi Bardenet, Yoshua Bengio, and B´al´azs K´egl. Algorithms for hyperparameter optimization. In Advances in Neural Information Processing Systems 25. 2011.
 Marc C. Kennedy and Anthony O'Hagan. Bayesian calibration of computer models. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 63(3), 2001.
 Frank Hutter, Holger H. Hoos, and Kevin Leyton-Brown. Sequential model-based optimization for general algorithm configuration. In Learning and Intelligent Optimization 5, 2011.
 Nimalan Mahendran, Ziyu Wang, Firas Hamze, and Nando de Freitas. Adaptive mcmc with bayesian optimization. In AISTATS, 2012.
 James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. Journal of Machine Learning Research, 13:281–305, 2012.
 Eric Brochu, Vlad M. Cora, and Nando de Freitas. A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning. pre-print, 2010. arXiv:1012.2599.
 Carl E. Rasmussen and Christopher Williams. Gaussian Processes for Machine Learning. MIT
Press, 2006.
 H. J. Kushner. A new method for locating the maximum point of an arbitrary multipeak curve in the presence of noise. Journal of Basic Engineering, 86, 1964.
 Iain Murray and Ryan P. Adams. Slice sampling covariance hyperparameters of latent Gaussian models. In Advances in Neural Information Processing Systems 24, pages 1723–1731. 2010.
 Yee Whye Teh, Matthias Seeger, and Michael I. Jordan. Semiparametric latent factor models.
In AISTATS, 2005.
 Edwin V. Bonilla, Kian Ming A. Chai, and Christopher K. I. Williams. Multi-task Gaussian process prediction. In Advances in Neural Information Processing Systems 22, 2008.
 David Ginsbourger and Rodolphe Le Riche. Dealing with asynchronicity in parallel Gaussian process based global optimization. http://hal.archives-ouvertes.fr/ hal-00507632, 2010.
 Matthew Hoffman, David M. Blei, and Francis Bach. Online learning for latent Dirichlet allocation. In Advances in Neural Information Processing Systems 24, 2010.
 Kevin Miller, M. Pawan Kumar, Benjamin Packer, Danny Goodman, and Daphne Koller. Maxmargin min-entropy models. In AISTATS, 2012.
 Chun-Nam John Yu and Thorsten Joachims. Learning structural SVMs with latent variables.
In Proceedings of the 26th International Conference on Machine Learning, 2009.
 M. Pawan Kumar, Benjamin Packer, and Daphne Koller. Self-paced learning for latent variable models. In Advances in Neural Information Processing Systems 25. 2010.
 Andrew Saxe, Pang Wei Koh, Zhenghao Chen, Maneesh Bhand, Bipin Suresh, and Andrew Ng.
On random weights and unsupervised feature learning. In Proceedings of the 28th International
Conference on Machine Learning, 2011.
 Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, Department of Computer Science, University of Toronto, 2009.
 Adam Coates and Andrew Y. Ng. Selecting receptive fields in deep networks. In Advances in Neural Information Processing Systems 25. 2011.Algorithms for Hyper-Parameter Optimization
James Bergstra
The Rowland Institute
Harvard University bergstra@rowland.harvard.edu
R´emi Bardenet
Laboratoire de Recherche en Informatique
Universit´e Paris-Sud bardenet@lri.fr
Yoshua Bengio
D´ept. d'Informatique et Recherche Op´erationelle
Universit´e de Montr´eal yoshua.bengio@umontreal.ca
Bal´azs K´egl
Linear Accelerator Laboratory
Universit´e Paris-Sud, CNRS balazs.kegl@gmail.com
Abstract
Several recent advances to the state of the art in image classification benchmarks have come from better configurations of existing techniques rather than novel approaches to feature learning. Traditionally, hyper-parameter optimization has been the job of humans because they can be very efficient in regimes where only a few trials are possible. Presently, computer clusters and GPU processors make it possible to run more trials and we show that algorithmic approaches can find better results. We present hyper-parameter optimization results on tasks of training neural networks and deep belief networks (DBNs). We optimize hyper-parameters using random search and two new greedy sequential methods based on the expected improvement criterion. Random search has been shown to be sufficiently efficient for learning neural networks for several datasets, but we show it is unreliable for training DBNs. The sequential algorithms are applied to the most difficult
DBN learning problems from and find significantly better results than the best previously reported. This work contributes novel techniques for making response surface models P(y|x) in which many elements of hyper-parameter assignment(x) are known to be irrelevant given particular values of other elements.
Introduction
Models such as Deep Belief Networks (DBNs), stacked denoising autoencoders, convolutional networks, as well as classifiers based on sophisticated feature extraction techniques have from ten to perhaps fifty hyper-parameters, depending on how the experimenter chooses to parametrize the model, and how many hyper-parameters the experimenter chooses to fix at a reasonable default. The difficulty of tuning these models makes published results difficult to reproduce and extend, and makes even the original investigation of such methods more of an art than a science.
Recent results such as,, and demonstrate that the challenge of hyper-parameter optimization in large and multilayer models is a direct impediment to scientific progress. These works have advanced state of the art performance on image classification problems by more concerted hyper-parameter optimization in simple algorithms, rather than by innovative modeling or machine learning strategies. It would be wrong to conclude from a result such as that feature learning is useless. Instead, hyper-parameter optimization should be regarded as a formal outer loop in the learning process. A learning algorithm, as a functional from data to classifier (taking classification problems as an example), includes a budgeting choice of how many CPU cycles are to be spent on hyper-parameter exploration, and how many CPU cycles are to be spent evaluating each hyperparameter choice (i.e. by tuning the regular parameters). The results of and suggest that with current generation hardware such as large computer clusters and GPUs, the optimal alloca1 tion of CPU cycles includes more hyper-parameter exploration than has been typical in the machine learning literature.
Hyper-parameter optimization is the problem of optimizing a loss function over a graph-structured configuration space. In this work we restrict ourselves to tree-structured configuration spaces. Configuration spaces are tree-structured in the sense that some leaf variables (e.g. the number of hidden units in the 2nd layer of a DBN) are only well-defined when node variables (e.g. a discrete choice of how many layers to use) take particular values. Not only must a hyper-parameter optimization algorithm optimize over variables which are discrete, ordinal, and continuous, but it must simultaneously choose which variables to optimize.
In this work we define a configuration space by a generative process for drawing valid samples.
Random search is the algorithm of drawing hyper-parameter assignments from that process and evaluating them. Optimization algorithms work by identifying hyper-parameter assignments that could have been drawn, and that appear promising on the basis of the loss function's value at other points. This paper makes two contributions: 1) Random search is competitive with the manual optimization of DBNs in, and 2) Automatic sequential optimization outperforms both manual and random search.
Section 2 covers sequential model-based optimization, and the expected improvement criterion. Section 3 introduces a Gaussian Process based hyper-parameter optimization algorithm. Section 4 introduces a second approach based on adaptive Parzen windows. Section 5 describes the problem of DBN hyper-parameter optimization, and shows the efficiency of random search. Section 6 shows the efficiency of sequential optimization on the two hardest datasets according to random search.
The paper concludes with discussion of results and concluding remarks in Section 7 and Section 8.
Sequential Model-based Global Optimization
Sequential Model-Based Global Optimization (SMBO) algorithms have been used in many applications where evaluation of the fitness function is expensive. In an application where the true fitness function f : X → R is costly to evaluate, model-based algorithms approximate f with a surrogate that is cheaper to evaluate. Typically the inner loop in an SMBO algorithm is the numerical optimization of this surrogate, or some transformation of the surrogate. The point x∗ that maximizes the surrogate (or its transformation) becomes the proposal for where the true function f should be evaluated. This active-learning-like algorithm template is summarized in Figure 1. SMBO algorithms differ in what criterion they optimize to obtain x∗ given a model (or surrogate) of f, and in they model f via observation history H.
SMBO
� f, M0, T, S
�
H ← ∅, For t ← 1 to T, 3 x∗ ← argminx S(x, Mt−1), Evaluate f(x∗), ▷ Expensive step
H ← H ∪ (x∗, f(x∗)), Fit a new model Mt to H.
7 return H
Figure 1: The pseudo-code of generic Sequential Model-Based Optimization.
The algorithms in this work optimize the criterion of Expected Improvement (EI). Other criteria have been suggested, such as Probability of Improvement and Expected Improvement, minimizing the Conditional Entropy of the Minimizer, and the bandit-based criterion described in. We chose to use the EI criterion in our work because it is intuitive, and has been shown to work well in a variety of settings. We leave the systematic exploration of improvement criteria for future work. Expected improvement is the expectation under some model M of f : X → RN that f(x) will exceed (negatively) some threshold y∗:
EIy∗(x) :=
� ∞
−∞ max(y∗ − y, 0)pM(y|x)dy.
The contribution of this work is two novel strategies for approximating f by modeling H: a hierarchical Gaussian Process and a tree-structured Parzen estimator. These are described in Section 3 and Section 4 respectively.
The Gaussian Process Approach (GP)
Gaussian Processes have long been recognized as a good method for modeling loss functions in model-based optimization literature. Gaussian Processes (GPs, ) are priors over functions that are closed under sampling, which means that if the prior distribution of f is believed to be a GP with mean 0 and kernel k, the conditional distribution of f knowing a sample H = (xi, f(xi))n i=1 of its values is also a GP, whose mean and covariance function are analytically derivable. GPs with generic mean functions can in principle be used, but it is simpler and sufficient for our purposes to only consider zero mean processes. We do this by centering the function values in the considered data sets. Modelling e.g. linear trends in the GP mean leads to undesirable extrapolation in unexplored regions during SMBO.
The above mentioned closedness property, along with the fact that GPs provide an assessment of prediction uncertainty incorporating the effect of data scarcity, make the GP an elegant candidate for both finding candidate x∗ (Figure 1, step 3) and fitting a model Mt (Figure 1, step 6). The runtime of each iteration of the GP approach scales cubically in |H| and linearly in the number of variables being optimized, however the expense of the function evaluations f(x∗) typically dominate even this cubic cost.
Optimizing EI in the GP
We model f with a GP and set y∗ to the best value found after observing H: y∗ = min{f(xi), 1 ≤ i ≤ n}. The model pM in (1) is then the posterior GP knowing H. The EI function in (1) encapsulates a compromise between regions where the mean function is close to or better than y∗ and under-explored regions where the uncertainty is high.
EI functions are usually optimized with an exhaustive grid search over the input space, or a Latin
Hypercube search in higher dimensions. However, some information on the landscape of the EI criterion can be derived from simple computations : 1) it is always non-negative and zero at training points from D, 2) it inherits the smoothness of the kernel k, which is in practice often at least once differentiable, and noticeably, 3) the EI criterion is likely to be highly multi-modal, especially as the number of training points increases. The authors of used the preceding remarks on the landscape of EI to design an evolutionary algorithm with mixture search, specifically aimed at optimizing EI, that is shown to outperform exhaustive search for a given budget in EI evaluations. We borrow here their approach and go one step further. We keep the Estimation of Distribution (EDA, ) approach on the discrete part of our input space (categorical and discrete hyper-parameters), where we sample candidate points according to binomial distributions, while we use the Covariance
Matrix Adaptation - Evolution Strategy (CMA-ES, ) for the remaining part of our input space(continuous hyper-parameters). CMA-ES is a state-of-the-art gradient-free evolutionary algorithm for optimization on continuous domains, which has been shown to outperform the Gaussian search
EDA. Notice that such a gradient-free approach allows non-differentiable kernels for the GP regression. We do not take on the use of mixtures in, but rather restart the local searches several times, starting from promising places. The use of tesselations suggested by is prohibitive here, as our task often means working in more than 10 dimensions, thus we start each local search at the center of mass of a simplex with vertices randomly picked among the training points.
Finally, we remark that all hyper-parameters are not relevant for each point. For example, a DBN with only one hidden layer does not have parameters associated to a second or third layer. Thus it is not enough to place one GP over the entire space of hyper-parameters. We chose to group the hyper-parameters by common use in a tree-like fashion and place different independent GPs over each group. As an example, for DBNs, this means placing one GP over common hyper-parameters, including categorical parameters that indicate what are the conditional groups to consider, three
GPs on the parameters corresponding to each of the three layers, and a few 1-dimensional GPs over individual conditional hyper-parameters, like ZCA energy (see Table 1 for DBN parameters).
Tree-structured Parzen Estimator Approach (TPE)
Anticipating that our hyper-parameter optimization tasks will mean high dimensions and small fitness evaluation budgets, we now turn to another modeling strategy and EI optimization scheme for the SMBO algorithm. Whereas the Gaussian-process based approach modeled p(y|x) directly, this strategy models p(x|y) and p(y).
Recall from the introduction that the configuration space X is described by a graph-structured generative process (e.g. first choose a number of DBN layers, then choose the parameters for each).
The tree-structured Parzen estimator (TPE) models p(x|y) by transforming that generative process, replacing the distributions of the configuration prior with non-parametric densities. In the experimental section, we will see that the configuation space is described using uniform, log-uniform, quantized log-uniform, and categorical variables. In these cases, the TPE algorithm makes the following replacements: uniform → truncated Gaussian mixture, log-uniform → exponentiated truncated Gaussian mixture, categorical → re-weighted categorical. Using different observations
{x(1),..., x(k)} in the non-parametric densities, these substitutions represent a learning algorithm that can produce a variety of densities over the configuration space X. The TPE defines p(x|y) using two such densities: p(x|y) =
�ℓ(x) if y < y∗ g(x) if y ≥ y∗, (2) where ℓ(x) is the density formed by using the observations {x(i)} such that corresponding loss f(x(i)) was less than y∗ and g(x) is the density formed by using the remaining observations.
Whereas the GP-based approach favoured quite an aggressive y∗ (typically less than the best observed loss), the TPE algorithm depends on a y∗ that is larger than the best observed f(x) so that some points can be used to form ℓ(x). The TPE algorithm chooses y∗ to be some quantile γ of the observed y values, so that p(y < y∗) = γ, but no specific model for p(y) is necessary. By maintaining sorted lists of observed variables in H, the runtime of each iteration of the TPE algorithm can scale linearly in |H| and linearly in the number of variables (dimensions) being optimized.
Optimizing EI in the TPE algorithm
The parametrization of p(x, y) as p(y)p(x|y) in the TPE algorithm was chosen to facilitate the optimization of EI.
EIy∗(x) =
� y∗
−∞(y∗ − y)p(y|x)dy =
� y∗
−∞(y∗ − y)p(x|y)p(y) p(x) dy
By construction, γ = p(y < y∗) and p(x) =
�
R p(x|y)p(y)dy = γℓ(x) + (1 − γ)g(x). Therefore
� y∗
−∞(y∗ − y)p(x|y)p(y)dy
= ℓ(x)
� y∗
−∞(y∗ − y)p(y)dy = γy∗ℓ(x) − ℓ(x)
� y∗
−∞ p(y)dy, so that finally EIy∗(x) = γy∗ℓ(x)−ℓ(x)
� y∗
−∞ p(y)dy γℓ(x)+(1−γ)g(x)
∝
� γ + g(x) ℓ(x) (1 − γ)
�−1. This last expression shows that to maximize improvement we would like points x with high probability under ℓ(x) and low probability under g(x). The tree-structured form of ℓ and g makes it easy to draw many candidates according to ℓ and evaluate them according to g(x)/ℓ(x). On each iteration, the algorithm returns the candidate x∗ with the greatest EI.
Details of the Parzen Estimator
The models ℓ(x) and g(x) are hierarchical processes involving discrete-valued and continuousvalued variables. The Adaptive Parzen Estimator yields a model over X by placing density in the vicinity of K observations B = {x(1),..., x(K)} ⊂ H. Each continuous hyper-parameter was specified by a uniform prior over some interval (a, b), or a Gaussian, or a log-uniform distribution.
The TPE substitutes an equally-weighted mixture of that prior with Gaussians centered at each of the x(i) ∈ B. The standard deviation of each Gaussian was set to the greater of the distances to the left and right neighbor, but clipped to remain in a reasonable range. In the case of the uniform, the points a and b were considered to be potential neighbors. For discrete variables, supposing the prior was a vector of N probabilities pi, the posterior vector elements were proportional to Npi + Ci where Ci counts the occurrences of choice i in B. The log-uniform hyper-parameters were treated as uniforms in the log domain.
Table 1: Distribution over DBN hyper-parameters for random sampling. Options separated by "or" such as pre-processing (and including the random seed) are weighted equally. Symbol U means uniform, N means Gaussian-distributed, and log U means uniformly distributed in the log-domain.
CD (also known as CD-1) stands for contrastive divergence, the algorithm used to initialize the layer parameters of the DBN.
Whole model
Per-layer
Parameter
Prior
Parameter
Prior pre-processing raw or ZCA n. hidden units log U(128, 4096)
ZCA energy
U(.5, 1)
W init
U(−a, a) or N(0, a2) random seed
5 choices a algo A or B (see text) classifier learn rate log U(0.001, 10) algo A coef
U(.2, 2) classifier anneal start log U(100, 104)
CD epochs log U(1, 104) classifier ℓ2-penalty
0 or log U(10−7, 10−4)
CD learn rate log U(10−4, 1) n. layers
1 to 3
CD anneal start log U(10, 104) batch size
20 or 100
CD sample data yes or no
Random Search for Hyper-Parameter Optimization in DBNs
One simple, but recent step toward formalizing hyper-parameter optimization is the use of random search. showed that random search was much more efficient than grid search for optimizing the parameters of one-layer neural network classifiers. In this section, we evaluate random search for DBN optimization, compared with the sequential grid-assisted manual search carried out in.
We chose the prior listed in Table 1 to define the search space over DBN configurations. The details of the datasets, the DBN model, and the greedy layer-wise training procedure based on CD are provided in. This prior corresponds to the search space of except for the following differences:(a) we allowed for ZCA pre-processing, (b) we allowed for each layer to have a different size, (c) we allowed for each layer to have its own training parameters for CD, (d) we allowed for the possibility of treating the continuous-valued data as either as Bernoulli means (more theoretically correct) or Bernoulli samples (more typical) in the CD algorithm, and (e) we did not discretize the possible values of real-valued hyper-parameters. These changes expand the hyper-parameter search problem, while maintaining the original hyper-parameter search space as a subset of the expanded search space.
The results of this preliminary random search are in Figure 2. Perhaps surprisingly, the result of manual search can be reliably matched with 32 random trials for several datasets. The efficiency of random search in this setting is explored further in. Where random search results match human performance, it is not clear from Figure 2 whether the reason is that it searched the original space as efficiently, or that it searched a larger space where good performance is easier to find. But the objection that random search is somehow cheating by searching a larger space is backward – the search space outlined in Table 1 is a natural description of the hyper-parameter optimization problem, and the restrictions to that space by were presumably made to simplify the search problem and make it tractable for grid-search assisted manual search. Critically, both methods train
DBNs on the same datasets.
The results in Figure 2 indicate that hyper-parameter optimization is harder for some datasets. For example, in the case of the "MNIST rotated background images" dataset (MRBI), random sampling appears to converge to a maximum relatively quickly (best models among experiments of 32 trials show little variance in performance), but this plateau is lower than what was found by manual search.
In another dataset (convex), the random sampling procedure exceeds the performance of manual search, but is slow to converge to any sort of plateau. There is considerable variance in generalization when the best of 32 models is selected. This slow convergence indicates that better performance is probably available, but we need to search the configuration space more efficiently to find it. The remainder of this paper explores sequential optimization strategies for hyper-parameter optimization for these two datasets: convex and MRBI.
Sequential Search for Hyper-Parameter Optimization in DBNs
We validated our GP approach of Section 3.1 by comparing with random sampling on the Boston
Housing dataset, a regression task with 506 points made of 13 scaled input variables and a scalar
128 experiment size (# trials)
1.0 accuracy mnist basic
128 experiment size (# trials)
0.9 accuracy mnist background images
128 experiment size (# trials)
0.6 accuracy mnist rotated background images
128 experiment size (# trials)
0.85 accuracy convex
128 experiment size (# trials)
1.0 accuracy rectangles
128 experiment size (# trials)
0.80 accuracy rectangles images
Figure 2: Deep Belief Network (DBN) performance according to random search. Random search is used to explore up to 32 hyper-parameters (see Table 1). Results found using a grid-search-assisted manual search over a similar domain with an average 41 trials are given in green (1-layer DBN) and red (3-layer DBN). Each box-plot (for N = 1, 2, 4,...) shows the distribution of test set performance when the best model among N random trials is selected. The datasets "convex" and "mnist rotated background images" are used for more thorough hyper-parameter optimization. regressed output. We trained a Multi-Layer Perceptron (MLP) with 10 hyper-parameters, including learning rate, ℓ1 and ℓ2 penalties, size of hidden layer, number of iterations, whether a PCA preprocessing was to be applied, whose energy was the only conditional hyper-parameter. Our results are depicted in Figure 3. The first 30 iterations were made using random sampling, while from the 30th on, we differentiated the random samples from the GP approach trained on the updated history. The experiment was repeated 20 times. Although the number of points is particularly small compared to the dimensionality, the surrogate modelling approach finds noticeably better points than random, which supports the application of SMBO approaches to more ambitious tasks and datasets.
Applying the GP to the problem of optimizing DBN performance, we allowed 3 random restarts to the CMA+ES algorithm per proposal x∗, and up to 500 iterations of conjugate gradient method in fitting the length scales of the GP. The squared exponential kernel was used for every node.
The CMA-ES part of GPs dealt with boundaries using a penalty method, the binomial sampling part dealt with it by nature. The GP algorithm was initialized with 30 randomly sampled points in H.
After 200 trials, the prediction of a point x∗ using this GP took around 150 seconds.
For the TPE-based algorithm, we chose γ = 0.15 and picked the best among 100 candidates drawn from ℓ(x) on each iteration as the proposal x∗. After 200 trials, the prediction of a point x∗ using this TPE algorithm took around 10 seconds. TPE was allowed to grow past the initial bounds used with for random sampling in the course of optimization, whereas the GP and random search were restricted to stay within the initial bounds throughout the course of optimization. The TPE algorithm was also initialized with the same 30 randomly sampled points as were used to seed the GP.
Parallelizing Sequential Search
Both the GP and TPE approaches were actually run asynchronously in order to make use of multiple compute nodes and to avoid wasting time waiting for trial evaluations to complete. For the GP approach, the so-called constant liar approach was used: each time a candidate point x∗ was proposed, a fake fitness evaluation equal to the mean of the y's within the training set D was assigned temporarily, until the evaluation completed and reported the actual loss f(x∗). For the TPE approach, we simply ignored recently proposed points and relied on the stochasticity of draws from ℓ(x) to provide different candidates from one iteration to the next. The consequence of parallelization is that each proposal x∗ is based on less feedback. This makes search less efficient, though faster in terms of wall time.
Time
Best value so far
Figure 3: After time 30, GP optimizing the MLP hyper-parameters on the Boston
Housing regression task.
Best minimum found so far every 5 iterations, against time. Red = GP, Blue = Random. Shaded areas = one-sigma error bars. convex
MRBI
TPE
14.13 ±0.30 %
44.55 ±0.44%
GP
16.70 ± 0.32%
47.08 ± 0.44%
Manual
18.63 ± 0.34%
47.39 ± 0.44%
Random
18.97 ± 0.34 %
50.52 ± 0.44%
Table 2: The test set classification error of the best model found by each search algorithm on each problem. Each search algorithm was allowed up to 200 trials. The manual searches used 82 trials for convex and 27 trials MRBI.
Runtime per trial was limited to 1 hour of GPU computation regardless of whether execution was on a GTX 285, 470, 480, or 580. The difference in speed between the slowest and fastest machine was roughly two-fold in theory, but the actual efficiency of computation depended also on the load of the machine and the configuration of the problem (the relative speed of the different cards is different in different hyper-parameter configurations). With the parallel evaluation of up to five proposals from the GP and TPE algorithms, each experiment took about 24 hours of wall time using five GPUs.
Discussion
The trajectories (H) constructed by each algorithm up to 200 steps are illustrated in Figure 4, and compared with random search and the manual search carried out in. The generalization scores of the best models found using these algorithms and others are listed in Table 2. On the convex dataset (2-way classification), both algorithms converged to a validation score of 13% error. In generalization, TPE's best model had 14.1% error and GP's best had 16.7%. TPE's best was significantly better than both manual search (19%) and random search with 200 trials (17%). On the MRBI dataset (10-way classification), random search was the worst performer (50% error), the GP approach and manual search approximately tied (47% error), while the TPE algorithm found a new best result (44% error). The models found by the TPE algorithm in particular are better than previously found ones on both datasets. The GP and TPE algorithms were slightly less efficient than manual search: GP and EI identified performance on par with manual search within 80 trials, the manual search of used 82 trials for convex and 27 trials for MRBI.
There are several possible reasons for why the TPE approach outperformed the GP approach in these two datasets. Perhaps the inverse factorization of p(x|y) is more accurate than the p(y|x) in the Gaussian process. Perhaps, conversely, the exploration induced by the TPE's lack of accuracy turned out to be a good heuristic for search. Perhaps the hyper-parameters of the GP approach itself were not set to correctly trade off exploitation and exploration in the DBN configuration space. More empirical work is required to test these hypotheses. Critically though, all four SMBO runs matched or exceeded both random search and a careful human-guided search, which are currently the state of the art methods for hyper-parameter optimization.
The GP and TPE algorithms work well in both of these settings, but there are certainly settings in which these algorithms, and in fact SMBO algorithm in general, would not be expected to do well. Sequential optimization algorithms work by leveraging structure in observed (x, y) pairs. It is possible for SMBO to be arbitrarily bad with a bad choice of p(y|x). It is also possible to be slower than random sampling at finding a global optimum with a apparently good p(y|x), if it extracts structure in H that leads only to a local optimum.
Conclusion
This paper has introduced two sequential hyper-parameter optimization algorithms, and shown them to meet or exceed human performance and the performance of a brute-force random search in two difficult hyper-parameter optimization tasks involving DBNs. We have relaxed standard constraints(e.g. equal layer sizes at all layers) on the search space, and fall back on a more natural hyperparameter space of 32 variables (including both discrete and continuous variables) in which many
200 time(trials)
0.50 error (fraction incorrect)
Dataset: convex manual
99.5'th q.
GP
TPE
200 time(trials)
0.9 error (fraction incorrect)
Dataset: mnist rotated background images manual
99.5'th q.
GP
TPE
Figure 4: Efficiency of Gaussian Process-based (GP) and graphical model-based (TPE) sequential optimization algorithms on the task of optimizing the validation set performance of a DBN of up to three layers on the convex task (left) and the MRBI task (right). The dots are the elements of the trajectory H produced by each SMBO algorithm. The solid coloured lines are the validation set accuracy of the best trial found before each point in time. Both the TPE and GP algorithms make significant advances from their random initial conditions, and substantially outperform the manual and random search methods. A
95% confidence interval about the best validation means on the convex task extends 0.018 above and below each point, and on the MRBI task extends 0.021 above and below each point. The solid black line is the test set accuracy obtained by domain experts using a combination of grid search and manual search. The dashed line is the 99.5% quantile of validation performance found among trials sampled from our prior distribution (see
Table 1), estimated from 457 and 361 random trials on the two datasets respectively. variables are sometimes irrelevant, depending on the value of other parameters (e.g. the number of layers). In this 32-dimensional search problem, the TPE algorithm presented here has uncovered new best results on both of these datasets that are significantly better than what DBNs were previously believed to achieve. Moreover, the GP and TPE algorithms are practical: the optimization for each dataset was done in just 24 hours using five GPU processors. Although our results are only for
DBNs, our methods are quite general, and extend naturally to any hyper-parameter optimization problem in which the hyper-parameters are drawn from a measurable set.
We hope that our work may spur researchers in the machine learning community to treat the hyperparameter optimization strategy as an interesting and important component of all learning algorithms. The question of "How well does a DBN do on the convex task?" is not a fully specified, empirically answerable question – different approaches to hyper-parameter optimization will give different answers. Algorithmic approaches to hyper-parameter optimization make machine learning results easier to disseminate, reproduce, and transfer to other domains. The specific algorithms we have presented here are also capable, at least in some cases, of finding better results than were previously known. Finally, powerful hyper-parameter optimization algorithms broaden the horizon of models that can realistically be studied; researchers need not restrict themselves to systems of a few variables that can readily be tuned by hand.
The TPE algorithm presented in this work, as well as parallel evaluation infrastructure, is available as BSD-licensed free open-source software, which has been designed not only to reproduce the results in this work, but also to facilitate the application of these and similar algorithms to other hyper-parameter optimization problems.1
Acknowledgements
This work was supported by the National Science and Engineering Research Council of Canada, Compute Canada, and by the ANR-2010-COSI-002 grant of the French National Research Agency.
GPU implementations of the DBN model were provided by Theano.
1"Hyperopt" software package: https://github.com/jaberg/hyperopt
References
 H. Larochelle, D. Erhan, A. Courville, J. Bergstra, and Y. Bengio. An empirical evaluation of deep architectures on problems with many factors of variation. In ICML 2007, pages 473–480, 2007.
 G. E. Hinton, S. Osindero, and Y. Teh. A fast learning algorithm for deep belief nets. Neural Computation, 18:1527–1554, 2006.
 P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P. A. Manzagol. Stacked denoising autoencoders:
Learning useful representations in a deep network with a local denoising criterion. Machine Learning
Research, 11:3371–3408, 2010.
 Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition.
Proceedings of the IEEE, 86(11):2278–2324, November 1998.
 Nicolas Pinto, David Doukhan, James J. DiCarlo, and David D. Cox. A high-throughput screening approach to discovering good forms of biologically inspired visual representation.
PLoS Comput Biol, 5(11):e1000579, 11 2009.
 A. Coates, H. Lee, and A. Ng. An analysis of single-layer networks in unsupervised feature learning.
NIPS Deep Learning and Unsupervised Feature Learning Workshop, 2010.
 A. Coates and A. Y. Ng. The importance of encoding versus training with sparse coding and vector quantization. In Proceedings of the Twenty-eighth International Conference on Machine Learning (ICML F. Hutter. Automated Configuration of Algorithms for Solving Hard Computational Problems. PhD thesis, University of British Columbia, 2009.
 F. Hutter, H. Hoos, and K. Leyton-Brown. Sequential model-based optimization for general algorithm configuration. In LION-5, 2011. Extended version as UBC Tech report TR-2010-10.
 D.R. Jones. A taxonomy of global optimization methods based on response surfaces. Journal of Global
Optimization, 21:345–383, 2001.
 J. Villemonteix, E. Vazquez, and E. Walter. An informational approach to the global optimization of expensive-to-evaluate functions. Journal of Global Optimization, 2006.
 N. Srinivas, A. Krause, S. Kakade, and M. Seeger. Gaussian process optimization in the bandit setting:
No regret and experimental design. In ICML, 2010.
 J. Mockus, V. Tiesis, and A. Zilinskas. The application of Bayesian methods for seeking the extremum.
In L.C.W. Dixon and G.P. Szego, editors, Towards Global Optimization, volume 2, pages 117–129. North
Holland, New York, 1978.
 C.E. Rasmussen and C. Williams. Gaussian Processes for Machine Learning.
 D. Ginsbourger, D. Dupuy, A. Badea, L. Carraro, and O. Roustant. A note on the choice and the estimation of kriging models for the analysis of deterministic computer experiments. 25:115–131, 2009.
 R. Bardenet and B. K´egl. Surrogating the surrogate: accelerating Gaussian Process optimization with mixtures. In ICML, 2010.
 P. Larra˜naga and J. Lozano, editors. Estimation of Distribution Algorithms: A New Tool for Evolutionary
Computation. Springer, 2001.
 N. Hansen. The CMA evolution strategy: a comparing review. In J.A. Lozano, P. Larranaga, I. Inza, and E. Bengoetxea, editors, Towards a new evolutionary computation. Advances on estimation of distribution algorithms, pages 75–102. Springer, 2006.
 J. Bergstra and Y. Bengio. Random search for hyper-parameter optimization. The Learning Workshop(Snowbird), 2011.
 A. Hyv¨arinen and E. Oja. Independent component analysis: Algorithms and applications. Neural Networks, 13(4–5):411–430, 2000.
 J. Bergstra and Y. Bengio. Random search for hyper-parameter optimization. JMLR, 2012. Accepted.
 C. Bishop. Neural networks for pattern recognition. 1995.
 J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu, G. Desjardins, J. Turian, and Y. Bengio.
Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientific Computing Conference (SciPy), June 2010.On Optimization Methods for Deep Learning
Quoc V. Le quocle@cs.stanford.edu
Jiquan Ngiam jngiam@cs.stanford.edu
Adam Coates acoates@cs.stanford.edu
Abhik Lahiri alahiri@cs.stanford.edu
Bobby Prochnow prochnow@cs.stanford.edu
Andrew Y. Ng ang@cs.stanford.edu
Computer Science Department, Stanford University, Stanford, CA 94305, USA
Abstract
The predominant methodology in training deep learning advocates the use of stochastic gradient descent methods (SGDs).
Despite its ease of implementation, SGDs are difficult to tune and parallelize. These problems make it challenging to develop, debug and scale up deep learning algorithms with SGDs.
In this paper, we show that more sophisticated off-the-shelf optimization methods such as Limited memory BFGS (L-BFGS) and Conjugate gradient (CG) with line search can significantly simplify and speed up the process of pretraining deep algorithms.
In our experiments, the difference between LBFGS/CG and SGDs are more pronounced if we consider algorithmic extensions (e.g., sparsity regularization) and hardware extensions (e.g., GPUs or computer clusters).
Our experiments with distributed optimization support the use of L-BFGS with locally connected networks and convolutional neural networks. Using L-BFGS, our convolutional network model achieves 0.69% on the standard MNIST dataset. This is a state-of-theart result on MNIST among algorithms that do not use distortions or pretraining.
1. Introduction
Stochastic
Gradient
Descent methods(SGDs) have been extensively employed in machine learning(Bottou, LeCun et al., Shalev-Shwartz et al., Bottou & Bousquet, Appearing in Proceedings of the 28 th International Conference on Machine Learning, Bellevue, WA, USA, 2011.
Copyright 2011 by the author(s)/owner(s).
2008; Zinkevich et al., 2010). A strength of SGDs is that they are simple to implement and also fast for problems that have many training examples.
However, SGD methods have many disadvantages.
One key disadvantage of SGDs is that they require much manual tuning of optimization parameters such as learning rates and convergence criteria. If one does not know the task at hand well, it is very difficult to find a good learning rate or a good convergence criterion. A standard strategy in this case is to run the learning algorithm with many optimization parameters and pick the model that gives the best performance on a validation set. Since one needs to search over the large space of possible optimization parameters, this makes SGDs difficult to train in settings where running the optimization procedure many times is computationally expensive. The second weakness of SGDs is that they are inherently sequential: it is very difficult to parallelize them using GPUs or distribute them using computer clusters.
Batch methods, such as Limited memory BFGS (LBFGS) or Conjugate Gradient (CG), with the presence of a line search procedure, are usually much more stable to train and easier to check for convergence. These methods also enjoy parallelism by computing the gradient on GPUs (Raina et al., 2009) and/or distributing that computation across machines (Chu et al., 2007). These methods, conventionally considered to be slow, can be fast thanks to the availability of large amounts of RAMs, multicore CPUs, GPUs and computer clusters with fast network hardware.
On a single machine, the speed benefits of L-BFGS come from using the approximated second-order information (modelling the interactions between variables). On the other hand, for CG, the benefits come from using conjugacy information during optimization.
Thanks to these bookkeeping steps, L-BFGS and CG
On Optimization Methods for Deep Learning can be faster and more stable than SGDs.
A weakness of batch L-BFGS and CG, which require the computation of the gradient on the entire dataset to make an update, is that they do not scale gracefully with the number of examples.
We found that minibatch training, which requires the computation of the gradient on a small subset of the dataset, addresses this weakness well. We found that minibatch
LBFGS/CG are fast when the dataset is large.
Our experimental results reflect the different strengths and weaknesses of the different optimization methods. Among the problems we considered, L-BFGS is highly competitive or sometimes superior to SGDs/CG for low dimensional problems, especially convolutional models. For high dimensional problems, CG is more competitive and usually outperforms L-BFGS and SGDs. Additionally, using a large minibatch and line search with SGDs can improve performance.
More significant speed improvements of L-BFGS and CG over SGDs are observed in our experiments with sparse autoencoders. This is because having a larger minibatch makes the optimization problem easier for sparse autoencoders: in this case, the cost of estimating the second-order and conjugate information is small compared to the cost of computing the gradient.
Furthermore, when training autoencoders, L-BFGS and CG can be both sped up significantly (2x) by simply performing the computations on GPUs. Conversely, only small speed improvements were observed when SGDs are used with GPUs on the same problem.
We also present results showing that Map-Reduce style optimization works well for L-BFGS when the model utilizes locally connected networks (Le et al., 2010) or convolutional neural networks (LeCun et al., 1998).
Our experimental results show that the speed improvements are close to linear in the number of machines when locally connected networks and convolutional networks are used (up to 8 machines considered in the experiments).
We applied our findings to train a convolutional network model (similar to Ranzato et al. (2007)) with LBFGS on a GPU cluster and obtained 0.69% test set error.
This is the state-of-the-art result on MNIST among algorithms that do not use pretraining or distortions.
Batch optimization is also behind the success of feature learning algorithms that achieve state-of-the-art performance on a variety of object recognition problems (Le et al., 2010; Coates et al., 2011) and action recognition problems (Le et al., 2011).
2. Related work
Optimization research has a long history.
Examples of successful unconstrained optimization methods include Newton-Raphson's method, BFGS methods, Conjugate Gradient methods and Stochastic Gradient
Descent methods. These methods are usually associated with a line search method to ensure that the algorithms consistently improve the objective function.
When it comes to large scale machine learning, the favorite optimization method is usually SGDs.
Recent work on SGDs focuses on adaptive strategies for the learning rate (Shalev-Shwartz et al., 2007;
Bartlett et al., 2008; Do et al., 2009) or improving
SGD convergence by approximating second-order information (Vishwanathan et al., 2007; Bordes et al., 2010). In practice, plain SGDs with constant learning rates or learning rates of the form α β+t are still popular thanks to their ease of implementation.
These simple methods are even more common in deep learning (Hinton, 2010) because the optimization problems are nonconvex and the convergence properties of complex methods (Shalev-Shwartz et al., 2007;
Bartlett et al., 2008; Do et al., 2009) no longer hold.
Recent proposals for training deep networks argue for the use of layerwise pretraining (Hinton et al., 2006;
Bengio et al., 2007; Bengio, 2009). Optimization techniques for training these models include Contrastive
Divergence (Hinton et al., 2006), Conjugate Gradient (Hinton & Salakhutdinov, 2006), stochastic diagonal Levenberg-Marquardt (LeCun et al., 1998) and Hessian-free optimization (Martens, 2010). Convolutional neural networks (LeCun et al., 1998) have traditionally employed SGDs with the stochastic diagonal
Levenberg-Marquardt, which uses a diagonal approximation to the Hessian (LeCun et al., 1998).
In this paper, it is our goal to empirically study the pros and cons of off-the-shelf optimization algorithms in the context of unsupervised feature learning and deep learning. In that direction, we focus on comparing L-BFGS, CG and SGDs.
Parallel optimization methods have recently attracted attention as a way to scale up machine learning algorithms.
Map-Reduce (Dean & Ghemawat, 2008) style optimization methods (Chu et al., 2007;
Teo et al., 2007) have been successful early approaches.
We also note recent studies (Mann et al., 2009; Zinkevich et al., 2010) that have parallelized
SGDs without using the Map-Reduce framework.
In our experiments, we found that if we use tiled (locally connected) networks (Le et al., 2010) (which includes convolutional architectures (LeCun et al., 1998;
On Optimization Methods for Deep Learning
Lee et al., 2009a)), Map-Reduce style parallelism is still an effective mechanism for scaling up.
In such cases, the cost of communicating the parameters across the network is small relative to the cost of computing the objective function value and gradient.
3. Deep learning algorithms
3.1. Restricted Boltzmann Machines
In RBMs (Smolensky, 1986; Hinton et al., 2006), the gradient used in training is an approximation formed by a taking small number of Gibbs sampling steps(Contrastive Divergence). Given the biased nature of the gradient and intractability of the objective function, it is difficult to use any optimization methods other than plain SGDs. For this reason we will not consider RBMs in our experiments.
3.2. Autoencoders and denoising autoencoders
Given an unlabelled dataset {x(i)}m i=1, an autoencoder is a two-layer network that learns nonlinear codes to represent (or "reconstruct") the data.
Specifically, we want to learn representations h(x(i); W, b) = σ(Wx(i) + b) such that σ(W T h(x(i); W, b) + c) is approximately x(i), minimize
W,b,c m
X i=1
'''σ
`
W T σ(Wx(i) + b) + c
´
− x(i)'''
Here, we use the L2 norm to penalize the difference between the reconstruction and the input.
In other studies, when x is binary, the cross entropy cost can also be used (Bengio et al., 2007). Typically, we set the activation function σ to be the sigmoid or hyperbolic tangent function.
Unlike RBMs, the gradient of the autoencoder objective can be computed exactly and this gives rise to an opportunity to use more advanced optimization methods, such as L-BFGS and CG, to train the networks.
Denoising autoencoders (Vincent et al., 2008) are also algorithms that can be trained by L-BFGS/CG.
3.3. Sparse RBMs and Autoencoders
Sparsity regularization typically leads to more interpretable features that perform well for classification.
Sparse coding was first proposed by (Olshausen & Field, 1996) as a model of simple cells in the visual cortex.
Lee et al. (2007); Raina et al.(2007) applied sparse coding to learn features for machine learning applications.
Lee et al. (2008) combined sparsity and RBMs to learn representations that mimic certain properties of the area V2 in the visual cortex.
The key idea in their approach is to penalize the deviation between the expected value of the hidden representations E
� hj(x; W, b)
� and a preferred target activation ρ. By setting ρ to be close to zero, the hidden unit will be sparsely activated.
Sparse representations have been employed successfully in many applications such as object recognition (Ranzato et al., 2007; Lee et al., 2009a;
Nair & Hinton, Yang et al., 2009), speech recognition (Lee et al., 2009b) and activity recognition (Taylor et al., 2010; Le et al., 2011).
Training sparse RBMs is usually difficult.
This is due to the stochastic nature of RBMs.
Specifically, in stochastic mode, the estimate of the expectation
E
� hj(x; W, b)
� is very noisy.
A common practice to train sparse RBMs is to use a running estimate of E
� hj(x; W, b)
� and penalizing only the bias (Lee et al., 2008; Larochelle & Bengio, 2008). This further complicates the optimization procedure and makes it hard to debug the learning algorithm. Moreover, it is important to tune the learning rates correctly for the different parameters W, b and c. Consequently, it can be difficult to train sparse RBMs.
In our experience, it is often faster and simpler to obtain sparse representations via autoencoders with the proposed sparsity penalties, especially when batch or large minibatch optimization methods are used.
In detail, we consider sparse autoencoders with a target activation of ρ and penalize it using the KL divergence (Hinton, 2010): n
X j=1
DKL
" ρ
''' 1 m m
X i=1 hj(x(i); W, b)
", (2) where m is the number of examples and n is the number of hidden units.
To train sparse autoencoders, we need to estimate the expected activation value for each hidden unit. However, we will not be able to compute this statistic unless we run the optimization method in batch mode. In practice, if we have a small dataset, it is better to use a batch method to train a sparse autoencoder because we do not have to tweak optimization parameters, such as minibatch size, λ as described below.
Using a minibatch of size m′
<< m, it is typical to keep a running estimate τ of the expectation
E
� h(x; W, b)
�. In this case, the KL penalty is n
X j=1
DKL
" ρ
'''λ 1 m′ m′
X i=1 hj(x(i); W, b) + (1 − λ)τj
"
On Optimization Methods for Deep Learning where λ is another tunable parameter.
3.4. Tiled and locally connected networks
RBMs and autoencoders have densely-connected network architectures which do not scale well to large images. For large images, the most common approach is to use convolutional neural networks (LeCun et al., 1998; Lee et al., 2009a).
Convolutional neural networks have local receptive field architectures: each hidden unit can only connect to a small region of the image. Translational invariance is usually hardwired by weight tying.
Recent approaches try to relax this constraint (Le et al., 2010) in their tiled convolutional architectures to also learn other invariances (Goodfellow et al., 2010).
Our experimental results show that local architectures, such as tiled convolutional or convolutional architectures, can be efficiently trained with a computer cluster using the Map-Reduce framework. With local architectures, the cost of communicating the gradient over the network is often smaller than the cost of computing it (e.g., cases considered in the experiments).
4. Experiments
4.1. Datasets and computers
Our experiments were carried out on the standard
MNIST dataset. We used up to 8 machines for our experiments; each machine has 4 Intel CPU cores (at
2.67 GHz) and a GeForce GTX 285 GPU. Most experiments below are done on a single machine unless indicated with "parallel."
We performed our experiments using Matlab and its
GPU-plugin Jacket.1
For parallel experiments, we used our custom toolbox that makes remote procedure calls in Matlab and Java.
In the experiments below, we report the standard metric in machine learning: the objective function evaluated on test data (i.e., test error) against time. We note that the objective function evaluated on the training shows similar trends.
4.2. Optimization methods
We are interested in off-the-shelf SGDs, L-BFGS and CG. For SGDs, we used a learning rate schedule of α β+t where t is the iteration number. In our experiments, we found that it is better to use this learning
1http://www.accelereyes.com/ rate schedule than a constant learning rate. We also use momentum, and vary the number of examples used to compute the gradient. In summary, the optimization parameters associated with SGDs are: α, β, momentum parameters (Hinton, 2010) and the number of examples in a minibatch.
We run L-BFGS and CG with a fixed minibatch for several iterations and then resample a new minibatch from the larger training set.
For each new minibatch, we discard the cached optimization history in L-BFGS/CG.
In our settings, for CG and L-BFGS, there are two optimization parameters: minibatch size and number of iterations per minibatch. We use the default values2 for other optimization parameters, such as line search parameters. For CG and LBFGS, we replaced the minibatch after 3 iterations and 20 iterations respectively. We found that these parameters generally work very well for many problems. Therefore, the only remaining tunable parameter is the minibatch size.
4.3. Autoencoder training
We compare L-BFGS, CG against SGDs for training autoencoders. Our autoencoders have 10000 hidden units and the sigmoid activation function (σ). As a result, our model has approximately 8 × 105 parameters, which is considered challenging for high order optimization methods like L-BFGS.3
Figure 1. Autoencoder training with 10000 units on one machine.
2We used LBFGS in minFunc by Mark Schmidt and a CG implementation from Carl Rasmussen. We note that both of these methods are fairly optimized implementations of these algorithms; less sophisticated implementations of these algorithms may perform worse.
3For lower dimensional problems, L-BFGS works much better than other candidates (we omit the results due to space constraints).
On Optimization Methods for Deep Learning
For
L-BFGS, we vary the minibatch size in {1000, 10000}; whereas for CG, we vary the minibatch size in {100, 10000}.
For SGDs, we tried 20 combinations of optimization parameters, including varying the minibatch size in {1, 10, 100, 1000} (when the minibatch is large, this method is also called minibatch
Gradient Descent).
We compared the reconstruction errors on the test set of different optimization methods and summarize the results in Figure 1.
For SGDs, we only report the results for two best parameter settings.
The results show that minibatch L-BFGS and CG with line search converge faster than carefully tuned plain
SGDs. In particular, CG performs better compared to
L-BFGS because computing the conjugate information can be less expensive than estimating the Hessian. CG also performs better than SGDs thanks to both the line search and conjugate information.
To understand how much estimating conjugacy helps
CG, we also performed a control experiment where we tuned (increased) the minibatch size and added a line search procedure to SGDs.
Figure 2. Control experiment with line search for SGDs.
The results are shown in Figure 2 which confirm that having a line search procedure makes SGDs simpler to tune and faster. Using information in the previous steps to form the Hessian approximation (L-BFGS) or conjugate directions (CG) further improves the results.
4.4. Sparse autoencoder training
In this experiment, we trained the autoencoders with the KL sparsity penalty.
The target activation ρ is set to be 10% (a typical value for sparse autoencoders or RBMs). The weighting between the estimate for the current sample and the old estimate (λ) is set to the ratio between the minibatch size m′ and 1000(= min
� m′
�
). This means that our estimates of the hidden unit activations are computed by averaging over at least about 1000 examples.
Figure 3. Sparse autoencoder training with 10000 units, ρ = 0.1, one machine.
We report the performance of different methods in Figure 3. The results show that L-BFGS/CG are much faster than SGDs. The difference, however, is more significant than in the case of standard autoencoders.
This is because L-BFGS and CG prefer larger minibatch size and consequently it is easier to estimate the expected value of the hidden activation. In contrast, SGDs have to deal with a noisy estimate of the hidden activation and we have to set the learning rate parameters to be small to make the algorithm more stable. Interestingly, the line search does not significantly improve SGDs, unlike the previous experiment.
A close inspection of the line search shows that initial step sizes are chosen to be slightly smaller (more conservative) than the tuned step size.
4.5. Training autoencoders with GPUs
Figure 4. Autoencoder training with 10000 units, one machine with GPUs. L-BFGS and CG enjoy a speed up of approximately 2x, while no significant improvement is observed for plain SGDs.
The idea of using GPUs for training deep learning alOn Optimization Methods for Deep Learning gorithms was first proposed in (Raina et al., 2009). In this section, we will consider GPUs and carry out experiments with standard autoencoders to understand how different optimization algorithms perform.
Using the same experimental protocols described in 4.3, we compared optimization methods and their gains in switching from CPUs to GPUs and present the results in Figure 4.
From the figure, the speed up gains are much higher for L-BFGS and CG than
SGDs. This is because L-BFGS and CG prefer larger minibatch sizes which can be parallelized more efficiently on the GPUs.
4.6. Parallel training of dense networks
In this experiment, we explore optimization methods for training autoencoders in a distributed fashion using the Map-Reduce framework (Chu et al., 2007).4
We also used the same settings for all algorithms as mentioned above in Section 4.3.
Our results with training dense autoencoders (omitted due to lack of space) show that parallelizing densely connected networks in this manner can result in slower convergence than running the method on a standalone machine. This can be attributed to the communication costs involve in passing the models and gradients across the network: the parameter vectors have a size of 64Mb, which can be a considerable amount of network traffic since it is frequently communicated.
4.7. Parallel training of local networks
If we use tiled (locally connected) networks (Le et al., 2010), Map-Reduce style gradient computation can be used as an effective way for training. In tiled networks, the number of parameters is small and thus the cost of transferring the gradient across the network can often be smaller than the cost of computing it. Specifically, in this experiment, we constrain each hidden unit to connect to a small section of the image. Furthermore, we do not share any weights across hidden units (no weight tying constraints). We learn 20 feature maps, where each map consists of 441 filters, each of size 8x8.
The results presented in Figure 5 show that SGDs are slower when a computer cluster is used. On the other hand, thanks to its preference of a larger minibatch size, L-BFGS enjoys more significant speed improve4In detail, for parallelized methods, one central machine ("master") runs the optimization procedure while the slaves compute the objective values and gradients. At every step during optimization, the master sends the parameter across all slaves, the slaves then compute the objective function and gradient and send back to the master. ments.5
Figure 5. Parallel training of locally connected networks.
With locally connected networks, the communication cost is reduced significantly.
The inset figure shows the (LBFGS) speed improvement as a function of number of slaves.
The speed up factor is measured by taking the amount of time that requires each method to reach a test objective equal or better than 2.
Also, the figure shows that L-BFGS enjoys an almost linear speed up (up to 8 slave machines considered in the experiments) when locally connected networks are used. On models where the number of parameters is small, L-BFGS's bookkeeping and communication cost are both small compared to gradient computations (which is distributed across the machines).
4.8. Parallel training of supervised CNNs
In this experiment, we compare different optimization methods for supervised training of two-layer convolutional neural networks (CNNs). Specifically, our model has has 16 maps of 5x5 filters in the first layer, followed by (non-overlapping) pooling units that pool over a 3x3 region. The second layer has 16 maps of 4x4 filters, without any pooling units. Additionally, we have a softmax classification layer which is connected to all the output units from the second layer. In this experiment, we distribute the gradient computations across many machines with GPUs.
The experimental results (Figure 4.8) show that LBFGS is better than CG and SGDs on this problem because of low dimensionality (less than 10000 parameters). Map-Reduce style parallelism also significantly improves the performance of both L-BFGS and CG.
5In this experiment, we did not tune the minibatch size, i.e., when we have 4 slaves, the minibatch size per computer is divided by 4. We expect that tuning this minibatch size will improve the results when the number of computers goes up.
On Optimization Methods for Deep Learning
Figure 6. Parallel training of CNNs.
4.9. Classification on standard MNIST
Finally, we carried out experiments to determine if LBFGS affects classification accuracy. We used a convolution network with a first layer having 32 maps of 5x5 filters and 3x3 pooling with subsampling. The second layer had 64 maps of 5x5 filters and 2x2 pooling with subsampling. This architecture is similar to that described in Ranzato et al. (2007), with the following two differences: (i) we did not use an additional hidden layer of 200 hidden units; (ii) the receptive field of our first layer pooling units is slightly larger (for computational reasons).
Table 1. Classification error on MNIST test set for some representative methods without pretraining.
SGDs with diagonal Levenberg-Marquardt are used in (LeCun et al., 1998; Ranzato et al., 2007).
LeNet-5, SGDs, no distortions (LeCun et al., 1998)
LeNet-5, SGDs, huge distortions (LeCun et al., 1998)
LeNet-5, SGDs, distortions (LeCun et al., 1998)
ConvNet, SGDs, no distortions (Ranzato et al., 2007)
ConvNet, L-BFGS, no distortions (this paper)
We trained our network using 4 machines (with
GPUs).
For every epoch, we saved the parameters to disk and used a hold-out validation set of 10000 examples6 to select the best model. The best model is used to make predictions on the test set. The results of our method (ConvNet) using minibatch LBFGS are reported in Table 1.
The results show that the CNN, trained with L-BFGS, achieves an encouraging classification result: 0.69%. We note that this is the best result for MNIST among algorithms that do not use unsupervised pretraining or distortions.
In particular, engineering distortions, typically viewed as a way to introduce domain knowledge, can improve classification results for MNIST. In fact, state-of-the-art results involve more careful dis6We used a reduced training set of 50000 examples throughout the classification experiments. tortion engineering and/or unsupervised pretraining, e.g., 0.4% (Simard et al., 2003), 0.53% (Jarrett et al., 2009), 0.39% (Ciresan et al., 2010).
5. Discussion
In our experiments, different optimization algorithms appear to be superior on different problems. On contrary to what appears to be a widely-held belief, that
SGDs are almost always preferred, we found that LBFGS and CG can be superior to SGDs in many cases.
Among the problems we considered, L-BFGS is a good candidate for optimization for low dimensional problems, where the number of parameters are relatively small (e.g., convolutional neural networks). For high dimensional problems, CG often does well.
Sparsity provides another compelling case for using LBFGS/CG. In our experiments, L-BFGS and CG outperform SGDs on training sparse autoencoders.
We note that there are cases where L-BFGS may not be expected to perform well (e.g., if the Hessian is not well approximated with a low-rank estimate). For instance, on local networks (Le et al., 2010) where the overlaps between receptive fields are small, the Hessian has a block-diagonal structure and L-BFGS, which uses low-rank updates, may not perform well.7 In such cases, algorithms that exploit the problem structures may perform much better.
CG and L-BFGS are also methods that can take better advantage of the GPUs thanks to their preference of larger minibatch sizes. Furthermore, if one uses tiled(locally connected) networks or other networks with a relatively small number of parameters, it is possible to compute the gradients in a Map-Reduce framework and speed up training with L-BFGS.
Acknowledgments:
We thank Andrew Maas, Andrew Saxe, Quinn Slack, Alex Smola and Will Zou for comments and discussions. This work is supported by the DARPA Deep Learning program under contract number FA8650-10-C-7020.
References
Bartlett, P., Hazan, E., and Rakhlin, A. Adaptive online gradient descent. In NIPS, 2008.
Bengio, Y. Learning deep architectures for AI. Foundations and Trends in Machine Learning, pp. 1–127, 2009.
Bengio, Y., Lamblin, P., Popovici, D., and Larochelle, H.
Greedy layerwise training of deep networks. In NIPS, 7Personal communications with Will Zou.
On Optimization Methods for Deep Learning
Bordes, A., Bottou, L., and Gallinari, P. SGD-QN: Careful quasi-newton stochastic gradient descent.
JMLR, pp.
1737–1754, 2010.
Bottou, L. Stochastic gradient learning in neural networks.
In Proceedings of Neuro-Nˆımes 91, 1991.
Bottou, L. and Bousquet, O. The tradeoffs of large scale learning. In NIPS. 2008.
Chu, C.T., Kim, S. K., Lin, Y. A., Yu, Y. Y., Bradski, G., Ng, A. Y., and Olukotun, K. Map-Reduce for machine learning on multicore. In NIPS 19, 2007.
Ciresan, D. C., Meier, U., Gambardella, L. M., and Schmidhuber, J. Deep big simple neural nets excel on handwritten digit recognition. CoRR, 2010.
Coates, A., Lee, H., and Ng, A. Y. An analysis of singlelayer networks in unsupervised feature learning. In AISTATS 14, 2011.
Dean, J. and Ghemawat, S. Map-Reduce: simplified data processing on large clusters. Comm. ACM, pp. 107–113, Do, C.B., Le, Q.V., and Foo, C.S. Proximal regularization for online and batch learning. In ICML, 2009.
Goodfellow, I., Le, Q.V., Saxe, A., Lee, H., and Ng, A.Y.
Measuring invariances in deep networks. In NIPS, 2010.
Hinton, G. A practical guide to training restricted boltzmann machines. Technical report, U. of Toronto, 2010.
Hinton, G. E. and Salakhutdinov, R.R. Reducing the dimensionality of data with neural networks.
Science, Hinton, G. E., Osindero, S., and Teh, Y.W. A fast learning algorithm for deep belief nets. Neu. Comp., 2006.
Jarrett, K., Kavukcuoglu, K., Ranzato, M.A., and LeCun, Y. What is the best multi-stage architecture for object recognition? In ICCV, 2009.
Larochelle, H. and Bengio, Y. Classification using discriminative restricted boltzmann machines. In ICML, 2008.
Le, Q. V., Ngiam, J., Chen, Z., Chia, D., Koh, P. W., and Ng, A. Y. Tiled convolutional neural networks. In NIPS, Le, Q. V., Zou, W., Yeung, S. Y., and Ng, A. Y. Learning hierarchical spatio-temporal features for action recognition with independent subspace analysis. In CVPR, LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient based learning applied to document recognition.
Proceeding of the IEEE, pp. 2278–2323, 1998.
LeCun, Y., Bottou, L., Orr, G., and Muller, K. Efficient backprop. In Neural Networks: Tricks of the trade, pp.
5–50. Springer, 1998.
Lee, H., Battle, A., Raina, R., and Ng, Andrew Y. Efficient sparse coding algorithms. In NIPS, 2007.
Lee, H., Ekanadham, C., and Ng, A. Y. Sparse deep belief net model for visual area V2. In NIPS, 2008.
Lee, H., Grosse, R., Ranganath, R., and Ng, A.Y. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. In ICML, 2009a.
Lee, H., Largman, Y., Pham, P., and Ng, A. Y. Unsupervised feature learning for audioclassification using convolutional deep belief networks. In NIPS, 2009b.
Mann, G., McDonald, R., Mohri, M., Silberman, N., and Walker, D. Efficient large-scale distributed training of conditional maximum entropy models. In NIPS, 2009.
Martens, J. Deep learning via hessian-free optimization.
In ICML, 2010.
Nair, V. and Hinton, G. E.
3D object recognition with deep belief nets. In NIPS, 2009.
Olshausen, B. and Field, D. Emergence of simple-cell receptive field properties by learning a sparse code for natural images. Nature, pp. 607–609, 1996.
Raina, R., Battle, A., Lee, H., Packer, B., and Ng, A.Y.
Self-taught learning: Transfer learning from unlabelled data. In ICML, 2007.
Raina, R., Madhavan, A., and Ng, A. Y.
Large-scale deep unsupervised learning using graphics processors. In
ICML, 2009.
Ranzato, M., Huang, F. J, Boureau, Y., and LeCun, Y. Unsupervised learning of invariant feature hierarchies with applications to object recognition. In CVPR, 2007.
Shalev-Shwartz, S., Singer, Y., and Srebro, N. Pegasos:
Primal estimated sub-gradient solver for svm. In ICML, Simard, P., Steinkraus, D., and Platt, J. Best practices for convolutional neural networks applied to visual document analysis. In ICDAR, 2003.
Smolensky, P.
Information processing in dynamical systems: foundations of harmony theory. In Parallel distributed processing, 1986.
Taylor, G.W., Fergus, R., Lecun, Y., and Bregler, C.
Convolutional learning of spatio-temporal features. In
ECCV, 2010.
Teo, C. H., Le, Q. V., Smola, A. J., and Vishwanathan, S. V. N. A scalable modular convex solver for regularized risk minimization. In KDD, 2007.
Vincent, P., Larochelle, H., Bengio, Y., and Manzagol, P. A. Extracting and composing robust features with denoising autoencoders. In ICML, 2008.
Vishwanathan, S. V. N., Schraudolph, N. N., Schmidt, M. W., and Murphy, K. P. Accelerated training of conditional random fields with stochastic gradient methods.
In ICML, 2007.
Yang, J., Yu, K., Gong, Y., and Huang, T. Linear spatial pyramid matching using sparse coding for image classification. In CVPR, 2009.
Zinkevich, M., Weimer, M., Smola, A., and Li, L. Parallelized stochastic gradient descent. In NIPS, 2010.Deep Learning in Neural Networks: An Overview
Technical Report IDSIA-03-14 / arXiv:1404.7828 v3 [cs.NE]
J¨urgen Schmidhuber
The Swiss AI Lab IDSIA
Istituto Dalle Molle di Studi sull'Intelligenza Artificiale
University of Lugano & SUPSI
Galleria 2, 6928 Manno-Lugano
Switzerland
2 July 2014
Abstract
In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning & evolutionary computation, and indirect search for short programs encoding deep and large networks.
LATEX source file: http://www.idsia.ch/˜juergen/DeepLearning2July2014.tex
Complete BIBTEX file: http://www.idsia.ch/˜juergen/bib.bib
Preface
This is the draft of an invited Deep Learning (DL) overview. One of its goals is to assign credit to those who contributed to the present state of the art. I acknowledge the limitations of attempting to achieve this goal. The DL research community itself may be viewed as a continually evolving, deep network of scientists who have influenced each other in complex ways. Starting from recent DL results, I tried to trace back the origins of relevant ideas through the past half century and beyond, sometimes using
"local search" to follow citations of citations backwards in time. Since not all DL publications properly acknowledge earlier relevant work, additional global search strategies were employed, aided by consulting numerous neural network experts. As a result, the present draft mostly consists of references (866 entries so far). Nevertheless, through an expert selection bias I may have missed important work. A related bias was surely introduced by my special familiarity with the work of my own DL research group in the past quarter-century. For these reasons, the present draft should be viewed as merely a snapshot of an ongoing credit assignment process. To help improve it, please do not hesitate to send corrections and suggestions to juergen@idsia.ch.
Contents
Introduction to Deep Learning (DL) in Neural Networks (NNs)
Event-Oriented Notation for Activation Spreading in FNNs / RNNs
Depth of Credit Assignment Paths (CAPs) and of Problems
Recurring Themes of Deep Learning
Dynamic Programming for Supervised / Reinforcement Learning (SL / RL)........
Unsupervised Learning (UL) Facilitating SL and RL
Learning Hierarchical Representations Through Deep SL, UL, RL.............
Occam's Razor: Compression and Minimum Description Length (MDL)..........
Fast Graphics Processing Units (GPUs) for DL in NNs...................
Supervised NNs, Some Helped by Unsupervised NNs
Early NNs Since the 1940s (and the 1800s)
Around 1960: Visual Cortex Provides Inspiration for DL (Sec. 5.4, 5.11)..........
1965: Deep Networks Based on the Group Method of Data Handling (GMDH)
1979: Convolution + Weight Replication + Subsampling (Neocognitron).........
1960-1981 and Beyond: Development of Backpropagation (BP) for NNs..........
BP for Weight-Sharing Feedforward NNs (FNNs) and Recurrent NNs (RNNs)
Late 1980s-2000 and Beyond: Numerous Improvements of NNs..............
Ideas for Dealing with Long Time Lags and Deep CAPs..............
Better BP Through Advanced Gradient Descent (Compare Sec. 5.24)
Searching For Simple, Low-Complexity, Problem-Solving NNs (Sec. 5.24)....
Potential Benefits of UL for SL (Compare Sec. 5.7, 5.10, 5.15)...........
1987: UL Through Autoencoder (AE) Hierarchies (Compare Sec. 5.15)..........
1989: BP for Convolutional NNs (CNNs, Sec. 5.4)
1991: Fundamental Deep Learning Problem of Gradient Descent..............
5.10 1991: UL-Based History Compression Through a Deep Hierarchy of RNNs........
5.11 1992: Max-Pooling (MP): Towards MPCNNs (Compare Sec. 5.16, 5.19)..........
5.12 1994: Early Contest-Winning NNs..............................
5.13 1995: Supervised Recurrent Very Deep Learner (LSTM RNN)...............
5.14 2003: More Contest-Winning/Record-Setting NNs; Successful Deep NNs.........
5.15 2006/7: UL For Deep Belief Networks (DBNs) / AE Stacks Fine-Tuned by BP
5.16 2006/7: Improved CNNs / GPU-CNNs / BP-Trained MPCNNs / LSTM Stacks
5.17 2009: First Official Competitions Won by RNNs, and with MPCNNs............
5.18 2010: Plain Backprop (+ Distortions) on GPU Breaks MNIST Record...........
5.19 2011: MPCNNs on GPU Achieve Superhuman Vision Performance
5.20 2011: Hessian-Free Optimization for RNNs.........................
5.21 2012: First Contests Won on ImageNet & Object Detection & Segmentation........
5.22 2013-: More Contests and Benchmark Records
5.23 Currently Successful Supervised Techniques: LSTM RNNs / GPU-MPCNNs.......
5.24 Recent Tricks for Improving SL Deep NNs (Compare Sec. 5.6.2, 5.6.3)
5.25 Consequences for Neuroscience
5.26 DL with Spiking Neurons?..................................
DL in FNNs and RNNs for Reinforcement Learning (RL)
RL Through NN World Models Yields RNNs With Deep CAPs...............
Deep FNNs for Traditional RL and Markov Decision Processes (MDPs)..........
Deep RL RNNs for Partially Observable MDPs (POMDPs).................
RL Facilitated by Deep UL in FNNs and RNNs.......................
Deep Hierarchical RL (HRL) and Subgoal Learning with FNNs and RNNs.........
Deep RL by Direct NN Search / Policy Gradients / Evolution................
Deep RL by Indirect Policy Search / Compressed NN Search................
Universal RL
Conclusion
Acknowledgments
Abbreviations in Alphabetical Order
AE: Autoencoder
ANN: Artificial Neural Network
BFGS: Broyden-Fletcher-Goldfarb-Shanno
BNN: Biological Neural Network
BM: Boltzmann Machine
BP: Backpropagation
BRNN: Bi-directional Recurrent Neural Network
CAP: Credit Assignment Path
CEC: Constant Error Carousel
CFL: Context Free Language
CMA-ES: Covariance Matrix Estimation Evolution
Strategies
CNN: Convolutional Neural Network
CoSyNE: Co-Synaptic Neuro-Evolution
CSL: Context Senistive Language
CTC : Connectionist Temporal Classification
DBN: Deep Belief Network
DCT: Discrete Cosine Transform
DL: Deep Learning
DP: Dynamic Programming
DS: Direct Policy Search
EA: Evolutionary Algorithm
EM: Expectation Maximization
FMS: Flat Minimum Search
FNN: Feedforward Neural Network
FSA: Finite State Automaton
GMDH: Group Method of Data Handling
GOFAI: Good Old-Fashioned Artificial Intelligence
GP: Genetic Programming
GPU: Graphics Processing Unit
GPU-MPCNN: GPU-Based Max-Pooling Convolutional Neural Network
HMM: Hidden Markov Model
HRL: Hierarchical Reinforcement Learning
HTM: Hierarchical Temporal Memory
HMAX: Hierarchical Model "and X"
LSTM: Long Short-Term Memory (Recurrent Neural Network)
MC: Multi-Column
MDL: Minimum Description Length
MDP: Markov Decision Process
MNIST: Mixed National Institute of Standards and Technology Database
MP: Max-Pooling
MPCNN: Max-Pooling Convolutional Neural Network
NEAT: NeuroEvolution of Augmenting Topologies
NES: Natural Evolution Strategies
NFQ: Neural Fitted Q-Learning
NN: Neural Network
PCC: Potential Causal Connection
PDCC: Potential Direct Causal Connection
PM: Predictability Minimization
POMDP: Partially Observable Markov Decision
Process
RAAM: Recursive Auto-Associative Memory
RBM: Restricted Boltzmann Machine
ReLU: Rectified Linear Unit
RL: Reinforcement Learning
RNN: Recurrent Neural Network
R-prop: Resilient Backpropagation
SL: Supervised Learning
SLIM NN: Self-Delimiting Neural Network
SOTA: Self-Organising Tree Algorithm
SVM: Support Vector Machine
TDNN: Time-Delay Neural Network
TIMIT: TI/SRI/MIT Acoustic-Phonetic Continuous
Speech Corpus
UL: Unsupervised Learning
WTA: Winner-Take-All
Introduction to Deep Learning (DL) in Neural Networks (NNs)
Which modifiable components of a learning system are responsible for its success or failure? What changes to them improve performance? This has been called the fundamental credit assignment problem (Minsky, 1963). There are general credit assignment methods for universal problem solvers that are time-optimal in various theoretical senses (Sec. 6.8). The present survey, however, will focus on the narrower, but now commercially important, subfield of Deep Learning (DL) in Artificial Neural Networks (NNs). We are interested in accurate credit assignment across possibly many, often nonlinear, computational stages of NNs.
Shallow NN-like models have been around for many decades if not centuries (Sec. 5.1). Models with several successive nonlinear layers of neurons date back at least to the 1960s (Sec. 5.3) and 1970s (Sec. 5.5).
An efficient gradient descent method for teacher-based Supervised Learning (SL) in discrete, differentiable networks of arbitrary depth called backpropagation (BP) was developed in the 1960s and 1970s, and applied to NNs in 1981 (Sec. 5.5). BP-based training of deep NNs with many layers, however, had been found to be difficult in practice by the late 1980s (Sec. 5.6), and had become an explicit research subject by the early 1990s (Sec. 5.9). DL became practically feasible to some extent through the help of Unsupervised
Learning (UL) (e.g., Sec. 5.10, 5.15). The 1990s and 2000s also saw many improvements of purely supervised DL (Sec. 5). In the new millennium, deep NNs have finally attracted wide-spread attention, mainly by outperforming alternative machine learning methods such as kernel machines (Vapnik, 1995; Sch¨olkopf et al., 1998) in numerous important applications. In fact, supervised deep NNs have won numerous official international pattern recognition competitions (e.g., Sec. 5.17, 5.19, 5.21, 5.22), achieving the first superhuman visual pattern recognition results in limited domains (Sec. 5.19). Deep NNs also have become relevant for the more general field of Reinforcement Learning (RL) where there is no supervising teacher(Sec. 6).
Both feedforward (acyclic) NNs (FNNs) and recurrent (cyclic) NNs (RNNs) have won contests (Sec.
5.12, 5.14, 5.17, 5.19, 5.21, 5.22). In a sense, RNNs are the deepest of all NNs (Sec. 3)—they are general computers more powerful than FNNs, and can in principle create and process memories of arbitrary sequences of input patterns (e.g., Siegelmann and Sontag, 1991; Schmidhuber, 1990a). Unlike traditional methods for automatic sequential program synthesis (e.g., Waldinger and Lee, 1969; Balzer, 1985; Soloway, 1986; Deville and Lau, 1994), RNNs can learn programs that mix sequential and parallel information processing in a natural and efficient way, exploiting the massive parallelism viewed as crucial for sustaining the rapid decline of computation cost observed over the past 75 years.
The rest of this paper is structured as follows. Sec. 2 introduces a compact, event-oriented notation that is simple yet general enough to accommodate both FNNs and RNNs. Sec. 3 introduces the concept of Credit Assignment Paths (CAPs) to measure whether learning in a given NN application is of the deep or shallow type. Sec. 4 lists recurring themes of DL in SL, UL, and RL. Sec. 5 focuses on SL and UL, and on how UL can facilitate SL, although pure SL has become dominant in recent competitions (Sec. 5.17–5.23).
Sec. 5 is arranged in a historical timeline format with subsections on important inspirations and technical contributions. Sec. 6 on deep RL discusses traditional Dynamic Programming (DP)-based RL combined with gradient-based search techniques for SL or UL in deep NNs, as well as general methods for direct and indirect search in the weight space of deep FNNs and RNNs, including successful policy gradient and evolutionary methods.
Event-Oriented Notation for Activation Spreading in FNNs / RNNs
Throughout this paper, let i, j, k, t, p, q, r denote positive integer variables assuming ranges implicit in the given contexts. Let n, m, T denote positive integer constants.
An NN's topology may change over time (e.g., Sec. 5.3, 5.6.3). At any given moment, it can be described as a finite subset of units (or nodes or neurons) N = {u1, u2,..., } and a finite set H ⊆ N × N of directed edges or connections between nodes. FNNs are acyclic graphs, RNNs cyclic. The first (input) layer is the set of input units, a subset of N. In FNNs, the k-th layer (k > 1) is the set of all nodes u ∈ N such that there is an edge path of length k − 1 (but no longer path) between some input unit and u. There may be shortcut connections between distant layers.
The NN's behavior or program is determined by a set of real-valued, possibly modifiable, parameters or weights wi (i = 1,..., n). We now focus on a single finite episode or epoch of information processing and activation spreading, without learning through weight changes. The following slightly unconventional notation is designed to compactly describe what is happening during the runtime of the system.
During an episode, there is a partially causal sequence xt(t = 1,..., T) of real values that I call events. Each xt is either an input set by the environment, or the activation of a unit that may directly depend on other xk(k < t) through a current NN topology-dependent set int of indices k representing incoming causal connections or links. Let the function v encode topology information and map such event index pairs (k, t) to weight indices. For example, in the non-input case we may have xt = ft(nett) with real-valued nett = � k∈int xkwv(k,t) (additive case) or nett = � k∈int xkwv(k,t) (multiplicative case), where ft is a typically nonlinear real-valued activation function such as tanh. In many recent competitionwinning NNs (Sec. 5.19, 5.21, 5.22) there also are events of the type xt = maxk∈int(xk); some network types may also use complex polynomial activation functions (Sec. 5.3). xt may directly affect certain xk(k > t) through outgoing connections or links represented through a current set outt of indices k with t ∈ ink. Some of the non-input events are called output events.
Note that many of the xt may refer to different, time-varying activations of the same unit in sequenceprocessing RNNs (e.g., Williams, 1989, "unfolding in time"), or also in FNNs sequentially exposed to time-varying input patterns of a large training set encoded as input events. During an episode, the same weight may get reused over and over again in topology-dependent ways, e.g., in RNNs, or in convolutional
NNs (Sec. 5.4, 5.8). I call this weight sharing across space and/or time. Weight sharing may greatly reduce the NN's descriptive complexity, which is the number of bits of information required to describe the NN(Sec. 4.4).
In Supervised Learning (SL), certain NN output events xt may be associated with teacher-given, realvalued labels or targets dt yielding errors et, e.g., et = 1/2(xt − dt)2. A typical goal of supervised NN training is to find weights that yield episodes with small total error E, the sum of all such et. The hope is that the NN will generalize well in later episodes, causing only small errors on previously unseen sequences of input events. Many alternative error functions for SL and UL are possible.
SL assumes that input events are independent of earlier output events (which may affect the environment through actions causing subsequent perceptions). This assumption does not hold in the broader fields of Sequential Decision Making and Reinforcement Learning (RL) (Kaelbling et al., 1996; Sutton and Barto, 1998; Hutter, 2005; Wiering and van Otterlo, 2012) (Sec. 6). In RL, some of the input events may encode real-valued reward signals given by the environment, and a typical goal is to find weights that yield episodes with a high sum of reward signals, through sequences of appropriate output actions.
Sec. 5.5 will use the notation above to compactly describe a central algorithm of DL, namely, backpropagation (BP) for supervised weight-sharing FNNs and RNNs. (FNNs may be viewed as RNNs with certain fixed zero weights.) Sec. 6 will address the more general RL case.
Depth of Credit Assignment Paths (CAPs) and of Problems
To measure whether credit assignment in a given NN application is of the deep or shallow type, I introduce the concept of Credit Assignment Paths or CAPs, which are chains of possibly causal links between events.
Let us first focus on SL. Consider two events xp and xq (1 ≤ p < q ≤ T). Depending on the application, they may have a Potential Direct Causal Connection (PDCC) expressed by the Boolean predicate pdcc(p, q), which is true if and only if p ∈ inq. Then the 2-element list (p, q) is defined to be a CAP (a minimal one) from p to q. A learning algorithm may be allowed to change wv(p,q) to improve performance in future episodes.
More general, possibly indirect, Potential Causal Connections (PCC) are expressed by the recursively defined Boolean predicate pcc(p, q), which in the SL case is true only if pdcc(p, q), or if pcc(p, k) for some k and pdcc(k, q). In the latter case, appending q to any CAP from p to k yields a CAP from p to q (this is a recursive definition, too). The set of such CAPs may be large but is finite. Note that the same weight may affect many different PDCCs between successive events listed by a given CAP, e.g., in the case of RNNs, or weight-sharing FNNs.
Suppose a CAP has the form (..., k, t,..., q), where k and t (possibly t = q) are the first successive elements with modifiable wv(k,t). Then the length of the suffix list (t,..., q) is called the CAP's depth(which is 0 if there are no modifiable links at all). This depth limits how far backwards credit assignment can move down the causal chain to find a modifiable weight.1
Suppose an episode and its event sequence x1,..., xT satisfy a computable criterion used to decide whether a given problem has been solved (e.g., total error E below some threshold). Then the set of used weights is called a solution to the problem, and the depth of the deepest CAP within the sequence is called the solution depth. There may be other solutions (yielding different event sequences) with different depths.
Given some fixed NN topology, the smallest depth of any solution is called the problem depth.
Sometimes we also speak of the depth of an architecture: SL FNNs with fixed topology imply a problem-independent maximal problem depth bounded by the number of non-input layers. Certain SL
RNNs with fixed weights for all connections except those to output units (Jaeger, 2001; Maass et al., 2002;
Jaeger, 2004; Schrauwen et al., 2007) have a maximal problem depth of 1, because only the final links in the corresponding CAPs are modifiable. In general, however, RNNs may learn to solve problems of potentially unlimited depth.
Note that the definitions above are solely based on the depths of causal chains, and agnostic to the temporal distance between events. For example, shallow FNNs perceiving large "time windows" of input events may correctly classify long input sequences through appropriate output events, and thus solve shallow problems involving long time lags between relevant events.
At which problem depth does Shallow Learning end, and Deep Learning begin? Discussions with DL experts have not yet yielded a conclusive response to this question. Instead of committing myself to a precise answer, let me just define for the purposes of this overview: problems of depth > 10 require Very
Deep Learning.
The difficulty of a problem may have little to do with its depth. Some NNs can quickly learn to solve certain deep problems, e.g., through random weight guessing (Sec. 5.9) or other types of direct search(Sec. 6.6) or indirect search (Sec. 6.7) in weight space, or through training an NN first on shallow problems whose solutions may then generalize to deep problems, or through collapsing sequences of (non)linear operations into a single (non)linear operation (but see an analysis of non-trivial aspects of deep linear networks, Baldi and Hornik, 1994, Section B). In general, however, finding an NN that precisely models a given training set is an NP-complete problem (Judd, 1990; Blum and Rivest, 1992), also in the case of deep NNs (S´ıma, 1994; de Souto et al., 1999; Windisch, 2005); compare a survey of negative results (S´ıma, 2002, Section 1).
Above we have focused on SL. In the more general case of RL in unknown environments, pcc(p, q) is also true if xp is an output event and xq any later input event—any action may affect the environment and thus any later perception. (In the real world, the environment may even influence non-input events computed on a physical hardware entangled with the entire universe, but this is ignored here.) It is possible to model and replace such unmodifiable environmental PCCs through a part of the NN that has already learned to predict (through some of its units) input events (including reward signals) from former input events and actions (Sec. 6.1). Its weights are frozen, but can help to assign credit to other, still modifiable weights used to compute actions (Sec. 6.1). This approach may lead to very deep CAPs though.
Some DL research is about automatically rephrasing problems such that their depth is reduced (Sec. 4).
In particular, sometimes UL is used to make SL problems less deep, e.g., Sec. 5.10. Often Dynamic
Programming (Sec. 4.1) is used to facilitate certain traditional RL problems, e.g., Sec. 6.2. Sec. 5 focuses on CAPs for SL, Sec. 6 on the more complex case of RL.
Recurring Themes of Deep Learning
Dynamic Programming for Supervised / Reinforcement Learning (SL / RL)
One recurring theme of DL is Dynamic Programming (DP) (Bellman, 1957), which can help to facilitate credit assignment under certain assumptions. For example, in SL NNs, backpropagation itself can
1An alternative would be to count only modifiable links when measuring depth. In many typical NN applications this would not make a difference, but in some it would, e.g., Sec. 6.1.
6 be viewed as a DP-derived method (Sec. 5.5). In traditional RL based on strong Markovian assumptions, DP-derived methods can help to greatly reduce problem depth (Sec. 6.2). DP algorithms are also essential for systems that combine concepts of NNs and graphical models, such as Hidden Markov Models(HMMs) (Stratonovich, 1960; Baum and Petrie, 1966) and Expectation Maximization (EM) (Dempster et al., 1977; Friedman et al., 2001), e.g., (Bottou, 1991; Bengio, 1991; Bourlard and Morgan, 1994; Baldi and Chauvin, 1996; Jordan and Sejnowski, 2001; Bishop, 2006; Hastie et al., 2009; Poon and Domingos, 2011; Dahl et al., 2012; Hinton et al., 2012a; Wu and Shao, 2014).
Unsupervised Learning (UL) Facilitating SL and RL
Another recurring theme is how UL can facilitate both SL (Sec. 5) and RL (Sec. 6). UL (Sec. 5.6.4) is normally used to encode raw incoming data such as video or speech streams in a form that is more convenient for subsequent goal-directed learning. In particular, codes that describe the original data in a less redundant or more compact way can be fed into SL (Sec. 5.10, 5.15) or RL machines (Sec. 6.4), whose search spaces may thus become smaller (and whose CAPs shallower) than those necessary for dealing with the raw data. UL is closely connected to the topics of regularization and compression (Sec. 4.4, 5.6.3).
Learning Hierarchical Representations Through Deep SL, UL, RL
Many methods of Good Old-Fashioned Artificial Intelligence (GOFAI) (Nilsson, 1980) as well as more recent approaches to AI (Russell et al., 1995) and Machine Learning (Mitchell, 1997) learn hierarchies of more and more abstract data representations. For example, certain methods of syntactic pattern recognition (Fu, 1977) such as grammar induction discover hierarchies of formal rules to model observations.
The partially (un)supervised Automated Mathematician / EURISKO (Lenat, 1983; Lenat and Brown, 1984) continually learns concepts by combining previously learnt concepts. Such hierarchical representation learning (Ring, 1994; Bengio et al., 2013; Deng and Yu, 2014) is also a recurring theme of DL NNs for SL(Sec. 5), UL-aided SL (Sec. 5.7, 5.10, 5.15), and hierarchical RL (Sec. 6.5). Often, abstract hierarchical representations are natural by-products of data compression (Sec. 4.4), e.g., Sec. 5.10.
Occam's Razor: Compression and Minimum Description Length (MDL)
Occam's razor favors simple solutions over complex ones. Given some programming language, the principle of Minimum Description Length (MDL) can be used to measure the complexity of a solution candidate by the length of the shortest program that computes it (e.g., Solomonoff, 1964; Kolmogorov, 1965b;
Chaitin, 1966; Wallace and Boulton, 1968; Levin, 1973a; Solomonoff, 1978; Rissanen, 1986; Blumer et al., 1987; Li and Vit´anyi, 1997; Gr¨unwald et al., 2005). Some methods explicitly take into account program runtime (Allender, 1992; Watanabe, 1992; Schmidhuber, 1997, 2002); many consider only programs with constant runtime, written in non-universal programming languages (e.g., Rissanen, 1986; Hinton and van
Camp, 1993). In the NN case, the MDL principle suggests that low NN weight complexity corresponds to high NN probability in the Bayesian view (e.g., MacKay, 1992; Buntine and Weigend, 1991; Neal, 1995; De Freitas, 2003), and to high generalization performance (e.g., Baum and Haussler, 1989), without overfitting the training data. Many methods have been proposed for regularizing NNs, that is, searching for solution-computing but simple, low-complexity SL NNs (Sec. 5.6.3) and RL NNs (Sec. 6.7). This is closely related to certain UL methods (Sec. 4.2, 5.6.4).
Fast Graphics Processing Units (GPUs) for DL in NNs
While the previous millennium saw several attempts at creating fast NN-specific hardware (e.g., Jackel et al., 1990; Faggin, 1992; Ramacher et al., 1993; Widrow et al., 1994; Heemskerk, 1995; Korkin et al., 1997; Urlbe, 1999), and at exploiting standard hardware (e.g., Anguita et al., 1994; Muller et al., 1995;
Anguita and Gomes, 1996), the new millennium brought a DL breakthrough in form of cheap, multiprocessor graphics cards or GPUs. GPUs are widely used for video games, a huge and competitive market that has driven down hardware prices. GPUs excel at the fast matrix and vector multiplications required not only for convincing virtual realities but also for NN training, where they can speed up learning by a factor
7 of 50 and more. Some of the GPU-based FNN implementations (Sec. 5.16–5.19) have greatly contributed to recent successes in contests for pattern recognition (Sec. 5.19–5.22), image segmentation (Sec. 5.21), and object detection (Sec. 5.21–5.22).
Supervised NNs, Some Helped by Unsupervised NNs
The main focus of current practical applications is on Supervised Learning (SL), which has dominated recent pattern recognition contests (Sec. 5.17–5.23). Several methods, however, use additional Unsupervised
Learning (UL) to facilitate SL (Sec. 5.7, 5.10, 5.15). It does make sense to treat SL and UL in the same section: often gradient-based methods, such as BP (Sec. 5.5.1), are used to optimize objective functions of both UL and SL, and the boundary between SL and UL may blur, for example, when it comes to time series prediction and sequence classification, e.g., Sec. 5.10, 5.12.
A historical timeline format will help to arrange subsections on important inspirations and technical contributions (although such a subsection may span a time interval of many years). Sec. 5.1 briefly mentions early, shallow NN models since the 1940s (and 1800s), Sec. 5.2 additional early neurobiological inspiration relevant for modern Deep Learning (DL). Sec. 5.3 is about GMDH networks (since 1965), to my knowledge the first (feedforward) DL systems. Sec. 5.4 is about the relatively deep Neocognitron
NN (1979) which is very similar to certain modern deep FNN architectures, as it combines convolutional
NNs (CNNs), weight pattern replication, and subsampling mechanisms. Sec. 5.5 uses the notation of Sec. 2 to compactly describe a central algorithm of DL, namely, backpropagation (BP) for supervised weight-sharing FNNs and RNNs. It also summarizes the history of BP 1960-1981 and beyond. Sec. 5.6 describes problems encountered in the late 1980s with BP for deep NNs, and mentions several ideas from the previous millennium to overcome them. Sec. 5.7 discusses a first hierarchical stack (1987) of coupled
UL-based Autoencoders (AEs)—this concept resurfaced in the new millennium (Sec. 5.15). Sec. 5.8 is about applying BP to CNNs (1989), which is important for today's DL applications. Sec. 5.9 explains
BP's Fundamental DL Problem (of vanishing/exploding gradients) discovered in 1991. Sec. 5.10 explains how a deep RNN stack of 1991 (the History Compressor) pre-trained by UL helped to solve previously unlearnable DL benchmarks requiring Credit Assignment Paths (CAPs, Sec. 3) of depth 1000 and more.
Sec. 5.11 discusses a particular winner-take-all (WTA) method called Max-Pooling (MP, 1992) widely used in today's deep FNNs. Sec. 5.12 mentions a first important contest won by SL NNs in 1994. Sec. 5.13 describes a purely supervised DL RNN (Long Short-Term Memory, LSTM, 1995) for problems of depth
1000 and more. Sec. 5.14 mentions an early contest of 2003 won by an ensemble of shallow FNNs, as well as good pattern recognition results with CNNs and deep FNNs and LSTM RNNs (2003). Sec. 5.15 is mostly about Deep Belief Networks (DBNs, 2006) and related stacks of Autoencoders (AEs, Sec. 5.7), both pre-trained by UL to facilitate subsequent BP-based SL (compare Sec. 5.6.1). Sec. 5.16 mentions the first SL-based GPU-CNNs (2006), BP-trained MPCNNs (2007), and LSTM stacks (2007). Sec. 5.17–
5.22 focus on official competitions with secret test sets won by (mostly purely supervised) deep NNs since
2009, in sequence recognition, image classification, image segmentation, and object detection. Many RNN results depended on LSTM (Sec. 5.13); many FNN results depended on GPU-based FNN code developed since 2004 (Sec. 5.16, 5.17, 5.18, 5.19), in particular, GPU-MPCNNs (2011, Sec. 5.19). Sec. 5.24 mentions recent tricks for improving DL in NNs, many of them closely related to earlier tricks from the previous millennium (e.g., Sec. 5.6.2, 5.6.3). Sec. 5.25 discusses how artificial NNs can help to understand biological
NNs; Sec. 5.26 addresses the possibility of DL in NNs with spiking neurons.
Early NNs Since the 1940s (and the 1800s)
Early NN architectures (McCulloch and Pitts, 1943) did not learn. The first ideas about UL were published a few years later (Hebb, 1949). The following decades brought simple NNs trained by SL (e.g., Rosenblatt, 1958, 1962; Widrow and Hoff, 1962; Narendra and Thathatchar, 1974) and UL (e.g., Grossberg, 1969;
Kohonen, 1972; von der Malsburg, 1973; Willshaw and von der Malsburg, 1976), as well as closely related associative memories (e.g., Palm, 1980; Hopfield, 1982).
In a sense NNs have been around even longer, since early supervised NNs were essentially variants of linear regression methods going back at least to the early 1800s (e.g., Legendre, 1805; Gauss, 1809, 1821);
Gauss also refers to his work of 1795. Early NNs had a maximal CAP depth of 1 (Sec. 3).
Around 1960: Visual Cortex Provides Inspiration for DL (Sec. 5.4, 5.11)
Simple cells and complex cells were found in the cat's visual cortex (e.g., Hubel and Wiesel, 1962; Wiesel and Hubel, 1959). These cells fire in response to certain properties of visual sensory inputs, such as the orientation of edges. Complex cells exhibit more spatial invariance than simple cells. This inspired later deep NN architectures (Sec. 5.4, 5.11) used in certain modern award-winning Deep Learners (Sec. 5.19–
1965: Deep Networks Based on the Group Method of Data Handling (GMDH)
Networks trained by the Group Method of Data Handling (GMDH) (Ivakhnenko and Lapa, 1965;
Ivakhnenko et al., 1967; Ivakhnenko, 1968, 1971) were perhaps the first DL systems of the Feedforward
Multilayer Perceptron type. The units of GMDH nets may have polynomial activation functions implementing Kolmogorov-Gabor polynomials (more general than other widely used NN activation functions, Sec. 2). Given a training set, layers are incrementally grown and trained by regression analysis (e.g., Legendre, 1805; Gauss, 1809, 1821) (Sec. 5.1), then pruned with the help of a separate validation set (using today's terminology), where Decision Regularisation is used to weed out superfluous units (compare
Sec. 5.6.3). The numbers of layers and units per layer can be learned in problem-dependent fashion. To my knowledge, this was the first example of hierarchical representation learning in NNs (Sec. 4.3). A paper of 1971 already described a deep GMDH network with 8 layers (Ivakhnenko, 1971). There have been numerous applications of GMDH-style nets, e.g. (Ikeda et al., 1976; Farlow, 1984; Madala and Ivakhnenko, 1994; Ivakhnenko, 1995; Kondo, 1998; Kord´ık et al., 2003; Witczak et al., 2006; Kondo and Ueno, 2008).
1979: Convolution + Weight Replication + Subsampling (Neocognitron)
Apart from deep GMDH networks (Sec. 5.3), the Neocognitron (Fukushima, 1979, 1980, 2013a) was perhaps the first artificial NN that deserved the attribute deep, and the first to incorporate the neurophysiological insights of Sec. 5.2. It introduced convolutional NNs (today often called CNNs or convnets), where the (typically rectangular) receptive field of a convolutional unit with given weight vector is shifted step by step across a 2-dimensional array of input values, such as the pixels of an image. The resulting 2D array of subsequent activation events of this unit can then provide inputs to higher-level units, and so on. Due to massive weight replication (Sec. 2), relatively few parameters (Sec. 4.4) may be necessary to describe the behavior of such a convolutional layer.
Subsampling or downsampling layers consist of units whose fixed-weight connections originate from physical neighbours in the convolutional layers below. Subsampling units become active if at least one of their inputs is active; their responses are insensitive to certain small image shifts (compare Sec. 5.2).
The Neocognitron is very similar to the architecture of modern, contest-winning, purely supervised, feedforward, gradient-based Deep Learners with alternating convolutional and downsampling layers (e.g., Sec. 5.19–5.22). Fukushima, however, did not set the weights by supervised backpropagation(Sec. 5.5, 5.8), but by local, WTA-based unsupervised learning rules (e.g., Fukushima, 2013b), or by prewiring. In that sense he did not care for the DL problem (Sec. 5.9), although his architecture was comparatively deep indeed. For downsampling purposes he used Spatial Averaging (Fukushima, 1980, 2011) instead of Max-Pooling (MP, Sec. 5.11), currently a particularly convenient and popular WTA mechanism.
Today's CNN-based DL machines also profit a lot from later CNN work (e.g., Sec. 5.8, 5.16, 5.16, 5.19).
1960-1981 and Beyond: Development of Backpropagation (BP) for NNs
The minimisation of errors through gradient descent (Hadamard, 1908) in the parameter space of complex, nonlinear, differentiable (Leibniz, 1684), multi-stage, NN-related systems has been discussed at least since the early 1960s (e.g., Kelley, 1960; Bryson, 1961; Bryson and Denham, 1961; Pontryagin et al., 1961;
Dreyfus, 1962; Wilkinson, 1965; Amari, 1967; Bryson and Ho, 1969; Director and Rohrer, 1969), initially within the framework of Euler-LaGrange equations in the Calculus of Variations (e.g., Euler, 1744).
Steepest descent in the weight space of such systems can be performed (Bryson, 1961; Kelley, 1960;
Bryson and Ho, 1969) by iterating the ancient chain rule (Leibniz, 1676; L'Hˆopital, 1696) in Dynamic
Programming (DP) style (Bellman, 1957). A simplified derivation of this backpropagation method uses the chain rule only (Dreyfus, 1962).
The systems of the 1960s were already efficient in the DP sense. However, they backpropagated derivative information through standard Jacobian matrix calculations from one "layer" to the previous one, explicitly addressing neither direct links across several layers nor potential additional efficiency gains due to network sparsity (but perhaps such enhancements seemed obvious to the authors). Given all the prior work on learning in multilayer NN-like systems (see also Sec. 5.3 on deep nonlinear nets since 1965), it seems surprising in hindsight that a book (Minsky and Papert, 1969) on the limitations of simple linear perceptrons with a single layer (Sec. 5.1) discouraged some researchers from further studying NNs.
Explicit, efficient error backpropagation (BP) in arbitrary, discrete, possibly sparsely connected, NNlike networks apparently was first described in a 1970 master's thesis (Linnainmaa, 1970, 1976), albeit without reference to NNs. BP is also known as the reverse mode of automatic differentiation (Griewank, 2012), where the costs of forward activation spreading essentially equal the costs of backward derivative calculation. See early FORTRAN code (Linnainmaa, 1970) and closely related work (Ostrovskii et al., 1971). Efficient BP was soon explicitly used to minimize cost functions by adapting control parameters(weights) (Dreyfus, 1973). Compare some preliminary, NN-specific discussion (Werbos, 1974, section
5.5.1), a method for multilayer threshold NNs (Bobrowski, 1978), and a computer program for automatically deriving and implementing BP for given differentiable systems (Speelpenning, 1980).
To my knowledge, the first NN-specific application of efficient BP as above was described in 1981 (Werbos, 1981, 2006). Related work was published several years later (Parker, 1985; LeCun, 1985, 1988). A paper of 1986 significantly contributed to the popularisation of BP (Rumelhart et al., 1986). See generalisations for sequence-processing recurrent NNs (e.g., Williams, 1989; Robinson and Fallside, 1987;
Werbos, 1988; Williams and Zipser, 1988, 1989b,a; Rohwer, 1989; Pearlmutter, 1989; Gherrity, 1989;
Williams and Peng, 1990; Schmidhuber, 1992a; Pearlmutter, 1995; Baldi, 1995; Kremer and Kolen, 2001;
Atiya and Parlos, 2000), also for equilibrium RNNs (Almeida, 1987; Pineda, 1987) with stationary inputs.
BP for Weight-Sharing Feedforward NNs (FNNs) and Recurrent NNs (RNNs)
Using the notation of Sec. 2 for weight-sharing FNNs or RNNs, after an episode of activation spreading through differentiable ft, a single iteration of gradient descent through BP computes changes of all wi in proportion to ∂E
∂wi = � t
∂E
∂nett
∂nett
∂wi as in Algorithm 5.5.1 (for the additive case), where each weight wi is associated with a real-valued variable △i initialized by 0.
Alg. 5.5.1: One iteration of BP for weight-sharing FNNs or RNNs for t = T,..., 1 do to compute
∂E
∂nett, inititalize real-valued error signal variable δt by 0; if xt is an input event then continue with next iteration; if there is an error et then δt := xt − dt; add to δt the value � k∈outt wv(t,k)δk; (this is the elegant and efficient recursive chain rule application collecting impacts of nett on future events) multiply δt by f ′ t(nett); for all k ∈ int add to △wv(k,t) the value xkδt end for change each wi in proportion to △i and a small real-valued learning rate
The computational costs of the backward (BP) pass are essentially those of the forward pass (Sec. 2).
Forward and backward passes are re-iterated until sufficient performance is reached.
As of 2014, this simple BP method is still the central learning algorithm for FNNs and RNNs. Notably, most contest-winning NNs up to 2014 (Sec. 5.12, 5.14, 5.17, 5.19, 5.21, 5.22) did not augment supervised
BP by some sort of unsupervised learning as discussed in Sec. 5.7, 5.10, 5.15.
Late 1980s-2000 and Beyond: Numerous Improvements of NNs
By the late 1980s it seemed clear that BP by itself (Sec. 5.5) was no panacea. Most FNN applications focused on FNNs with few hidden layers. Additional hidden layers often did not seem to offer empirical benefits. Many practitioners found solace in a theorem (Kolmogorov, 1965a; Hecht-Nielsen, 1989; Hornik et al., 1989) stating that an NN with a single layer of enough hidden units can approximate any multivariate continous function with arbitrary accuracy.
Likewise, most RNN applications did not require backpropagating errors far. Many researchers helped their RNNs by first training them on shallow problems (Sec. 3) whose solutions then generalized to deeper problems.
In fact, some popular RNN algorithms restricted credit assignment to a single step backwards (Elman, 1990; Jordan, 1986, 1997), also in more recent studies (Jaeger, 2002; Maass et al., 2002;
Jaeger, 2004).
Generally speaking, although BP allows for deep problems in principle, it seemed to work only for shallow problems. The late 1980s and early 1990s saw a few ideas with a potential to overcome this problem, which was fully understood only in 1991 (Sec. 5.9).
Ideas for Dealing with Long Time Lags and Deep CAPs
To deal with long time lags between relevant events, several sequence processing methods were proposed, including Focused BP based on decay factors for activations of units in RNNs (Mozer, 1989, 1992), TimeDelay Neural Networks (TDNNs) (Lang et al., 1990) and their adaptive extension (Bodenhausen and Waibel, 1991), Nonlinear AutoRegressive with eXogenous inputs (NARX) RNNs (Lin et al., 1996), certain hierarchical RNNs (Hihi and Bengio, 1996) (compare Sec. 5.10), RL economies in RNNs with WTA units and local learning rules (Schmidhuber, 1989b), and other methods (e.g., Ring, 1993, 1994; Plate, 1993; de Vries and Principe, 1991; Sun et al., 1993a; Bengio et al., 1994). However, these algorithms either worked for shallow CAPs only, could not generalize to unseen CAP depths, had problems with greatly varying time lags between relevant events, needed external fine tuning of delay constants, or suffered from other problems. In fact, it turned out that certain simple but deep benchmark problems used to evaluate such methods are more quickly solved by randomly guessing RNN weights until a solution is found (Hochreiter and Schmidhuber, 1996).
While the RNN methods above were designed for DL of temporal sequences, the Neural Heat Exchanger (Schmidhuber, 1990c) consists of two parallel deep FNNs with opposite flow directions. Input patterns enter the first FNN and are propagated "up". Desired outputs (targets) enter the "opposite" FNN and are propagated "down". Using a local learning rule, each layer in each net tries to be similar (in information content) to the preceding layer and to the adjacent layer of the other net. The input entering the first net slowly "heats up" to become the target. The target entering the opposite net slowly "cools down" to become the input. The Helmholtz Machine (Dayan et al., 1995; Dayan and Hinton, 1996) may be viewed as an unsupervised (Sec. 5.6.4) variant thereof (Peter Dayan, personal communication, 1994).
A hybrid approach (Shavlik and Towell, 1989; Towell and Shavlik, 1994) initializes a potentially deep
FNN through a domain theory in propositional logic, which may be acquired through explanation-based learning (Mitchell et al., 1986; DeJong and Mooney, 1986; Minton et al., 1989). The NN is then finetuned through BP (Sec. 5.5). The NN's depth reflects the longest chain of reasoning in the original set of logical rules. An extension of this approach (Maclin and Shavlik, 1993; Shavlik, 1994) initializes an RNN by domain knowledge expressed as a Finite State Automaton (FSA). BP-based fine-tuning has become important for later DL systems pre-trained by UL, e.g., Sec. 5.10, 5.15.
Better BP Through Advanced Gradient Descent (Compare Sec. 5.24)
Numerous improvements of steepest descent through BP (Sec. 5.5) have been proposed. Least-squares methods (Gauss-Newton, Levenberg-Marquardt) (Gauss, 1809; Newton, 1687; Levenberg, 1944; Marquardt, 1963; Schaback and Werner, 1992) and quasi-Newton methods (Broyden-Fletcher-GoldfarbShanno, BFGS) (Broyden et al., 1965; Fletcher and Powell, 1963; Goldfarb, 1970; Shanno, 1970) are computationally too expensive for large NNs.
Partial BFGS (Battiti, 1992; Saito and Nakano, 1997) and conjugate gradient (Hestenes and Stiefel, 1952; Møller, 1993) as well as other methods (Solla, 1988;
Schmidhuber, 1989a; Cauwenberghs, 1993) provide sometimes useful fast alternatives. BP can be treated as a linear least-squares problem (Biegler-K¨onig and B¨armann, 1993), where second-order gradient information is passed back to preceding layers.
To speed up BP, momentum was introduced (Rumelhart et al., 1986), ad-hoc constants were added to the slope of the linearized activation function (Fahlman, 1988), or the nonlinearity of the slope was exaggerated (West and Saad, 1995).
Only the signs of the error derivatives are taken into account by the successful and widely used BP variant R-prop (Riedmiller and Braun, 1993) and the robust variation iRprop+ (Igel and H¨usken, 2003), which was also successfully applied to RNNs.
The local gradient can be normalized based on the NN architecture (Schraudolph and Sejnowski, 1996), through a diagonalized Hessian approach (Becker and Le Cun, 1989), or related efficient methods (Schraudolph, 2002).
Some algorithms for controlling BP step size adapt a global learning rate (Lapedes and Farber, 1986;
Vogl et al., 1988; Battiti, 1989; LeCun et al., 1993; Yu et al., 1995), while others compute individual learning rates for each weight (Jacobs, 1988; Silva and Almeida, 1990). In online learning, where BP is applied after each pattern presentation, the vario-η algorithm (Neuneier and Zimmermann, 1996) sets each weight's learning rate inversely proportional to the empirical standard deviation of its local gradient, thus normalizing the stochastic weight fluctuations. Compare a local online step size adaptation method for nonlinear NNs (Almeida et al., 1997).
Many additional tricks for improving NNs have been described (e.g., Orr and M¨uller, 1998; Montavon et al., 2012). Compare Sec. 5.6.3 and recent developments mentioned in Sec. 5.24.
Searching For Simple, Low-Complexity, Problem-Solving NNs (Sec. 5.24)
Many researchers used BP-like methods to search for "simple," low-complexity NNs (Sec. 4.4) with high generalization capability. Most approaches address the bias/variance dilemma (Geman et al., 1992) through strong prior assumptions. For example, weight decay (Hanson and Pratt, 1989; Weigend et al., 1991;
Krogh and Hertz, 1992) encourages near-zero weights, by penalizing large weights. In a Bayesian framework (Bayes, 1763), weight decay can be derived (Hinton and van Camp, 1993) from Gaussian or Laplacian weight priors (Gauss, 1809; Laplace, 1774); see also (Murray and Edwards, 1993). An extension of this approach postulates that a distribution of networks with many similar weights generated by Gaussian mixtures is "better" a priori (Nowlan and Hinton, 1992).
Often weight priors are implicit in additional penalty terms (MacKay, 1992) or in methods based on validation sets (Mosteller and Tukey, 1968; Stone, 1974; Eubank, 1988; Hastie and Tibshirani, 1990;
Craven and Wahba, 1979; Golub et al., 1979), Akaike's information criterion and final prediction error (Akaike, 1970, 1973, 1974), or generalized prediction error (Moody and Utans, 1994; Moody, 1992).
See also (Holden, 1994; Wang et al., 1994; Amari and Murata, 1993; Wang et al., 1994; Guyon et al., 1992;
Vapnik, 1992; Wolpert, 1994). Similar priors (or biases towards simplicity) are implicit in constructive and pruning algorithms, e.g., layer-by-layer sequential network construction (e.g., Ivakhnenko, 1968, 1971;
Ash, 1989; Moody, 1989; Gallant, 1988; Honavar and Uhr, 1988; Ring, 1991; Fahlman, 1991; Weng et al., 1992; Honavar and Uhr, 1993; Burgess, 1994; Fritzke, 1994; Parekh et al., 2000; Utgoff and Stracuzzi, 2002) (see also Sec. 5.3, 5.11), input pruning (Moody, 1992; Refenes et al., 1994), unit pruning (e.g., Ivakhnenko, 1968, 1971; White, 1989; Mozer and Smolensky, 1989; Levin et al., 1994), weight pruning, e.g., optimal brain damage (LeCun et al., 1990b), and optimal brain surgeon (Hassibi and Stork, 1993).
A very general but not always practical approach for discovering low-complexity SL NNs or RL NNs searches among weight matrix-computing programs written in a universal programming language, with a bias towards fast and short programs (Schmidhuber, 1997) (Sec. 6.7).
Flat Minimum Search (FMS) (Hochreiter and Schmidhuber, 1997a, 1999) searches for a "flat" minimum of the error function: a large connected region in weight space where error is low and remains approximately constant, that is, few bits of information are required to describe low-precision weights with high variance. Compare perturbation tolerance conditions (Minai and Williams, 1994; Murray and Edwards, 1993; Neti et al., 1992; Matsuoka, 1992; Bishop, 1993; Kerlirzin and Vallet, 1993; Carter et al., 1990). An MDL-based, Bayesian argument suggests that flat minima correspond to "simple" NNs and low expected overfitting. Compare Sec. 5.6.4 and more recent developments mentioned in Sec. 5.24.
Potential Benefits of UL for SL (Compare Sec. 5.7, 5.10, 5.15)
The notation of Sec. 2 introduced teacher-given labels dt. Many papers of the previous millennium, however, were about unsupervised learning (UL) without a teacher (e.g., Hebb, 1949; von der Malsburg, 1973;
Kohonen, 1972, 1982, 1988; Willshaw and von der Malsburg, 1976; Grossberg, 1976a,b; Watanabe, 1985;
Pearlmutter and Hinton, 1986; Barrow, 1987; Field, 1987; Oja, 1989; Barlow et al., 1989; Baldi and Hornik, 1989; Rubner and Tavan, 1989; Sanger, 1989; Ritter and Kohonen, 1989; Rubner and Schulten, 1990;
F¨oldi´ak, 1990; Martinetz et al., 1990; Kosko, 1990; Mozer, 1991; Palm, 1992; Atick et al., 1992; Miller, 1994; Saund, 1994; F¨oldi´ak and Young, 1995; Deco and Parra, 1997); see also post-2000 work (e.g., Carreira-Perpinan, 2001; Wiskott and Sejnowski, 2002; Franzius et al., 2007; Waydo and Koch, 2008).
Many UL methods are designed to maximize entropy-related, information-theoretic (Boltzmann, 1909;
Shannon, 1948; Kullback and Leibler, 1951) objectives (e.g., Linsker, 1988; Barlow et al., 1989; MacKay and Miller, 1990; Plumbley, 1991; Schmidhuber, 1992b,c; Schraudolph and Sejnowski, 1993; Redlich, 1993; Zemel, 1993; Zemel and Hinton, 1994; Field, 1994; Hinton et al., 1995; Dayan and Zemel, 1995;
Amari et al., 1996; Deco and Parra, 1997). Many do this to uncover and disentangle hidden underlying sources of signals (e.g., Jutten and Herault, 1991; Schuster, 1992; Andrade et al., 1993; Molgedey and Schuster, 1994; Comon, 1994; Cardoso, 1994; Bell and Sejnowski, 1995; Karhunen and Joutsensalo, 1995;
Belouchrani et al., 1997; Hyv¨arinen et al., 2001; Szab´o et al., 2006; Shan et al., 2007; Shan and Cottrell, Many UL methods automatically and robustly generate distributed, sparse representations of input patterns (F¨oldi´ak, 1990; Hinton and Ghahramani, 1997; Lewicki and Olshausen, 1998; Hyv¨arinen et al., 1999;
Hochreiter and Schmidhuber, 1999; Falconbridge et al., 2006) through well-known feature detectors (e.g., Olshausen and Field, 1996; Schmidhuber et al., 1996), such as off-center-on-surround-like structures, as well as orientation sensitive edge detectors and Gabor filters (Gabor, 1946). They extract simple features related to those observed in early visual pre-processing stages of biological systems (e.g., De Valois et al., 1982; Jones and Palmer, 1987).
UL can also serve to extract invariant features from different data items (e.g., Becker, 1991) through coupled NNs observing two different inputs (Schmidhuber and Prelinger, 1992), also called Siamese
NNs (e.g., Bromley et al., 1993; Hadsell et al., 2006; Taylor et al., 2011; Chen and Salman, 2011).
UL can help to encode input data in a form advantageous for further processing. In the context of DL, one important goal of UL is redundancy reduction. Ideally, given an ensemble of input patterns, redundancy reduction through a deep NN will create a factorial code (a code with statistically independent components) of the ensemble (Barlow et al., 1989; Barlow, 1989), to disentangle the unknown factors of variation (compare Bengio et al., 2013). Such codes may be sparse and can be advantageous for (1) data compression, (2) speeding up subsequent BP (Becker, 1991), (3) trivialising the task of subsequent naive yet optimal Bayes classifiers (Schmidhuber et al., 1996).
Most early UL FNNs had a single layer. Methods for deeper UL FNNs include hierarchical (Sec. 4.3) self-organizing Kohonen maps (e.g., Koikkalainen and Oja, 1990; Lampinen and Oja, 1992; Versino and Gambardella, 1996; Dittenbach et al., 2000; Rauber et al., 2002), hierarchical Gaussian potential function networks (Lee and Kil, 1991), the Self-Organising Tree Algorithm (SOTA) (Herrero et al., 2001), and nonlinear Autoencoders (AEs) with more than 3 (e.g., 5) layers (Kramer, 1991; Oja, 1991; DeMers and Cottrell, 1993). Such AE NNs (Rumelhart et al., 1986) can be trained to map input patterns to themselves, for example, by compactly encoding them through activations of units of a narrow bottleneck hidden layer.
Certain nonlinear AEs suffer from certain limitations (Baldi, 2012).
LOCOCODE (Hochreiter and Schmidhuber, 1999) uses FMS (Sec. 5.6.3) to find low-complexity AEs with low-precision weights describable by few bits of information, often producing sparse or factorial codes. Predictability Minimization (PM) (Schmidhuber, 1992c) searches for factorial codes through nonlinear feature detectors that fight nonlinear predictors, trying to become both as informative and as unpredictable as possible. PM-based UL was applied not only to FNNs but also to RNNs (e.g., Schmidhuber, 1993b; Lindst¨adt, 1993a,b). Compare Sec. 5.10 on UL-based RNN stacks (1991), as well as later UL
RNNs (e.g., Klapper-Rybicka et al., 2001; Steil, 2007).
1987: UL Through Autoencoder (AE) Hierarchies (Compare Sec. 5.15)
Perhaps the first work to study potential benefits of UL-based pre-training was published in 1987. It proposed unsupervised AE hierarchies (Ballard, 1987), closely related to certain post-2000 feedforward
Deep Learners based on UL (Sec. 5.15). The lowest-level AE NN with a single hidden layer is trained to map input patterns to themselves. Its hidden layer codes are then fed into a higher-level AE of the same type, and so on. The hope is that the codes in the hidden AE layers have properties that facilitate subsequent learning. In one experiment, a particular AE-specific learning algorithm (different from traditional BP of Sec. 5.5.1) was used to learn a mapping in an AE stack pre-trained by this type of UL (Ballard, 1987). This was faster than learning an equivalent mapping by BP through a single deeper AE without pre-training.
On the other hand, the task did not really require a deep AE, that is, the benefits of UL were not that obvious from this experiment. Compare an early survey (Hinton, 1989) and the somewhat related Recursive
Auto-Associative Memory (RAAM) (Pollack, 1988, 1990; Melnik et al., 2000), originally used to encode sequential linguistic structures of arbitrary size through a fixed number of hidden units. More recently, RAAMs were also used as unsupervised pre-processors to facilitate deep credit assignment for RL (Gisslen et al., 2011) (Sec. 6.4).
In principle, many UL methods (Sec. 5.6.4) could be stacked like the AEs above, the historycompressing RNNs of Sec. 5.10, the Restricted Boltzmann Machines (RBMs) of Sec. 5.15, or hierarchical
Kohonen nets (Sec. 5.6.4), to facilitate subsequent SL. Compare Stacked Generalization (Wolpert, 1992;
Ting and Witten, 1997), and FNNs that profit from pre-training by competitive UL (e.g., Rumelhart and Zipser, 1986) prior to BP-based fine-tuning (Maclin and Shavlik, 1995). See also more recent methods using UL to improve SL (e.g., Escalante-B. and Wiskott, 2013).
1989: BP for Convolutional NNs (CNNs, Sec. 5.4)
In 1989, backpropagation (Sec. 5.5) was applied (LeCun et al., 1989, 1990a, 1998) to Neocognitron-like, weight-sharing, convolutional neural layers (Sec. 5.4) with adaptive connections. This combination, augmented by Max-Pooling (MP, Sec. 5.11, 5.16), and sped up on graphics cards (Sec. 5.19), has become an essential ingredient of many modern, competition-winning, feedforward, visual Deep Learners (Sec. 5.19–
5.23). This work also introduced the MNIST data set of handwritten digits (LeCun et al., 1989), which over time has become perhaps the most famous benchmark of Machine Learning. CNNs helped to achieve good performance on MNIST (LeCun et al., 1990a) (CAP depth 5) and on fingerprint recognition (Baldi and Chauvin, 1993); similar CNNs were used commercially in the 1990s.
1991: Fundamental Deep Learning Problem of Gradient Descent
A diploma thesis (Hochreiter, 1991) represented a milestone of explicit DL research. As mentioned in Sec. 5.6, by the late 1980s, experiments had indicated that traditional deep feedforward or recurrent networks are hard to train by backpropagation (BP) (Sec. 5.5). Hochreiter's work formally identified a major reason: Typical deep NNs suffer from the now famous problem of vanishing or exploding gradients. With standard activation functions (Sec. 1), cumulative backpropagated error signals (Sec. 5.5.1) either shrink rapidly, or grow out of bounds. In fact, they decay exponentially in the number of layers or CAP depth(Sec. 3), or they explode. This is also known as the long time lag problem. Much subsequent DL research of the 1990s and 2000s was motivated by this insight. Later work (Bengio et al., 1994) also studied basins of attraction and their stability under noise from a dynamical systems point of view: either the dynamics are not robust to noise, or the gradients vanish. See also (Hochreiter et al., 2001a; Tiˇno and Hammer, 2004). Over the years, several ways of partially overcoming the Fundamental Deep Learning Problem were explored:
I A Very Deep Learner of 1991 (the History Compressor, Sec. 5.10) alleviates the problem through unsupervised pre-training for a hierarchy of RNNs. This greatly facilitates subsequent supervised credit assignment through BP (Sec. 5.5). Compare conceptually related AE stacks (Sec. 5.7) and Deep Belief Networks (DBNs) (Sec. 5.15) for the FNN case.
II LSTM-like networks (Sec. 5.13, 5.16, 5.17, 5.21–5.23) alleviate the problem through a special architecture unaffected by it.
III Today's GPU-based computers have a million times the computational power of desktop machines of the early 1990s. This allows for propagating errors a few layers further down within reasonable time, even in traditional NNs (Sec. 5.18). That is basically what is winning many of the image recognition competitions now (Sec. 5.19, 5.21, 5.22). (Although this does not really overcome the problem in a fundamental way.)
IV Hessian-free optimization (Sec. 5.6.2) can alleviate the problem for FNNs (Møller, 1993; Pearlmutter, 1994; Schraudolph, 2002; Martens, 2010) (Sec. 5.6.2) and RNNs (Martens and Sutskever, 2011)(Sec. 5.20).
V The space of NN weight matrices can also be searched without relying on error gradients, thus avoiding the Fundamental Deep Learning Problem altogether. Random weight guessing sometimes works better than more sophisticated methods (Hochreiter and Schmidhuber, 1996). Certain more complex problems are better solved by using Universal Search (Levin, 1973b) for weight matrixcomputing programs written in a universal programming language (Schmidhuber, 1997). Some are better solved by using linear methods to obtain optimal weights for connections to output events(Sec. 2), and evolving weights of connections to other events—this is called Evolino (Schmidhuber et al., 2007). Compare related RNNs pre-trained by certain UL rules (Steil, 2007), also in the case of spiking neurons (Klampfl and Maass, 2013) (Sec. 5.26). Direct search methods are relevant not only for SL but also for more general RL, and are discussed in more detail in Sec. 6.6.
1991: UL-Based History Compression Through a Deep Hierarchy of RNNs
A working Very Deep Learner (Sec. 3) of 1991 (Schmidhuber, 1992b, 2013a) could perform credit assignment across hundreds of nonlinear operators or neural layers, by using unsupervised pre-training for a stack of RNNs.
The basic idea is still relevant today. Each RNN is trained for a while in unsupervised fashion to predict its next input (e.g., Connor et al., 1994; Dorffner, 1996). From then on, only unexpected inputs (errors) convey new information and get fed to the next higher RNN which thus ticks on a slower, self-organising time scale. It can easily be shown that no information gets lost. It just gets compressed (much of machine learning is essentially about compression, e.g., Sec. 4.4, 5.6.3, 6.7). For each individual input sequence, we get a series of less and less redundant encodings in deeper and deeper levels of this History Compressor or Neural Sequence Chunker, which can compress data in both space (like feedforward NNs) and time.
This is another good example of hierarchical representation learning (Sec. 4.3). There also is a continuous variant of the history compressor (Schmidhuber et al., 1993).
The RNN stack is essentially a deep generative model of the data, which can be reconstructed from its compressed form. Adding another RNN to the stack improves a bound on the data's description length— equivalent to the negative logarithm of its probability (Huffman, 1952; Shannon, 1948)—as long as there is remaining local learnable predictability in the data representation on the corresponding level of the hierarchy. Compare a similar result for feedforward Deep Belief Networks (DBNs, 2006, Sec. 5.15).
The system was able to learn many previously unlearnable DL tasks. One ancient illustrative DL experiment (Schmidhuber, 1993b) required CAPs (Sec. 3) of depth 1200. The top level code of the initially unsupervised RNN stack, however, got so compact that (previously infeasible) sequence classification through additional BP-based SL became possible. Essentially the system used UL to greatly reduce problem depth. Compare earlier BP-based fine-tuning of NNs initialized by rules of propositional logic (Shavlik and Towell, 1989) (Sec. 5.6.1).
There is a way of compressing higher levels down into lower levels, thus fully or partially collapsing the RNN stack. The trick is to retrain a lower-level RNN to continually imitate (predict) the hidden units of an already trained, slower, higher-level RNN (the "conscious" chunker), through additional predictive output neurons (Schmidhuber, 1992b). This helps the lower RNN (the "automatizer") to develop appropriate, rarely changing memories that may bridge very long time lags. Again, this procedure can greatly reduce the required depth of the BP process.
The 1991 system was a working Deep Learner in the modern post-2000 sense, and also a first Neural Hierarchical Temporal Memory (HTM). It is conceptually similar to earlier AE hierarchies (1987, Sec. 5.7) and later Deep Belief Networks (2006, Sec. 5.15), but more general in the sense that it uses sequence-processing RNNs instead of FNNs with unchanging inputs. More recently, well-known entrepreneurs (Hawkins and George, 2006; Kurzweil, 2012) also got interested in HTMs; compare also hierarchical HMMs (e.g., Fine et al., 1998), as well as later UL-based recurrent systems (Klapper-Rybicka et al., 2001; Steil, 2007; Klampfl and Maass, 2013; Young et al., 2014). Clockwork RNNs (Koutn´ık et al., 2014) also consist of interacting RNN modules with different clock rates, but do not require UL to set those rates. Stacks of RNNs were used in later work on SL with great success, e.g., Sec. 5.13, 5.16, 5.17, 5.22.
1992: Max-Pooling (MP): Towards MPCNNs (Compare Sec. 5.16, 5.19)
The Neocognitron (Sec. 5.4) inspired the Cresceptron (Weng et al., 1992), which adapts its topology during training (Sec. 5.6.3); compare the incrementally growing and shrinking GMDH networks (1965, Sec. 5.3).
Instead of using alternative local subsampling or WTA methods (e.g., Fukushima, 1980; Schmidhuber, 1989b; Maass, 2000; Fukushima, 2013a), the Cresceptron uses Max-Pooling (MP) layers. Here a 2-dimensional layer or array of unit activations is partitioned into smaller rectangular arrays. Each is replaced in a downsampling layer by the activation of its maximally active unit. A later, more complex version of the Cresceptron (Weng et al., 1997) also included "blurring" layers to improve object location tolerance.
The neurophysiologically plausible topology of the feedforward HMAX model (Riesenhuber and Poggio, 1999) is very similar to the one of the 1992 Cresceptron (and thus to the 1979 Neocognitron). HMAX does not learn though. Its units have hand-crafted weights; biologically plausible learning rules were later proposed for similar models (e.g., Serre et al., 2002; Teichmann et al., 2012).
When CNNs or convnets (Sec. 5.4, 5.8) are combined with MP, they become Cresceptron-like or HMAX-like MPCNNs with alternating convolutional and max-pooling layers. Unlike Cresceptron and HMAX, however, MPCNNs are trained by BP (Sec. 5.5, 5.16) (Ranzato et al., 2007). Advantages of doing this were pointed out subsequently (Scherer et al., 2010). BP-trained MPCNNs have become central to many modern, competition-winning, feedforward, visual Deep Learners (Sec. 5.17, 5.19–5.23).
1994: Early Contest-Winning NNs
Back in the 1990s, certain NNs already won certain controlled pattern recognition contests with secret test sets. Notably, an NN with internal delay lines won the Santa Fe time-series competition on chaotic intensity pulsations of an NH3 laser (Wan, 1994; Weigend and Gershenfeld, 1993). No very deep CAPs (Sec. 3) were needed though.
1995: Supervised Recurrent Very Deep Learner (LSTM RNN)
Supervised Long Short-Term Memory (LSTM) RNN (Hochreiter and Schmidhuber, 1997b; Gers et al., 2000; P´erez-Ortiz et al., 2003) could eventually perform similar feats as the deep RNN hierarchy of 1991(Sec. 5.10), overcoming the Fundamental Deep Learning Problem (Sec. 5.9) without any unsupervised pretraining. LSTM could also learn DL tasks without local sequence predictability (and thus unlearnable by the partially unsupervised 1991 History Compressor, Sec. 5.10), dealing with very deep problems (Sec. 3) (e.g., Gers et al., 2002).
The basic LSTM idea is very simple. Some of the units are called Constant Error Carousels (CECs).
Each CEC uses as an activation function f, the identity function, and has a connection to itself with fixed weight of 1.0. Due to f's constant derivative of 1.0, errors backpropagated through a CEC cannot vanish or explode (Sec. 5.9) but stay as they are (unless they "flow out" of the CEC to other, typically adaptive parts of the NN). CECs are connected to several nonlinear adaptive units (some with multiplicative activation functions) needed for learning nonlinear behavior. Weight changes of these units often profit from error signals propagated far back in time through CECs. CECs are the main reason why LSTM nets can learn to discover the importance of (and memorize) events that happened thousands of discrete time steps ago, while previous RNNs already failed in case of minimal time lags of 10 steps.
Many different LSTM variants and topologies are allowed. It is possible to evolve good problemspecific topologies (Bayer et al., 2009). Some LSTM variants also use modifiable self-connections of CECs (Gers and Schmidhuber, 2001).
To a certain extent, LSTM is biologically plausible (O'Reilly, 2003). LSTM learned to solve many previously unlearnable DL tasks involving: Recognition of the temporal order of widely separated events in noisy input streams; Robust storage of high-precision real numbers across extended time intervals; Arithmetic operations on continuous input streams; Extraction of information conveyed by the temporal distance between events; Recognition of temporally extended patterns in noisy input sequences (Hochreiter and Schmidhuber, 1997b; Gers et al., 2000); Stable generation of precisely timed rhythms, as well as smooth and non-smooth periodic trajectories (Gers and Schmidhuber, 2000). LSTM clearly outperformed previous
RNNs on tasks that require learning the rules of regular languages describable by deterministic Finite State
Automata (FSAs) (Watrous and Kuhn, 1992; Casey, 1996; Siegelmann, 1992; Blair and Pollack, 1997;
Kalinke and Lehmann, 1998; Zeng et al., 1994; Manolios and Fanelli, 1994; Omlin and Giles, 1996; Vahed and Omlin, 2004), both in terms of reliability and speed.
LSTM also worked on tasks involving context free languages (CFLs) that cannot be represented by
HMMs or similar FSAs discussed in the RNN literature (Sun et al., 1993b; Wiles and Elman, 1995; Andrews et al., 1995; Steijvers and Grunwald, 1996; Tonkes and Wiles, 1997; Rodriguez et al., 1999; Rodriguez and Wiles, 1998). CFL recognition (Lee, 1996) requires the functional equivalent of a runtime stack. Some previous RNNs failed to learn small CFL training sets (Rodriguez and Wiles, 1998). Those that did not (Rodriguez et al., 1999; Bod´en and Wiles, 2000) failed to extract the general rules, and did not generalize well on substantially larger test sets. Similar for context-sensitive languages (CSLs) (e.g., Chalup and Blair, 2003). LSTM generalized well though, requiring only the 30 shortest exemplars (n ≤ 10) of the CSL anbncn to correctly predict the possible continuations of sequence prefixes for n up to 1000 and more. A combination of a decoupled extended Kalman filter (Kalman, 1960; Williams, 1992b; Puskorius and Feldkamp, 1994; Feldkamp et al., 1998; Haykin, 2001; Feldkamp et al., 2003) and an LSTM
RNN (P´erez-Ortiz et al., 2003) learned to deal correctly with values of n up to 10 million and more. That is, after training the network was able to read sequences of 30,000,000 symbols and more, one symbol at a time, and finally detect the subtle differences between legal strings such as a10,000,000b10,000,000c10,000,000 and very similar but illegal strings such as a10,000,000b9,999,999c10,000,000. Compare also more recent RNN algorithms able to deal with long time lags (Sch¨afer et al., 2006; Martens and Sutskever, 2011; Zimmermann et al., 2012; Koutn´ık et al., 2014).
Bi-directional RNNs (BRNNs) (Schuster and Paliwal, 1997; Schuster, 1999) are designed for input sequences whose starts and ends are known in advance, such as spoken sentences to be labeled by their phonemes; compare (Fukada et al., 1999). To take both past and future context of each sequence element into account, one RNN processes the sequence from start to end, the other backwards from end to start.
At each time step their combined outputs predict the corresponding label (if there is any). BRNNs were successfully applied to secondary protein structure prediction (Baldi et al., 1999). DAG-RNNs (Baldi and Pollastri, 2003; Wu and Baldi, 2008) generalize BRNNs to multiple dimensions. They learned to predict properties of small organic molecules (Lusci et al., 2013) as well as protein contact maps (Tegge et al., 2009), also in conjunction with a growing deep FNN (Di Lena et al., 2012) (Sec. 5.21). BRNNs and DAGRNNs unfold their full potential when combined with the LSTM concept (Graves and Schmidhuber, 2005, 2009; Graves et al., 2009).
Particularly successful in recent competitions are stacks (Sec. 5.10) of LSTM RNNs (Fernandez et al., 2007; Graves and Schmidhuber, 2009) trained by Connectionist Temporal Classification (CTC) (Graves et al., 2006), a gradient-based method for finding RNN weights that maximize the probability of teachergiven label sequences, given (typically much longer and more high-dimensional) streams of real-valued input vectors. CTC-LSTM performs simultaneous segmentation (alignment) and recognition (Sec. 5.22).
In the early 2000s, speech recognition was still dominated by HMMs combined with FNNs (e.g., Bourlard and Morgan, 1994). Nevertheless, when trained from scratch on utterances from the TIDIGITS speech database, in 2003 LSTM already obtained results comparable to those of HMM-based systems (Graves et al., 2003; Beringer et al., 2005; Graves et al., 2006). A decade later, LSTM achieved best known results on the famous TIMIT phoneme recognition benchmark (Graves et al., 2013) (Sec. 5.22). Besides speech recognition and keyword spotting (Fern´andez et al., 2007), important applications of LSTM include protein analysis (Hochreiter and Obermayer, 2005), robot localization (F¨orster et al., 2007) and 17 robot control (Mayer et al., 2008), handwriting recognition (Graves et al., 2008, 2009; Graves and Schmidhuber, 2009; Bluche et al., 2014), optical character recognition (Breuel et al., 2013), and others. RNNs can also be used for metalearning (Schmidhuber, 1987; Schaul and Schmidhuber, 2010; Prokhorov et al., 2002), because they can in principle learn to run their own weight change algorithm (Schmidhuber, 1993a). A successful metalearner (Hochreiter et al., 2001b) used an LSTM RNN to quickly learn a learning algorithm for quadratic functions (compare Sec. 6.8).
More recently, LSTM RNNs won several international pattern recognition competitions and set benchmark records on large and complex data sets, e.g., Sec. 5.17, 5.21, 5.22. Gradient-based LSTM is no panacea though—other methods sometimes outperformed it at least on certain tasks (Jaeger, 2004; Schmidhuber et al., 2007; Martens and Sutskever, 2011; Pascanu et al., 2013b; Koutn´ık et al., 2014); compare
Sec. 5.20.
2003: More Contest-Winning/Record-Setting NNs; Successful Deep NNs
In the decade around 2000, many practical and commercial pattern recognition applications were dominated by non-neural machine learning methods such as Support Vector Machines (SVMs) (Vapnik, 1995;
Sch¨olkopf et al., 1998). Nevertheless, at least in certain domains, NNs outperformed other techniques.
A Bayes NN (Neal, 2006) based on an ensemble (Breiman, 1996; Schapire, 1990; Wolpert, 1992;
Hashem and Schmeiser, 1992; Ueda, 2000; Dietterich, 2000a) of NNs won the NIPS 2003 Feature Selection Challenge with secret test set (Neal and Zhang, 2006). The NN was not very deep though—it had two hidden layers and thus rather shallow CAPs (Sec. 3) of depth 3.
Important for many present competition-winning pattern recognisers (Sec. 5.19, 5.21, 5.22) were developments in the CNN department. A BP-trained (LeCun et al., 1989) CNN (Sec. 5.4, Sec. 5.8) set a new MNIST record of 0.4% (Simard et al., 2003), using training pattern deformations (Baird, 1990) but no unsupervised pre-training (Sec. 5.7, 5.10, 5.15). A standard BP net achieved 0.7% (Simard et al., 2003).
Again, the corresponding CAP depth was low. Compare further improvements in Sec. 5.16, 5.18, 5.19.
Good image interpretation results (Behnke, 2003) were achieved with rather deep NNs trained by the BP variant R-prop (Riedmiller and Braun, 1993) (Sec. 5.6.2). FNNs with CAP depth up to 6 were used to successfully classify high-dimensional data (Vieira and Barradas, 2003).
Deep LSTM RNNs started to obtain certain first speech recognition results comparable to those of HMM-based systems (Graves et al., 2003); compare Sec. 5.13, 5.16, 5.21, 5.22.
2006/7: UL For Deep Belief Networks (DBNs) / AE Stacks Fine-Tuned by BP
While learning networks with numerous non-linear layers date back at least to 1965 (Sec. 5.3), and explicit
DL research results have been published at least since 1991 (Sec. 5.9, 5.10), the expression Deep Learning was actually coined around 2006, when unsupervised pre-training of deep FNNs helped to accelerate subsequent SL through BP (Hinton and Salakhutdinov, 2006; Hinton et al., 2006). Compare earlier terminology on loading deep networks (S´ıma, 1994; Windisch, 2005) and learning deep memories (Gomez and Schmidhuber, 2005). Compare also BP-based (Sec. 5.5) fine-tuning (Sec. 5.6.1) of (not so deep) FNNs pre-trained by competitive UL (Maclin and Shavlik, 1995).
The Deep Belief Network (DBN) is a stack of Restricted Boltzmann Machines (RBMs) (Smolensky, 1986), which in turn are Boltzmann Machines (BMs) (Hinton and Sejnowski, 1986) with a single layer of feature-detecting units; compare also Higher-Order BMs (Memisevic and Hinton, 2010). Each RBM perceives pattern representations from the level below and learns to encode them in unsupervised fashion. At least in theory under certain assumptions, adding more layers improves a bound on the data's negative log probability (Hinton et al., 2006) (equivalent to the data's description length—compare the corresponding observation for RNN stacks, Sec. 5.10). There are extensions for Temporal RBMs (Sutskever et al., 2008).
Without any training pattern deformations (Sec. 5.14), a DBN fine-tuned by BP achieved 1.2% error rate (Hinton and Salakhutdinov, 2006) on the MNIST handwritten digits (Sec. 5.8, 5.14). This result helped to arouse interest in DBNs. DBNs also achieved good results on phoneme recognition, with an error rate of 26.7% on the TIMIT core test set (Mohamed et al., 2009; Mohamed and Hinton, 2010); compare further improvements through FNNs (Hinton et al., 2012a; Deng and Yu, 2014) and LSTM RNNs (Sec. 5.22).
A DBN-based technique called Semantic Hashing (Salakhutdinov and Hinton, 2009) maps semantically similar documents (of variable size) to nearby addresses in a space of document representations. It outperformed previous searchers for similar documents, such as Locality Sensitive Hashing (Buhler, 2001;
Datar et al., 2004). See the RBM/DBN tutorial (Fischer and Igel, 2014).
Autoencoder (AE) stacks (Ballard, 1987) (Sec. 5.7) became a popular alternative way of pre-training deep FNNs in unsupervised fashion, before fine-tuning (Sec. 5.6.1) them through BP (Sec. 5.5) (Bengio et al., 2007; Vincent et al., 2008; Erhan et al., 2010). Sparse coding (Sec. 5.6.4) was formulated as a combination of convex optimization problems (Lee et al., 2007a). Recent surveys of stacked RBM and AE methods focus on post-2006 developments (Bengio, 2009; Arel et al., 2010). Unsupervised DBNs and AE stacks are conceptually similar to, but in a certain sense less general than, the unsupervised RNN stackbased History Compressor of 1991 (Sec. 5.10), which can process and re-encode not only stationary input patterns, but entire pattern sequences.
2006/7: Improved CNNs / GPU-CNNs / BP-Trained MPCNNs / LSTM Stacks
Also in 2006, a BP-trained (LeCun et al., 1989) CNN (Sec. 5.4, Sec. 5.8) set a new MNIST record of 0.39% (Ranzato et al., 2006), using training pattern deformations (Sec. 5.14) but no unsupervised pretraining. Compare further improvements in Sec. 5.18, 5.19. Similar CNNs were used for off-road obstacle avoidance (LeCun et al., 2006). A combination of CNNs and TDNNs later learned to map fixed-size representations of variable-size sentences to features relevant for language processing, using a combination of SL and UL (Collobert and Weston, 2008).
2006 also saw an early GPU-based CNN implementation (Chellapilla et al., 2006) up to 4 times faster than CPU-CNNs; compare also earlier GPU implementations of standard FNNs with a reported speed-up factor of 20 (Oh and Jung, 2004). GPUs or graphics cards have become more and more important for DL in subsequent years (Sec. 5.18–5.22).
In 2007, BP (Sec. 5.5) was applied for the first time (Ranzato et al., 2007) to Neocognitron-inspired(Sec. 5.4), Cresceptron-like (or HMAX-like) MPCNNs (Sec. 5.11) with alternating convolutional and maxpooling layers. BP-trained MPCNNs have become an essential ingredient of many modern, competitionwinning, feedforward, visual Deep Learners (Sec. 5.17, 5.19–5.23).
Also in 2007, hierarchical stacks of LSTM RNNs were introduced (Fernandez et al., 2007). They can be trained by hierarchical Connectionist Temporal Classification (CTC) (Graves et al., 2006). For tasks of sequence labelling, every LSTM RNN level (Sec. 5.13) predicts a sequence of labels fed to the next level.
Error signals at every level are back-propagated through all the lower levels. On spoken digit recognition, LSTM stacks outperformed HMMs, despite making fewer assumptions about the domain. LSTM stacks do not necessarily require unsupervised pre-training like the earlier UL-based RNN stacks (Schmidhuber, 1992b) of Sec. 5.10.
2009: First Official Competitions Won by RNNs, and with MPCNNs
Stacks of LSTM RNNs trained by CTC (Sec. 5.13, 5.16) became the first RNNs to win official international pattern recognition contests (with secret test sets known only to the organisers). More precisely, three connected handwriting competitions at ICDAR 2009 in three different languages (French, Arab, Farsi) were won by deep LSTM RNNs without any a priori linguistic knowledge, performing simultaneous segmentation and recognition. Compare (Graves and Schmidhuber, 2005; Graves et al., 2009; Schmidhuber et al., 2011; Graves et al., 2013; Graves and Jaitly, 2014) (Sec. 5.22).
To detect human actions in surveillance videos, a 3-dimensional CNN (e.g., Jain and Seung, 2009;
Prokhorov, 2010), combined with SVMs, was part of a larger system (Yang et al., 2009) using a bag of features approach (Nowak et al., 2006) to extract regions of interest. The system won three 2009 TRECVID competitions. These were possibly the first official international contests won with the help of (MP)CNNs(Sec. 5.16). An improved version of the method was published later (Ji et al., 2013).
2009 also saw a GPU-DBN implementation (Raina et al., 2009) orders of magnitudes faster than previous CPU-DBNs (see Sec. 5.15); see also (Coates et al., 2013). The Convolutional DBN (Lee et al., 2009a)(with a probabilistic variant of MP, Sec. 5.11) combines ideas from CNNs and DBNs, and was successfully applied to audio classification (Lee et al., 2009b).
2010: Plain Backprop (+ Distortions) on GPU Breaks MNIST Record
In 2010, a new MNIST (Sec. 5.8) record of 0.35% error rate was set by good old BP (Sec. 5.5) in deep but otherwise standard NNs (Ciresan et al., 2010), using neither unsupervised pre-training (e.g., Sec. 5.7, 5.10, 5.15) nor convolution (e.g., Sec. 5.4, 5.8, 5.14, 5.16). However, training pattern deformations (e.g., Sec. 5.14) were important to generate a big training set and avoid overfitting. This success was made possible mainly through a GPU implementation of BP that was up to 50 times faster than standard CPU versions. A good value of 0.95% was obtained without distortions except for small saccadic eye movement-like translations—compare Sec. 5.15.
Since BP was 3-5 decades old by then (Sec. 5.5), and pattern deformations 2 decades (Baird, 1990)(Sec. 5.14), these results seemed to suggest that advances in exploiting modern computing hardware were more important than advances in algorithms.
2011: MPCNNs on GPU Achieve Superhuman Vision Performance
In 2011, the first GPU-implementation (Ciresan et al., 2011a) of Max-Pooling (MP) CNNs or Convnets was described (the GPU-MPCNN), extending earlier work on MP (Weng et al., 1992) (Sec. 5.11) CNNs(Fukushima, 1979; LeCun et al., 1989) (Sec. 5.4, 5.8, 5.16), and on early GPU-based CNNs without
MP (Chellapilla et al., 2006) (Sec. 5.16); compare early GPU-NNs (Oh and Jung, 2004) and GPUDBNs (Raina et al., 2009) (Sec. 5.17). MPCNNs have alternating convolutional layers (Sec. 5.4) and max-pooling layers (MP, Sec. 5.11) topped by standard fully connected layers. All weights are trained by
BP (Sec. 5.5, 5.8, 5.16) (Ranzato et al., 2007; Scherer et al., 2010). GPU-MPCNNs have become essential for many contest-winning FNNs (Sec. 5.21, Sec. 5.22).
Multi-Column (MC) GPU-MPCNNs (Ciresan et al., 2011b) are committees (Breiman, 1996; Schapire, 1990; Wolpert, 1992; Hashem and Schmeiser, 1992; Ueda, 2000; Dietterich, 2000a) of GPU-MPCNNs with simple democratic output averaging. Several MPCNNs see the same input; their output vectors are used to assign probabilities to the various possible classes. The class with the on average highest probability is chosen as the system's classification of the present input. Compare earlier, more sophisticated ensemble methods (Schapire, 1990), the contest-winning ensemble Bayes-NN (Neal, 2006) of Sec. 5.14, and recent related work (Shao et al., 2014).
An MC-GPU-MPCNN was the first system to achieve superhuman visual pattern recognition (Ciresan et al., 2011b, 2012b) in a controlled competition, namely, the IJCNN 2011 traffic sign recognition contest in San Jose (CA) (Stallkamp et al., 2011, 2012). This is of interest for fully autonomous, self-driving cars in traffic (e.g., Dickmanns et al., 1994). The MC-GPU-MPCNN obtained 0.56% error rate and was twice better than human test subjects, three times better than the closest artificial NN competitor (Sermanet and LeCun, 2011), and six times better than the best non-neural method.
A few months earlier, the qualifying round was won in a 1st stage online competition, albeit by a much smaller margin: 1.02% (Ciresan et al., 2011b) vs 1.03% for second place (Sermanet and LeCun, 2011). After the deadline, the organisers revealed that human performance on the test set was 1.19%. That is, the best methods already seemed human-competitive. However, during the qualifying it was possible to incrementally gain information about the test set by probing it through repeated submissions. This is illustrated by better and better results obtained by various teams over time (Stallkamp et al., 2012) (the organisers eventually imposed a limit of 10 resubmissions). In the final competition this was not possible.
This illustrates a general problem with benchmarks whose test sets are public, or at least can be probed to some extent: competing teams tend to overfit on the test set even when it cannot be directly used for training, only for evaluation.
In 1997 many thought it a big deal that human chess world champion Kasparov was beaten by an IBM computer. But back then computers could not at all compete with little kids in visual pattern recognition, which seems much harder than chess from a computational perspective. Of course, the traffic sign domain is highly restricted, and kids are still much better general pattern recognisers. Nevertheless, by 2011, deep
NNs could already learn to rival them in important limited visual domains.
An MC-GPU-MPCNN was also the first method to achieve human-competitive performance (around
0.2%) on MNIST (Ciresan et al., 2012c). This represented a dramatic improvement, since by then the MNIST record had hovered around 0.4% for almost a decade (Sec. 5.14, 5.16, 5.18).
Given all the prior work on (MP)CNNs (Sec. 5.4, 5.8, 5.11, 5.16) and GPU-CNNs (Sec. 5.16), GPUMPCNNs are not a breakthrough in the scientific sense. But they are a commercially relevant breakthrough in efficient coding that has made a difference in several contests since 2011. Today, most feedforward competition-winning deep NNs are GPU-MPCNNs (Sec. 5.21–5.23).
2011: Hessian-Free Optimization for RNNs
Also in 2011 it was shown (Martens and Sutskever, 2011) that Hessian-free optimization (e.g., Møller, 1993; Pearlmutter, 1994; Schraudolph, 2002) (Sec. 5.6.2) can alleviate the Fundamental Deep Learning
Problem (Sec. 5.9) in RNNs, outperforming standard gradient-based LSTM RNNs (Sec. 5.13) on several tasks. Compare other RNN algorithms (Jaeger, 2004; Schmidhuber et al., 2007; Pascanu et al., 2013b;
Koutn´ık et al., 2014) that also at least sometimes yield better results than steepest descent for LSTM RNNs.
2012: First Contests Won on ImageNet & Object Detection & Segmentation
In 2012, an ensemble of GPU-MPCNNs (Sec. 5.19) achieved best results on the ImageNet classification benchmark (Krizhevsky et al., 2012), which is popular in the computer vision community. Here relatively large image sizes of 256x256 pixels were necessary, as opposed to only 48x48 pixels for the 2011 traffic sign competition (Sec. 5.19). See further improvements in Sec. 5.22.
Also in 2012, the biggest NN so far (109 free parameters) was trained in unsupervised mode (Sec. 5.7, 5.15) on unlabeled data (Le et al., 2012), then applied to ImageNet. The codes across its top layer were used to train a simple supervised classifier, which achieved best results so far on 20,000 classes. Instead of relying on efficient GPU programming, this was done by brute force on 1,000 standard machines with
16,000 cores.
So by 2011/2012, excellent results had been achieved by Deep Learners in image recognition and classification (Sec. 5.19, 5.21). The computer vision community, however, is especially interested in object detection in large images, for applications such as image-based search engines, or for biomedical diagnosis where the goal may be to automatically detect tumors etc in images of human tissue. Object detection presents additional challenges. One natural approach is to train a deep NN classifier on patches of big images, then use it as a feature detector to be shifted across unknown visual scenes, using various rotations and zoom factors. Image parts that yield highly active output units are likely to contain objects similar to those the NN was trained on.
2012 finally saw the first DL system (an MC-GPU-MPCNN, Sec. 5.19) to win a contest on visual object detection (Ciresan et al., 2013) in large images of several million pixels (ICPR 2012 Contest on Mitosis
Detection in Breast Cancer Histological Images, 2012; Roux et al., 2013). Such biomedical applications may turn out to be among the most important applications of DL. The world spends over 10% of GDP on healthcare (> 6 trillion USD per year), much of it on medical diagnosis through expensive experts. Partial automation of this could not only save lots of money, but also make expert diagnostics accessible to many who currently cannot afford it. It is gratifying to observe that today deep NNs may actually help to improve healthcare and perhaps save human lives.
2012 also saw the first pure image segmentation contest won by DL (Ciresan et al., 2012a), again through an MC-GPU-MPCNN (Segmentation of Neuronal Structures in EM Stacks Challenge, 2012).2
EM stacks are relevant for the recently approved huge brain projects in Europe and the US (e.g., Markram, 2012). Given electron microscopy images of stacks of thin slices of animal brains, the goal is to build a detailed 3D model of the brain's neurons and dendrites. But human experts need many hours and days and weeks to annotate the images: Which parts depict neuronal membranes? Which parts are irrelevant background? This needs to be automated (e.g., Turaga et al., 2010). Deep MC-GPU-MPCNNs learned to solve this task through experience with many training images, and won the contest on all three evaluation metrics by a large margin, with superhuman performance in terms of pixel error.
Both object detection (Ciresan et al., 2013) and image segmentation (Ciresan et al., 2012a) profit from fast MPCNN-based image scans that avoid redundant computations. Recent MPCNN scanners speed up
2It should be mentioned, however, that LSTM RNNs already performed simultaneous segmentation and recognition when they became the first recurrent Deep Learners to win official international pattern recognition contests—see Sec. 5.17.
21 naive implementations by up to three orders of magnitude (Masci et al., 2013; Giusti et al., 2013); compare earlier efficient methods for CNNs without MP (Vaillant et al., 1994).
Also in 2012, a system consisting of growing deep FNNs and 2D-BRNNs (Di Lena et al., 2012) won the CASP 2012 contest on protein contact map prediction. On the IAM-OnDoDB benchmark, LSTM
RNNs (Sec. 5.13) outperformed all other methods (HMMs, SVMs) on online mode detection (Otte et al., 2012; Indermuhle et al., 2012) and keyword spotting (Indermuhle et al., 2011). On the long time lag problem of language modelling, LSTM RNNs outperformed all statistical approaches on the IAM-DB benchmark (Frinken et al., 2012); improved results were later obtained through a combination of NNs and HMMs (Zamora-Martnez et al., 2014). Compare other recent RNNs for object recognition (Wyatte et al., 2012; OReilly et al., 2013), extending earlier work on biologically plausible learning rules for RNNs (O'Reilly, 1996).
2013-: More Contests and Benchmark Records
A stack (Fernandez et al., 2007; Graves and Schmidhuber, 2009) (Sec. 5.10) of bi-directional LSTM recurrent NNs (Graves and Schmidhuber, 2005) trained by CTC (Sec. 5.13, 5.17) broke a famous TIMIT speech(phoneme) recognition record, achieving 17.7% test set error rate (Graves et al., 2013), despite thousands of man years previously spent on Hidden Markov Model (HMMs)-based speech recognition research. Compare earlier DBN results (Sec. 5.15). CTC-LSTM also helped to score first at NIST's OpenHaRT2013 evaluation (Bluche et al., 2014). For Optical Character Recognition (OCR), LSTM RNNs outperformed commercial recognizers of historical data (Breuel et al., 2013).
A new record on the ICDAR Chinese handwriting recognition benchmark (over 3700 classes) was set on a desktop machine by an MC-GPU-MPCNN (Sec. 5.19) with almost human performance (Ciresan and Schmidhuber, 2013); compare (Yin et al., 2013).
The MICCAI 2013 Grand Challenge on Mitosis Detection (Veta et al., 2013) also was won by an objectdetecting MC-GPU-MPCNN (Ciresan et al., 2013). Its data set was even larger and more challenging than the one of ICPR 2012 (Sec. 5.21): a real-world dataset including many ambiguous cases and frequently encountered problems such as imperfect slide staining.
Three 2D-CNNs (with mean-pooling instead of MP, Sec. 5.11) observing three orthogonal projections of 3D images outperformed traditional full 3D methods on the task of segmenting tibial cartilage in low field knee MRI scans (Prasoon et al., 2013).
Deep GPU-MPCNNs (Sec. 5.19) also helped to achieve new best results on important benchmarks of the computer vision community: ImageNet classification (Zeiler and Fergus, 2013) and—in conjunction with traditional approaches—PASCAL object detection (Girshick et al., 2013). They also learned to predict bounding box coordinates of objects in the Imagenet 2013 database, and obtained state-of-the-art results on tasks of localization and detection (Sermanet et al., 2013). GPU-MPCNNs also helped to recognise multidigit numbers in Google Street View images (Goodfellow et al., 2014b), where part of the NN was trained to count visible digits; compare earlier work on detecting "numerosity" through DBNs (Stoianov and Zorzi, 2012). This system also excelled at recognising distorted synthetic text in reCAPTCHA puzzles. Other successful CNN applications include scene parsing (Farabet et al., 2013), object detection (Szegedy et al., 2013), shadow detection (Khan et al., 2014), video classification (Karpathy et al., 2014), and Alzheimers disease neuroimaging (Li et al., 2014).
Additional contests are mentioned in the web pages of the Swiss AI Lab IDSIA, the University of Toronto, NY University, and the University of Montreal. (Unlike in most academic contests, winners of contests listed at the commercial web site kaggle.com have to hand their code over to companies.)
Currently Successful Supervised Techniques: LSTM RNNs / GPU-MPCNNs
Most competition-winning or benchmark record-setting Deep Learners actually use one of two supervised techniques: (a) recurrent LSTM (1997) trained by CTC (2006) (Sec. 5.13, 5.17, 5.21, 5.22), or (b) feedforward GPU-MPCNNs (2011, Sec. 5.19, 5.21, 5.22) based on CNNs (1979, Sec. 5.4) with MP (1992, Sec. 5.11) trained through BP (1989–2007, Sec. 5.8, 5.16).
Exceptions include two 2011 contests (Goodfellow et al., 2011; Mesnil et al., 2011; Goodfellow et al., 2012) specialised on Transfer Learning from one dataset to another (e.g., Caruana, 1997; Schmidhuber, 2004; Pan and Yang, 2010). However, deep GPU-MPCNNs do allow for pure SL-based transfer (Ciresan et al., 2012d), where pre-training on one training set greatly improves performance on quite different sets, also in more recent studies (Oquab et al., 2013; Donahue et al., 2013). In fact, deep MPCNNs pre-trained by SL can extract useful features from quite diverse off-training-set images, yielding better results than traditional, widely used features such as SIFT (Lowe, 1999, 2004) on many vision tasks (Razavian et al., 2014). To deal with changing datasets, slowly learning deep NNs were also combined with rapidly adapting
"surface" NNs (Kak et al., 2010).
Remarkably, in the 1990s a trend went from partially unsupervised RNN stacks (Sec. 5.10) to purely supervised LSTM RNNs (Sec. 5.13), just like in the 2000s a trend went from partially unsupervised FNN stacks (Sec. 5.15) to purely supervised MPCNNs (Sec. 5.16–5.22). Nevertheless, in many applications it can still be advantageous to combine the best of both worlds—supervised learning and unsupervised pre-training (Sec. 5.10, 5.15).
Recent Tricks for Improving SL Deep NNs (Compare Sec. 5.6.2, 5.6.3)
DBN training (Sec. 5.15) can be improved through gradient enhancements and automatic learning rate adjustments during stochastic gradient descent (Cho et al., 2013; Cho, 2014), and through Tikhonovtype (Tikhonov et al., 1977) regularization of RBMs (Cho et al., 2012). Contractive AEs (Rifai et al., 2011) discourage hidden unit perturbations in response to input perturbations, similar to how FMS (Sec. 5.6.3) for LOCOCODE AEs (Sec. 5.6.4) discourages output perturbations in response to weight perturbations.
Dropout (Hinton et al., 2012b; Ba and Frey, 2013) removes units from NNs during training to improve generalisation. Some view it as an ensemble method that trains multiple data models simultaneously (Baldi and Sadowski, 2014). Under certain circumstances, it could also be viewed as a form of training set augmentation: effectively, more and more informative complex features are removed from the training data.
Compare dropout for RNNs (Pham et al., 2013; Pachitariu and Sahani, 2013; Pascanu et al., 2013a). A deterministic approximation coined fast dropout (Wang and Manning, 2013) can lead to faster learning and evaluation and was adapted for RNNs (Bayer et al., 2013). Dropout is closely related to older, biologically plausible techniques for adding noise to neurons or synapses during training (e.g., Murray and Edwards, 1993; Schuster, 1992; Nadal and Parga, 1994; Jim et al., 1995; An, 1996), which in turn are closely related to finding perturbation-resistant low-complexity NNs, e.g., through FMS (Sec. 5.6.3). MDL-based stochastic variational methods (Graves, 2011) are also related to FMS. They are useful for RNNs, where classic regularizers such as weight decay (Sec. 5.6.3) represent a bias towards limited memory capacity (e.g., Pascanu et al., 2013b).
The activation function f of Rectified Linear Units (ReLUs) is f(x) = x for x > 0, f(x) = 0 otherwise. ReLU NNs are useful for RBMs (Nair and Hinton, 2010; Maas et al., 2013), outperformed sigmoidal activation functions in deep NNs (Glorot et al., 2011), and helped to obtain best results on several benchmark problems across multiple domains (e.g., Krizhevsky et al., 2012; Dahl et al., 2013).
NNs with competing linear units tend to outperform those with non-competing nonlinear units, and avoid catastrophic forgetting through BP when training sets change over time (Srivastava et al., 2013).
In this context, choosing a learning algorithm may be more important than choosing activation functions (Goodfellow et al., 2014a). Maxout NNs (Goodfellow et al., 2013) combine competitive interactions and dropout (see above) to achieve excellent results on certain benchmarks. Compare early RNNs with competing units for SL and RL (Schmidhuber, 1989b). To address overfitting, instead of depending on pre-wired regularizers and hyper-parameters (Hertz et al., 1991; Bishop, 2006), self-delimiting RNNs(SLIM NNs) with competing units (Schmidhuber, 2012) can in principle learn to select their own runtime and their own numbers of effective free parameters, thus learning their own computable regularisers(Sec. 4.4, 5.6.3), becoming fast and slim when necessary. One may penalize the task-specific total length of connections (e.g., Legenstein and Maass, 2002; Schmidhuber, 2012, 2013b; Clune et al., 2013) and communication costs of SLIM NNs implemented on the 3-dimensional brain-like multi-processor hardware to be expected in the future.
RmsProp (Tieleman and Hinton, 2012; Schaul et al., 2013) can speed up first order gradient descent methods (Sec. 5.5, 5.6.2); compare vario-η (Neuneier and Zimmermann, 1996), Adagrad (Duchi et al., 2011) and Adadelta (Zeiler, 2012). DL in NNs can also be improved by transforming hidden unit activations such that they have zero output and slope on average (Raiko et al., 2012). Many additional, older
23 tricks (Sec. 5.6.2, 5.6.3) should also be applicable to today's deep NNs; compare (Orr and M¨uller, 1998;
Montavon et al., 2012).
Consequences for Neuroscience
It is ironic that artificial NNs (ANNs) can help to better understand biological NNs (BNNs)—see the ISBI
2012 results mentioned in Sec. 5.21 (Segmentation of Neuronal Structures in EM Stacks Challenge, 2012;
Ciresan et al., 2012a).
The feature detectors learned by single-layer visual ANNs are similar to those found in early visual processing stages of BNNs (e.g., Sec. 5.6.4). Likewise, the feature detectors learned in deep layers of visual
ANNs should be highly predictive of what neuroscientists will find in deep layers of BNNs. While the visual cortex of BNNs may use quite different learning algorithms, its objective function to be minimised may be quite similar to the one of visual ANNs. In fact, results obtained with relatively deep artificial
DBNs (Lee et al., 2007b) and CNNs (Yamins et al., 2013) seem compatible with insights about the visual pathway in the primate cerebral cortex, which has been studied for many decades (e.g., Hubel and Wiesel, 1968; Perrett et al., 1982; Desimone et al., 1984; Felleman and Van Essen, 1991; Perrett et al., 1992;
Kobatake and Tanaka, 1994; Logothetis et al., 1995; Bichot et al., 2005; Hung et al., 2005; Lennie and Movshon, 2005; Connor et al., 2007; Kriegeskorte et al., 2008; DiCarlo et al., 2012); compare a computer vision-oriented survey (Kruger et al., 2013).
DL with Spiking Neurons?
Many recent DL results profit from GPU-based traditional deep NNs, e.g., Sec. 5.16–5.19. Current GPUs, however, are little ovens, much hungrier for energy than biological brains, whose neurons efficiently communicate by brief spikes (Hodgkin and Huxley, 1952; FitzHugh, 1961; Nagumo et al., 1962), and often remain quiet. Many computational models of such spiking neurons have been proposed and analyzed (e.g., Gerstner and van Hemmen, 1992; Zipser et al., 1993; Stemmler, 1996; Tsodyks et al., 1996; Maex and Orban, 1996; Maass, 1996, 1997; Kistler et al., 1997; Amit and Brunel, 1997; Tsodyks et al., 1998; Kempter et al., 1999; Song et al., 2000; Stoop et al., 2000; Brunel, 2000; Bohte et al., 2002; Gerstner and Kistler, 2002; Izhikevich et al., 2003; Seung, 2003; Deco and Rolls, 2005; Brette et al., 2007; Brea et al., 2013;
Nessler et al., 2013; Kasabov, 2014; Hoerzer et al., 2014; Rezende and Gerstner, 2014).
Future energy-efficient hardware for DL in NNs may implement aspects of such models; see (e.g., Liu et al., 2001; Roggen et al., 2003; Glackin et al., 2005; Schemmel et al., 2006; Fieres et al., 2008; Khan et al., 2008; Serrano-Gotarredona et al., 2009; Jin et al., 2010; Indiveri et al., 2011; Neil and Liu, 2014). A simulated, event-driven, spiking variant (Neftci et al., 2014) of an RBM (Sec. 5.15) was trained by a variant of the Contrastive Divergence algorithm (Hinton, 2002). A spiking DBN with about 250,000 neurons (as part of a larger NN; Eliasmith et al., 2012; Eliasmith, 2013) achieved 6% error rate on MNIST; compare similar results with a spiking DBN variant of depth 3 using a neuromorphic event-based sensor (O'Connor et al., 2013). In practical applications, however, current artificial networks of spiking neurons cannot yet compete with the best traditional deep NNs (e.g., compare MNIST results of Sec. 5.19).
DL in FNNs and RNNs for Reinforcement Learning (RL)
So far we have focused on Deep Learning (DL) in supervised or unsupervised NNs. Such NNs learn to perceive / encode / predict / classify patterns or pattern sequences, but they do not learn to act in the more general sense of Reinforcement Learning (RL) in unknown environments (see surveys, e.g., Kaelbling et al., 1996; Sutton and Barto, 1998; Wiering and van Otterlo, 2012). Here we add a discussion of DL FNNs and RNNs for RL. It will be shorter than the discussion of FNNs and RNNs for SL and UL (Sec. 5), reflecting the current size of the various fields.
Without a teacher, solely from occasional real-valued pain and pleasure signals, RL agents must discover how to interact with a dynamic, initially unknown environment to maximize their expected cumulative reward signals (Sec. 2). There may be arbitrary, a priori unknown delays between actions and perceivable consequences. The problem is as hard as any problem of computer science, since any task with a computable description can be formulated in the RL framework (e.g., Hutter, 2005). For example, an answer to the famous question of whether P = NP (Levin, 1973b; Cook, 1971) would also set limits for what is achievable by general RL. Compare more specific limitations, e.g., (Blondel and Tsitsiklis, 2000; Madani et al., 2003; Vlassis et al., 2012). The following subsections mostly focus on certain obvious intersections between DL and RL—they cannot serve as a general RL survey.
RL Through NN World Models Yields RNNs With Deep CAPs
In the special case of an RL FNN controller C interacting with a deterministic, predictable environment, a separate FNN called M can learn to become C's world model through system identification, predicting C's inputs from previous actions and inputs (e.g., Werbos, 1981, 1987; Munro, 1987; Jordan, 1988; Werbos, 1989b,a; Robinson and Fallside, 1989; Jordan and Rumelhart, 1990; Schmidhuber, 1990d; Narendra and Parthasarathy, 1990; Werbos, 1992; Gomi and Kawato, 1993; Cochocki and Unbehauen, 1993; Levin and Narendra, 1995; Miller et al., 1995; Ljung, 1998; Prokhorov et al., 2001; Ge et al., 2010). Assume M has learned to produce accurate predictions. We can use M to substitute the environment. Then M and C form an RNN where M's outputs become inputs of C, whose outputs (actions) in turn become inputs of M. Now BP for RNNs (Sec. 5.5.1) can be used to achieve desired input events such as high real-valued reward signals: While M's weights remain fixed, gradient information for C's weights is propagated back through M down into C and back through M etc. To a certain extent, the approach is also applicable in probabilistic or uncertain environments, as long as the inner products of M's C-based gradient estimates and M's "true" gradients tend to be positive.
In general, this approach implies deep CAPs for C, unlike in DP-based traditional RL (Sec. 6.2).
Decades ago, the method was used to learn to back up a model truck (Nguyen and Widrow, 1989). An
RL active vision system used it to learn sequential shifts (saccades) of a fovea, to detect targets in visual scenes (Schmidhuber and Huber, 1991), thus learning to control selective attention. Compare RL-based attention learning without NNs (Whitehead, 1992).
To allow for memories of previous events in partially observable worlds (Sec. 6.3), the most general variant of this technique uses RNNs instead of FNNs to implement both M and C (Schmidhuber, 1990d, 1991c; Feldkamp and Puskorius, 1998). This may cause deep CAPs not only for C but also for M.
M can also be used to optimize expected reward by planning future action sequences (Schmidhuber, 1990d). In fact, the winners of the 2004 RoboCup World Championship in the fast league (Egorova et al., 2004) trained NNs to predict the effects of steering signals on fast robots with 4 motors for 4 different wheels. During play, such NN models were used to achieve desirable subgoals, by optimizing action sequences through quickly planning ahead. The approach also was used to create self-healing robots able to compensate for faulty motors whose effects do not longer match the predictions of the NN models (Gloye et al., 2005; Schmidhuber, 2007).
Typically M is not given in advance. Then an essential question is: which experiments should C conduct to quickly improve M? The Formal Theory of Fun and Creativity (e.g., Schmidhuber, 2006a, 2013b) formalizes driving forces and value functions behind such curious and exploratory behavior: A measure of the learning progress of M becomes the intrinsic reward of C (Schmidhuber, 1991a); compare (Singh et al., 2005; Oudeyer et al., 2013). This motivates C to create action sequences (experiments) such that M makes quick progress.
Deep FNNs for Traditional RL and Markov Decision Processes (MDPs)
The classical approach to RL (Samuel, 1959; Bertsekas and Tsitsiklis, 1996) makes the simplifying assumption of Markov Decision Processes (MDPs): the current input of the RL agent conveys all information necessary to compute an optimal next output event or decision. This allows for greatly reducing CAP depth in RL NNs (Sec. 3, 6.1) by using the Dynamic Programming (DP) trick (Bellman, 1957). The latter is often explained in a probabilistic framework (e.g., Sutton and Barto, 1998), but its basic idea can already be conveyed in a deterministic setting. For simplicity, using the notation of Sec. 2, let input events xt encode the entire current state of the environment, including a real-valued reward rt (no need to introduce additional vector-valued notation, since real values can encode arbitrary vectors of real values). The original RL goal (find weights that maximize the sum of all rewards of an episode) is replaced by an equivalent set of alternative goals set by a real-valued value function V defined on input events. Consider any two subsequent input events xt, xk. Recursively define V (xt) = rt + V (xk), where V (xk) = rk if xk is the last input event. Now search for weights that maximize the V of all input events, by causing appropriate output events or actions.
Due to the Markov assumption, an FNN suffices to implement the policy that maps input to output events. Relevant CAPs are not deeper than this FNN. V itself is often modeled by a separate FNN (also yielding typically short CAPs) learning to approximate V (xt) only from local information rt, V (xk).
Many variants of traditional RL exist (e.g., Barto et al., 1983; Watkins, 1989; Watkins and Dayan, 1992; Moore and Atkeson, 1993; Schwartz, 1993; Baird, 1994; Rummery and Niranjan, 1994; Singh, 1994; Baird, 1995; Kaelbling et al., 1995; Peng and Williams, 1996; Mahadevan, 1996; Tsitsiklis and van Roy, 1996; Bradtke et al., 1996; Santamar´ıa et al., 1997; Prokhorov and Wunsch, 1997; Sutton and Barto, 1998; Wiering and Schmidhuber, 1998b; Baird and Moore, 1999; Meuleau et al., 1999; Morimoto and Doya, 2000; Bertsekas, 2001; Brafman and Tennenholtz, 2002; Abounadi et al., 2002; Lagoudakis and Parr, 2003; Sutton et al., 2008; Maei and Sutton, 2010; van Hasselt, 2012). Most are formulated in a probabilistic framework, and evaluate pairs of input and output (action) events (instead of input events only). To facilitate certain mathematical derivations, some discount delayed rewards, but such distortions of the original RL problem are problematic.
Perhaps the most well-known RL NN is the world-class RL backgammon player (Tesauro, 1994), which achieved the level of human world champions by playing against itself. Its nonlinear, rather shallow FNN maps a large but finite number of discrete board states to values. More recently, a rather deep GPU-CNN was used in a traditional RL framework to play several Atari 2600 computer games directly from 84x84 pixel 60 Hz video input (Mnih et al., 2013), using experience replay (Lin, 1993), extending previous work on Neural Fitted Q-Learning (NFQ) (Riedmiller, 2005). Compare RBM-based RL (Sallans and Hinton, 2004) with high-dimensional inputs (Elfwing et al., 2010), earlier RL Atari players (Gr¨uttner et al., 2010), and an earlier, raw video-based RL NN for computer games (Koutn´ık et al., 2013) trained by Indirect Policy
Search (Sec. 6.7).
Deep RL RNNs for Partially Observable MDPs (POMDPs)
The Markov assumption (Sec. 6.2) is often unrealistic. We cannot directly perceive what is behind our back, let alone the current state of the entire universe. However, memories of previous events can help to deal with partially observable Markov decision problems (POMDPs) (e.g., Schmidhuber, 1990d, 1991c; Ring, 1991, 1993, 1994; Williams, 1992a; Lin, 1993; Teller, 1994; Kaelbling et al., 1995; Littman et al., 1995; Boutilier and Poole, 1996; Littman, 1996; Jaakkola et al., 1995; McCallum, 1996; Kimura et al., 1997; Wiering and Schmidhuber, 1996, 1998a; Otsuka et al., 2010). A naive way of implementing memories without leaving the MDP framework (Sec. 6.2) would be to simply consider a possibly huge state space, namely, the set of all possible observation histories and their prefixes. A more realistic way is to use function approximators such as RNNs that produce compact state features as a function of the entire history seen so far. Generally speaking, POMDP RL often uses DL RNNs to learn which events to memorize and which to ignore. Three basic alternatives are:
1. Use an RNN as a value function mapping arbitrary event histories to values (e.g., Schmidhuber, 1990b, 1991c; Lin, 1993; Bakker, 2002). For example, deep LSTM RNNs were used in this way for
RL robots (Bakker et al., 2003).
2. Use an RNN controller in conjunction with a second RNN as predictive world model, to obtain a combined RNN with deep CAPs—see Sec. 6.1.
3. Use an RNN for RL by Direct Search (Sec. 6.6) or Indirect Search (Sec. 6.7) in weight space.
In general, however, POMDPs may imply greatly increased CAP depth.
RL Facilitated by Deep UL in FNNs and RNNs
RL machines may profit from UL for input preprocessing (e.g., Jodogne and Piater, 2007). In particular, an UL NN can learn to compactly encode environmental inputs such as images or videos, e.g., Sec. 5.7, 5.10, 5.15. The compact codes (instead of the high-dimensional raw data) can be fed into an RL machine, whose job thus may become much easier (Legenstein et al., 2010; Cuccu et al., 2011), just like SL may profit from UL, e.g., Sec. 5.7, 5.10, 5.15. For example, NFQ (Riedmiller, 2005) was applied to realworld control tasks (Lange and Riedmiller, 2010; Riedmiller et al., 2012) where purely visual inputs were compactly encoded by deep autoencoders (Sec. 5.7, 5.15). RL combined with UL based on Slow Feature
Analysis (Wiskott and Sejnowski, 2002; Kompella et al., 2012) enabled a real humanoid robot to learn skills from raw high-dimensional video streams (Luciw et al., 2013). To deal with POMDPs (Sec. 6.3) involving high-dimensional inputs, RBM-based RL was used (Otsuka, 2010), and a RAAM (Pollack, 1988) (Sec. 5.7) was employed as a deep unsupervised sequence encoder for RL (Gisslen et al., 2011). Certain types of RL and UL also were combined in biologically plausible RNNs with spiking neurons (Sec. 5.26) (e.g., Klampfl and Maass, 2013; Rezende and Gerstner, 2014).
Deep Hierarchical RL (HRL) and Subgoal Learning with FNNs and RNNs
Multiple learnable levels of abstraction (Fu, 1977; Lenat and Brown, 1984; Ring, 1994; Bengio et al., 2013; Deng and Yu, 2014) seem as important for RL as for SL. Work on NN-based Hierarchical RL (HRL) has been published since the early 1990s. In particular, gradient-based subgoal discovery with FNNs or RNNs decomposes RL tasks into subtasks for RL submodules (Schmidhuber, 1991b; Schmidhuber and Wahnsiedler, 1992). Numerous alternative HRL techniques have been proposed (e.g., Ring, 1991, 1994;
Jameson, 1991; Tenenberg et al., 1993; Weiss, 1994; Moore and Atkeson, 1995; Precup et al., 1998; Dietterich, 2000b; Menache et al., 2002; Doya et al., 2002; Ghavamzadeh and Mahadevan, 2003; Barto and Mahadevan, 2003; Samejima et al., 2003; Bakker and Schmidhuber, 2004; Whiteson et al., 2005; Simsek and Barto, 2008). While HRL frameworks such as Feudal RL (Dayan and Hinton, 1993) and options (Sutton et al., 1999b; Barto et al., 2004; Singh et al., 2005) do not directly address the problem of automatic subgoal discovery, HQ-Learning (Wiering and Schmidhuber, 1998a) automatically decomposes POMDPs(Sec. 6.3) into sequences of simpler subtasks that can be solved by memoryless policies learnable by reactive sub-agents. Recent HRL organizes potentially deep NN-based RL sub-modules into self-organizing, 2-dimensional motor control maps (Ring et al., 2011) inspired by neurophysiological findings (Graziano, Deep RL by Direct NN Search / Policy Gradients / Evolution
Not quite as universal as the methods of Sec. 6.8, yet both practical and more general than most traditional
RL algorithms (Sec. 6.2), are methods for Direct Policy Search (DS). Without a need for value functions or Markovian assumptions (Sec. 6.2, 6.3), the weights of an FNN or RNN are directly evaluated on the given RL problem. The results of successive trials inform further search for better weights. Unlike with
RL supported by BP (Sec. 5.5, 6.3, 6.1), CAP depth (Sec. 3, 5.9) is not a crucial issue. DS may solve the credit assignment problem without backtracking through deep causal chains of modifiable parameters—it neither cares for their existence, nor tries to exploit them.
An important class of DS methods for NNs are Policy Gradient methods (Williams, 1986, 1988, 1992a;
Baxter and Bartlett, 1999; Sutton et al., 1999a; Baxter and Bartlett, 2001; Aberdeen, 2003; Ghavamzadeh and Mahadevan, 2003; Kohl and Stone, 2004; Wierstra et al., 2007, 2008; R¨uckstieß et al., 2008; Peters and Schaal, 2008b,a; Sehnke et al., 2010; Gr¨uttner et al., 2010; Wierstra et al., 2010; Peters, 2010; Grondman
27 et al., 2012; Heess et al., 2012). Gradients of the total reward with respect to policies (NN weights) are estimated (and then exploited) through repeated NN evaluations.
RL NNs can also be evolved through Evolutionary Algorithms (EAs) (Rechenberg, 1971; Schwefel, 1974; Holland, 1975; Fogel et al., 1966; Goldberg, 1989) in a series of trials. Here several policies are represented by a population of NNs improved through mutations and/or repeated recombinations of the population's fittest individuals (e.g., Montana and Davis, 1989; Fogel et al., 1990; Maniezzo, 1994; Happel and Murre, 1994; Nolfi et al., 1994b). Compare Genetic Programming (GP) (Cramer, 1985) (see also Smith, 1980) which can be used to evolve computer programs of variable size (Dickmanns et al., 1987; Koza, 1992), and Cartesian GP (Miller and Thomson, 2000; Miller and Harding, 2009) for evolving graph-like programs, including NNs (Khan et al., 2010) and their topology (Turner and Miller, 2013). Related methods include probability distribution-based EAs (Baluja, 1994; Saravanan and Fogel, 1995; Sałustowicz and Schmidhuber, 1997; Larraanaga and Lozano, 2001), Covariance Matrix Estimation Evolution Strategies(CMA-ES) (Hansen and Ostermeier, 2001; Hansen et al., 2003; Igel, 2003; Heidrich-Meisner and Igel, 2009), and NeuroEvolution of Augmenting Topologies (NEAT) (Stanley and Miikkulainen, 2002). Hybrid methods combine traditional NN-based RL (Sec. 6.2) and EAs (e.g., Whiteson and Stone, 2006).
Since RNNs are general computers, RNN evolution is like GP in the sense that it can evolve general programs. Unlike sequential programs learned by traditional GP, however, RNNs can mix sequential and parallel information processing in a natural and efficient way, as already mentioned in Sec. 1. Many RNN evolvers have been proposed (e.g., Miller et al., 1989; Wieland, 1991; Cliff et al., 1993; Yao, 1993; Nolfi et al., 1994a; Sims, 1994; Yamauchi and Beer, 1994; Miglino et al., 1995; Moriarty, 1997; Pasemann et al., 1999; Juang, 2004; Whiteson, 2012). One particularly effective family of methods coevolves neurons, combining them into networks, and selecting those neurons for reproduction that participated in the bestperforming networks (Moriarty and Miikkulainen, 1996; Gomez, 2003; Gomez and Miikkulainen, 2003).
This can help to solve deep POMDPs (Gomez and Schmidhuber, 2005). Co-Synaptic Neuro-Evolution(CoSyNE) does something similar on the level of synapses or weights (Gomez et al., 2008); benefits of this were shown on difficult nonlinear POMDP benchmarks.
Natural Evolution Strategies (NES) (Wierstra et al., 2008; Glasmachers et al., 2010; Sun et al., 2009, 2013) link policy gradient methods and evolutionary approaches through the concept of Natural Gradients (Amari, 1998). RNN evolution may also help to improve SL for deep RNNs through Evolino (Schmidhuber et al., 2007) (Sec. 5.9).
Deep RL by Indirect Policy Search / Compressed NN Search
Some DS methods (Sec. 6.6) can evolve NNs with hundreds of weights, but not millions. How to search for large and deep NNs? Most SL and RL methods mentioned so far somehow search the space of weights wi. Some profit from a reduction of the search space through shared wi that get reused over and over again, e.g., in CNNs (Sec. 5.4, 5.8, 5.16, 5.21), or in RNNs for SL (Sec. 5.5, 5.13, 5.17) and RL (Sec. 6.1, 6.3, 6.6).
It may be possible, however, to exploit additional regularities/compressibilities in the space of solutions, through indirect search in weight space. Instead of evolving large NNs directly (Sec. 6.6), one can sometimes greatly reduce the search space by evolving compact encodings of NNs, e.g., through Lindenmeyer Systems (Lindenmayer, 1968; Jacob et al., 1994), graph rewriting (Kitano, 1990), Cellular Encoding (Gruau et al., 1996), HyperNEAT (D'Ambrosio and Stanley, 2007; Stanley et al., 2009; Clune et al., 2011; van den Berg and Whiteson, 2013) (extending NEAT; Sec. 6.6), and extensions thereof (e.g., Risi and Stanley, 2012). This helps to avoid overfitting (compare Sec. 5.6.3, 5.24) and is closely related to the topics of regularisation and MDL (Sec. 4.4).
A general approach (Schmidhuber, 1997) for both SL and RL seeks to compactly encode weights of large NNs (Schmidhuber, 1997) through programs written in a universal programming language (G¨odel, 1931; Church, 1936; Turing, 1936; Post, 1936). Often it is much more efficient to systematically search the space of such programs with a bias towards short and fast programs (Levin, 1973b; Schmidhuber, 1997, 2004), instead of directly searching the huge space of possible NN weight matrices. A previous universal language for encoding NNs was assembler-like (Schmidhuber, 1997). More recent work uses more practical languages based on coefficients of popular transforms (Fourier, wavelet, etc). In particular, RNN weight matrices may be compressed like images, by encoding them through the coefficients of a discrete cosine transform (DCT) (Koutn´ık et al., 2010, 2013). Compact DCT-based descriptions can be
28 evolved through NES or CoSyNE (Sec. 6.6). An RNN with over a million weights learned (without a teacher) to drive a simulated car in the TORCS driving game (Loiacono et al., 2009, 2011), based on a high-dimensional video-like visual input stream (Koutn´ık et al., 2013). The RNN learned both control and visual processing from scratch, without being aided by UL. (Of course, UL might help to generate more compact image codes (Sec. 6.4, 4.2) to be fed into a smaller RNN, to reduce the overall computational effort.)
Universal RL
General purpose learning algorithms may improve themselves in open-ended fashion and environmentspecific ways in a lifelong learning context (Schmidhuber, 1987; Schmidhuber et al., 1997b,a; Schaul and Schmidhuber, 2010). The most general type of RL is constrained only by the fundamental limitations of computability identified by the founders of theoretical computer science (G¨odel, 1931; Church, 1936; Turing, 1936; Post, 1936). Remarkably, there exist blueprints of universal problem solvers or universal RL machines for unlimited problem depth that are time-optimal in various theoretical senses (Hutter, 2005, 2002; Schmidhuber, 2002, 2006b). In particular, the G¨odel Machine can be implemented on general computers such as RNNs and may improve any part of its software (including the learning algorithm itself) in a way that is provably time-optimal in a certain sense (Schmidhuber, 2006b). It can be initialized by an asymptotically optimal meta-method (Hutter, 2002) (also applicable to RNNs) which will solve any well-defined problem as quickly as the unknown fastest way of solving it, save for an additive constant overhead that becomes negligible as problem size grows. Note that most problems are large; only few are small. AI and DL researchers are still in business because many are interested in problems so small that it is worth trying to reduce the overhead through less general methods, including heuristics. Here I won't further discuss universal RL methods, which go beyond what is usually called DL.
Conclusion
Deep Learning (DL) in Neural Networks (NNs) is relevant for Supervised Learning (SL) (Sec. 5), Unsupervised Learning (UL) (Sec. 5), and Reinforcement Learning (RL) (Sec. 6). By alleviating problems with deep Credit Assignment Paths (CAPs, Sec. 3, 5.9), UL (Sec. 5.6.4) can not only facilitate SL of sequences(Sec. 5.10) and stationary patterns (Sec. 5.7, 5.15), but also RL (Sec. 6.4, 4.2). Dynamic Programming (DP, Sec. 4.1) is important for both deep SL (Sec. 5.5) and traditional RL with deep NNs (Sec. 6.2). A search for solution-computing, perturbation-resistant (Sec. 5.6.3, 5.15, 5.24), low-complexity NNs describable by few bits of information (Sec. 4.4) can reduce overfitting and improve deep SL & UL (Sec. 5.6.3, 5.6.4) as well as RL (Sec. 6.7), also in the case of partially observable environments (Sec. 6.3). Deep SL, UL, RL often create hierarchies of more and more abstract representations of stationary data (Sec. 5.3, 5.7, 5.15), sequential data (Sec. 5.10), or RL policies (Sec. 6.5). While UL can facilitate SL, pure SL for feedforward NNs(FNNs) (Sec. 5.5, 5.8, 5.16, 5.18) and recurrent NNs (RNNs) (Sec. 5.5, 5.13) did not only win early contests(Sec. 5.12, 5.14) but also most of the recent ones (Sec. 5.17–5.22). Especially DL in FNNs profited from
GPU implementations (Sec. 5.16–5.19). In particular, GPU-based (Sec. 5.19) Max-Pooling (Sec. 5.11)
Convolutional NNs (Sec. 5.4, 5.8, 5.16) won competitions not only in pattern recognition (Sec. 5.19–5.22) but also image segmentation (Sec. 5.21) and object detection (Sec. 5.21, 5.22). Unlike these systems, humans learn to actively perceive patterns by sequentially directing attention to relevant parts of the available data. Near future deep NNs may do so, too, extending previous work on learning selective attention through
RL of (a) motor actions such as saccade control (Sec. 6.1) and (b) internal actions controlling spotlights of attention within RNNs, thus closing the general sensorimotor loop through both external and internal feedback (e.g., Sec. 2, 5.21, 6.6, 6.7). The more distant future may belong to general purpose learning algorithms that improve themselves in provably optimal ways (Sec. 6.8), but these are not yet practical or commercially relevant.
Acknowledgments
Since 16 April 2014, drafts of this paper have undergone massive open online peer review through public mailing lists including connectionists@cs.cmu.edu, ml-news@googlegroups.com, compneuro@neuroinf.org, genetic programming@yahoogroups.com, rl-list@googlegroups.com, imageworld@diku.dk. Thanks to numerous NN / DL experts for valuable comments.
The contents of this paper may be used for educational and non-commercial purposes, including articles for Wikipedia and similar sites.
References
Aberdeen, D. (2003). Policy-Gradient Algorithms for Partially Observable Markov Decision Processes.
PhD thesis, Australian National University.
Abounadi, J., Bertsekas, D., and Borkar, V. S. (2002). Learning algorithms for Markov decision processes with average cost. SIAM Journal on Control and Optimization, 40(3):681–698.
Akaike, H. (1970). Statistical predictor identification. Ann. Inst. Statist. Math., 22:203–217.
Akaike, H. (1973). Information theory and an extension of the maximum likelihood principle. In Second
Intl. Symposium on Information Theory, pages 267–281. Akademinai Kiado.
Akaike, H. (1974). A new look at the statistical model identification. IEEE Transactions on Automatic
Control, 19(6):716–723.
Allender, A. (1992). Application of time-bounded Kolmogorov complexity in complexity theory. In Watanabe, O., editor, Kolmogorov complexity and computational complexity, pages 6–22. EATCS Monographs on Theoretical Computer Science, Springer.
Almeida, L. B. (1987). A learning rule for asynchronous perceptrons with feedback in a combinatorial environment. In IEEE 1st International Conference on Neural Networks, San Diego, volume 2, pages
609–618.
Almeida, L. B., Almeida, L. B., Langlois, T., Amaral, J. D., and Redol, R. A. (1997). On-line step size adaptation. Technical report, INESC, 9 Rua Alves Redol, 1000.
Amari, S. (1967). A theory of adaptive pattern classifiers. IEEE Trans. EC, 16(3):299–307.
Amari, S., Cichocki, A., and Yang, H. (1996). A new learning algorithm for blind signal separation.
In Touretzky, D. S., Mozer, M. C., and Hasselmo, M. E., editors, Advances in Neural Information
Processing Systems (NIPS), volume 8. The MIT Press.
Amari, S. and Murata, N. (1993). Statistical theory of learning curves under entropic loss criterion. Neural
Computation, 5(1):140–153.
Amari, S.-I. (1998). Natural gradient works efficiently in learning. Neural Computation, 10(2):251–276.
Amit, D. J. and Brunel, N. (1997). Dynamics of a recurrent network of spiking neurons before and following learning. Network: Computation in Neural Systems, 8(4):373–404.
An, G. (1996). The effects of adding noise during backpropagation training on a generalization performance. Neural Computation, 8(3):643–674.
Andrade, M. A., Chacon, P., Merelo, J. J., and Moran, F. (1993). Evaluation of secondary structure of proteins from UV circular dichroism spectra using an unsupervised learning neural network. Protein
Engineering, 6(4):383–390.
Andrews, R., Diederich, J., and Tickle, A. B. (1995). Survey and critique of techniques for extracting rules from trained artificial neural networks. Knowledge-Based Systems, 8(6):373–389.
Anguita, D. and Gomes, B. A. (1996). Mixing floating- and fixed-point formats for neural network learning on neuroprocessors. Microprocessing and Microprogramming, 41(10):757 – 769.
Anguita, D., Parodi, G., and Zunino, R. (1994). An efficient implementation of BP on RISC-based workstations. Neurocomputing, 6(1):57 – 65.
Arel, I., Rose, D. C., and Karnowski, T. P. (2010). Deep machine learning – a new frontier in artificial intelligence research. Computational Intelligence Magazine, IEEE, 5(4):13–18.
Ash, T. (1989).
Dynamic node creation in backpropagation neural networks.
Connection Science, 1(4):365–375.
Atick, J. J., Li, Z., and Redlich, A. N. (1992). Understanding retinal color coding from first principles.
Neural Computation, 4:559–572.
Atiya, A. F. and Parlos, A. G. (2000). New results on recurrent network training: unifying the algorithms and accelerating convergence. IEEE Transactions on Neural Networks, 11(3):697–709.
Ba, J. and Frey, B. (2013). Adaptive dropout for training deep neural networks. In Advances in Neural
Information Processing Systems (NIPS), pages 3084–3092.
Baird, H. (1990). Document image defect models. In Proceddings, IAPR Workshop on Syntactic and Structural Pattern Recognition, Murray Hill, NJ.
Baird, L. and Moore, A. W. (1999). Gradient descent for general reinforcement learning. In Advances in neural information processing systems 12 (NIPS), pages 968–974. MIT Press.
Baird, L. C. (1994). Reinforcement learning in continuous time: Advantage updating. In IEEE World
Congress on Computational Intelligence, volume 4, pages 2448–2453. IEEE.
Baird, L. C. (1995). Residual algorithms: Reinforcement learning with function approximation. In International Conference on Machine Learning, pages 30–37.
Bakker, B. (2002). Reinforcement learning with Long Short-Term Memory. In Dietterich, T. G., Becker, S., and Ghahramani, Z., editors, Advances in Neural Information Processing Systems 14, pages 1475–1482.
MIT Press, Cambridge, MA.
Bakker, B. and Schmidhuber, J. (2004). Hierarchical reinforcement learning based on subgoal discovery and subpolicy specialization. In et al., F. G., editor, Proc. 8th Conference on Intelligent Autonomous
Systems IAS-8, pages 438–445, Amsterdam, NL. IOS Press.
Bakker, B., Zhumatiy, V., Gruener, G., and Schmidhuber, J. (2003). A robot that reinforcement-learns to identify and memorize important previous observations. In Proceedings of the 2003 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2003, pages 430–435.
Baldi, P. (1995). Gradient descent learning algorithms overview: A general dynamical systems perspective.
IEEE Transactions on Neural Networks, 6(1):182–195.
Baldi, P. (2012). Autoencoders, unsupervised learning, and deep architectures. Journal of Machine Learning Research (Proc. 2011 ICML Workshop on Unsupervised and Transfer Learning), 27:37–50.
Baldi, P., Brunak, S., Frasconi, P., Pollastri, G., and Soda, G. (1999). Exploiting the past and the future in protein secondary structure prediction. Bioinformatics, 15:937–946.
Baldi, P. and Chauvin, Y. (1993).
Neural networks for fingerprint recognition.
Neural Computation, 5(3):402–418.
Baldi, P. and Chauvin, Y. (1996). Hybrid modeling, HMM/NN architectures, and protein applications.
Neural Computation, 8(7):1541–1565.
Baldi, P. and Hornik, K. (1989). Neural networks and principal component analysis: Learning from examples without local minima. Neural Networks, 2:53–58.
Baldi, P. and Hornik, K. (1994). Learning in linear networks: a survey. IEEE Transactions on Neural
Networks, 6(4):837–858. 1995.
Baldi, P. and Pollastri, G. (2003). The principled design of large-scale recursive neural network architectures – DAG-RNNs and the protein structure prediction problem. J. Mach. Learn. Res., 4:575–602.
Baldi, P. and Sadowski, P. (2014). The dropout learning algorithm. Artificial Intelligence, 210C:78–122.
Ballard, D. H. (1987). Modular learning in neural networks. In Proc. AAAI, pages 279–284.
Baluja, S. (1994). Population-based incremental learning: A method for integrating genetic search based function optimization and competitive learning. Technical Report CMU-CS-94-163, Carnegie Mellon
University.
Balzer, R. (1985). A 15 year perspective on automatic programming. IEEE Transactions on Software
Engineering, 11(11):1257–1268.
Barlow, H. B. (1989). Unsupervised learning. Neural Computation, 1(3):295–311.
Barlow, H. B., Kaushal, T. P., and Mitchison, G. J. (1989). Finding minimum entropy codes. Neural
Computation, 1(3):412–423.
Barrow, H. G. (1987). Learning receptive fields. In Proceedings of the IEEE 1st Annual Conference on
Neural Networks, volume IV, pages 115–121. IEEE.
Barto, A. G. and Mahadevan, S. (2003). Recent advances in hierarchical reinforcement learning. Discrete
Event Dynamic Systems, 13(4):341–379.
Barto, A. G., Singh, S., and Chentanez, N. (2004). Intrinsically motivated learning of hierarchical collections of skills. In Proceedings of International Conference on Developmental Learning (ICDL), pages
112–119. MIT Press, Cambridge, MA.
Barto, A. G., Sutton, R. S., and Anderson, C. W. (1983). Neuronlike adaptive elements that can solve difficult learning control problems. IEEE Transactions on Systems, Man, and Cybernetics, SMC-13:834–
Battiti, R. (1989). Accelerated backpropagation learning: two optimization methods. Complex Systems, 3(4):331–342.
Battiti, T. (1992). First- and second-order methods for learning: Between steepest descent and Newton's method. Neural Computation, 4(2):141–166.
Baum, E. B. and Haussler, D. (1989). What size net gives valid generalization?
Neural Computation, 1(1):151–160.
Baum, L. E. and Petrie, T. (1966). Statistical inference for probabilistic functions of finite state Markov chains. The Annals of Mathematical Statistics, pages 1554–1563.
Baxter, J. and Bartlett, P. (1999). Direct gradient-based reinforcement learning. Technical report, Research
School of Information Sciences and Engineering, Australian National University.
Baxter, J. and Bartlett, P. L. (2001).
Infinite-horizon policy-gradient estimation.
J. Artif. Int. Res., 15(1):319–350.
Bayer, J., Osendorfer, C., Chen, N., Urban, S., and van der Smagt, P. (2013). On fast dropout and its applicability to recurrent networks. arXiv preprint arXiv:1311.0701.
Bayer, J., Wierstra, D., Togelius, J., and Schmidhuber, J. (2009). Evolving memory cell structures for sequence learning. In Proc. ICANN (2), pages 755–764.
Bayes, T. (1763). An essay toward solving a problem in the doctrine of chances. Philosophical Transactions of the Royal Society of London, 53:370–418. Communicated by R. Price, in a letter to J. Canton.
Becker, S. (1991). Unsupervised learning procedures for neural networks. International Journal of Neural
Systems, 2(1 & 2):17–33.
Becker, S. and Le Cun, Y. (1989). Improving the convergence of back-propagation learning with second order methods. In Touretzky, D., Hinton, G., and Sejnowski, T., editors, Proc. 1988 Connectionist
Models Summer School, pages 29–37, Pittsburg 1988. Morgan Kaufmann, San Mateo.
Behnke, S. (2003). Hierarchical Neural Networks for Image Interpretation, volume LNCS 2766 of Lecture
Notes in Computer Science. Springer.
Bell, A. J. and Sejnowski, T. J. (1995). An information-maximization approach to blind separation and blind deconvolution. Neural Computation, 7(6):1129–1159.
Bellman, R. (1957). Dynamic Programming. Princeton University Press, Princeton, NJ, USA, 1st edition.
Belouchrani, A., Abed-Meraim, K., Cardoso, J.-F., and Moulines, E. (1997). A blind source separation technique using second-order statistics. IEEE Transactions on Signal Processing, 45(2):434–444.
Bengio, Y. (1991). Artificial Neural Networks and their Application to Sequence Recognition. PhD thesis, McGill University, (Computer Science), Montreal, Qc., Canada.
Bengio, Y. (2009). Learning Deep Architectures for AI. Foundations and Trends in Machine Learning, V2(1). Now Publishers.
Bengio, Y., Courville, A., and Vincent, P. (2013). Representation learning: A review and new perspectives.
Pattern Analysis and Machine Intelligence, IEEE Transactions on, 35(8):1798–1828.
Bengio, Y., Lamblin, P., Popovici, D., and Larochelle, H. (2007). Greedy layer-wise training of deep networks. In Cowan, J. D., Tesauro, G., and Alspector, J., editors, Advances in Neural Information
Processing Systems 19 (NIPS), pages 153–160. MIT Press.
Bengio, Y., Simard, P., and Frasconi, P. (1994). Learning long-term dependencies with gradient descent is difficult. IEEE Transactions on Neural Networks, 5(2):157–166.
Beringer, N., Graves, A., Schiel, F., and Schmidhuber, J. (2005).
Classifying unprompted speech by retraining LSTM nets. In Duch, W., Kacprzyk, J., Oja, E., and Zadrozny, S., editors, Artificial Neural
Networks: Biological Inspirations - ICANN 2005, LNCS 3696, pages 575–581. Springer-Verlag Berlin
Heidelberg.
Bertsekas, D. P. (2001). Dynamic Programming and Optimal Control. Athena Scientific.
Bertsekas, D. P. and Tsitsiklis, J. N. (1996). Neuro-dynamic Programming. Athena Scientific, Belmont, MA.
Bichot, N. P., Rossi, A. F., and Desimone, R. (2005). Parallel and serial neural mechanisms for visual search in macaque area V4. Science, 308:529–534.
Biegler-K¨onig, F. and B¨armann, F. (1993). A learning algorithm for multilayered neural networks based on linear least squares problems. Neural Networks, 6(1):127–131.
Bishop, C. M. (1993). Curvature-driven smoothing: A learning algorithm for feed-forward networks. IEEE
Transactions on Neural Networks, 4(5):882–884.
Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.
Blair, A. D. and Pollack, J. B. (1997). Analysis of dynamical recognizers. Neural Computation, 9(5):1127–
Blondel, V. D. and Tsitsiklis, J. N. (2000). A survey of computational complexity results in systems and control. Automatica, 36(9):1249–1274.
Bluche, T., Louradour, J., Knibbe, M., Moysset, B., Benzeghiba, F., and Kermorvant., C. (2014). The A2iA Arabic Handwritten Text Recognition System at the OpenHaRT2013 Evaluation. In International
Workshop on Document Analysis Systems.
Blum, A. L. and Rivest, R. L. (1992). Training a 3-node neural network is np-complete. Neural Networks, 5(1):117–127.
Blumer, A., Ehrenfeucht, A., Haussler, D., and Warmuth, M. K. (1987). Occam's razor. Information
Processing Letters, 24:377–380.
Bobrowski, L. (1978). Learning processes in multilayer threshold nets. Biological Cybernetics, 31:1–6.
Bod´en, M. and Wiles, J. (2000). Context-free and context-sensitive dynamics in recurrent neural networks.
Connection Science, 12(3-4):197–210.
Bodenhausen, U. and Waibel, A. (1991). The Tempo 2 algorithm: Adjusting time-delays by supervised learning. In Lippman, D. S., Moody, J. E., and Touretzky, D. S., editors, Advances in Neural Information
Processing Systems 3, pages 155–161. Morgan Kaufmann.
Bohte, S. M., Kok, J. N., and La Poutre, H. (2002). Error-backpropagation in temporally encoded networks of spiking neurons. Neurocomputing, 48(1):17–37.
Boltzmann, L. (1909). In Hasen¨ohrl, F., editor, Wissenschaftliche Abhandlungen (collection of Boltzmann's articles in scientific journals). Barth, Leipzig.
Bottou, L. (1991). Une approche th´eorique de l'apprentissage connexioniste; applications `a la reconnaissance de la parole. PhD thesis, Universit´e de Paris XI.
Bourlard, H. and Morgan, N. (1994). Connnectionist Speech Recognition: A Hybrid Approach. Kluwer
Academic Publishers.
Boutilier, C. and Poole, D. (1996). Computing optimal policies for partially observable Markov decision processes using compact representations. In Proceedings of the AAAI, Portland, OR.
Bradtke, S. J., Barto, A. G., and Kaelbling, L. P. (1996). Linear least-squares algorithms for temporal difference learning. In Machine Learning, pages 22–33.
Brafman, R. I. and Tennenholtz, M. (2002). R-MAX—a general polynomial time algorithm for nearoptimal reinforcement learning. Journal of Machine Learning Research, 3:213–231.
Brea, J., Senn, W., and Pfister, J.-P. (2013). Matching recall and storage in sequence learning with spiking neural networks. The Journal of Neuroscience, 33(23):9565–9575.
Breiman, L. (1996). Bagging predictors. Machine Learning, 24:123–140.
Brette, R., Rudolph, M., Carnevale, T., Hines, M., Beeman, D., Bower, J. M., Diesmann, M., Morrison, A., Goodman, P. H., Harris Jr, F. C., et al. (2007). Simulation of networks of spiking neurons: a review of tools and strategies. Journal of Computational Neuroscience, 23(3):349–398.
Breuel, T. M., Ul-Hasan, A., Al-Azawi, M. A., and Shafait, F. (2013). High-performance OCR for printed
English and Fraktur using LSTM networks. In 12th International Conference on Document Analysis and Recognition (ICDAR), pages 683–687. IEEE.
Bromley, J., Bentz, J. W., Bottou, L., Guyon, I., LeCun, Y., Moore, C., Sackinger, E., and Shah, R. (1993).
Signature verification using a Siamese time delay neural network.
International Journal of Pattern
Recognition and Artificial Intelligence, 7(4):669–688.
Broyden, C. G. et al. (1965). A class of methods for solving nonlinear simultaneous equations. Math.
Comp, 19(92):577–593.
Brunel, N. (2000). Dynamics of sparsely connected networks of excitatory and inhibitory spiking neurons.
Journal of Computational Neuroscience, 8(3):183–208.
Bryson, A. and Ho, Y. (1969). Applied optimal control: optimization, estimation, and control. Blaisdell
Pub. Co.
Bryson, A. E. (1961). A gradient method for optimizing multi-stage allocation processes. In Proc. Harvard
Univ. Symposium on digital computers and their applications.
Bryson, Jr., A. E. and Denham, W. F. (1961). A steepest-ascent method for solving optimum programming problems. Technical Report BR-1303, Raytheon Company, Missle and Space Division.
Buhler, J. (2001). Efficient large-scale sequence comparison by locality-sensitive hashing. Bioinformatics, 17(5):419–428.
Buntine, W. L. and Weigend, A. S. (1991). Bayesian back-propagation. Complex Systems, 5:603–643.
Burgess, N. (1994). A constructive algorithm that converges for real-valued input patterns. International
Journal of Neural Systems, 5(1):59–66.
Cardoso, J.-F. (1994). On the performance of orthogonal source separation algorithms. In Proc. EUSIPCO, pages 776–779.
Carreira-Perpinan, M. A. (2001). Continuous latent variable models for dimensionality reduction and sequential data reconstruction. PhD thesis, University of Sheffield UK.
Carter, M. J., Rudolph, F. J., and Nucci, A. J. (1990). Operational fault tolerance of CMAC networks. In
Touretzky, D. S., editor, Advances in Neural Information Processing Systems (NIPS) 2, pages 340–347.
San Mateo, CA: Morgan Kaufmann.
Caruana, R. (1997). Multitask learning. Machine Learning, 28(1):41–75.
Casey, M. P. (1996). The dynamics of discrete-time computation, with application to recurrent neural networks and finite state machine extraction. Neural Computation, 8(6):1135–1178.
Cauwenberghs, G. (1993). A fast stochastic error-descent algorithm for supervised learning and optimization. In Lippman, D. S., Moody, J. E., and Touretzky, D. S., editors, Advances in Neural Information
Processing Systems 5, pages 244–244. Morgan Kaufmann.
Chaitin, G. J. (1966). On the length of programs for computing finite binary sequences. Journal of the ACM, 13:547–569.
Chalup, S. K. and Blair, A. D. (2003). Incremental training of first order recurrent neural networks to predict a context-sensitive language. Neural Networks, 16(7):955–972.
Chellapilla, K., Puri, S., and Simard, P. (2006). High performance convolutional neural networks for document processing. In International Workshop on Frontiers in Handwriting Recognition.
Chen, K. and Salman, A. (2011). Learning speaker-specific characteristics with a deep neural architecture.
IEEE Transactions on Neural Networks, 22(11):1744–1756.
Cho, K. (2014). Foundations and Advances in Deep Learning. PhD thesis, Aalto University School of Science.
Cho, K., Ilin, A., and Raiko, T. (2012). Tikhonov-type regularization for restricted Boltzmann machines.
In Intl. Conf. on Artificial Neural Networks (ICANN) 2012, pages 81–88. Springer.
Cho, K., Raiko, T., and Ilin, A. (2013). Enhanced gradient for training restricted Boltzmann machines.
Neural Computation, 25(3):805–831.
Church, A. (1936). An unsolvable problem of elementary number theory. American Journal of Mathematics, 58:345–363.
Ciresan, D. C., Giusti, A., Gambardella, L. M., and Schmidhuber, J. (2012a).
Deep neural networks segment neuronal membranes in electron microscopy images. In Advances in Neural Information Processing Systems (NIPS), pages 2852–2860.
Ciresan, D. C., Giusti, A., Gambardella, L. M., and Schmidhuber, J. (2013). Mitosis detection in breast cancer histology images with deep neural networks. In Proc. MICCAI, volume 2, pages 411–418.
Ciresan, D. C., Meier, U., Gambardella, L. M., and Schmidhuber, J. (2010). Deep big simple neural nets for handwritten digit recogntion. Neural Computation, 22(12):3207–3220.
Ciresan, D. C., Meier, U., Masci, J., Gambardella, L. M., and Schmidhuber, J. (2011a). Flexible, high performance convolutional neural networks for image classification. In Intl. Joint Conference on Artificial
Intelligence IJCAI, pages 1237–1242.
Ciresan, D. C., Meier, U., Masci, J., and Schmidhuber, J. (2011b). A committee of neural networks for traffic sign classification. In International Joint Conference on Neural Networks (IJCNN), pages 1918–
Ciresan, D. C., Meier, U., Masci, J., and Schmidhuber, J. (2012b). Multi-column deep neural network for traffic sign classification. Neural Networks, 32:333–338.
Ciresan, D. C., Meier, U., and Schmidhuber, J. (2012c). Multi-column deep neural networks for image classification. In IEEE Conference on Computer Vision and Pattern Recognition CVPR 2012. Long preprint arXiv:1202.2745v1 [cs.CV].
Ciresan, D. C., Meier, U., and Schmidhuber, J. (2012d). Transfer learning for Latin and Chinese characters with deep neural networks. In International Joint Conference on Neural Networks (IJCNN), pages 1301–
Ciresan, D. C. and Schmidhuber, J. (2013). Multi-column deep neural networks for offline handwritten
Chinese character classification. Technical report, IDSIA. arXiv:1309.0261.
Cliff, D. T., Husbands, P., and Harvey, I. (1993). Evolving recurrent dynamical networks for robot control.
In Artificial Neural Nets and Genetic Algorithms, pages 428–435. Springer.
Clune, J., Mouret, J.-B., and Lipson, H. (2013). The evolutionary origins of modularity. Proceedings of the Royal Society B: Biological Sciences, 280(1755):20122863.
Clune, J., Stanley, K. O., Pennock, R. T., and Ofria, C. (2011). On the performance of indirect encoding across the continuum of regularity. Trans. Evol. Comp, 15(3):346–367.
Coates, A., Huval, B., Wang, T., Wu, D. J., Ng, A. Y., and Catanzaro, B. (2013). Deep learning with COTS
HPC systems. In Proc. International Conference on Machine learning (ICML'13).
Cochocki, A. and Unbehauen, R. (1993). Neural networks for optimization and signal processing. John
Wiley & Sons, Inc.
Collobert, R. and Weston, J. (2008). A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th International Conference on Machine
Learning (ICML), pages 160–167. ACM.
Comon, P. (1994). Independent component analysis – a new concept? Signal Processing, 36(3):287–314.
Connor, C. E., Brincat, S. L., and Pasupathy, A. (2007). Transformation of shape information in the ventral pathway. Current Opinion in Neurobiology, 17(2):140–147.
Connor, J., Martin, D. R., and Atlas, L. E. (1994). Recurrent neural networks and robust time series prediction. IEEE Transactions on Neural Networks, 5(2):240–254.
Cook, S. A. (1971). The complexity of theorem-proving procedures. In Proceedings of the 3rd Annual
ACM Symposium on the Theory of Computing (STOC'71), pages 151–158. ACM, New York.
Cramer, N. L. (1985). A representation for the adaptive generation of simple sequential programs. In
Grefenstette, J., editor, Proceedings of an International Conference on Genetic Algorithms and Their
Applications, Carnegie-Mellon University, July 24-26, 1985, Hillsdale NJ. Lawrence Erlbaum Associates.
Craven, P. and Wahba, G. (1979). Smoothing noisy data with spline functions: Estimating the correct degree of smoothing by the method of generalized cross-validation. Numer. Math., 31:377–403.
Cuccu, G., Luciw, M., Schmidhuber, J., and Gomez, F. (2011). Intrinsically motivated evolutionary search for vision-based reinforcement learning. In Proceedings of the 2011 IEEE Conference on Development and Learning and Epigenetic Robotics IEEE-ICDL-EPIROB, volume 2, pages 1–7. IEEE.
Dahl, G., Yu, D., Deng, L., and Acero, A. (2012). Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition. Audio, Speech, and Language Processing, IEEE Transactions on, 20(1):30–42.
Dahl, G. E., Sainath, T. N., and Hinton, G. E. (2013). Improving deep neural networks for LVCSR using rectified linear units and dropout. In IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP), pages 8609–8613. IEEE.
D'Ambrosio, D. B. and Stanley, K. O. (2007). A novel generative encoding for exploiting neural network sensor and output geometry. In Proceedings of the Conference on Genetic and Evolutionary Computation(GECCO), pages 974–981.
Datar, M., Immorlica, N., Indyk, P., and Mirrokni, V. S. (2004). Locality-sensitive hashing scheme based on p-stable distributions. In Proceedings of the 20th Annual Symposium on Computational Geometry, pages 253–262. ACM.
Dayan, P. and Hinton, G. (1993). Feudal reinforcement learning. In Lippman, D. S., Moody, J. E., and Touretzky, D. S., editors, Advances in Neural Information Processing Systems (NIPS) 5, pages 271–278.
Morgan Kaufmann.
Dayan, P. and Hinton, G. E. (1996). Varieties of Helmholtz machine. Neural Networks, 9(8):1385–1403.
Dayan, P., Hinton, G. E., Neal, R. M., and Zemel, R. S. (1995). The Helmholtz machine. Neural Computation, 7:889–904.
Dayan, P. and Zemel, R. (1995). Competition and multiple cause models. Neural Computation, 7:565–579.
De Freitas, J. F. G. (2003). Bayesian methods for neural networks. PhD thesis, University of Cambridge. de Souto, M. C., Souto, M. C. P. D., and Oliveira, W. R. D. (1999). The loading problem for pyramidal neural networks. In Electronic Journal on Mathematics of Computation.
De Valois, R. L., Albrecht, D. G., and Thorell, L. G. (1982). Spatial frequency selectivity of cells in macaque visual cortex. Vision Research, 22(5):545–559. de Vries, B. and Principe, J. C. (1991). A theory for neural networks with time delays. In Lippmann, R. P., Moody, J. E., and Touretzky, D. S., editors, Advances in Neural Information Processing Systems (NIPS)
3, pages 162–168. Morgan Kaufmann.
Deco, G. and Parra, L. (1997). Non-linear feature extraction by redundancy reduction in an unsupervised stochastic neural network. Neural Networks, 10(4):683–691.
Deco, G. and Rolls, E. T. (2005). Neurodynamics of biased competition and cooperation for attention: a model with spiking neurons. Journal of Neurophysiology, 94(1):295–313.
DeJong, G. and Mooney, R. (1986). Explanation-based learning: An alternative view. Machine Learning, 1(2):145–176.
DeMers, D. and Cottrell, G. (1993). Non-linear dimensionality reduction. In Hanson, S. J., Cowan, J. D., and Giles, C. L., editors, Advances in Neural Information Processing Systems (NIPS) 5, pages 580–587.
Morgan Kaufmann.
Dempster, A. P., Laird, N. M., and Rubin, D. B. (1977). Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, B, 39.
Deng, L. and Yu, D. (2014). Deep Learning: Methods and Applications. NOW Publishers.
Desimone, R., Albright, T. D., Gross, C. G., and Bruce, C. (1984). Stimulus-selective properties of inferior temporal neurons in the macaque. The Journal of Neuroscience, 4(8):2051–2062.
Deville, Y. and Lau, K. K. (1994). Logic program synthesis. Journal of Logic Programming, 19(20):321–
Di Lena, P., Nagata, K., and Baldi, P. (2012). Deep architectures for protein contact map prediction.
Bioinformatics, 28:2449–2457.
DiCarlo, J. J., Zoccolan, D., and Rust, N. C. (2012). How does the brain solve visual object recognition?
Neuron, 73(3):415–434.
Dickmanns, D., Schmidhuber, J., and Winklhofer, A. (1987).
Der genetische Algorithmus:
Eine Implementierung in Prolog. Technical Report, Inst. of Informatics, Tech. Univ. Munich. http://www.idsia.ch/˜juergen/geneticprogramming.html.
Dickmanns, E. D., Behringer, R., Dickmanns, D., Hildebrandt, T., Maurer, M., Thomanek, F., and Schiehlen, J. (1994). The seeing passenger car 'VaMoRs-P'. In Proc. Int. Symp. on Intelligent Vehicles '94, Paris, pages 68–73.
Dietterich, T. G. (2000a). Ensemble methods in machine learning. In Multiple classifier systems, pages
1–15. Springer.
Dietterich, T. G. (2000b). Hierarchical reinforcement learning with the MAXQ value function decomposition. J. Artif. Intell. Res. (JAIR), 13:227–303.
Director, S. W. and Rohrer, R. A. (1969). Automated network design - the frequency-domain case. IEEE
Trans. Circuit Theory, CT-16:330–337.
Dittenbach, M., Merkl, D., and Rauber, A. (2000). The growing hierarchical self-organizing map. In
IEEE-INNS-ENNS International Joint Conference on Neural Networks, volume 6, pages 6015–6015.
IEEE Computer Society.
Donahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E., and Darrell, T. (2013). DeCAF: A deep convolutional activation feature for generic visual recognition. arXiv preprint arXiv:1310.1531.
Dorffner, G. (1996). Neural networks for time series processing. In Neural Network World.
Doya, K., Samejima, K., ichi Katagiri, K., and Kawato, M. (2002). Multiple model-based reinforcement learning. Neural Computation, 14(6):1347–1369.
Dreyfus, S. E. (1962). The numerical solution of variational problems. Journal of Mathematical Analysis and Applications, 5(1):30–45.
Dreyfus, S. E. (1973). The computational solution of optimal control problems with time lag. IEEE
Transactions on Automatic Control, 18(4):383–385.
Duchi, J., Hazan, E., and Singer, Y. (2011). Adaptive subgradient methods for online learning and stochastic optimization. The Journal of Machine Learning, 12:2121–2159.
Egorova, A., Gloye, A., G¨oktekin, C., Liers, A., Luft, M., Rojas, R., Simon, M., Tenchio, O., and Wiesel, F. (2004). FU-Fighters Small Size 2004, Team Description. RoboCup 2004 Symposium: Papers and Team Description Papers. CD edition.
Elfwing, S., Otsuka, M., Uchibe, E., and Doya, K. (2010). Free-energy based reinforcement learning for vision-based navigation with high-dimensional sensory inputs. In Neural Information Processing.
Theory and Algorithms (ICONIP), volume 1, pages 215–222. Springer.
Eliasmith, C. (2013).
How to build a brain: A neural architecture for biological cognition.
Oxford
University Press, New York, NY.
Eliasmith, C., Stewart, T. C., Choo, X., Bekolay, T., DeWolf, T., Tang, Y., and Rasmussen, D. (2012). A large-scale model of the functioning brain. Science, 338(6111):1202–1205.
Elman, J. L. (1990). Finding structure in time. Cognitive Science, 14(2):179–211.
Erhan, D., Bengio, Y., Courville, A., Manzagol, P.-A., Vincent, P., and Bengio, S. (2010). Why does unsupervised pre-training help deep learning? J. Mach. Learn. Res., 11:625–660.
Escalante-B., A. N. and Wiskott, L. (2013). How to solve classification and regression problems on highdimensional data with a supervised extension of slow feature analysis. Journal of Machine Learning
Research, 14:3683–3719.
Eubank, R. L. (1988).
Spline smoothing and nonparametric regression.
In Farlow, S., editor, SelfOrganizing Methods in Modeling. Marcel Dekker, New York.
Euler, L. (1744). Methodus inveniendi.
Faggin, F. (1992).
Neural network hardware.
In International Joint Conference on Neural Networks(IJCNN), volume 1, page 153.
Fahlman, S. E. (1988). An empirical study of learning speed in back-propagation networks. Technical
Report CMU-CS-88-162, Carnegie-Mellon Univ.
Fahlman, S. E. (1991). The recurrent cascade-correlation learning algorithm. In Lippmann, R. P., Moody, J. E., and Touretzky, D. S., editors, Advances in Neural Information Processing Systems (NIPS) 3, pages
190–196. Morgan Kaufmann.
Falconbridge, M. S., Stamps, R. L., and Badcock, D. R. (2006). A simple Hebbian/anti-Hebbian network learns the sparse, independent components of natural images. Neural Computation, 18(2):415–429.
Farabet, C., Couprie, C., Najman, L., and LeCun, Y. (2013). Learning hierarchical features for scene labeling. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1915–1929.
Farlow, S. J. (1984). Self-organizing methods in modeling: GMDH type algorithms, volume 54. CRC
Press.
Feldkamp, L. A., Prokhorov, D. V., Eagen, C. F., and Yuan, F. (1998). Enhanced multi-stream Kalman filter training for recurrent networks. In Nonlinear Modeling, pages 29–53. Springer.
Feldkamp, L. A., Prokhorov, D. V., and Feldkamp, T. M. (2003). Simple and conditioned adaptive behavior from Kalman filter trained recurrent networks. Neural Networks, 16(5):683–689.
Feldkamp, L. A. and Puskorius, G. V. (1998). A signal processing framework based on dynamic neural networks with application to problems in adaptation, filtering, and classification. Proceedings of the IEEE, 86(11):2259–2277.
Felleman, D. J. and Van Essen, D. C. (1991). Distributed hierarchical processing in the primate cerebral cortex. Cerebral Cortex, 1(1):1–47.
Fern´andez, S., Graves, A., and Schmidhuber, J. (2007). An application of recurrent neural networks to discriminative keyword spotting. In Proc. ICANN (2), pages 220–229.
Fernandez, S., Graves, A., and Schmidhuber, J. (2007). Sequence labelling in structured domains with hierarchical recurrent neural networks. In Proceedings of the 20th International Joint Conference on
Artificial Intelligence (IJCAI).
Field, D. J. (1987). Relations between the statistics of natural images and the response properties of cortical cells. Journal of the Optical Society of America, 4:2379–2394.
Field, D. J. (1994). What is the goal of sensory coding? Neural Computation, 6:559–601.
Fieres, J., Schemmel, J., and Meier, K. (2008). Realizing biological spiking network models in a configurable wafer-scale hardware system. In IEEE International Joint Conference on Neural Networks, pages
969–976.
Fine, S., Singer, Y., and Tishby, N. (1998). The hierarchical hidden Markov model: Analysis and applications. Machine Learning, 32(1):41–62.
Fischer, A. and Igel, C. (2014). Training restricted Boltzmann machines: An introduction. Pattern Recognition, 47:25–39.
FitzHugh, R. (1961). Impulses and physiological states in theoretical models of nerve membrane. Biophysical Journal, 1(6):445–466.
Fletcher, R. and Powell, M. J. (1963). A rapidly convergent descent method for minimization. The Computer Journal, 6(2):163–168.
Fogel, D. B., Fogel, L. J., and Porto, V. (1990).
Evolving neural networks.
Biological Cybernetics, 63(6):487–493.
Fogel, L., Owens, A., and Walsh, M. (1966). Artificial Intelligence through Simulated Evolution. Wiley, New York.
F¨oldi´ak, P. (1990). Forming sparse representations by local anti-Hebbian learning. Biological Cybernetics, 64:165–170.
F¨oldi´ak, P. and Young, M. P. (1995). Sparse coding in the primate cortex. In Arbib, M. A., editor, The Handbook of Brain Theory and Neural Networks, pages 895–898. The MIT Press.
F¨orster, A., Graves, A., and Schmidhuber, J. (2007). RNN-based Learning of Compact Maps for Efficient
Robot Localization. In 15th European Symposium on Artificial Neural Networks, ESANN, pages 537–
542, Bruges, Belgium.
Franzius, M., Sprekeler, H., and Wiskott, L. (2007). Slowness and sparseness lead to place, head-direction, and spatial-view cells. PLoS Computational Biology, 3(8):166.
Friedman, J., Hastie, T., and Tibshirani, R. (2001). The elements of statistical learning, volume 1. Springer
Series in Statistics, New York.
Frinken, V., Zamora-Martinez, F., Espana-Boquera, S., Castro-Bleda, M. J., Fischer, A., and Bunke, H.(2012). Long-short term memory neural networks language modeling for handwriting recognition. In
Pattern Recognition (ICPR), 2012 21st International Conference on, pages 701–704. IEEE.
Fritzke, B. (1994). A growing neural gas network learns topologies. In Tesauro, G., Touretzky, D. S., and Leen, T. K., editors, NIPS, pages 625–632. MIT Press.
Fu, K. S. (1977). Syntactic Pattern Recognition and Applications. Berlin, Springer.
Fukada, T., Schuster, M., and Sagisaka, Y. (1999).
Phoneme boundary estimation using bidirectional recurrent neural networks and its applications. Systems and Computers in Japan, 30(4):20–30.
Fukushima, K. (1979). Neural network model for a mechanism of pattern recognition unaffected by shift in position - Neocognitron. Trans. IECE, J62-A(10):658–665.
Fukushima, K. (1980). Neocognitron: A self-organizing neural network for a mechanism of pattern recognition unaffected by shift in position. Biological Cybernetics, 36(4):193–202.
Fukushima, K. (2011). Increasing robustness against background noise: visual pattern recognition by a Neocognitron. Neural Networks, 24(7):767–778.
Fukushima, K. (2013a). Artificial vision by multi-layered neural networks: Neocognitron and its advances.
Neural Networks, 37:103–119.
Fukushima, K. (2013b). Training multi-layered neural network Neocognitron. Neural Networks, 40:18–31.
Gabor, D. (1946). Theory of communication. Part 1: The analysis of information. Electrical EngineersPart III: Journal of the Institution of Radio and Communication Engineering, 93(26):429–441.
Gallant, S. I. (1988). Connectionist expert systems. Communications of the ACM, 31(2):152–169.
Gauss, C. F. (1809). Theoria motus corporum coelestium in sectionibus conicis solem ambientium.
Gauss, C. F. (1821). Theoria combinationis observationum erroribus minimis obnoxiae (Theory of the combination of observations least subject to error).
Ge, S., Hang, C. C., Lee, T. H., and Zhang, T. (2010). Stable adaptive neural network control. Springer.
Geman, S., Bienenstock, E., and Doursat, R. (1992). Neural networks and the bias/variance dilemma.
Neural Computation, 4:1–58.
Gers, F. A. and Schmidhuber, J. (2000). Recurrent nets that time and count. In Neural Networks, 2000.
IJCNN 2000, Proceedings of the IEEE-INNS-ENNS International Joint Conference on, volume 3, pages
189–194. IEEE.
Gers, F. A. and Schmidhuber, J. (2001). LSTM recurrent networks learn simple context free and context sensitive languages. IEEE Transactions on Neural Networks, 12(6):1333–1340.
Gers, F. A., Schmidhuber, J., and Cummins, F. (2000). Learning to forget: Continual prediction with
LSTM. Neural Computation, 12(10):2451–2471.
Gers, F. A., Schraudolph, N., and Schmidhuber, J. (2002). Learning precise timing with LSTM recurrent networks. Journal of Machine Learning Research, 3:115–143.
Gerstner, W. and Kistler, W. K. (2002). Spiking Neuron Models. Cambridge University Press.
Gerstner, W. and van Hemmen, J. L. (1992). Associative memory in a network of spiking neurons. Network:
Computation in Neural Systems, 3(2):139–164.
Ghavamzadeh, M. and Mahadevan, S. (2003). Hierarchical policy gradient algorithms. In Proceedings of the Twentieth Conference on Machine Learning (ICML-2003), pages 226–233.
Gherrity, M. (1989). A learning algorithm for analog fully recurrent neural networks. In IEEE/INNS
International Joint Conference on Neural Networks, San Diego, volume 1, pages 643–644.
Girshick, R., Donahue, J., Darrell, T., and Malik, J. (2013). Rich feature hierarchies for accurate object detection and semantic segmentation. Technical Report arxiv.org/abs/1311.2524, UC Berkeley and ICSI.
Gisslen, L., Luciw, M., Graziano, V., and Schmidhuber, J. (2011). Sequential constant size compressor for reinforcement learning. In Proc. Fourth Conference on Artificial General Intelligence (AGI), Google, Mountain View, CA, pages 31–40. Springer.
Giusti, A., Ciresan, D. C., Masci, J., Gambardella, L. M., and Schmidhuber, J. (2013). Fast image scanning with deep max-pooling convolutional neural networks. In Proc. ICIP.
Glackin, B., McGinnity, T. M., Maguire, L. P., Wu, Q., and Belatreche, A. (2005). A novel approach for the implementation of large scale spiking neural networks on FPGA hardware. In Computational
Intelligence and Bioinspired Systems, pages 552–563. Springer.
Glasmachers, T., Schaul, T., Sun, Y., Wierstra, D., and Schmidhuber, J. (2010). Exponential natural evolution strategies. In Proceedings of the Genetic and Evolutionary Computation Conference (GECCO), pages 393–400. ACM.
Glorot, X., Bordes, A., and Bengio, Y. (2011). Deep sparse rectifier networks. In AISTATS, volume 15, pages 315–323.
Gloye, A., Wiesel, F., Tenchio, O., and Simon, M. (2005). Reinforcing the driving quality of soccer playing robots by anticipation. IT - Information Technology, 47(5).
G¨odel, K. (1931). ¨Uber formal unentscheidbare S¨atze der Principia Mathematica und verwandter Systeme
I. Monatshefte f¨ur Mathematik und Physik, 38:173–198.
Goldberg, D. E. (1989). Genetic Algorithms in Search, Optimization and Machine Learning. AddisonWesley, Reading, MA.
Goldfarb, D. (1970). A family of variable-metric methods derived by variational means. Mathematics of computation, 24(109):23–26.
Golub, G., Heath, H., and Wahba, G. (1979). Generalized cross-validation as a method for choosing a good ridge parameter. Technometrics, 21:215–224.
Gomez, F. J. (2003). Robust Nonlinear Control through Neuroevolution. PhD thesis, Department of Computer Sciences, University of Texas at Austin.
Gomez, F. J. and Miikkulainen, R. (2003). Active guidance for a finless rocket using neuroevolution. In
Proc. GECCO 2003, Chicago.
Gomez, F. J. and Schmidhuber, J. (2005). Co-evolving recurrent neurons learn deep memory POMDPs.
In Proc. of the 2005 conference on genetic and evolutionary computation (GECCO), Washington, D. C.
ACM Press, New York, NY, USA.
Gomez, F. J., Schmidhuber, J., and Miikkulainen, R. (2008). Accelerated neural evolution through cooperatively coevolved synapses. Journal of Machine Learning Research, 9(May):937–965.
Gomi, H. and Kawato, M. (1993). Neural network control for a closed-loop system using feedback-errorlearning. Neural Networks, 6(7):933–946.
Goodfellow, I., Mirza, M., Da, X., Courville, A., and Bengio, Y. (2014a). An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks. TR arXiv:1312.6211v2.
Goodfellow, I. J., Bulatov, Y., Ibarz, J., Arnoud, S., and Shet, V. (2014b). Multi-digit number recognition from street view imagery using deep convolutional neural networks. arXiv preprint arXiv:1312.6082 v4.
Goodfellow, I. J., Courville, A., and Bengio, Y. (2011). Spike-and-slab sparse coding for unsupervised feature discovery. In NIPS Workshop on Challenges in Learning Hierarchical Models.
Goodfellow, I. J., Courville, A. C., and Bengio, Y. (2012). Large-scale feature learning with spike-and-slab sparse coding. In Proceedings of the 29th International Conference on Machine Learning (ICML).
Goodfellow, I. J., Warde-Farley, D., Mirza, M., Courville, A., and Bengio, Y. (2013). Maxout networks. In
International Conference on Machine Learning (ICML).
Graves, A. (2011). Practical variational inference for neural networks. In Advances in Neural Information
Processing Systems (NIPS), pages 2348–2356.
Graves, A., Eck, D., Beringer, N., and Schmidhuber, J. (2003). Isolated digit recognition with LSTM recurrent networks. In First International Workshop on Biologically Inspired Approaches to Advanced
Information Technology, Lausanne.
Graves, A., Fernandez, S., Gomez, F. J., and Schmidhuber, J. (2006). Connectionist temporal classification:
Labelling unsegmented sequence data with recurrent neural nets. In ICML'06: Proceedings of the 23rd
International Conference on Machine Learning, pages 369–376.
Graves, A., Fernandez, S., Liwicki, M., Bunke, H., and Schmidhuber, J. (2008). Unconstrained on-line handwriting recognition with recurrent neural networks. In Platt, J., Koller, D., Singer, Y., and Roweis, S., editors, Advances in Neural Information Processing Systems (NIPS) 20, pages 577–584. MIT Press, Cambridge, MA.
Graves, A. and Jaitly, N. (2014). Towards end-to-end speech recognition with recurrent neural networks.
In Proc. 31st International Conference on Machine Learning (ICML), pages 1764–1772.
Graves, A., Liwicki, M., Fernandez, S., Bertolami, R., Bunke, H., and Schmidhuber, J. (2009). A novel connectionist system for improved unconstrained handwriting recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 31(5).
Graves, A., Mohamed, A.-R., and Hinton, G. E. (2013). Speech recognition with deep recurrent neural networks. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 6645–6649. IEEE.
Graves, A. and Schmidhuber, J. (2005). Framewise phoneme classification with bidirectional LSTM and other neural network architectures. Neural Networks, 18(5-6):602–610.
Graves, A. and Schmidhuber, J. (2009). Offline handwriting recognition with multidimensional recurrent neural networks. In Advances in Neural Information Processing Systems (NIPS) 21, pages 545–552.
MIT Press, Cambridge, MA.
Graziano, M. (2009). The Intelligent Movement Machine: An Ethological Perspective on the Primate
Motor System. Oxford University Press, USA.
Griewank, A. (2012). Documenta Mathematica - Extra Volume ISMP, pages 389–400.
Grondman, I., Busoniu, L., Lopes, G. A. D., and Babuska, R. (2012). A survey of actor-critic reinforcement learning: Standard and natural policy gradients. Systems, Man, and Cybernetics, Part C: Applications and Reviews, IEEE Transactions on, 42(6):1291–1307.
Grossberg, S. (1969). Some networks that can learn, remember, and reproduce any number of complicated space-time patterns, I. Journal of Mathematics and Mechanics, 19:53–91.
Grossberg, S. (1976a). Adaptive pattern classification and universal recoding, 1: Parallel development and coding of neural feature detectors. Biological Cybernetics, 23:187–202.
Grossberg, S. (1976b). Adaptive pattern classification and universal recoding, 2: Feedback, expectation, olfaction, and illusions. Biological Cybernetics, 23.
Gruau, F., Whitley, D., and Pyeatt, L. (1996). A comparison between cellular encoding and direct encoding for genetic neural networks. NeuroCOLT Technical Report NC-TR-96-048, ESPRIT Working Group in Neural and Computational Learning, NeuroCOLT 8556.
Gr¨unwald, P. D., Myung, I. J., and Pitt, M. A. (2005). Advances in minimum description length: Theory and applications. MIT Press.
Gr¨uttner, M., Sehnke, F., Schaul, T., and Schmidhuber, J. (2010). Multi-Dimensional Deep Memory AtariGo Players for Parameter Exploring Policy Gradients. In Proceedings of the International Conference on Artificial Neural Networks ICANN, pages 114–123. Springer.
Guyon, I., Vapnik, V., Boser, B., Bottou, L., and Solla, S. A. (1992). Structural risk minimization for character recognition. In Lippman, D. S., Moody, J. E., and Touretzky, D. S., editors, Advances in Neural Information Processing Systems (NIPS) 4, pages 471–479. Morgan Kaufmann.
Hadamard, J. (1908). M´emoire sur le probl`eme d'analyse relatif `a l'´equilibre des plaques ´elastiques encastr´ees.
M´emoires pr´esent´es par divers savants `a l'Acad´emie des sciences de l'Institut de France:
´Extrait. Imprimerie nationale.
Hadsell, R., Chopra, S., and LeCun, Y. (2006). Dimensionality reduction by learning an invariant mapping.
In Proc. Computer Vision and Pattern Recognition Conference (CVPR'06). IEEE Press.
Hansen, N., M¨uller, S. D., and Koumoutsakos, P. (2003). Reducing the time complexity of the derandomized evolution strategy with covariance matrix adaptation (CMA-ES). Evolutionary Computation, 11(1):1–18.
Hansen, N. and Ostermeier, A. (2001). Completely derandomized self-adaptation in evolution strategies.
Evolutionary Computation, 9(2):159–195.
Hanson, S. J. and Pratt, L. Y. (1989). Comparing biases for minimal network construction with backpropagation. In Touretzky, D. S., editor, Advances in Neural Information Processing Systems (NIPS) 1, pages 177–185. San Mateo, CA: Morgan Kaufmann.
Happel, B. L. and Murre, J. M. (1994). Design and evolution of modular neural network architectures.
Neural Networks, 7(6):985–1004.
Hashem, S. and Schmeiser, B. (1992). Improving model accuracy using optimal linear combinations of trained neural networks. IEEE Transactions on Neural Networks, 6:792–794.
Hassibi, B. and Stork, D. G. (1993). Second order derivatives for network pruning: Optimal brain surgeon. In Lippman, D. S., Moody, J. E., and Touretzky, D. S., editors, Advances in Neural Information
Processing Systems 5, pages 164–171. Morgan Kaufmann.
Hastie, T., Tibshirani, R., and Friedman, J. (2009). The elements of statistical learning. Springer Series in Statistics.
Hastie, T. J. and Tibshirani, R. J. (1990). Generalized additive models. Monographs on Statisics and Applied Probability, 43.
Hawkins, J. and George, D. (2006). Hierarchical Temporal Memory - Concepts, Theory, and Terminology.
Numenta Inc.
Haykin, S. S. (2001). Kalman filtering and neural networks. Wiley Online Library.
Hebb, D. O. (1949). The Organization of Behavior. Wiley, New York.
Hecht-Nielsen, R. (1989). Theory of the backpropagation neural network. In International Joint Conference on Neural Networks (IJCNN), pages 593–605. IEEE.
Heemskerk, J. N. (1995). Overview of neural hardware. Neurocomputers for Brain-Style Processing.
Design, Implementation and Application.
Heess, N., Silver, D., and Teh, Y. W. (2012). Actor-critic reinforcement learning with energy-based policies.
In Proc. European Workshop on Reinforcement Learning, pages 43–57.
Heidrich-Meisner, V. and Igel, C. (2009). Neuroevolution strategies for episodic reinforcement learning.
Journal of Algorithms, 64(4):152–168.
Herrero, J., Valencia, A., and Dopazo, J. (2001). A hierarchical unsupervised growing neural network for clustering gene expression patterns. Bioinformatics, 17(2):126–136.
Hertz, J., Krogh, A., and Palmer, R. (1991). Introduction to the Theory of Neural Computation. AddisonWesley, Redwood City.
Hestenes, M. R. and Stiefel, E. (1952). Methods of conjugate gradients for solving linear systems. Journal of research of the National Bureau of Standards, 49:409–436.
Hihi, S. E. and Bengio, Y. (1996). Hierarchical recurrent neural networks for long-term dependencies.
In Touretzky, D. S., Mozer, M. C., and Hasselmo, M. E., editors, Advances in Neural Information
Processing Systems 8, pages 493–499. MIT Press.
Hinton, G. and Salakhutdinov, R. (2006).
Reducing the dimensionality of data with neural networks.
Science, 313(5786):504–507.
Hinton, G. E. (1989). Connectionist learning procedures. Artificial intelligence, 40(1):185–234.
Hinton, G. E. (2002). Training products of experts by minimizing contrastive divergence. Neural Comp., 14(8):1771–1800.
Hinton, G. E., Dayan, P., Frey, B. J., and Neal, R. M. (1995). The wake-sleep algorithm for unsupervised neural networks. Science, 268:1158–1160.
Hinton, G. E., Deng, L., Yu, D., Dahl, G. E., Mohamed, A., Jaitly, N., Senior, A., Vanhoucke, V., Nguyen, P., Sainath, T. N., and Kingsbury, B. (2012a). Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. IEEE Signal Process. Mag., 29(6):82–97.
Hinton, G. E. and Ghahramani, Z. (1997). Generative models for discovering sparse distributed representations. Philosophical Transactions of the Royal Society B, 352:1177–1190.
Hinton, G. E., Osindero, S., and Teh, Y.-W. (2006). A fast learning algorithm for deep belief nets. Neural
Computation, 18(7):1527–1554.
Hinton, G. E. and Sejnowski, T. E. (1986). Learning and relearning in Boltzmann machines. In Parallel
Distributed Processing, volume 1, pages 282–317. MIT Press.
Hinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. R. (2012b). Improving neural networks by preventing co-adaptation of feature detectors. Technical Report arXiv:1207.0580.
Hinton, G. E. and van Camp, D. (1993). Keeping neural networks simple. In Proceedings of the International Conference on Artificial Neural Networks, Amsterdam, pages 11–18. Springer.
Hochreiter, S. (1991). Untersuchungen zu dynamischen neuronalen Netzen. Diploma thesis, Institut f¨ur
Informatik, Lehrstuhl Prof. Brauer, Technische Universit¨at M¨unchen. Advisor: J. Schmidhuber.
Hochreiter, S., Bengio, Y., Frasconi, P., and Schmidhuber, J. (2001a). Gradient flow in recurrent nets: the difficulty of learning long-term dependencies. In Kremer, S. C. and Kolen, J. F., editors, A Field Guide to Dynamical Recurrent Neural Networks. IEEE Press.
Hochreiter, S. and Obermayer, K. (2005). Sequence classification for protein analysis. In Snowbird Workshop, Snowbird, Utah. Computational and Biological Learning Society.
Hochreiter, S. and Schmidhuber, J. (1996). Bridging long time lags by weight guessing and "Long ShortTerm Memory". In Silva, F. L., Principe, J. C., and Almeida, L. B., editors, Spatiotemporal models in biological and artificial systems, pages 65–72. IOS Press, Amsterdam, Netherlands. Serie: Frontiers in Artificial Intelligence and Applications, Volume 37.
Hochreiter, S. and Schmidhuber, J. (1997a). Flat minima. Neural Computation, 9(1):1–42.
Hochreiter, S. and Schmidhuber, J. (1997b). Long Short-Term Memory. Neural Computation, 9(8):1735–
1780. Based on TR FKI-207-95, TUM (1995).
Hochreiter, S. and Schmidhuber, J. (1999). Feature extraction through LOCOCODE. Neural Computation, 11(3):679–714.
Hochreiter, S., Younger, A. S., and Conwell, P. R. (2001b). Learning to learn using gradient descent. In
Lecture Notes on Comp. Sci. 2130, Proc. Intl. Conf. on Artificial Neural Networks (ICANN-2001), pages
87–94. Springer: Berlin, Heidelberg.
Hodgkin, A. L. and Huxley, A. F. (1952). A quantitative description of membrane current and its application to conduction and excitation in nerve. The Journal of Physiology, 117(4):500.
Hoerzer, G. M., Legenstein, R., and Maass, W. (2014). Emergence of complex computational structures from chaotic neural networks through reward-modulated Hebbian learning. Cerebral Cortex, 24:677–
Holden, S. B. (1994). On the Theory of Generalization and Self-Structuring in Linearly Weighted Connectionist Networks. PhD thesis, Cambridge University, Engineering Department.
Holland, J. H. (1975). Adaptation in Natural and Artificial Systems. University of Michigan Press, Ann
Arbor.
Honavar, V. and Uhr, L. (1993). Generative learning structures and processes for generalized connectionist networks. Information Sciences, 70(1):75–108.
Honavar, V. and Uhr, L. M. (1988). A network of neuron-like units that learns to perceive by generation as well as reweighting of its links. In Touretzky, D., Hinton, G. E., and Sejnowski, T., editors, Proc. of the 1988 Connectionist Models Summer School, pages 472–484, San Mateo. Morgan Kaufman.
Hopfield, J. J. (1982).
Neural networks and physical systems with emergent collective computational abilities. Proc. of the National Academy of Sciences, 79:2554–2558.
Hornik, K., Stinchcombe, M., and White, H. (1989). Multilayer feedforward networks are universal approximators. Neural Networks, 2(5):359–366.
Hubel, D. H. and Wiesel, T. (1962). Receptive fields, binocular interaction, and functional architecture in the cat's visual cortex. Journal of Physiology (London), 160:106–154.
Hubel, D. H. and Wiesel, T. N. (1968). Receptive fields and functional architecture of monkey striate cortex. The Journal of Physiology, 195(1):215–243.
Huffman, D. A. (1952). A method for construction of minimum-redundancy codes. Proceedings IRE, 40:1098–1101.
Hung, C. P., Kreiman, G., Poggio, T., and DiCarlo, J. J. (2005). Fast readout of object identity from macaque inferior temporal cortex. Science, 310(5749):863–866.
Hutter, M. (2002). The fastest and shortest algorithm for all well-defined problems. International Journal of Foundations of Computer Science, 13(3):431–443. (On J. Schmidhuber's SNF grant 20-61847).
Hutter, M. (2005). Universal Artificial Intelligence: Sequential Decisions based on Algorithmic Probability. Springer, Berlin. (On J. Schmidhuber's SNF grant 20-61847).
Hyv¨arinen, A., Hoyer, P., and Oja, E. (1999). Sparse code shrinkage: Denoising by maximum likelihood estimation. In Kearns, M., Solla, S. A., and Cohn, D., editors, Advances in Neural Information
Processing Systems (NIPS) 12. MIT Press.
Hyv¨arinen, A., Karhunen, J., and Oja, E. (2001). Independent component analysis. John Wiley & Sons.
ICPR 2012 Contest on Mitosis Detection in Breast Cancer Histological Images (2012).
IPAL Laboratory and TRIBVN Company and Pitie-Salpetriere Hospital and CIALAB of Ohio State Univ., http://ipal.cnrs.fr/ICPR2012/.
Igel, C. (2003). Neuroevolution for reinforcement learning using evolution strategies. In Reynolds, R., Abbass, H., Tan, K. C., Mckay, B., Essam, D., and Gedeon, T., editors, Congress on Evolutionary
Computation (CEC 2003), volume 4, pages 2588–2595. IEEE.
Igel, C. and H¨usken, M. (2003). Empirical evaluation of the improved Rprop learning algorithm. Neurocomputing, 50(C):105–123.
Ikeda, S., Ochiai, M., and Sawaragi, Y. (1976). Sequential GMDH algorithm and its application to river flow prediction. IEEE Transactions on Systems, Man and Cybernetics, (7):473–479.
Indermuhle, E., Frinken, V., and Bunke, H. (2012). Mode detection in online handwritten documents using BLSTM neural networks. In Frontiers in Handwriting Recognition (ICFHR), 2012 International
Conference on, pages 302–307. IEEE.
Indermuhle, E., Frinken, V., Fischer, A., and Bunke, H. (2011). Keyword spotting in online handwritten documents containing text and non-text using BLSTM neural networks. In Document Analysis and Recognition (ICDAR), 2011 International Conference on, pages 73–77. IEEE.
Indiveri, G., Linares-Barranco, B., Hamilton, T. J., Van Schaik, A., Etienne-Cummings, R., Delbruck, T., Liu, S.-C., Dudek, P., H¨afliger, P., Renaud, S., et al. (2011). Neuromorphic silicon neuron circuits.
Frontiers in Neuroscience, 5(73).
Ivakhnenko, A. G. (1968).
The group method of data handling – a rival of the method of stochastic approximation. Soviet Automatic Control, 13(3):43–55.
Ivakhnenko, A. G. (1971). Polynomial theory of complex systems. IEEE Transactions on Systems, Man and Cybernetics, (4):364–378.
Ivakhnenko, A. G. (1995). The review of problems solvable by algorithms of the group method of data handling (GMDH). Pattern Recognition and Image Analysis / Raspoznavaniye Obrazov I Analiz Izobrazhenii, 5:527–535.
Ivakhnenko, A. G. and Lapa, V. G. (1965). Cybernetic Predicting Devices. CCM Information Corporation.
Ivakhnenko, A. G., Lapa, V. G., and McDonough, R. N. (1967). Cybernetics and forecasting techniques.
American Elsevier, NY.
Izhikevich, E. M. et al. (2003). Simple model of spiking neurons. IEEE Transactions on Neural Networks, 14(6):1569–1572.
Jaakkola, T., Singh, S. P., and Jordan, M. I. (1995). Reinforcement learning algorithm for partially observable Markov decision problems. In Tesauro, G., Touretzky, D. S., and Leen, T. K., editors, Advances in Neural Information Processing Systems (NIPS) 7, pages 345–352. MIT Press.
Jackel, L., Boser, B., Graf, H.-P., Denker, J., LeCun, Y., Henderson, D., Matan, O., Howard, R., and Baird, H. (1990). VLSI implementation of electronic neural networks: and example in character recognition.
In IEEE, editor, IEEE International Conference on Systems, Man, and Cybernetics, pages 320–322, Los
Angeles, CA.
Jacob, C., Lindenmayer, A., and Rozenberg, G. (1994). Genetic L-System Programming. In Parallel
Problem Solving from Nature III, Lecture Notes in Computer Science.
Jacobs, R. A. (1988). Increased rates of convergence through learning rate adaptation. Neural Networks, 1(4):295–307.
Jaeger, H. (2001). The "echo state" approach to analysing and training recurrent neural networks. Technical
Report GMD Report 148, German National Research Center for Information Technology.
Jaeger, H. (2002). Short term memory in echo state networks. GMD-Report 152, GMD - German National
Research Institute for Computer Science.
Jaeger, H. (2004). Harnessing nonlinearity: Predicting chaotic systems and saving energy in wireless communication. Science, 304:78–80.
Jain, V. and Seung, S. (2009).
Natural image denoising with convolutional networks.
In Koller, D., Schuurmans, D., Bengio, Y., and Bottou, L., editors, Advances in Neural Information Processing Systems(NIPS) 21, pages 769–776. Curran Associates, Inc.
Jameson, J. (1991). Delayed reinforcement learning with multiple time scale hierarchical backpropagated adaptive critics. In Neural Networks for Control.
Ji, S., Xu, W., Yang, M., and Yu, K. (2013). 3D convolutional neural networks for human action recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(1):221–231.
Jim, K., Giles, C. L., and Horne, B. G. (1995). Effects of noise on convergence and generalization in recurrent networks. In Tesauro, G., Touretzky, D., and Leen, T., editors, Advances in Neural Information
Processing Systems (NIPS) 7, page 649. San Mateo, CA: Morgan Kaufmann.
Jin, X., Lujan, M., Plana, L. A., Davies, S., Temple, S., and Furber, S. B. (2010). Modeling spiking neural networks on SpiNNaker. Computing in Science & Engineering, 12(5):91–97.
Jodogne, S. R. and Piater, J. H. (2007).
Closed-loop learning of visual control policies.
J. Artificial
Intelligence Research, 28:349–391.
Jones, J. P. and Palmer, L. A. (1987). An evaluation of the two-dimensional Gabor filter model of simple receptive fields in cat striate cortex. Journal of Neurophysiology, 58(6):1233–1258.
Jordan, M. I. (1986). Serial order: A parallel distributed processing approach. Technical Report ICS Report
8604, Institute for Cognitive Science, University of California, San Diego.
Jordan, M. I. (1988). Supervised learning and systems with excess degrees of freedom. Technical Report
COINS TR 88-27, Massachusetts Institute of Technology.
Jordan, M. I. (1997). Serial order: A parallel distributed processing approach. Advances in Psychology, 121:471–495.
Jordan, M. I. and Rumelhart, D. E. (1990). Supervised learning with a distal teacher. Technical Report
Occasional Paper #40, Center for Cog. Sci., Massachusetts Institute of Technology.
Jordan, M. I. and Sejnowski, T. J. (2001). Graphical models: Foundations of neural computation. MIT
Press.
Juang, C.-F. (2004). A hybrid of genetic algorithm and particle swarm optimization for recurrent network design. Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE Transactions on, 34(2):997–1006.
Judd, J. S. (1990). Neural network design and the complexity of learning. Neural network modeling and connectionism. MIT Press.
Jutten, C. and Herault, J. (1991). Blind separation of sources, part I: An adaptive algorithm based on neuromimetic architecture. Signal Processing, 24(1):1–10.
Kaelbling, L. P., Littman, M. L., and Cassandra, A. R. (1995). Planning and acting in partially observable stochastic domains. Technical report, Brown University, Providence RI.
Kaelbling, L. P., Littman, M. L., and Moore, A. W. (1996). Reinforcement learning: a survey. Journal of AI research, 4:237–285.
Kak, S., Chen, Y., and Wang, L. (2010). Data mining using surface and deep agents based on neural networks. AMCIS 2010 Proceedings.
Kalinke, Y. and Lehmann, H. (1998). Computation in recurrent neural networks: From counters to iterated function systems. In Antoniou, G. and Slaney, J., editors, Advanced Topics in Artificial Intelligence, Proceedings of the 11th Australian Joint Conference on Artificial Intelligence, volume 1502 of LNAI, Berlin, Heidelberg. Springer.
Kalman, R. E. (1960). A new approach to linear filtering and prediction problems. Journal of Basic
Engineering, 82(1):35–45.
Karhunen, J. and Joutsensalo, J. (1995). Generalizations of principal component analysis, optimization problems, and neural networks. Neural Networks, 8(4):549–562.
Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., and Fei-Fei, L. (2014). Large-scale video classification with convolutional neural networks. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR).
Kasabov, N. K. (2014). Neucube: A spiking neural network architecture for mapping, learning and understanding of spatio-temporal brain data. Neural Networks.
Kelley, H. J. (1960). Gradient theory of optimal flight paths. ARS Journal, 30(10):947–954.
Kempter, R., Gerstner, W., and Van Hemmen, J. L. (1999). Hebbian learning and spiking neurons. Physical
Review E, 59(4):4498.
Kerlirzin, P. and Vallet, F. (1993). Robustness in multilayer perceptrons. Neural Computation, 5(1):473–
Khan, M. M., Khan, G. M., and Miller, J. F. (2010). Evolution of neural networks using Cartesian Genetic
Programming. In IEEE Congress on Evolutionary Computation (CEC), pages 1–8.
Khan, M. M., Lester, D. R., Plana, L. A., Rast, A., Jin, X., Painkras, E., and Furber, S. B. (2008). SpiNNaker: mapping neural networks onto a massively-parallel chip multiprocessor. In International Joint
Conference on Neural Networks (IJCNN), pages 2849–2856. IEEE.
Khan, S. H., Bennamoun, M., Sohel, F., and Togneri, R. (2014). Automatic feature learning for robust shadow detection. In IEEE Conference on Computer Vision and Pattern Recognition CVPR.
Kimura, H., Miyazaki, K., and Kobayashi, S. (1997). Reinforcement learning in POMDPs with function approximation. In ICML, volume 97, pages 152–160.
Kistler, W. M., Gerstner, W., and van Hemmen, J. L. (1997). Reduction of the Hodgkin-Huxley equations to a single-variable threshold model. Neural Computation, 9(5):1015–1045.
Kitano, H. (1990). Designing neural networks using genetic algorithms with graph generation system.
Complex Systems, 4:461–476.
Klampfl, S. and Maass, W. (2013). Emergence of dynamic memory traces in cortical microcircuit models through STDP. The Journal of Neuroscience, 33(28):11515–11529.
Klapper-Rybicka, M., Schraudolph, N. N., and Schmidhuber, J. (2001). Unsupervised learning in LSTM recurrent neural networks. In Lecture Notes on Comp. Sci. 2130, Proc. Intl. Conf. on Artificial Neural
Networks (ICANN-2001), pages 684–691. Springer: Berlin, Heidelberg.
Kobatake, E. and Tanaka, K. (1994). Neuronal selectivities to complex object features in the ventral visual pathway of the macaque cerebral cortex. J. Neurophysiol., 71:856–867.
Kohl, N. and Stone, P. (2004). Policy gradient reinforcement learning for fast quadrupedal locomotion.
In Robotics and Automation, 2004. Proceedings. ICRA'04. 2004 IEEE International Conference on, volume 3, pages 2619–2624. IEEE.
Kohonen, T. (1972). Correlation matrix memories. Computers, IEEE Transactions on, 100(4):353–359.
Kohonen, T. (1982). Self-organized formation of topologically correct feature maps. Biological Cybernetics, 43(1):59–69.
Kohonen, T. (1988). Self-Organization and Associative Memory. Springer, second edition.
Koikkalainen, P. and Oja, E. (1990). Self-organizing hierarchical feature maps. In International Joint
Conference on Neural Networks (IJCNN), pages 279–284. IEEE.
Kolmogorov, A. N. (1965a). On the representation of continuous functions of several variables by superposition of continuous functions of one variable and addition.
Doklady Akademii. Nauk USSR,, 114:679–681.
Kolmogorov, A. N. (1965b). Three approaches to the quantitative definition of information. Problems of Information Transmission, 1:1–11.
Kompella, V. R., Luciw, M. D., and Schmidhuber, J. (2012). Incremental slow feature analysis: Adaptive low-complexity slow feature updating from high-dimensional input streams. Neural Computation, 24(11):2994–3024.
Kondo, T. (1998). GMDH neural network algorithm using the heuristic self-organization method and its application to the pattern identification problem. In Proceedings of the 37th SICE Annual Conference
SICE'98, pages 1143–1148. IEEE.
Kondo, T. and Ueno, J. (2008). Multi-layered GMDH-type neural network self-selecting optimum neural network architecture and its application to 3-dimensional medical image recognition of blood vessels.
International Journal of Innovative Computing, Information and Control, 4(1):175–187.
Kord´ık, P., N´aplava, P., Snorek, M., and Genyk-Berezovskyj, M. (2003). Modified GMDH method and models quality evaluation by visualization. Control Systems and Computers, 2:68–75.
Korkin, M., de Garis, H., Gers, F., and Hemmi, H. (1997). CBM (CAM-Brain Machine) - a hardware tool which evolves a neural net module in a fraction of a second and runs a million neuron artificial brain in real time.
Kosko, B. (1990). Unsupervised learning in noise. IEEE Transactions on Neural Networks, 1(1):44–57.
Koutn´ık, J., Cuccu, G., Schmidhuber, J., and Gomez, F. (July 2013). Evolving large-scale neural networks for vision-based reinforcement learning. In Proceedings of the Genetic and Evolutionary Computation
Conference (GECCO), pages 1061–1068, Amsterdam. ACM.
Koutn´ık, J., Gomez, F., and Schmidhuber, J. (2010). Evolving neural networks in compressed weight space. In Proceedings of the 12th Annual Conference on Genetic and Evolutionary Computation, pages
619–626.
Koutn´ık, J., Greff, K., Gomez, F., and Schmidhuber, J. (2014).
A Clockwork RNN.
In Proceedings of the 31th International Conference on Machine Learning (ICML), volume 32, pages 1845–1853. arXiv:1402.3511 [cs.NE].
Koza, J. R. (1992). Genetic Programming – On the Programming of Computers by Means of Natural
Selection. MIT Press.
Kramer, M. (1991). Nonlinear principal component analysis using autoassociative neural networks. AIChE
Journal, 37:233–243.
Kremer, S. C. and Kolen, J. F. (2001). Field guide to dynamical recurrent networks. Wiley-IEEE Press.
Kriegeskorte, N., Mur, M., Ruff, D. A., Kiani, R., Bodurka, J., Esteky, H., Tanaka, K., and Bandettini, P. A.(2008). Matching categorical object representations in inferior temporal cortex of man and monkey.
Neuron, 60(6):1126–1141.
Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems (NIPS 2012), page 4.
Krogh, A. and Hertz, J. A. (1992). A simple weight decay can improve generalization. In Lippman, D. S., Moody, J. E., and Touretzky, D. S., editors, Advances in Neural Information Processing Systems 4, pages
950–957. Morgan Kaufmann.
Kruger, N., Janssen, P., Kalkan, S., Lappe, M., Leonardis, A., Piater, J., Rodriguez-Sanchez, A., and Wiskott, L. (2013). Deep hierarchies in the primate visual cortex: What can we learn for computer vision? IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1847–1871.
Kullback, S. and Leibler, R. A. (1951). On information and sufficiency. The Annals of Mathematical
Statistics, pages 79–86.
Kurzweil, R. (2012). How to Create a Mind: The Secret of Human Thought Revealed.
Lagoudakis, M. G. and Parr, R. (2003). Least-squares policy iteration. JMLR, 4:1107–1149.
Lampinen, J. and Oja, E. (1992). Clustering properties of hierarchical self-organizing maps. Journal of Mathematical Imaging and Vision, 2(2-3):261–272.
Lang, K., Waibel, A., and Hinton, G. E. (1990). A time-delay neural network architecture for isolated word recognition. Neural Networks, 3:23–43.
Lange, S. and Riedmiller, M. (2010). Deep auto-encoder neural networks in reinforcement learning. In
Neural Networks (IJCNN), The 2010 International Joint Conference on, pages 1–8.
Lapedes, A. and Farber, R. (1986). A self-optimizing, nonsymmetrical neural net for content addressable memory and pattern recognition. Physica D, 22:247–259.
Laplace, P. (1774). M´emoire sur la probabilit´e des causes par les ´ev`enements. M´emoires de l'Academie
Royale des Sciences Present´es par Divers Savan, 6:621–656.
Larraanaga, P. and Lozano, J. A. (2001). Estimation of Distribution Algorithms: A New Tool for Evolutionary Computation. Kluwer Academic Publishers, Norwell, MA, USA.
Le, Q. V., Ranzato, M., Monga, R., Devin, M., Corrado, G., Chen, K., Dean, J., and Ng, A. Y. (2012).
Building high-level features using large scale unsupervised learning. In Proc. ICML'12.
LeCun, Y. (1985). Une proc´edure d'apprentissage pour r´eseau `a seuil asym´etrique. Proceedings of Cognitiva 85, Paris, pages 599–604.
LeCun, Y. (1988).
A theoretical framework for back-propagation.
In Touretzky, D., Hinton, G., and Sejnowski, T., editors, Proceedings of the 1988 Connectionist Models Summer School, pages 21–28, CMU, Pittsburgh, Pa. Morgan Kaufmann.
LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., and Jackel, L. D. (1989).
Back-propagation applied to handwritten zip code recognition. Neural Computation, 1(4):541–551.
LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., and Jackel, L. D. (1990a).
Handwritten digit recognition with a back-propagation network. In Touretzky, D. S., editor, Advances in Neural Information Processing Systems 2, pages 396–404. Morgan Kaufmann.
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324.
LeCun, Y., Denker, J. S., and Solla, S. A. (1990b). Optimal brain damage. In Touretzky, D. S., editor, Advances in Neural Information Processing Systems 2, pages 598–605. Morgan Kaufmann.
LeCun, Y., Muller, U., Cosatto, E., and Flepp, B. (2006). Off-road obstacle avoidance through end-to-end learning. In Advances in Neural Information Processing Systems (NIPS 2005).
LeCun, Y., Simard, P., and Pearlmutter, B. (1993).
Automatic learning rate maximization by on-line estimation of the Hessian's eigenvectors. In Hanson, S., Cowan, J., and Giles, L., editors, Advances in Neural Information Processing Systems (NIPS 1992), volume 5. Morgan Kaufmann Publishers, San
Mateo, CA.
Lee, H., Battle, A., Raina, R., and Ng, A. Y. (2007a). Efficient sparse coding algorithms. In Advances in Neural Information Processing Systems (NIPS) 19, pages 801–808.
Lee, H., Ekanadham, C., and Ng, A. Y. (2007b). Sparse deep belief net model for visual area V2. In
Advances in Neural Information Processing Systems (NIPS), volume 7, pages 873–880.
Lee, H., Grosse, R., Ranganath, R., and Ng, A. Y. (2009a). Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. In Proceedings of the 26th International
Conference on Machine Learning (ICML), pages 609–616.
Lee, H., Pham, P. T., Largman, Y., and Ng, A. Y. (2009b). Unsupervised feature learning for audio classification using convolutional deep belief networks. In Proc. NIPS, volume 9, pages 1096–1104.
Lee, L. (1996). Learning of context-free languages: A survey of the literature. Technical Report TR-12-96, Center for Research in Computing Technology, Harvard University, Cambridge, Massachusetts.
Lee, S. and Kil, R. M. (1991). A Gaussian potential function network with hierarchically self-organizing learning. Neural Networks, 4(2):207–224.
Legendre, A. M. (1805). Nouvelles m´ethodes pour la d´etermination des orbites des cometes. F. Didot.
Legenstein, R., Wilbert, N., and Wiskott, L. (2010). Reinforcement learning on slow features of highdimensional input streams. PLoS Computational Biology, 6(8).
Legenstein, R. A. and Maass, W. (2002). Neural circuits for pattern recognition with small total wire length. Theor. Comput. Sci., 287(1):239–249.
Leibniz, G. W. (1676). Memoir using the chain rule (cited in TMME 7:2&3 p 321-332, 2010).
Leibniz, G. W. (1684). Nova methodus pro maximis et minimis, itemque tangentibus, quae nec fractas, nec irrationales quantitates moratur, et singulare pro illis calculi genus. Acta Eruditorum, pages 467–473.
Lenat, D. B. (1983). Theory formation by heuristic search. Machine Learning, 21.
Lenat, D. B. and Brown, J. S. (1984). Why AM an EURISKO appear to work. Artificial Intelligence, 23(3):269–294.
Lennie, P. and Movshon, J. A. (2005). Coding of color and form in the geniculostriate visual pathway.
Journal of the Optical Society of America A, 22(10):2013–2033.
Levenberg, K. (1944). A method for the solution of certain problems in least squares. Quarterly of applied mathematics, 2:164–168.
Levin, A. U., Leen, T. K., and Moody, J. E. (1994). Fast pruning using principal components. In Advances in Neural Information Processing Systems 6, page 35. Morgan Kaufmann.
Levin, A. U. and Narendra, K. S. (1995). Control of nonlinear dynamical systems using neural networks. ii. observability, identification, and control. IEEE Transactions on Neural Networks, 7(1):30–42.
Levin, L. A. (1973a). On the notion of a random sequence. Soviet Math. Dokl., 14(5):1413–1416.
Levin, L. A. (1973b).
Universal sequential search problems.
Problems of Information Transmission, 9(3):265–266.
Lewicki, M. S. and Olshausen, B. A. (1998). Inferring sparse, overcomplete image codes using an efficient coding framework. In Jordan, M. I., Kearns, M. J., and Solla, S. A., editors, Advances in Neural
Information Processing Systems (NIPS) 10, pages 815–821.
L'Hˆopital, G. F. A. (1696). Analyse des infiniment petits, pour l'intelligence des lignes courbes. Paris:
L'Imprimerie Royale.
Li, M. and Vit´anyi, P. M. B. (1997). An Introduction to Kolmogorov Complexity and its Applications (2nd edition). Springer.
Li, R., Zhang, W., Suk, H.-I., Wang, L., Li, J., Shen, D., and Ji, S. (2014). Deep learning based imaging data completion for improved brain disease diagnosis. In Proc. MICCAI. Springer.
Lin, L. (1993). Reinforcement Learning for Robots Using Neural Networks. PhD thesis, Carnegie Mellon
University, Pittsburgh.
Lin, T., Horne, B., Tino, P., and Giles, C. (1996). Learning long-term dependencies in NARX recurrent neural networks. IEEE Transactions on Neural Networks, 7(6):1329–1338.
Lindenmayer, A. (1968). Mathematical models for cellular interaction in development. J. Theoret. Biology, 18:280–315.
Lindst¨adt, S. (1993a). Comparison of two unsupervised neural network models for redundancy reduction.
In Mozer, M. C., Smolensky, P., Touretzky, D. S., Elman, J. L., and Weigend, A. S., editors, Proc. of the 1993 Connectionist Models Summer School, pages 308–315. Hillsdale, NJ: Erlbaum Associates.
Lindst¨adt, S. (1993b). Comparison of unsupervised neural networks for redundancy reduction. Master's thesis, Dept. of Comp. Sci., University of Colorado at Boulder. Advisor: J. S.
Linnainmaa, S. (1970). The representation of the cumulative rounding error of an algorithm as a Taylor expansion of the local rounding errors. Master's thesis, Univ. Helsinki.
Linnainmaa, S. (1976). Taylor expansion of the accumulated rounding error. BIT Numerical Mathematics, 16(2):146–160.
Linsker, R. (1988). Self-organization in a perceptual network. IEEE Computer, 21:105–117.
Littman, M. L. (1996). Algorithms for Sequential Decision Making. PhD thesis, Brown University.
Littman, M. L., Cassandra, A. R., and Kaelbling, L. P. (1995). Learning policies for partially observable environments: Scaling up. In Prieditis, A. and Russell, S., editors, Machine Learning: Proceedings of the Twelfth International Conference, pages 362–370. Morgan Kaufmann Publishers, San Francisco, CA.
Liu, S.-C., Kramer, J., Indiveri, G., Delbr¨uck, T., Burg, T., Douglas, R., et al. (2001). Orientation-selective aVLSI spiking neurons. Neural Networks, 14(6-7):629–643.
Ljung, L. (1998). System identification. Springer.
Logothetis, N. K., Pauls, J., and Poggio, T. (1995). Shape representation in the inferior temporal cortex of monkeys. Current Biology, 5(5):552–563.
Loiacono, D., Cardamone, L., and Lanzi, P. L. (2011). Simulated car racing championship competition software manual. Technical report, Dipartimento di Elettronica e Informazione, Politecnico di Milano, Italy.
Loiacono, D., Lanzi, P. L., Togelius, J., Onieva, E., Pelta, D. A., Butz, M. V., L¨onneker, T. D., Cardamone, L., Perez, D., S´aez, Y., Preuss, M., and Quadflieg, J. (2009). The 2009 simulated car racing championship.
Lowe, D. (1999). Object recognition from local scale-invariant features. In The Proceedings of the Seventh
IEEE International Conference on Computer Vision (ICCV), volume 2, pages 1150–1157.
Lowe, D. (2004). Distinctive image features from scale-invariant key-points. Intl. Journal of Computer
Vision, 60:91–110.
Luciw, M., Kompella, V. R., Kazerounian, S., and Schmidhuber, J. (2013). An intrinsic value system for developing multiple invariant representations with incremental slowness learning. Frontiers in Neurorobotics, 7(9).
Lusci, A., Pollastri, G., and Baldi, P. (2013). Deep architectures and deep learning in chemoinformatics: the prediction of aqueous solubility for drug-like molecules.
Journal of Chemical Information and Modeling, 53(7):1563–1575.
Maas, A. L., Hannun, A. Y., and Ng, A. Y. (2013). Rectifier nonlinearities improve neural network acoustic models. In International Conference on Machine Learning (ICML).
Maass, W. (1996). Lower bounds for the computational power of networks of spiking neurons. Neural
Computation, 8(1):1–40.
Maass, W. (1997). Networks of spiking neurons: the third generation of neural network models. Neural
Networks, 10(9):1659–1671.
Maass, W. (2000). On the computational power of winner-take-all. Neural Computation, 12:2519–2535.
Maass, W., Natschl¨ager, T., and Markram, H. (2002). Real-time computing without stable states: A new framework for neural computation based on perturbations. Neural Computation, 14(11):2531–2560.
MacKay, D. J. C. (1992). A practical Bayesian framework for backprop networks. Neural Computation, 4:448–472.
MacKay, D. J. C. and Miller, K. D. (1990). Analysis of Linsker's simulation of Hebbian rules. Neural
Computation, 2:173–187.
Maclin, R. and Shavlik, J. W. (1993). Using knowledge-based neural networks to improve algorithms:
Refining the Chou-Fasman algorithm for protein folding. Machine Learning, 11(2-3):195–215.
Maclin, R. and Shavlik, J. W. (1995). Combining the predictions of multiple classifiers: Using competitive learning to initialize neural networks. In Proc. IJCAI, pages 524–531.
Madala, H. R. and Ivakhnenko, A. G. (1994). Inductive learning algorithms for complex systems modeling.
CRC Press, Boca Raton.
Madani, O., Hanks, S., and Condon, A. (2003). On the undecidability of probabilistic planning and related stochastic optimization problems. Artificial Intelligence, 147(1):5–34.
Maei, H. R. and Sutton, R. S. (2010). GQ(λ): A general gradient algorithm for temporal-difference prediction learning with eligibility traces. In Proceedings of the Third Conference on Artificial General
Intelligence, volume 1, pages 91–96.
Maex, R. and Orban, G. (1996). Model circuit of spiking neurons generating directional selectivity in simple cells. Journal of Neurophysiology, 75(4):1515–1545.
Mahadevan, S. (1996). Average reward reinforcement learning: Foundations, algorithms, and empirical results. Machine Learning, 22:159.
Maniezzo, V. (1994). Genetic evolution of the topology and weight distribution of neural networks. IEEE
Transactions on Neural Networks, 5(1):39–53.
Manolios, P. and Fanelli, R. (1994). First-order recurrent neural networks and deterministic finite state automata. Neural Computation, 6:1155–1173.
Markram, H. (2012). The human brain project. Scientific American, 306(6):50–55.
Marquardt, D. W. (1963). An algorithm for least-squares estimation of nonlinear parameters. Journal of the Society for Industrial & Applied Mathematics, 11(2):431–441.
Martens, J. (2010). Deep learning via Hessian-free optimization. In F¨urnkranz, J. and Joachims, T., editors, Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 735–742, Haifa, Israel. Omnipress.
Martens, J. and Sutskever, I. (2011). Learning recurrent neural networks with Hessian-free optimization.
In Proceedings of the 28th International Conference on Machine Learning (ICML), pages 1033–1040.
Martinetz, T. M., Ritter, H. J., and Schulten, K. J. (1990). Three-dimensional neural net for learning visuomotor coordination of a robot arm. IEEE Transactions on Neural Networks, 1(1):131–136.
Masci, J., Giusti, A., Ciresan, D. C., Fricout, G., and Schmidhuber, J. (2013). A fast learning algorithm for image segmentation with max-pooling convolutional networks. In International Conference on Image
Processing (ICIP13), pages 2713–2717.
Matsuoka, K. (1992). Noise injection into inputs in back-propagation learning. IEEE Transactions on
Systems, Man, and Cybernetics, 22(3):436–440.
Mayer, H., Gomez, F., Wierstra, D., Nagy, I., Knoll, A., and Schmidhuber, J. (2008).
A system for robotic heart surgery that learns to tie knots using recurrent neural networks. Advanced Robotics, 22(1314):1521–1537.
McCallum, R. A. (1996). Learning to use selective attention and short-term memory in sequential tasks.
In Maes, P., Mataric, M., Meyer, J.-A., Pollack, J., and Wilson, S. W., editors, From Animals to Animats
4: Proceedings of the Fourth International Conference on Simulation of Adaptive Behavior, Cambridge, MA, pages 315–324. MIT Press, Bradford Books.
McCulloch, W. and Pitts, W. (1943). A logical calculus of the ideas immanent in nervous activity. Bulletin of Mathematical Biophysics, 7:115–133.
Melnik, O., Levy, S. D., and Pollack, J. B. (2000). RAAM for infinite context-free languages. In Proc.
IJCNN (5), pages 585–590.
Memisevic, R. and Hinton, G. E. (2010).
Learning to represent spatial transformations with factored higher-order Boltzmann machines. Neural Computation, 22(6):1473–1492.
Menache, I., Mannor, S., and Shimkin, N. (2002). Q-cut – dynamic discovery of sub-goals in reinforcement learning. In Proc. ECML'02, pages 295–306.
Mesnil, G., Dauphin, Y., Glorot, X., Rifai, S., Bengio, Y., Goodfellow, I., Lavoie, E., Muller, X., Desjardins, G., Warde-Farley, D., Vincent, P., Courville, A., and Bergstra, J. (2011). Unsupervised and transfer learning challenge: a deep learning approach. In JMLR W&CP: Proc. Unsupervised and Transfer Learning, volume 7.
Meuleau, N., Peshkin, L., Kim, K. E., and Kaelbling, L. P. (1999). Learning finite state controllers for partially observable environments. In 15th International Conference of Uncertainty in AI, pages 427–
Miglino, O., Lund, H., and Nolfi, S. (1995). Evolving mobile robots in simulated and real environments.
Artificial Life, 2(4):417–434.
Miller, G., Todd, P., and Hedge, S. (1989). Designing neural networks using genetic algorithms. In Proceedings of the 3rd International Conference on Genetic Algorithms, pages 379–384. Morgan Kauffman.
Miller, J. F. and Harding, S. L. (2009). Cartesian genetic programming. In Proceedings of the 11th Annual
Conference Companion on Genetic and Evolutionary Computation Conference: Late Breaking Papers, pages 3489–3512. ACM.
Miller, J. F. and Thomson, P. (2000). Cartesian genetic programming. In Genetic Programming, pages
121–132. Springer.
Miller, K. D. (1994). A model for the development of simple cell receptive fields and the ordered arrangement of orientation columns through activity-dependent competition between on- and off-center inputs.
Journal of Neuroscience, 14(1):409–441.
Miller, W. T., Werbos, P. J., and Sutton, R. S. (1995). Neural networks for control. MIT Press.
Minai, A. A. and Williams, R. D. (1994). Perturbation response in feedforward networks. Neural Networks, 7(5):783–796.
Minsky, M. (1963).
Steps toward artificial intelligence.
In Feigenbaum, E. and Feldman, J., editors, Computers and Thought, pages 406–450. McGraw-Hill, New York.
Minsky, M. and Papert, S. (1969). Perceptrons. Cambridge, MA: MIT Press.
Minton, S., Carbonell, J. G., Knoblock, C. A., Kuokka, D. R., Etzioni, O., and Gil, Y. (1989). Explanationbased learning: A problem solving perspective. Artificial Intelligence, 40(1):63–118.
Mitchell, T. (1997). Machine Learning. McGraw Hill.
Mitchell, T. M., Keller, R. M., and Kedar-Cabelli, S. T. (1986). Explanation-based generalization: A unifying view. Machine Learning, 1(1):47–80.
Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M. (Dec
2013). Playing Atari with deep reinforcement learning. Technical Report arXiv:1312.5602 [cs.LG], Deepmind Technologies.
Mohamed, A., Dahl, G. E., and Hinton, G. E. (2009). Deep belief networks for phone recognition. In
NIPS'22 workshop on deep learning for speech recognition.
Mohamed, A. and Hinton, G. E. (2010). Phone recognition using restricted Boltzmann machines. In IEEE
International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 4354–4357.
Molgedey, L. and Schuster, H. G. (1994). Separation of independent signals using time-delayed correlations. Phys. Reviews Letters, 72(23):3634–3637.
Møller, M. F. (1993). Exact calculation of the product of the Hessian matrix of feed-forward network error functions and a vector in O(N) time. Technical Report PB-432, Computer Science Department, Aarhus
University, Denmark.
Montana, D. J. and Davis, L. (1989). Training feedforward neural networks using genetic algorithms. In
Proceedings of the 11th International Joint Conference on Artificial Intelligence (IJCAI) - Volume 1, IJCAI'89, pages 762–767, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.
Montavon, G., Orr, G., and M¨uller, K. (2012). Neural Networks: Tricks of the Trade. Number LNCS 7700 in Lecture Notes in Computer Science Series. Springer Verlag.
Moody, J. E. (1989). Fast learning in multi-resolution hierarchies. In Touretzky, D. S., editor, Advances in Neural Information Processing Systems (NIPS) 1, pages 29–39. Morgan Kaufmann.
Moody, J. E. (1992). The effective number of parameters: An analysis of generalization and regularization in nonlinear learning systems. In Lippman, D. S., Moody, J. E., and Touretzky, D. S., editors, Advances in Neural Information Processing Systems (NIPS) 4, pages 847–854. Morgan Kaufmann.
Moody, J. E. and Utans, J. (1994). Architecture selection strategies for neural networks: Application to corporate bond rating prediction. In Refenes, A. N., editor, Neural Networks in the Capital Markets.
John Wiley & Sons.
Moore, A. and Atkeson, C. (1995). The parti-game algorithm for variable resolution reinforcement learning in multidimensional state-spaces. Machine Learning, 21(3):199–233.
Moore, A. and Atkeson, C. G. (1993). Prioritized sweeping: Reinforcement learning with less data and less time. Machine Learning, 13:103–130.
Moriarty, D. E. (1997). Symbiotic Evolution of Neural Networks in Sequential Decision Tasks. PhD thesis, Department of Computer Sciences, The University of Texas at Austin.
Moriarty, D. E. and Miikkulainen, R. (1996). Efficient reinforcement learning through symbiotic evolution.
Machine Learning, 22:11–32.
Morimoto, J. and Doya, K. (2000). Robust reinforcement learning. In Leen, T. K., Dietterich, T. G., and Tresp, V., editors, Advances in Neural Information Processing Systems (NIPS) 13, pages 1061–1067.
MIT Press.
Mosteller, F. and Tukey, J. W. (1968). Data analysis, including statistics. In Lindzey, G. and Aronson, E., editors, Handbook of Social Psychology, Vol. 2. Addison-Wesley.
Mozer, M. C. (1989). A focused back-propagation algorithm for temporal sequence recognition. Complex
Systems, 3:349–381.
Mozer, M. C. (1991). Discovering discrete distributed representations with iterative competitive learning. In Lippmann, R. P., Moody, J. E., and Touretzky, D. S., editors, Advances in Neural Information
Processing Systems 3, pages 627–634. Morgan Kaufmann.
Mozer, M. C. (1992). Induction of multiscale temporal structure. In Lippman, D. S., Moody, J. E., and Touretzky, D. S., editors, Advances in Neural Information Processing Systems (NIPS) 4, pages 275–282.
Morgan Kaufmann.
Mozer, M. C. and Smolensky, P. (1989). Skeletonization: A technique for trimming the fat from a network via relevance assessment. In Touretzky, D. S., editor, Advances in Neural Information Processing
Systems (NIPS) 1, pages 107–115. Morgan Kaufmann.
Muller, U. A., Gunzinger, A., and Guggenb¨uhl, W. (1995). Fast neural net simulation with a DSP processor array. IEEE Transactions on Neural Networks, 6(1):203–213.
Munro, P. W. (1987). A dual back-propagation scheme for scalar reinforcement learning. Proceedings of the Ninth Annual Conference of the Cognitive Science Society, Seattle, WA, pages 165–176.
Murray, A. F. and Edwards, P. J. (1993). Synaptic weight noise during MLP learning enhances faulttolerance, generalisation and learning trajectory. In S. J. Hanson, J. D. C. and Giles, C. L., editors, Advances in Neural Information Processing Systems (NIPS) 5, pages 491–498. San Mateo, CA: Morgan
Kaufmann.
Nadal, J.-P. and Parga, N. (1994). Non-linear neurons in the low noise limit: a factorial code maximises information transfer. Network, 5:565–581.
Nagumo, J., Arimoto, S., and Yoshizawa, S. (1962). An active pulse transmission line simulating nerve axon. Proceedings of the IRE, 50(10):2061–2070.
Nair, V. and Hinton, G. E. (2010). Rectified linear units improve restricted Boltzmann machines. In
International Conference on Machine Learning (ICML).
Narendra, K. S. and Parthasarathy, K. (1990). Identification and control of dynamical systems using neural networks. Neural Networks, IEEE Transactions on, 1(1):4–27.
Narendra, K. S. and Thathatchar, M. A. L. (1974). Learning automata – a survey. IEEE Transactions on
Systems, Man, and Cybernetics, 4:323–334.
Neal, R. M. (1995). Bayesian learning for neural networks. PhD thesis, University of Toronto.
Neal, R. M. (2006). Classification with Bayesian neural networks. In Quinonero-Candela, J., Magnini, B., Dagan, I., and D'Alche-Buc, F., editors, Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Textual Entailment, volume 3944 of Lecture Notes in Computer Science, pages 28–32. Springer.
Neal, R. M. and Zhang, J. (2006). High dimensional classification with Bayesian neural networks and Dirichlet diffusion trees. In Guyon, I., Gunn, S., Nikravesh, M., and Zadeh, L. A., editors, Feature
Extraction: Foundations and Applications, Studies in Fuzziness and Soft Computing, pages 265–295.
Springer.
Neftci, E., Das, S., Pedroni, B., Kreutz-Delgado, K., and Cauwenberghs, G. (2014). Event-driven contrastive divergence for spiking neuromorphic systems. Frontiers in Neuroscience, 7(272).
Neil, D. and Liu, S.-C. (2014). Minitaur, an event-driven FPGA-based spiking network accelerator. IEEE
Transactions on Very Large Scale Integration (VLSI) Systems, PP(99):1–8.
Nessler, B., Pfeiffer, M., Buesing, L., and Maass, W. (2013).
Bayesian computation emerges in generic cortical microcircuits through spike-timing-dependent plasticity. PLoS Computational Biology, 9(4):e1003037.
Neti, C., Schneider, M. H., and Young, E. D. (1992). Maximally fault tolerant neural networks. In IEEE
Transactions on Neural Networks, volume 3, pages 14–23.
Neuneier, R. and Zimmermann, H.-G. (1996). How to train neural networks. In Orr, G. B. and M¨uller, K.R., editors, Neural Networks: Tricks of the Trade, volume 1524 of Lecture Notes in Computer Science, pages 373–423. Springer.
Newton, I. (1687). Philosophiae naturalis principia mathematica. William Dawson & Sons Ltd., London.
Nguyen, N. and Widrow, B. (1989). The truck backer-upper: An example of self learning in neural networks. In Proceedings of the International Joint Conference on Neural Networks, pages 357–363. IEEE
Press.
Nilsson, N. J. (1980). Principles of artificial intelligence. Morgan Kaufmann, San Francisco, CA, USA.
Nolfi, S., Floreano, D., Miglino, O., and Mondada, F. (1994a). How to evolve autonomous robots: Different approaches in evolutionary robotics. In Brooks, R. A. and Maes, P., editors, Fourth International
Workshop on the Synthesis and Simulation of Living Systems (Artificial Life IV), pages 190–197. MIT.
Nolfi, S., Parisi, D., and Elman, J. L. (1994b). Learning and evolution in neural networks. Adaptive
Behavior, 3(1):5–28.
Nowak, E., Jurie, F., and Triggs, B. (2006). Sampling strategies for bag-of-features image classification.
In Proc. ECCV 2006, pages 490–503. Springer.
Nowlan, S. J. and Hinton, G. E. (1992). Simplifying neural networks by soft weight sharing. Neural
Computation, 4:173–193.
O'Connor, P., Neil, D., Liu, S.-C., Delbruck, T., and Pfeiffer, M. (2013). Real-time classification and sensor fusion with a spiking deep belief network. Frontiers in Neuroscience, 7(178).
Oh, K.-S. and Jung, K. (2004). GPU implementation of neural networks. Pattern Recognition, 37(6):1311–
Oja, E. (1989). Neural networks, principal components, and subspaces. International Journal of Neural
Systems, 1(1):61–68.
Oja, E. (1991). Data compression, feature extraction, and autoassociation in feedforward neural networks.
In Kohonen, T., M¨akisara, K., Simula, O., and Kangas, J., editors, Artificial Neural Networks, volume 1, pages 737–745. Elsevier Science Publishers B.V., North-Holland.
Olshausen, B. A. and Field, D. J. (1996). Emergence of simple-cell receptive field properties by learning a sparse code for natural images. Nature, 381(6583):607–609.
Omlin, C. and Giles, C. L. (1996). Extraction of rules from discrete-time recurrent neural networks. Neural
Networks, 9(1):41–52.
Oquab, M., Bottou, L., Laptev, I., and Sivic, J. (2013). Learning and transferring mid-level image representations using convolutional neural networks. Technical Report hal-00911179.
O'Reilly, R. (2003). Making working memory work: A computational model of learning in the prefrontal cortex and basal ganglia. Technical Report ICS-03-03, ICS.
O'Reilly, R. C. (1996). Biologically plausible error-driven learning using local activation differences: The generalized recirculation algorithm. Neural Computation, 8(5):895–938.
Orr, G. and M¨uller, K. (1998). Neural Networks: Tricks of the Trade. Number LNCS 1524 in Lecture
Notes in Computer Science Series. Springer Verlag.
Ostrovskii, G. M., Volin, Y. M., and Borisov, W. W. (1971). ¨Uber die Berechnung von Ableitungen. Wiss.
Z. Tech. Hochschule f¨ur Chemie, 13:382–384.
Otsuka, M. (2010). Goal-Oriented Representation of the External World: A Free-Energy-Based Approach.
PhD thesis, Nara Institute of Science and Technology.
Otsuka, M., Yoshimoto, J., and Doya, K. (2010). Free-energy-based reinforcement learning in a partially observable environment. In Proc. ESANN.
Otte, S., Krechel, D., Liwicki, M., and Dengel, A. (2012). Local feature based online mode detection with recurrent neural networks. In Proceedings of the 2012 International Conference on Frontiers in Handwriting Recognition, pages 533–537. IEEE Computer Society.
Oudeyer, P.-Y., Baranes, A., and Kaplan, F. (2013). Intrinsically motivated learning of real world sensorimotor skills with developmental constraints. In Baldassarre, G. and Mirolli, M., editors, Intrinsically
Motivated Learning in Natural and Artificial Systems. Springer.
OReilly, R. C., Wyatte, D., Herd, S., Mingus, B., and Jilk, D. J. (2013). Recurrent processing during object recognition. Frontiers in Psychology, 4:124.
Pachitariu, M. and Sahani, M. (2013). Regularization and nonlinearities for neural language models: when are they needed? arXiv preprint arXiv:1301.5650.
Palm, G. (1980). On associative memory. Biological Cybernetics, 36.
Palm, G. (1992).
On the information storage capacity of local learning rules.
Neural Computation, 4(2):703–711.
Pan, S. J. and Yang, Q. (2010). A survey on transfer learning. IEEE Transactions on Knowledge and Data
Engineering, 22(10):1345–1359.
Parekh, R., Yang, J., and Honavar, V. (2000). Constructive neural network learning algorithms for multicategory pattern classification. IEEE Transactions on Neural Networks, 11(2):436–451.
Parker, D. B. (1985). Learning-logic. Technical Report TR-47, Center for Comp. Research in Economics and Management Sci., MIT.
Pascanu, R., Gulcehre, C., Cho, K., and Bengio, Y. (2013a). How to construct deep recurrent neural networks. arXiv preprint arXiv:1312.6026.
Pascanu, R., Mikolov, T., and Bengio, Y. (2013b). On the difficulty of training recurrent neural networks.
In ICML'13: JMLR: W&CP volume 28.
Pasemann, F., Steinmetz, U., and Dieckman, U. (1999). Evolving structure and function of neurocontrollers. In Angeline, P. J., Michalewicz, Z., Schoenauer, M., Yao, X., and Zalzala, A., editors, Proceedings of the Congress on Evolutionary Computation, volume 3, pages 1973–1978, Mayflower Hotel, Washington D.C., USA. IEEE Press.
Pearlmutter, B. A. (1989). Learning state space trajectories in recurrent neural networks. Neural Computation, 1(2):263–269.
Pearlmutter, B. A. (1994). Fast exact multiplication by the Hessian. Neural Computation, 6(1):147–160.
Pearlmutter, B. A. (1995). Gradient calculations for dynamic recurrent neural networks: A survey. IEEE
Transactions on Neural Networks, 6(5):1212–1228.
Pearlmutter, B. A. and Hinton, G. E. (1986). G-maximization: An unsupervised learning procedure for discovering regularities. In Denker, J. S., editor, Neural Networks for Computing: American Institute of Physics Conference Proceedings 151, volume 2, pages 333–338.
Peng, J. and Williams, R. J. (1996). Incremental multi-step Q-learning. Machine Learning, 22:283–290.
P´erez-Ortiz, J. A., Gers, F. A., Eck, D., and Schmidhuber, J. (2003). Kalman filters improve LSTM network performance in problems unsolvable by traditional recurrent nets. Neural Networks, (16):241–250.
Perrett, D., Hietanen, J., Oram, M., Benson, P., and Rolls, E. (1992). Organization and functions of cells responsive to faces in the temporal cortex [and discussion]. Philosophical Transactions of the Royal
Society of London. Series B: Biological Sciences, 335(1273):23–30.
Perrett, D., Rolls, E., and Caan, W. (1982). Visual neurones responsive to faces in the monkey temporal cortex. Experimental Brain Research, 47(3):329–342.
Peters, J. (2010). Policy gradient methods. Scholarpedia, 5(11):3698.
Peters, J. and Schaal, S. (2008a). Natural actor-critic. Neurocomputing, 71:1180–1190.
Peters, J. and Schaal, S. (2008b). Reinforcement learning of motor skills with policy gradients. Neural
Network, 21(4):682–697.
Pham, V., Kermorvant, C., and Louradour, J. (2013). Dropout Improves Recurrent Neural Networks for
Handwriting Recognition. arXiv preprint arXiv:1312.4569.
Pineda, F. J. (1987). Generalization of back-propagation to recurrent neural networks. Physical Review
Letters, 19(59):2229–2232.
Plate, T. A. (1993). Holographic recurrent networks. In S. J. Hanson, J. D. C. and Giles, C. L., editors, Advances in Neural Information Processing Systems (NIPS) 5, pages 34–41. Morgan Kaufmann.
Plumbley, M. D. (1991). On information theory and unsupervised neural networks. Dissertation, published as technical report CUED/F-INFENG/TR.78, Engineering Department, Cambridge University.
Pollack, J. B. (1988). Implications of recursive distributed representations. In Proc. NIPS, pages 527–536.
Pollack, J. B. (1990). Recursive distributed representation. Artificial Intelligence, 46:77–105.
Pontryagin, L. S., Boltyanskii, V. G., Gamrelidze, R. V., and Mishchenko, E. F. (1961). The Mathematical
Theory of Optimal Processes.
Poon, H. and Domingos, P. (2011). Sum-product networks: A new deep architecture. In IEEE International
Conference on Computer Vision (ICCV) Workshops, pages 689–690. IEEE.
Post, E. L. (1936). Finite combinatory processes-formulation 1. The Journal of Symbolic Logic, 1(3):103–
Prasoon, A., Petersen, K., Igel, C., Lauze, F., Dam, E., and Nielsen, M. (2013). Voxel classification based on triplanar convolutional neural networks applied to cartilage segmentation in knee MRI. In Medical
Image Computing and Computer Assisted Intervention (MICCAI), volume 8150 of LNCS, pages 246–
253. Springer.
Precup, D., Sutton, R. S., and Singh, S. (1998). Multi-time models for temporally abstract planning. In
Advances in Neural Information Processing Systems (NIPS), pages 1050–1056. Morgan Kaufmann.
Prokhorov, D. (2010). A convolutional learning system for object classification in 3-D LIDAR data. IEEE
Transactions on Neural Networks, 21(5):858–863.
Prokhorov, D., Puskorius, G., and Feldkamp, L. (2001). Dynamical neural networks for control. In Kolen, J. and Kremer, S., editors, A field guide to dynamical recurrent networks, pages 23–78. IEEE Press.
Prokhorov, D. and Wunsch, D. (1997). Adaptive critic design. IEEE Transactions on Neural Networks, 8(5):997–1007.
Prokhorov, D. V., Feldkamp, L. A., and Tyukin, I. Y. (2002). Adaptive behavior with fixed weights in RNN: an overview. In Proceedings of the IEEE International Joint Conference on Neural Networks (IJCNN), pages 2018–2023.
Puskorius, G. V. and Feldkamp, L. A. (1994). Neurocontrol of nonlinear dynamical systems with Kalman filter trained recurrent networks. IEEE Transactions on Neural Networks, 5(2):279–297.
Raiko, T., Valpola, H., and LeCun, Y. (2012). Deep learning made easier by linear transformations in perceptrons. In International Conference on Artificial Intelligence and Statistics, pages 924–932.
Raina, R., Madhavan, A., and Ng, A. (2009). Large-scale deep unsupervised learning using graphics processors. In Proceedings of the 26th Annual International Conference on Machine Learning (ICML), pages 873–880. ACM.
Ramacher, U., Raab, W., Anlauf, J., Hachmann, U., Beichter, J., Bruels, N., Wesseling, M., Sicheneder, E., Maenner, R., Glaess, J., and Wurz, A. (1993).
Multiprocessor and memory architecture of the neurocomputer SYNAPSE-1. International Journal of Neural Systems, 4(4):333–336.
Ranzato, M., Poultney, C., Chopra, S., and LeCun, Y. (2006). Efficient learning of sparse representations with an energy-based model. In et al., J. P., editor, Advances in Neural Information Processing Systems(NIPS 2006). MIT Press.
Ranzato, M. A., Huang, F., Boureau, Y., and LeCun, Y. (2007). Unsupervised learning of invariant feature hierarchies with applications to object recognition. In Proc. Computer Vision and Pattern Recognition
Conference (CVPR'07), pages 1–8. IEEE Press.
Rauber, A., Merkl, D., and Dittenbach, M. (2002). The growing hierarchical self-organizing map: exploratory analysis of high-dimensional data. IEEE Transactions on Neural Networks, 13(6):1331–1341.
Razavian, A. S., Azizpour, H., Sullivan, J., and Carlsson, S. (2014).
CNN features off-the-shelf: an astounding baseline for recognition. arXiv preprint arXiv:1403.6382.
Rechenberg, I. (1971). Evolutionsstrategie - Optimierung technischer Systeme nach Prinzipien der biologischen Evolution. Dissertation. Published 1973 by Fromman-Holzboog.
Redlich, A. N. (1993). Redundancy reduction as a strategy for unsupervised learning. Neural Computation, 5:289–304.
Refenes, N. A., Zapranis, A., and Francis, G. (1994). Stock performance modeling using neural networks: a comparative study with regression models. Neural Networks, 7(2):375–388.
Rezende, D. J. and Gerstner, W. (2014). Stochastic variational learning in recurrent spiking networks.
Frontiers in Computational Neuroscience, 8:38.
Riedmiller, M. (2005). Neural fitted Q iteration—first experiences with a data efficient neural reinforcement learning method. In Proc. ECML-2005, pages 317–328. Springer-Verlag Berlin Heidelberg.
Riedmiller, M. and Braun, H. (1993). A direct adaptive method for faster backpropagation learning: The Rprop algorithm. In Proc. IJCNN, pages 586–591. IEEE Press.
Riedmiller, M., Lange, S., and Voigtlaender, A. (2012). Autonomous reinforcement learning on raw visual input data in a real world application. In International Joint Conference on Neural Networks (IJCNN), pages 1–8, Brisbane, Australia.
Riesenhuber, M. and Poggio, T. (1999). Hierarchical models of object recognition in cortex. Nat. Neurosci., 2(11):1019–1025.
Rifai, S., Vincent, P., Muller, X., Glorot, X., and Bengio, Y. (2011). Contractive auto-encoders: Explicit invariance during feature extraction. In Proceedings of the 28th International Conference on Machine
Learning (ICML-11), pages 833–840.
Ring, M., Schaul, T., and Schmidhuber, J. (2011). The two-dimensional organization of behavior. In
Proceedings of the First Joint Conference on Development Learning and on Epigenetic Robotics ICDLEPIROB, Frankfurt.
Ring, M. B. (1991). Incremental development of complex behaviors through automatic construction of sensory-motor hierarchies. In Birnbaum, L. and Collins, G., editors, Machine Learning: Proceedings of the Eighth International Workshop, pages 343–347. Morgan Kaufmann.
Ring, M. B. (1993). Learning sequential tasks by incrementally adding higher orders. In S. J. Hanson, J.
D. C. and Giles, C. L., editors, Advances in Neural Information Processing Systems 5, pages 115–122.
Morgan Kaufmann.
Ring, M. B. (1994). Continual Learning in Reinforcement Environments. PhD thesis, University of Texas at Austin, Austin, Texas 78712.
Risi, S. and Stanley, K. O. (2012). A unified approach to evolving plasticity and neural geometry. In
International Joint Conference on Neural Networks (IJCNN), pages 1–8. IEEE.
Rissanen, J. (1986). Stochastic complexity and modeling. The Annals of Statistics, 14(3):1080–1100.
Ritter, H. and Kohonen, T. (1989). Self-organizing semantic maps. Biological Cybernetics, 61(4):241–254.
Robinson, A. J. and Fallside, F. (1987). The utility driven dynamic error propagation network. Technical
Report CUED/F-INFENG/TR.1, Cambridge University Engineering Department.
Robinson, T. and Fallside, F. (1989).
Dynamic reinforcement driven error propagation networks with application to game playing. In Proceedings of the 11th Conference of the Cognitive Science Society, Ann Arbor, pages 836–843.
Rodriguez, P. and Wiles, J. (1998). Recurrent neural networks can learn to implement symbol-sensitive counting. In Advances in Neural Information Processing Systems (NIPS), volume 10, pages 87–93. The MIT Press.
Rodriguez, P., Wiles, J., and Elman, J. (1999). A recurrent neural network that learns to count. Connection
Science, 11(1):5–40.
Roggen, D., Hofmann, S., Thoma, Y., and Floreano, D. (2003). Hardware spiking neural network with runtime reconfigurable connectivity in an autonomous robot. In Proc. NASA/DoD Conference on Evolvable
Hardware, 2003, pages 189–198. IEEE.
Rohwer, R. (1989). The 'moving targets' training method. In Kindermann, J. and Linden, A., editors, Proceedings of 'Distributed Adaptive Neural Information Processing', St.Augustin, 24.-25.5,. Oldenbourg.
Rosenblatt, F. (1958). The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review, 65(6):386.
Rosenblatt, F. (1962). Principles of Neurodynamics. Spartan, New York.
Roux, L., Racoceanu, D., Lomenie, N., Kulikova, M., Irshad, H., Klossa, J., Capron, F., Genestie, C., Naour, G. L., and Gurcan, M. N. (2013). Mitosis detection in breast cancer histological images - an
ICPR 2012 contest. J. Pathol. Inform., 4:8.
Rubner, J. and Schulten, K. (1990). Development of feature detectors by self-organization: A network model. Biological Cybernetics, 62:193–199.
Rubner, J. and Tavan, P. (1989). A self-organization network for principal-component analysis. Europhysics Letters, 10:693–698.
R¨uckstieß, T., Felder, M., and Schmidhuber, J. (2008). State-Dependent Exploration for policy gradient methods. In et al., W. D., editor, European Conference on Machine Learning (ECML) and Principles and Practice of Knowledge Discovery in Databases 2008, Part II, LNAI 5212, pages 234–249.
Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1986). Learning internal representations by error propagation. In Rumelhart, D. E. and McClelland, J. L., editors, Parallel Distributed Processing, volume 1, pages 318–362. MIT Press.
Rumelhart, D. E. and Zipser, D. (1986). Feature discovery by competitive learning. In Parallel Distributed
Processing, pages 151–193. MIT Press.
Rummery, G. and Niranjan, M. (1994). On-line Q-learning using connectionist sytems. Technical Report
CUED/F-INFENG-TR 166, Cambridge University, UK.
Russell, S. J., Norvig, P., Canny, J. F., Malik, J. M., and Edwards, D. D. (1995). Artificial Intelligence: a Modern Approach, volume 2. Englewood Cliffs: Prentice Hall.
Saito, K. and Nakano, R. (1997). Partial BFGS update and efficient step-length calculation for three-layer neural networks. Neural Computation, 9(1):123–141.
Salakhutdinov, R. and Hinton, G. (2009). Semantic hashing. Int. J. Approx. Reasoning, 50(7):969–978.
Sallans, B. and Hinton, G. (2004). Reinforcement learning with factored states and actions. Journal of Machine Learning Research, 5:1063–1088.
Sałustowicz, R. P. and Schmidhuber, J. (1997). Probabilistic incremental program evolution. Evolutionary
Computation, 5(2):123–141.
Samejima, K., Doya, K., and Kawato, M. (2003). Inter-module credit assignment in modular reinforcement learning. Neural Networks, 16(7):985–994.
Samuel, A. L. (1959). Some studies in machine learning using the game of checkers. IBM Journal on
Research and Development, 3:210–229.
Sanger, T. D. (1989). An optimality principle for unsupervised learning. In Touretzky, D. S., editor, Advances in Neural Information Processing Systems (NIPS) 1, pages 11–19. Morgan Kaufmann.
Santamar´ıa, J. C., Sutton, R. S., and Ram, A. (1997). Experiments with reinforcement learning in problems with continuous state and action spaces. Adaptive Behavior, 6(2):163–217.
Saravanan, N. and Fogel, D. B. (1995). Evolving neural control systems. IEEE Expert, pages 23–27.
Saund, E. (1994). Unsupervised learning of mixtures of multiple causes in binary data. In Cowan, J. D., Tesauro, G., and Alspector, J., editors, Advances in Neural Information Processing Systems (NIPS) 6, pages 27–34. Morgan Kaufmann.
Schaback, R. and Werner, H. (1992). Numerische Mathematik, volume 4. Springer.
Sch¨afer, A. M., Udluft, S., and Zimmermann, H.-G. (2006). Learning long term dependencies with recurrent neural networks. In Kollias, S. D., Stafylopatis, A., Duch, W., and Oja, E., editors, ICANN (1), volume 4131 of Lecture Notes in Computer Science, pages 71–80. Springer.
Schapire, R. E. (1990). The strength of weak learnability. Machine Learning, 5:197–227.
Schaul, T. and Schmidhuber, J. (2010). Metalearning. Scholarpedia, 6(5):4650.
Schaul, T., Zhang, S., and LeCun, Y. (2013). No more pesky learning rates. In Proc. 30th International
Conference on Machine Learning (ICML).
Schemmel, J., Grubl, A., Meier, K., and Mueller, E. (2006). Implementing synaptic plasticity in a VLSI spiking neural network model. In International Joint Conference on Neural Networks (IJCNN), pages
1–6. IEEE.
Scherer, D., M¨uller, A., and Behnke, S. (2010). Evaluation of pooling operations in convolutional architectures for object recognition. In Proc. International Conference on Artificial Neural Networks (ICANN), pages 92–101.
Schmidhuber, J. (1987). Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... hook. Inst. f. Inf., Tech. Univ. Munich. http://www.idsia.ch/˜juergen/diploma.html.
Schmidhuber, J. (1989a).
Accelerated learning in back-propagation nets.
In Pfeifer, R., Schreter, Z., Fogelman, Z., and Steels, L., editors, Connectionism in Perspective, pages 429 – 438. Amsterdam:
Elsevier, North-Holland.
Schmidhuber, J. (1989b). A local learning algorithm for dynamic feedforward and recurrent networks.
Connection Science, 1(4):403–412.
Schmidhuber, J. (1990a). Dynamische neuronale Netze und das fundamentale raumzeitliche Lernproblem.(Dynamic neural nets and the fundamental spatio-temporal credit assignment problem.) Dissertation, Inst. f. Inf., Tech. Univ. Munich.
Schmidhuber, J. (1990b). Learning algorithms for networks with internal and external feedback. In Touretzky, D. S., Elman, J. L., Sejnowski, T. J., and Hinton, G. E., editors, Proc. of the 1990 Connectionist
Models Summer School, pages 52–61. Morgan Kaufmann.
Schmidhuber, J. (1990c). The Neural Heat Exchanger. Talks at TU Munich (1990), University of Colorado at Boulder (1992), and Z. Li's NIPS*94 workshop on unsupervised learning. Also published at the Intl.
Conference on Neural Information Processing (ICONIP'96), vol. 1, pages 194-197, 1996.
Schmidhuber, J. (1990d). An on-line algorithm for dynamic reinforcement learning and planning in reactive environments. In Proc. IEEE/INNS International Joint Conference on Neural Networks, San Diego, volume 2, pages 253–258.
Schmidhuber, J. (1991a). Curious model-building control systems. In Proceedings of the International
Joint Conference on Neural Networks, Singapore, volume 2, pages 1458–1463. IEEE press.
Schmidhuber, J. (1991b). Learning to generate sub-goals for action sequences. In Kohonen, T., M¨akisara, K., Simula, O., and Kangas, J., editors, Artificial Neural Networks, pages 967–972. Elsevier Science
Publishers B.V., North-Holland.
Schmidhuber, J. (1991c). Reinforcement learning in Markovian and non-Markovian environments. In
Lippman, D. S., Moody, J. E., and Touretzky, D. S., editors, Advances in Neural Information Processing
Systems 3 (NIPS 3), pages 500–506. Morgan Kaufmann.
Schmidhuber, J. (1992a). A fixed size storage O(n3) time complexity learning algorithm for fully recurrent continually running networks. Neural Computation, 4(2):243–248.
Schmidhuber, J. (1992b). Learning complex, extended sequences using the principle of history compression. Neural Computation, 4(2):234–242. (Based on TR FKI-148-91, TUM, 1991).
Schmidhuber, J. (1992c). Learning factorial codes by predictability minimization. Neural Computation, 4(6):863–879.
Schmidhuber, J. (1993a). An introspective network that can learn to run its own weight change algorithm.
In Proc. of the Intl. Conf. on Artificial Neural Networks, Brighton, pages 191–195. IEE.
Schmidhuber, J. (1993b). Netzwerkarchitekturen, Zielfunktionen und Kettenregel. (Network architectures, objective functions, and chain rule.) Habilitation Thesis, Inst. f. Inf., Tech. Univ. Munich.
Schmidhuber, J. (1997). Discovering neural nets with low Kolmogorov complexity and high generalization capability. Neural Networks, 10(5):857–873.
Schmidhuber, J. (2002). The Speed Prior: a new simplicity measure yielding near-optimal computable predictions. In Kivinen, J. and Sloan, R. H., editors, Proceedings of the 15th Annual Conference on
Computational Learning Theory (COLT 2002), Lecture Notes in Artificial Intelligence, pages 216–228.
Springer, Sydney, Australia.
Schmidhuber, J. (2004). Optimal ordered problem solver. Machine Learning, 54:211–254.
Schmidhuber, J. (2006a). Developmental robotics, optimal artificial curiosity, creativity, music, and the fine arts. Connection Science, 18(2):173–187.
Schmidhuber, J. (2006b). G¨odel machines: Fully self-referential optimal universal self-improvers. In
Goertzel, B. and Pennachin, C., editors, Artificial General Intelligence, pages 199–226. Springer Verlag.
Variant available as arXiv:cs.LO/0309048.
Schmidhuber, J. (2007). Prototype resilient, self-modeling robots. Science, 316(5825):688.
Schmidhuber, J. (2012).
Self-delimiting neural networks.
Technical Report IDSIA-08-12, arXiv:1210.0118v1 [cs.NE], The Swiss AI Lab IDSIA.
Schmidhuber, J. (2013a). My first Deep Learning system of 1991 + Deep Learning timeline 1962-2013.
Technical Report arXiv:1312.5548v1 [cs.NE], The Swiss AI Lab IDSIA.
Schmidhuber, J. (2013b). POWERPLAY: Training an Increasingly General Problem Solver by Continually
Searching for the Simplest Still Unsolvable Problem. Frontiers in Psychology.
Schmidhuber, J., Ciresan, D., Meier, U., Masci, J., and Graves, A. (2011). On fast deep nets for AGI vision.
In Proc. Fourth Conference on Artificial General Intelligence (AGI), Google, Mountain View, CA, pages
243–246.
Schmidhuber, J., Eldracher, M., and Foltin, B. (1996). Semilinear predictability minimization produces well-known feature detectors. Neural Computation, 8(4):773–786.
Schmidhuber, J. and Huber, R. (1991). Learning to generate artificial fovea trajectories for target detection.
International Journal of Neural Systems, 2(1 & 2):135–141.
Schmidhuber, J., Mozer, M. C., and Prelinger, D. (1993). Continuous history compression. In H¨uning, H., Neuhauser, S., Raus, M., and Ritschel, W., editors, Proc. of Intl. Workshop on Neural Networks, RWTH
Aachen, pages 87–95. Augustinus.
Schmidhuber, J. and Prelinger, D. (1992). Discovering predictable classifications. Technical Report CUCS-626-92, Dept. of Comp. Sci., University of Colorado at Boulder. Published in Neural Computation
5(4):625-635 (1993).
Schmidhuber, J. and Wahnsiedler, R. (1992). Planning simple trajectories using neural subgoal generators.
In Meyer, J. A., Roitblat, H. L., and Wilson, S. W., editors, Proc. of the 2nd International Conference on
Simulation of Adaptive Behavior, pages 196–202. MIT Press.
Schmidhuber, J., Wierstra, D., Gagliolo, M., and Gomez, F. J. (2007). Training recurrent networks by
Evolino. Neural Computation, 19(3):757–779.
Schmidhuber, J., Zhao, J., and Schraudolph, N. (1997a). Reinforcement learning with self-modifying policies. In Thrun, S. and Pratt, L., editors, Learning to learn, pages 293–309. Kluwer.
Schmidhuber, J., Zhao, J., and Wiering, M. (1997b). Shifting inductive bias with success-story algorithm, adaptive Levin search, and incremental self-improvement. Machine Learning, 28:105–130.
Sch¨olkopf, B., Burges, C. J. C., and Smola, A. J., editors (1998). Advances in Kernel Methods - Support
Vector Learning. MIT Press, Cambridge, MA.
Schraudolph, N. and Sejnowski, T. J. (1993). Unsupervised discrimination of clustered data via optimization of binary information gain. In Hanson, S. J., Cowan, J. D., and Giles, C. L., editors, Advances in Neural Information Processing Systems, volume 5, pages 499–506. Morgan Kaufmann, San Mateo.
Schraudolph, N. N. (2002). Fast curvature matrix-vector products for second-order gradient descent. Neural Computation, 14(7):1723–1738.
Schraudolph, N. N. and Sejnowski, T. J. (1996). Tempering backpropagation networks: Not all weights are created equal. In Touretzky, D. S., Mozer, M. C., and Hasselmo, M. E., editors, Advances in Neural
Information Processing Systems (NIPS), volume 8, pages 563–569. The MIT Press, Cambridge, MA.
Schrauwen, B., Verstraeten, D., and Van Campenhout, J. (2007). An overview of reservoir computing: theory, applications and implementations. In Proceedings of the 15th European Symposium on Artificial
Neural Networks. p. 471-482 2007, pages 471–482.
Schuster, H. G. (1992). Learning by maximization the information transfer through nonlinear noisy neurons and "noise breakdown". Phys. Rev. A, 46(4):2131–2138.
Schuster, M. (1999). On supervised learning from sequential data with applications for speech recognition.
PhD thesis, Nara Institute of Science and Technolog, Kyoto, Japan.
Schuster, M. and Paliwal, K. K. (1997). Bidirectional recurrent neural networks. IEEE Transactions on
Signal Processing, 45:2673–2681.
Schwartz, A. (1993). A reinforcement learning method for maximizing undiscounted rewards. In Proc.
ICML, pages 298–305.
Schwefel, H. P. (1974). Numerische Optimierung von Computer-Modellen. Dissertation. Published 1977 by Birkh¨auser, Basel.
Segmentation of Neuronal Structures in EM Stacks Challenge (2012). IEEE International Symposium on
Biomedical Imaging (ISBI), http://tinyurl.com/d2fgh7g.
Sehnke, F., Osendorfer, C., R¨uckstieß, T., Graves, A., Peters, J., and Schmidhuber, J. (2010). Parameterexploring policy gradients. Neural Networks, 23(4):551–559.
Sermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., and LeCun, Y. (2013). OverFeat: Integrated recognition, localization and detection using convolutional networks. arXiv preprint arXiv:1312.6229.
Sermanet, P. and LeCun, Y. (2011). Traffic sign recognition with multi-scale convolutional networks. In
Proceedings of International Joint Conference on Neural Networks (IJCNN'11), pages 2809–2813.
Serrano-Gotarredona, R., Oster, M., Lichtsteiner, P., Linares-Barranco, A., Paz-Vicente, R., G´omezRodr´ıguez, F., Camu˜nas-Mesa, L., Berner, R., Rivas-P´erez, M., Delbruck, T., et al. (2009). Caviar:
A 45k neuron, 5m synapse, 12g connects/s AER hardware sensory–processing–learning–actuating system for high-speed visual object recognition and tracking. IEEE Transactions on Neural Networks, 20(9):1417–1438.
Serre, T., Riesenhuber, M., Louie, J., and Poggio, T. (2002). On the role of object-specific features for real world object recognition in biological vision. In Biologically Motivated Computer Vision, pages
387–397.
Seung, H. S. (2003). Learning in spiking neural networks by reinforcement of stochastic synaptic transmission. Neuron, 40(6):1063–1073.
Shan, H. and Cottrell, G. (2014). Efficient visual coding: From retina to V2. In Proc. International
Conference on Learning Representations (ICLR). arXiv preprint arXiv:1312.6077.
Shan, H., Zhang, L., and Cottrell, G. W. (2007). Recursive ICA. Advances in Neural Information Processing Systems (NIPS), 19:1273.
Shanno, D. F. (1970). Conditioning of quasi-Newton methods for function minimization. Mathematics of computation, 24(111):647–656.
Shannon, C. E. (1948). A mathematical theory of communication (parts I and II). Bell System Technical
Journal, XXVII:379–423.
Shao, L., Wu, D., and Li, X. (2014).
Learning deep and wide: A spectral method for learning deep networks. IEEE Transactions on Neural Networks and Learning Systems.
Shavlik, J. W. (1994). Combining symbolic and neural learning. Machine Learning, 14(3):321–331.
Shavlik, J. W. and Towell, G. G. (1989). Combining explanation-based and neural learning: An algorithm and empirical results. Connection Science, 1(3):233–255.
Siegelmann, H. (1992). Theoretical Foundations of Recurrent Neural Networks. PhD thesis, Rutgers, New
Brunswick Rutgers, The State of New Jersey.
Siegelmann, H. T. and Sontag, E. D. (1991). Turing computability with neural nets. Applied Mathematics
Letters, 4(6):77–80.
Silva, F. M. and Almeida, L. B. (1990). Speeding up back-propagation. In Eckmiller, R., editor, Advanced
Neural Computers, pages 151–158, Amsterdam. Elsevier.
S´ıma, J. (1994). Loading deep networks is hard. Neural Computation, 6(5):842–850.
S´ıma, J. (2002). Training a single sigmoidal neuron is hard. Neural Computation, 14(11):2709–2728.
Simard, P., Steinkraus, D., and Platt, J. (2003). Best practices for convolutional neural networks applied to visual document analysis. In Seventh International Conference on Document Analysis and Recognition, pages 958–963.
Sims, K. (1994). Evolving virtual creatures.
In Glassner, A., editor, Proceedings of SIGGRAPH '94(Orlando, Florida, July 1994), Computer Graphics Proceedings, Annual Conference, pages 15–22. ACM
SIGGRAPH, ACM Press. ISBN 0-89791-667-0.
Simsek, ¨O. and Barto, A. G. (2008). Skill characterization based on betweenness. In NIPS'08, pages
1497–1504.
Singh, S., Barto, A. G., and Chentanez, N. (2005). Intrinsically motivated reinforcement learning. In
Advances in Neural Information Processing Systems 17 (NIPS). MIT Press, Cambridge, MA.
Singh, S. P. (1994). Reinforcement learning algorithms for average-payoff Markovian decision processes.
In National Conference on Artificial Intelligence, pages 700–705.
Smith, S. F. (1980). A Learning System Based on Genetic Adaptive Algorithms,. PhD thesis, Univ. Pittsburgh.
Smolensky, P. (1986). Parallel distributed processing: Explorations in the microstructure of cognition, vol. 1. chapter Information Processing in Dynamical Systems: Foundations of Harmony Theory, pages
194–281. MIT Press, Cambridge, MA, USA.
Solla, S. A. (1988). Accelerated learning in layered neural networks. Complex Systems, 2:625–640.
Solomonoff, R. J. (1964). A formal theory of inductive inference. Part I. Information and Control, 7:1–22.
Solomonoff, R. J. (1978). Complexity-based induction systems. IEEE Transactions on Information Theory, IT-24(5):422–432.
Soloway, E. (1986). Learning to program = learning to construct mechanisms and explanations. Communications of the ACM, 29(9):850–858.
Song, S., Miller, K. D., and Abbott, L. F. (2000). Competitive Hebbian learning through spike-timingdependent synaptic plasticity. Nature Neuroscience, 3(9):919–926.
Speelpenning, B. (1980). Compiling Fast Partial Derivatives of Functions Given by Algorithms. PhD thesis, Department of Computer Science, University of Illinois, Urbana-Champaign.
Srivastava, R. K., Masci, J., Kazerounian, S., Gomez, F., and Schmidhuber, J. (2013). Compete to compute.
In Advances in Neural Information Processing Systems (NIPS), pages 2310–2318.
Stallkamp, J., Schlipsing, M., Salmen, J., and Igel, C. (2011). The German traffic sign recognition benchmark: A multi-class classification competition. In International Joint Conference on Neural Networks(IJCNN 2011), pages 1453–1460. IEEE Press.
Stallkamp, J., Schlipsing, M., Salmen, J., and Igel, C. (2012). Man vs. computer: Benchmarking machine learning algorithms for traffic sign recognition. Neural Networks, 32:323–332.
Stanley, K. O., D'Ambrosio, D. B., and Gauci, J. (2009). A hypercube-based encoding for evolving largescale neural networks. Artificial Life, 15(2):185–212.
Stanley, K. O. and Miikkulainen, R. (2002). Evolving neural networks through augmenting topologies.
Evolutionary Computation, 10:99–127.
Steijvers, M. and Grunwald, P. (1996). A recurrent network that performs a contextsensitive prediction task. In Proceedings of the 18th Annual Conference of the Cognitive Science Society. Erlbaum.
Steil, J. J. (2007). Online reservoir adaptation by intrinsic plasticity for backpropagation–decorrelation and echo state learning. Neural Networks, 20(3):353–364.
Stemmler, M. (1996). A single spike suffices: the simplest form of stochastic resonance in model neurons.
Network: Computation in Neural Systems, 7(4):687–716.
Stoianov, I. and Zorzi, M. (2012). Emergence of a 'visual number sense' in hierarchical generative models.
Nature Neuroscience, 15(2):194–6.
Stone, M. (1974). Cross-validatory choice and assessment of statistical predictions. Roy. Stat. Soc., 36:111–
Stoop, R., Schindler, K., and Bunimovich, L. (2000). When pyramidal neurons lock, when they respond chaotically, and when they like to synchronize. Neuroscience research, 36(1):81–91.
Stratonovich, R. (1960).
Conditional Markov processes.
Theory of Probability And Its Applications, 5(2):156–178.
Sun, G., Chen, H., and Lee, Y. (1993a). Time warping invariant neural networks. In S. J. Hanson, J. D. C. and Giles, C. L., editors, Advances in Neural Information Processing Systems (NIPS) 5, pages 180–187.
Morgan Kaufmann.
Sun, G. Z., Giles, C. L., Chen, H. H., and Lee, Y. C. (1993b). The neural network pushdown automaton:
Model, stack and learning simulations. Technical Report CS-TR-3118, University of Maryland, College
Park.
Sun, Y., Gomez, F., Schaul, T., and Schmidhuber, J. (2013). A Linear Time Natural Evolution Strategy for
Non-Separable Functions. In Proceedings of the Genetic and Evolutionary Computation Conference, page 61, Amsterdam, NL. ACM.
Sun, Y., Wierstra, D., Schaul, T., and Schmidhuber, J. (2009). Efficient natural evolution strategies. In
Proc. 11th Genetic and Evolutionary Computation Conference (GECCO), pages 539–546.
Sutskever, I., Hinton, G. E., and Taylor, G. W. (2008). The recurrent temporal restricted Boltzmann machine. In NIPS, volume 21, page 2008.
Sutton, R. and Barto, A. (1998). Reinforcement learning: An introduction. Cambridge, MA, MIT Press.
Sutton, R. S., McAllester, D. A., Singh, S. P., and Mansour, Y. (1999a). Policy gradient methods for reinforcement learning with function approximation. In Advances in Neural Information Processing
Systems (NIPS) 12, pages 1057–1063.
Sutton, R. S., Precup, D., and Singh, S. P. (1999b). Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning. Artif. Intell., 112(1-2):181–211.
Sutton, R. S., Szepesv´ari, C., and Maei, H. R. (2008). A convergent O(n) algorithm for off-policy temporaldifference learning with linear function approximation. In Advances in Neural Information Processing
Systems (NIPS'08), volume 21, pages 1609–1616.
Szab´o, Z., P´oczos, B., and L˝orincz, A. (2006). Cross-entropy optimization for independent process analysis. In Independent Component Analysis and Blind Signal Separation, pages 909–916. Springer.
Szegedy, C., Toshev, A., and Erhan, D. (2013). Deep neural networks for object detection. pages 2553–
Taylor, G. W., Spiro, I., Bregler, C., and Fergus, R. (2011). Learning invariance through imitation. In
Conference on Computer Vision and Pattern Recognition (CVPR), pages 2729–2736. IEEE.
Tegge, A. N., Wang, Z., Eickholt, J., and Cheng, J. (2009). NNcon: improved protein contact map prediction using 2D-recursive neural networks. Nucleic Acids Research, 37(Suppl 2):W515–W518.
Teichmann, M., Wiltschut, J., and Hamker, F. (2012). Learning invariance from natural images inspired by observations in the primary visual cortex. Neural Computation, 24(5):1271–1296.
Teller, A. (1994). The evolution of mental models. In Kenneth E. Kinnear, J., editor, Advances in Genetic
Programming, pages 199–219. MIT Press.
Tenenberg, J., Karlsson, J., and Whitehead, S. (1993). Learning via task decomposition. In Meyer, J. A., Roitblat, H., and Wilson, S., editors, From Animals to Animats 2: Proceedings of the Second International Conference on Simulation of Adaptive Behavior, pages 337–343. MIT Press.
Tesauro, G. (1994). TD-gammon, a self-teaching backgammon program, achieves master-level play. Neural Computation, 6(2):215–219.
Tieleman, T. and Hinton, G. (2012). Lecture 6.5—RmsProp: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning.
Tikhonov, A. N., Arsenin, V. I., and John, F. (1977). Solutions of ill-posed problems. Winston.
Ting, K. M. and Witten, I. H. (1997). Stacked generalization: when does it work? In in Proc. International
Joint Conference on Artificial Intelligence (IJCAI).
Tiˇno, P. and Hammer, B. (2004). Architectural bias in recurrent neural networks: Fractal analysis. Neural
Computation, 15(8):1931–1957.
Tonkes, B. and Wiles, J. (1997). Learning a context-free task with a recurrent neural network: An analysis of stability. In Proceedings of the Fourth Biennial Conference of the Australasian Cognitive Science
Society.
Towell, G. G. and Shavlik, J. W. (1994). Knowledge-based artificial neural networks. Artificial Intelligence, 70(1):119–165.
Tsitsiklis, J. N. and van Roy, B. (1996). Feature-based methods for large scale dynamic programming.
Machine Learning, 22(1-3):59–94.
Tsodyks, M., Pawelzik, K., and Markram, H. (1998). Neural networks with dynamic synapses. Neural
Computation, 10(4):821–835.
Tsodyks, M. V., Skaggs, W. E., Sejnowski, T. J., and McNaughton, B. L. (1996). Population dynamics and theta rhythm phase precession of hippocampal place cell firing: a spiking neuron model. Hippocampus, 6(3):271–280.
Turaga, S. C., Murray, J. F., Jain, V., Roth, F., Helmstaedter, M., Briggman, K., Denk, W., and Seung, H. S.(2010). Convolutional networks can learn to generate affinity graphs for image segmentation. Neural
Computation, 22(2):511–538.
Turing, A. M. (1936). On computable numbers, with an application to the Entscheidungsproblem. Proceedings of the London Mathematical Society, Series 2, 41:230–267.
Turner, A. J. and Miller, J. F. (2013). Cartesian Genetic Programming encoded artificial neural networks:
A comparison using three benchmarks. In Proceedings of the Conference on Genetic and Evolutionary
Computation (GECCO), pages 1005–1012.
Ueda, N. (2000). Optimal linear combination of neural networks for improving classification performance.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 22(2):207–215.
Urlbe, A. P. (1999). Structure-adaptable digital neural networks. PhD thesis, Universidad del Valle.
Utgoff, P. E. and Stracuzzi, D. J. (2002). Many-layered learning. Neural Computation, 14(10):2497–2529.
Vahed, A. and Omlin, C. W. (2004). A machine learning method for extracting symbolic knowledge from recurrent neural networks. Neural Computation, 16(1):59–71.
Vaillant, R., Monrocq, C., and LeCun, Y. (1994). Original approach for the localisation of objects in images. IEE Proc on Vision, Image, and Signal Processing, 141(4):245–250. van den Berg, T. and Whiteson, S. (2013). Critical factors in the performance of HyperNEAT. In GECCO
2013: Proceedings of the Genetic and Evolutionary Computation Conference, pages 759–766. van Hasselt, H. (2012). Reinforcement learning in continuous state and action spaces. In Wiering, M. and van Otterlo, M., editors, Reinforcement Learning, pages 207–251. Springer.
Vapnik, V. (1992). Principles of risk minimization for learning theory. In Lippman, D. S., Moody, J. E., and Touretzky, D. S., editors, Advances in Neural Information Processing Systems (NIPS) 4, pages 831–838.
Morgan Kaufmann.
Vapnik, V. (1995). The Nature of Statistical Learning Theory. Springer, New York.
Versino, C. and Gambardella, L. M. (1996). Learning fine motion by using the hierarchical extended
Kohonen map. In Proc. Intl. Conf. on Artificial Neural Networks (ICANN), pages 221–226. Springer.
Veta, M., Viergever, M., Pluim, J., Stathonikos, N., and van Diest, P. J. (2013). MICCAI 2013 Grand
Challenge on Mitosis Detection.
Vieira, A. and Barradas, N. (2003). A training algorithm for classification of high-dimensional data. Neurocomputing, 50:461–472.
Vincent, P., Hugo, L., Bengio, Y., and Manzagol, P.-A. (2008). Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th international conference on Machine learning, ICML '08, pages 1096–1103, New York, NY, USA. ACM.
Vlassis, N., Littman, M. L., and Barber, D. (2012). On the computational complexity of stochastic controller optimization in POMDPs. ACM Transactions on Computation Theory, 4(4):12.
Vogl, T., Mangis, J., Rigler, A., Zink, W., and Alkon, D. (1988). Accelerating the convergence of the back-propagation method. Biological Cybernetics, 59:257–263. von der Malsburg, C. (1973). Self-organization of orientation sensitive cells in the striate cortex. Kybernetik, 14(2):85–100.
Waldinger, R. J. and Lee, R. C. T. (1969). PROW: a step toward automatic program writing. In Walker, D. E. and Norton, L. M., editors, Proceedings of the 1st International Joint Conference on Artificial
Intelligence (IJCAI), pages 241–252. Morgan Kaufmann.
Wallace, C. S. and Boulton, D. M. (1968). An information theoretic measure for classification. Computer
Journal, 11(2):185–194.
Wan, E. A. (1994). Time series prediction by using a connectionist network with internal delay lines.
In Weigend, A. S. and Gershenfeld, N. A., editors, Time series prediction: Forecasting the future and understanding the past, pages 265–295. Addison-Wesley.
Wang, C., Venkatesh, S. S., and Judd, J. S. (1994). Optimal stopping and effective machine complexity in learning. In Advances in Neural Information Processing Systems (NIPS'6), pages 303–310. Morgan
Kaufmann.
Wang, S. and Manning, C. (2013). Fast dropout training. In Proceedings of the 30th International Conference on Machine Learning (ICML-13), pages 118–126.
Watanabe, O. (1992). Kolmogorov complexity and computational complexity. EATCS Monographs on
Theoretical Computer Science, Springer.
Watanabe, S. (1985). Pattern Recognition: Human and Mechanical. Willey, New York.
Watkins, C. J. C. H. (1989). Learning from Delayed Rewards. PhD thesis, King's College, Oxford.
Watkins, C. J. C. H. and Dayan, P. (1992). Q-learning. Machine Learning, 8:279–292.
Watrous, R. L. and Kuhn, G. M. (1992). Induction of finite-state automata using second-order recurrent networks. In Moody, J. E., Hanson, S. J., and Lippman, R. P., editors, Advances in Neural Information
Processing Systems 4, pages 309–316. Morgan Kaufmann.
Waydo, S. and Koch, C. (2008). Unsupervised learning of individuals and categories from images. Neural
Computation, 20(5):1165–1178.
Weigend, A. S. and Gershenfeld, N. A. (1993). Results of the time series prediction competition at the Santa Fe Institute. In Neural Networks, 1993., IEEE International Conference on, pages 1786–1793.
IEEE.
Weigend, A. S., Rumelhart, D. E., and Huberman, B. A. (1991). Generalization by weight-elimination with application to forecasting. In Lippmann, R. P., Moody, J. E., and Touretzky, D. S., editors, Advances in Neural Information Processing Systems (NIPS) 3, pages 875–882. San Mateo, CA: Morgan Kaufmann.
Weiss, G. (1994). Hierarchical chunking in classifier systems. In Proceedings of the 12th National Conference on Artificial Intelligence, volume 2, pages 1335–1340. AAAI Press/The MIT Press.
Weng, J., Ahuja, N., and Huang, T. S. (1992). Cresceptron: a self-organizing neural network which grows adaptively. In International Joint Conference on Neural Networks (IJCNN), volume 1, pages 576–581.
IEEE.
Weng, J. J., Ahuja, N., and Huang, T. S. (1997). Learning recognition and segmentation using the cresceptron. International Journal of Computer Vision, 25(2):109–143.
Werbos, P. J. (1974). Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences.
PhD thesis, Harvard University.
Werbos, P. J. (1981). Applications of advances in nonlinear sensitivity analysis. In Proceedings of the 10th
IFIP Conference, 31.8 - 4.9, NYC, pages 762–770.
Werbos, P. J. (1987). Building and understanding adaptive systems: A statistical/numerical approach to factory automation and brain research. IEEE Transactions on Systems, Man, and Cybernetics, 17.
Werbos, P. J. (1988). Generalization of backpropagation with application to a recurrent gas market model.
Neural Networks, 1.
Werbos, P. J. (1989a). Backpropagation and neurocontrol: A review and prospectus. In IEEE/INNS International Joint Conference on Neural Networks, Washington, D.C., volume 1, pages 209–216.
Werbos, P. J. (1989b). Neural networks for control and system identification. In Proceedings of IEEE/CDC
Tampa, Florida.
Werbos, P. J. (1992). Neural networks, system identification, and control in the chemical industries. In
D. A. White, D. A. S., editor, Handbook of Intelligent Control: Neural, Fuzzy, and Adaptive Approaches, pages 283–356. Thomson Learning.
Werbos, P. J. (2006). Backwards differentiation in AD and neural nets: Past links and new opportunities.
In Automatic Differentiation: Applications, Theory, and Implementations, pages 15–34. Springer.
West, A. H. L. and Saad, D. (1995). Adaptive back-propagation in on-line learning of multilayer networks.
In Touretzky, D. S., Mozer, M., and Hasselmo, M. E., editors, NIPS, pages 323–329. MIT Press.
White, H. (1989). Learning in artificial neural networks: A statistical perspective. Neural Computation, 1(4):425–464.
Whitehead, S. (1992). Reinforcement Learning for the adaptive control of perception and action. PhD thesis, University of Rochester.
Whiteson, S. (2012). Evolutionary computation for reinforcement learning. In Wiering, M. and van Otterlo, M., editors, Reinforcement Learning, pages 325–355. Springer, Berlin, Germany.
Whiteson, S., Kohl, N., Miikkulainen, R., and Stone, P. (2005). Evolving keepaway soccer players through task decomposition. Machine Learning, 59(1):5–30.
Whiteson, S. and Stone, P. (2006). Evolutionary function approximation for reinforcement learning. Journal of Machine Learning Research, 7:877–917.
Widrow, B. and Hoff, M. (1962). Associative storage and retrieval of digital information in networks of adaptive neurons. Biological Prototypes and Synthetic Systems, 1:160.
Widrow, B., Rumelhart, D. E., and Lehr, M. A. (1994). Neural networks: Applications in industry, business and science. Commun. ACM, 37(3):93–105.
Wieland, A. P. (1991). Evolving neural network controllers for unstable systems. In International Joint
Conference on Neural Networks (IJCNN), volume 2, pages 667–673. IEEE.
Wiering, M. and Schmidhuber, J. (1996). Solving POMDPs with Levin search and EIRA. In Saitta, L., editor, Machine Learning: Proceedings of the Thirteenth International Conference, pages 534–542.
Morgan Kaufmann Publishers, San Francisco, CA.
Wiering, M. and Schmidhuber, J. (1998a). HQ-learning. Adaptive Behavior, 6(2):219–246.
Wiering, M. and van Otterlo, M. (2012). Reinforcement Learning. Springer.
Wiering, M. A. and Schmidhuber, J. (1998b). Fast online Q(λ). Machine Learning, 33(1):105–116.
Wierstra, D., Foerster, A., Peters, J., and Schmidhuber, J. (2007). Solving deep memory POMDPs with recurrent policy gradients. In ICANN (1), volume 4668 of Lecture Notes in Computer Science, pages
697–706. Springer.
Wierstra, D., Foerster, A., Peters, J., and Schmidhuber, J. (2010). Recurrent policy gradients. Logic Journal of IGPL, 18(2):620–634.
Wierstra, D., Schaul, T., Peters, J., and Schmidhuber, J. (2008). Natural evolution strategies. In Congress of Evolutionary Computation (CEC 2008).
Wiesel, D. H. and Hubel, T. N. (1959). Receptive fields of single neurones in the cat's striate cortex. J.
Physiol., 148:574–591.
Wiles, J. and Elman, J. (1995).
Learning to count without a counter: A case study of dynamics and activation landscapes in recurrent networks. In In Proceedings of the Seventeenth Annual Conference of the Cognitive Science Society, pages pages 482 – 487, Cambridge, MA. MIT Press.
Wilkinson, J. H., editor (1965). The Algebraic Eigenvalue Problem. Oxford University Press, Inc., New
York, NY, USA.
Williams, R. J. (1986). Reinforcement-learning in connectionist networks: A mathematical analysis. Technical Report 8605, Institute for Cognitive Science, University of California, San Diego.
Williams, R. J. (1988). Toward a theory of reinforcement-learning connectionist systems. Technical Report
NU-CCS-88-3, College of Comp. Sci., Northeastern University, Boston, MA.
Williams, R. J. (1989). Complexity of exact gradient computation algorithms for recurrent neural networks. Technical Report Technical Report NU-CCS-89-27, Boston: Northeastern University, College of Computer Science.
Williams, R. J. (1992a). Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8:229–256.
Williams, R. J. (1992b). Training recurrent networks using the extended Kalman filter. In International
Joint Conference on Neural Networks (IJCNN), volume 4, pages 241–246. IEEE.
Williams, R. J. and Peng, J. (1990). An efficient gradient-based algorithm for on-line training of recurrent network trajectories. Neural Computation, 4:491–501.
Williams, R. J. and Zipser, D. (1988). A learning algorithm for continually running fully recurrent networks. Technical Report ICS Report 8805, Univ. of California, San Diego, La Jolla.
Williams, R. J. and Zipser, D. (1989a). Experimental analysis of the real-time recurrent learning algorithm.
Connection Science, 1(1):87–111.
Williams, R. J. and Zipser, D. (1989b). A learning algorithm for continually running fully recurrent networks. Neural Computation, 1(2):270–280.
Willshaw, D. J. and von der Malsburg, C. (1976). How patterned neural connections can be set up by self-organization. Proc. R. Soc. London B, 194:431–445.
Windisch, D. (2005). Loading deep networks is hard: The pyramidal case. Neural Computation, 17(2):487–
Wiskott, L. and Sejnowski, T. (2002). Slow feature analysis: Unsupervised learning of invariances. Neural
Computation, 14(4):715–770.
Witczak, M., Korbicz, J., Mrugalski, M., and Patton, R. J. (2006). A GMDH neural network-based approach to robust fault diagnosis: Application to the DAMADICS benchmark problem. Control Engineering Practice, 14(6):671–683.
Wolpert, D. H. (1992). Stacked generalization. Neural Networks, 5(2):241–259.
Wolpert, D. H. (1994). Bayesian backpropagation over i-o functions rather than weights. In Cowan, J. D., Tesauro, G., and Alspector, J., editors, Advances in Neural Information Processing Systems (NIPS) 6, pages 200–207. Morgan Kaufmann.
Wu, D. and Shao, L. (2014). Leveraging hierarchical parametric networks for skeletal joints based action segmentation and recognition. In Proc. Conference on Computer Vision and Pattern Recognition(CVPR).
Wu, L. and Baldi, P. (2008). Learning to play Go using recursive neural networks. Neural Networks, 21(9):1392–1400.
Wyatte, D., Curran, T., and O'Reilly, R. (2012). The limits of feedforward vision: Recurrent processing promotes robust object recognition when objects are degraded.
Journal of Cognitive Neuroscience, 24(11):2248–2261.
Yamauchi, B. M. and Beer, R. D. (1994). Sequential behavior and learning in evolved dynamical neural networks. Adaptive Behavior, 2(3):219–246.
Yamins, D., Hong, H., Cadieu, C., and DiCarlo, J. J. (2013). Hierarchical modular optimization of convolutional networks achieves representations similar to macaque IT and human ventral stream. Advances in Neural Information Processing Systems (NIPS), pages 1–9.
Yang, M., Ji, S., Xu, W., Wang, J., Lv, F., Yu, K., Gong, Y., Dikmen, M., Lin, D. J., and Huang, T. S.(2009). Detecting human actions in surveillance videos. In TREC Video Retrieval Evaluation Workshop.
Yao, X. (1993). A review of evolutionary artificial neural networks. International Journal of Intelligent
Systems, 4:203–222.
Yin, F., Wang, Q.-F., Zhang, X.-Y., and Liu, C.-L. (2013). ICDAR 2013 Chinese handwriting recognition competition. In 12th International Conference on Document Analysis and Recognition (ICDAR), pages
1464–1470.
Young, S., Davis, A., Mishtal, A., and Arel, I. (2014). Hierarchical spatiotemporal feature extraction using recurrent online clustering. Pattern Recognition Letters, 37:115–123.
Yu, X.-H., Chen, G.-A., and Cheng, S.-X. (1995). Dynamic learning rate optimization of the backpropagation algorithm. IEEE Transactions on Neural Networks, 6(3):669–677.
Zamora-Martnez, F., Frinken, V., Espaa-Boquera, S., Castro-Bleda, M., Fischer, A., and Bunke, H. (2014).
Neural network language models for off-line handwriting recognition. Pattern Recognition, 47(4):1642–
Zeiler, M. D. (2012). ADADELTA: An Adaptive Learning Rate Method. CoRR, abs/1212.5701.
Zeiler, M. D. and Fergus, R. (2013). Visualizing and understanding convolutional networks. Technical
Report arXiv:1311.2901 [cs.CV], NYU.
Zemel, R. S. (1993). A minimum description length framework for unsupervised learning. PhD thesis, University of Toronto.
Zemel, R. S. and Hinton, G. E. (1994). Developing population codes by minimizing description length.
In Cowan, J. D., Tesauro, G., and Alspector, J., editors, Advances in Neural Information Processing
Systems 6, pages 11–18. Morgan Kaufmann.
Zeng, Z., Goodman, R., and Smyth, P. (1994). Discrete recurrent neural networks for grammatical inference. IEEE Transactions on Neural Networks, 5(2).
Zimmermann, H.-G., Tietz, C., and Grothmann, R. (2012). Forecasting with recurrent neural networks: 12 tricks. In Montavon, G., Orr, G. B., and M¨uller, K.-R., editors, Neural Networks: Tricks of the Trade(2nd ed.), volume 7700 of Lecture Notes in Computer Science, pages 687–707. Springer.
Zipser, D., Kehoe, B., Littlewort, G., and Fuster, J. (1993). A spiking network model of short-term active memory. The Journal of Neuroscience, 13(8):3406–3420.Li Deng and Dong Yu
Microsoft Research
One Microsoft Way
Redmond, WA 98052
NOW PUBLISHERS, 2014
DEEP LEARNING:
METHODS AND APPLICATIONS
Table of Contents
Chapter 1 Introduction.................................................................................................................... 5
Definitions and Background............................................................................................. 5
Organization of This Book............................................................................................... 8
Chapter 2 Some Historical Context of Deep Learning................................................................ 11
Chapter 3 Three Classes of Deep Learning Networks................................................................. 18
A Three-Way Categorization......................................................................................... 18
Deep Networks for Unsupervised or Generative Learning............................................ 21
Deep Networks for Supervised Learning....................................................................... 24
Hybrid Deep Networks................................................................................................... 26
Chapter 4 Deep Autoencoders --- Unsupervised Learning........................................................... 29
Introduction.................................................................................................................... 29
Use of Deep Autoencoders to Extract Speech Features................................................. 30
Stacked Denoising Autoencoders................................................................................... 35
Transforming Autoencoders........................................................................................... 35
Chapter 5 Pre-Trained Deep Neural Networks --- A Hybrid...................................................... 37
Restricted Boltzmann Machines..................................................................................... 37
Unsupervised Layer-wise Pretraining............................................................................ 40
Interfacing DNNs with HMMs...................................................................................... 42
Chapter 6 Deep Stacking Networks and Variants --- Supervised Learning................................ 44
Introduction.................................................................................................................... 44
A Basic Architecture of the Deep Stacking Network.................................................... 45
A Method for Learning the DSN Weights..................................................................... 46
The Tensor Deep Stacking Network.............................................................................. 48
The Kernelized Deep Stacking Network........................................................................ 50
Chapter 7 Selected Applications in Speech and Audio Processing............................................. 53
7.1 Acoustic Modeling for Speech Recognition................................................................... 53
7.1.1 Back to primitive spectral features of speech................................................................. 54
7.1.2 The DNN-HMM architecture vs. use of DNN-derived features.................................... 56
7.1.3 Noise robustness by deep learning................................................................................. 59
7.1.4 Output representations in the DNN................................................................................ 60
7.1.5 Adaptation of the DNN-based speech recognizers........................................................ 62
7.1.6 Better architectures and nonlinear units......................................................................... 63
7.1.7 Better optimization and regularization …………………………………………………67
Speech Synthesis............................................................................................................ 70
Audio and Music Processing.......................................................................................... 71
Chapter 8 Selected Applications in Language Modeling and Natural Language Processing...... 73
Language Modeling........................................................................................................ 73
Natural Language Processing......................................................................................... 77
Chapter 9 Selected Applications in Information Retrieval.......................................................... 84
A Brief Introduction to Information Retrieval............................................................... 84
Semantic Hashing with Deep Autoencoders for Document Indexing and Retrieval..... 85
Deep-Structured Semantic Modeling for Document Retrieval...................................... 86
Use of Deep Stacking Networks for Information Retrieval........................................... 91
Chapter 10 Selected Applications in Object Recognition and Computer Vision........................ 92
10.1 Unsupervised or Generative Feature Learning............................................................... 92
10.2 Supervised Feature Learning and Classification............................................................ 94
Chapter 11 Selected Applications in Multi-modal and Multi-task Learning............................. 101
11.1 Multi-Modalities: Text and Image............................................................................... 101
11.2 Multi-Modalities: Speech and Image........................................................................... 104
11.3 Multi-Task Learning within the Speech, NLP or Image Domain................................ 106
Chapter 12 Epilogues................................................................................................................. 110
BIBLIOGRAPHY....................................................................................................................... 114
Abstract
This book is aimed to provide an overview of general deep learning methodology and its applications to a variety of signal and information processing tasks. The application areas are chosen with the following three criteria: 1) expertise or knowledge of the authors; 2) the application areas that have already been transformed by the successful use of deep learning technology, such as speech recognition and computer vision; and 3) the application areas that have the potential to be impacted significantly by deep learning and that have gained concentrated research efforts, including natural language and text processing, information retrieval, and multimodal information processing empowered by multi-task deep learning.
In Chapter 1, we provide the background of deep learning, as intrinsically connected to the use of multiple layers of nonlinear transformations to derive features from the sensory signals such as speech and visual images. In the most recent literature, deep learning is embodied also as representation learning, which involves a hierarchy of features or concepts where higher-level representations of them are defined from lower-level ones and where the same lower-level representations help to define higher-level ones. In Chapter 2, a brief historical account of deep learning is presented. In particular, selected chronological development of speech recognition is used to illustrate the recent impact of deep learning that has become a dominant technology in speech recognition industry within only a few years since the start of a collaboration between academic and industrial researchers in applying deep learning to speech recognition. In Chapter 3, a three-way classification scheme for a large body of work in deep learning is developed. We classify a growing number of deep learning techniques into unsupervised, supervised, and hybrid categories, and present qualitative descriptions and a literature survey for each category. From
Chapter 4 to Chapter 6, we discuss in detail three popular deep networks and related learning methods, one in each category. Chapter 4 is devoted to deep autoencoders as a prominent example of the unsupervised deep learning techniques. Chapter 5 gives a major example in the hybrid deep network category, which is the discriminative feed-forward neural network for supervised learning with many layers initialized using layer-by-layer generative, unsupervised pre-training. In Chapter
6, deep stacking networks and several of the variants are discussed in detail, which exemplify the discriminative or supervised deep learning techniques in the three-way categorization scheme.
In Chapters 7-11, we select a set of typical and successful applications of deep learning in diverse areas of signal and information processing and of applied artificial intelligence. In Chapter 7, we review the applications of deep learning to speech and audio processing, with emphasis on speech recognition organized according to several prominent themes. In Chapters 8, we present recent results of applying deep learning to language modeling and natural language processing. Chapter
9 is devoted to selected applications of deep learning to information retrieval including Web search.
In Chapter 10, we cover selected applications of deep learning to image object recognition in computer vision. Selected applications of deep learning to multi-modal processing and multi-task learning are reviewed in Chapter 11. Finally, an epilogue is given in Chapter 12 to summarize what we presented in earlier chapters and to discuss future challenges and directions.
CHAPTER 1
INTRODUCTION
1.1 Definitions and Background
Since 2006, deep structured learning, or more commonly called deep learning or hierarchical learning, has emerged as a new area of machine learning research (Hinton et al., 2006; Bengio, 2009). During the past several years, the techniques developed from deep learning research have already been impacting a wide range of signal and information processing work within the traditional and the new, widened scopes including key aspects of machine learning and artificial intelligence; see overview articles in (Bengio, 2009; Arel et al., 2010; Yu and Deng, 2011; Deng, 2011, 2013; Hinton et al., 2012; Bengio et al., 2013a), and also the media coverage of this progress in (Markoff, 2012; Anthes, 2013). A series of workshops, tutorials, and special issues or conference special sessions in recent years have been devoted exclusively to deep learning and its applications to various signal and information processing areas. These include:
 2008 NIPS Deep Learning Workshop;
 2009 NIPS Workshop on Deep Learning for Speech Recognition and Related Applications;
 2009 ICML Workshop on Learning Feature Hierarchies;
 2011 ICML Workshop on Learning Architectures, Representations, and Optimization for
Speech and Visual Information Processing;
 2012 ICASSP Tutorial on Deep Learning for Signal and Information Processing;
 2012 ICML Workshop on Representation Learning;
 2012 Special Section on Deep Learning for Speech and Language Processing in IEEE
Transactions on Audio, Speech, and Language Processing (T-ASLP, January);
 2010, 2011, and 2012 NIPS Workshops on Deep Learning and Unsupervised Feature
Learning;
 2013 NIPS Workshops on Deep Learning and on Output Representation Learning;
 2013 Special Issue on Learning Deep Architectures in IEEE Transactions on Pattern
Analysis and Machine Intelligence (T-PAMI, September).
 2013 International Conference on Learning Representations;
 2013 ICML Workshop on Representation Learning Challenges;
 2013 ICML Workshop on Deep Learning for Audio, Speech, and Language Processing;
 2013 ICASSP Special Session on New Types of Deep Neural Network Learning for Speech
Recognition and Related Applications.
The authors have been actively involved in deep learning research and in organizing or providing several of the above events, tutorials, and editorials. In particular, they gave tutorials and invited lectures on this topic at various places. Part of this book is based on their tutorials and lecture material.
Before embarking on describing details of deep learning, let's provide necessary definitions. Deep learning has various closely related definitions or high-level descriptions:

Definition 1: A class of machine learning techniques that exploit many layers of non-linear information processing for supervised or unsupervised feature extraction and transformation, and for pattern analysis and classification.

Definition 2: "A sub-field within machine learning that is based on algorithms for learning multiple levels of representation in order to model complex relationships among data.
Higher-level features and concepts are thus defined in terms of lower-level ones, and such a hierarchy of features is called a deep architecture. Most of these models are based on unsupervised learning of representations." (Wikipedia on "Deep Learning" around March

Definition 3: "A sub-field of machine learning that is based on learning several levels of representations, corresponding to a hierarchy of features or factors or concepts, where higherlevel concepts are defined from lower-level ones, and the same lower-level concepts can help to define many higher-level concepts. Deep learning is part of a broader family of machine learning methods based on learning representations. An observation (e.g., an image) can be represented in many ways (e.g., a vector of pixels), but some representations make it easier to learn tasks of interest (e.g., is this the image of a human face?) from examples, and research in this area attempts to define what makes better representations and how to learn them."(Wikipedia on "Deep Learning" around February 2013.)

Definition 4: "Deep learning is a set of algorithms in machine learning that attempt to learn in multiple levels, corresponding to different levels of abstraction. It typically uses artificialneural networks. The levels in these learned statistical models correspond to distinct levels of concepts, where higher-level concepts are defined from lower-level ones, and the same lower-level concepts can help to define many higher-level concepts." See Wikipedia http://en.wikipedia.org/wiki/Deep_learning on "Deep Learning" as of this most recent update in October 2013.

Definition 5: "Deep Learning is a new area of Machine Learning research, which has been introduced with the objective of moving Machine Learning closer to one of its original goals:
Artificial Intelligence. Deep Learning is about learning multiple levels of representation and abstraction that help to make sense of data such as images, sound, and text." See https://github.com/lisa-lab/DeepLearningTutorials
Note that the deep learning that we discuss in this book is about learning in deep architectures for signal and information processing. It is not about deep understanding of the signal or information, although in many cases they may be related. It should also be distinguished from the overloaded term in educational psychology: "Deep learning describes an approach to learning that is characterized by active engagement, intrinsic motivation, and a personal search for meaning." http://www.blackwellreference.com/public/tocnode?id=g9781405161251_chunk_g97814051612
516_ss1-1
Common among the various high-level descriptions of deep learning above are two key aspects:
1) models consisting of multiple layers or stages of nonlinear information processing; and 2) methods for supervised or unsupervised learning of feature representation at successively higher, more abstract layers. Deep learning is in the intersections among the research areas of neural networks, artificial intelligence, graphical modeling, optimization, pattern recognition, and signal processing. Three important reasons for the popularity of deep learning today are the drastically increased chip processing abilities (e.g., general-purpose graphical processing units or GPGPUs), the significantly lowered cost of computing hardware, and the recent advances in machine learning and signal/information processing research. These advances have enabled the deep learning methods to effectively exploit complex, compositional nonlinear functions, to learn distributed and hierarchical feature representations, and to make effective use of both labeled and unlabeled data.
Active researchers in this area include those at University of Toronto, New York University, University of Montreal, Stanford University, Microsoft Research (since 2009), Google (since about 2011), IBM Research (since about 2011), Baidu (since 2012), Facebook (since 2013), UCBerkeley, UC-Irvine, IDIAP, IDSIA, University College London, University of Michigan, Massachusetts Institute of Technology, University of Washington, and numerous other places; see http://deeplearning.net/deep-learning-research-groups-and-labs/ for a more detailed list. These researchers have demonstrated empirical successes of deep learning in diverse applications of computer vision, phonetic recognition, voice search, conversational speech recognition, speech and image feature coding, semantic utterance classification, natural language understanding, handwriting recognition, audio processing, information retrieval, robotics, and even in the analysis of molecules that may lead to discovery of new drugs as reported recently by Markoff (2012).
In addition to the reference list provided at the end of this book, which may be outdated not long after the publication of this book, there are a number of excellent and frequently updated reading lists, tutorials, software, and video lectures online at:
 http://deeplearning.net/reading-list/
 http://ufldl.stanford.edu/wiki/index.php/UFLDL_Recommended_Readings
 http://www.cs.toronto.edu/~hinton/
 http://deeplearning.net/tutorial/
 http://ufldl.stanford.edu/wiki/index.php/UFLDL_Tutorial
1.2 Organization of This Book
The rest of the book is organized as follows:
In Chapter 2, we provide a brief historical account of deep learning, mainly from the perspective of how speech recognition technology has been hugely impacted by deep learning, and how the revolution got started and has gained and sustained immense momentum.
In Chapter 3, a three-way categorization scheme for a majority of the work in deep learning is developed. They include unsupervised, supervised, and hybrid deep learning networks, where in the latter category unsupervised learning (or pre-training) is exploited to assist the subsequent stage of supervised learning when the final tasks pertain to classification. The supervised and hybrid deep networks often have the same type of architectures or the structures in the deep networks, but the unsupervised deep networks tend to have different architectures from the others.
Chapters 4-6 are devoted, respectively, to three popular types of deep architectures, one from each of the classes in the three-way categorization scheme reviewed in Chapter 3. In Chapter 4, we discuss in detail deep autoencoders as a prominent example of the unsupervised deep learning networks. No class labels are used in the learning, although supervised learning methods such as back-propagation are cleverly exploited when the input signal itself, instead of any label information of interest to possible classification tasks, is treated as the "supervised" signal.
In Chapter 5, as a major example in the hybrid deep network category, we present in detail the deep neural networks with unsupervised and largely generative pre-training to boost the effectiveness of supervised training. This benefit is found critical when the training data are limited and no other appropriate regularization ways (i.e., dropout) are exploited. The particular pretraining method based on restricted Boltzmann machines and the related deep belief networks described in this chapter has been historically significant as it ignited the intense interest in the early applications of deep learning to speech recognition and other information processing tasks.
In addition to this retrospective review, subsequent development and different paths from the more recent perspective are discussed.
In Chapter 6, the basic deep stacking networks and their several extensions are discussed in detail, which exemplify the discriminative, supervised deep learning networks in the three-way classification scheme. This group of deep networks operate in many ways that are distinct from the deep neural networks. Most notably, they use target labels in constructing each of many layers or modules in the overall deep networks. Assumptions made about part of the networks, such as linear output units in each of the modules, simplify the learning algorithms and enable a much wider variety of network architectures to be constructed and learned than the networks discussed in Chapters 4 and 5.
In Chapters 7-11, we select a set of typical and successful applications of deep learning in diverse areas of signal and information processing. In Chapter 7, we review the applications of deep learning to speech recognition, speech synthesis, and audio processing. Subsections surrounding the main subject of speech recognition are created based on several prominent themes on the topic in the literature.
In Chapters 8, we present recent results of applying deep learning to language modeling and natural language processing, where we highlight the key recent development in embedding symbolic entities such as words into low-dimensional, continuous-valued vectors.
Chapter 9 is devoted to selected applications of deep learning to information retrieval including web search.
In Chapter 10, we cover selected applications of deep learning to image object recognition in computer vision. The chapter is divided to two main classes of deep learning approaches: 1) unsupervised feature learning, and 2) supervised learning for end-to-end and joint feature learning and classification.
Selected applications to multi-modal processing and multi-task learning are reviewed in Chapter
11, divided into three categories according to the nature of the multi-modal data as inputs to the deep learning systems. For single-modality data of speech, text, or image, a number of recent multi-task learning studies based on deep learning methods are reviewed in the literature.
Finally, an epilogue is given in Chapter 12 to summarize the book and to discuss future challenges and directions.
This short monograph contains the material expanded from two tutorials that the authors gave, one at APSIPA in October 2011 and the other at ICASSP in March 2012. Substantial updates have been made based on the literature up to January 2014 (including the materials presented at NIPS2013 and at IEEE-ASRU-2013 both held in December of 2013), focusing on practical aspects in the fast development of deep learning research and technology during the interim years.
CHAPTER 2
SOME HISTORICAL CONTEXT OF DEEP
LEARNING
Until recently, most machine learning and signal processing techniques had exploited shallowstructured architectures. These architectures typically contain at most one or two layers of nonlinear feature transformations. Examples of the shallow architectures are Gaussian mixture models (GMMs), linear or nonlinear dynamical systems, conditional random fields (CRFs), maximum entropy (MaxEnt) models, support vector machines (SVMs), logistic regression, kernel regression, multi-layer perceptrons (MLPs) with a single hidden layer including extreme learning machines (ELMs). For instance, SVMs use a shallow linear pattern separation model with one or zero feature transformation layer when the kernel trick is used or otherwise. (Notable exceptions are the recent kernel methods that have been inspired by and integrated with deep learning; e.g.
Cho and Saul, 2009; Deng et al., 2012; Vinyals et al., 2012; Aslan et al., 2013). Shallow architectures have been shown effective in solving many simple or well-constrained problems, but their limited modeling and representational power can cause difficulties when dealing with more complicated real-world applications involving natural signals such as human speech, natural sound and language, and natural image and visual scenes.
Human information processing mechanisms (e.g., vision and audition), however, suggest the need of deep architectures for extracting complex structure and building internal representation from rich sensory inputs. For example, human speech production and perception systems are both equipped with clearly layered hierarchical structures in transforming the information from the waveform level to the linguistic level (Baker et al., 2009, 2009a; Deng, 1999, 2003). In a similar vein, the human visual system is also hierarchical in nature, mostly in the perception side but interestingly also in the "generation" side (George, 2008; Bouvrie, 2009; Poggio, 2007). It is natural to believe that the state-of-the-art can be advanced in processing these types of natural signals if efficient and effective deep learning algorithms can be developed.
Historically, the concept of deep learning originated from artificial neural network research.(Hence, one may occasionally hear the discussion of "new-generation neural networks".) Feedforward neural networks or MLPs with many hidden layers, which are often referred to as deep neural networks (DNNs), are good examples of the models with a deep architecture. Backpropagation (BP), popularized in 1980's, has been a well-known algorithm for learning the parameters of these networks. Unfortunately back-propagation alone did not work well in practice then for learning networks with more than a small number of hidden layers (see a review and analysis in (Bengio, 2009; Glorot and Bengio, 2010). The pervasive presence of local optima and other optimization challenges in the non-convex objective function of the deep networks are the main source of difficulties in the learning. Back-propagation is based on local gradient information, and starts usually at some random initial points. It often gets trapped in poor local optima when the batch-mode or even stochastic gradient descent BP algorithm is used. The severity increasessignificantly as the depth of the networks increases. This difficulty is partially responsible for steering away most of the machine learning and signal processing research from neural networks to shallow models that have convex loss functions (e.g., SVMs, CRFs, and MaxEnt models), for which the global optimum can be efficiently obtained at the cost of reduced modeling power, although there had been continuing work on neural networks with limited scale and impact (e.g., Hochreiter and Schmidhuber, 1997; LeCun et al., 1998; Bourlard and Morgan, 1993; Deng et al., 1994s; Bridle et al., 1998; Robinson, 1994; Morgan, et al., 2005).
The optimization difficulty associated with the deep models was empirically alleviated when a reasonably efficient, unsupervised learning algorithm was introduced in the two seminar papers(Hinton et al., 2006; Hinton and Salakhutdinov, 2006). In these papers, a class of deep generative models, called deep belief network (DBN), was introduced. A DBN is composed of a stack of restricted Boltzmann machines (RBMs). A core component of the DBN is a greedy, layer-by-layer learning algorithm which optimizes DBN weights at time complexity linear to the size and depth of the networks. Separately and with some surprise, initializing the weights of an MLP with a correspondingly configured DBN often produces much better results than that with the random weights. As such, MLPs with many hidden layers, or deep neural networks (DNN), which are learned with unsupervised DBN pre-training followed by back-propagation fine-tuning is sometimes also called DBNs in the literature (e.g., Dahl et al., 2011; Mohamed et al., 2010, 2012).
More recently, researchers have been more careful in distinguishing DNNs from DBNs (Dahl et al., 2012; Hinton et al., 2012), and when DBN is used to initialize the training of a DNN, the resulting network is sometimes called the DBN-DNN (Hinton et al., 2012).
Independently of the RBM development, in 2006 two alternative, non-probabilistic, nongenerative, unsupervised deep models were published. One is an autoencoder variant with greedy layer-wise training much like the DBN training (Bengio et al., 2006). Another is an energy-based model with unsupervised learning of sparse over-complete representations (Ranzato et al., 2006).
They both can be effectively used to pre-train a deep neural network, much like the DBN.
In addition to the supply of good initialization points, the DBN comes with additional attractive properties. First, the learning algorithm makes effective use of unlabeled data. Second, it can be interpreted as Bayesian probabilistic generative model. Third, the over-fitting problem, which is often observed in the models with millions of parameters such as DBNs, and the under-fitting problem, which occurs often in deep networks, can be effectively addressed by the generative pretraining step. An insightful analysis on what speech information DBNs can capture is provided in(Mohamed et al. 2012a).
Using hidden layers with many neurons in a DNN significantly improves the modeling power of the DNN and creates many closely optimal configurations. Even if parameter learning is trapped into a local optimum, the resulting DNN can still perform quite well since the chance of having a poor local optimum is lower than when a small number of neurons are used in the network. Using deep and wide neural networks, however, would cast great demand to the computational power during the training process and this is one of the reasons why it is not until recent years that researchers have started exploring both deep and wide neural networks in a serious manner.
Better learning algorithms and different nonlinearities also contributed to the success of DNNs.
Stochastic gradient descend (SGD) algorithms are the most efficient algorithm when the training set is large and redundant as is the case for most applications (Bottou and LeCun, 2004). Recently, SGD is shown to be effective for parallelizing over many machines with an asynchronous mode(Dean et al., 2012) or over multiple GPUs through pipelined BP (Chen et al., 2012). Further, SGD can often allow the training to jump out of local optima due to the noisy gradients estimated from a single or a small batch of samples. Other learning algorithms such as Hessian free (Martens 2010, Kingsbury et al., 2012) or Krylov subspace methods (Vinyals and Povey, 2011) have shown a similar ability.
For the highly non-convex optimization problem of DNN learning, it is obvious that better parameter initialization techniques will lead to better models since optimization starts from these initial models. What was not obvious, however, is how to efficiently and effectively initialize DNN parameters and how the use of very large amounts of training data can alleviate the learning problem until more recently (Hinton et al. 2006; Hinton and Salakhutdinov, 2006; Bengio, 2009;
Vincent et al., 2010; Deng et al., 2010; Yu et al., 2010c; Dahl et al., 2010, 2012; Seide et al. 2011;
Hinton et al., 2012). The DNN parameter initialization technique that attracted the most attention is the unsupervised pretraining technique proposed in (Hinton et al. 2006; Hinton and Salakhutdinov, 2006) discussed earlier.
The DBN pretraining procedure is not the only one that allows effective initialization of DNNs.
An alternative unsupervised approach that performs equally well is to pretrain DNNs layer by layer by considering each pair of layers as a de-noising autoencoder regularized by setting a random subset of the input nodes to zero (Bengio, 2009; Vincent et al., 2010). Another alternative is to use contractive autoencoders for the same purpose by favoring representations that are more robust to the input variations, i.e., penalizing the gradient of the activities of the hidden units with respect to the inputs (Rifai et al., 2011). Further, Ranzato et al. (2007) developed the Sparse Encoding
Symmetric Machine (SESM), which has a very similar architecture to RBMs as building blocks of a DBN. The SESM may also be used to effectively initialize the DNN training. In addition to unsupervised pretraining using greedy layer-wise procedures (Hinton and Salakhutdinov, 2006;
Bengio et al., 2006; Ranzato et al., 2007), the supervised pretraining, or sometimes called discriminative pretraining, has also been shown to be effective (Seide et al., 2011; Yu et al., 2011;
Hinton et al., 2012) and in cases where labeled training data are abundant performs better than the unsupervised pretraining techniques. The idea of the discriminative pretraining is to start from a one-hidden-layer MLP trained with the BP algorithm. Every time when we want to add a new hidden layer we replace the output layer with a randomly initialized new hidden and output layer and train the whole new MLP (or DNN) using the BP algorithm. Different from the unsupervised pretraining techniques, the discriminative pretraining technique requires labels.
Researchers who apply deep learning to speech and vision analyzed what DNNs capture in speech and images. For example, Mohamed et al. (2012a) applied a dimensionality reduction method to visualize the relationship among the feature vectors learned by the DNN. They found that the DNN's hidden activity vectors preserve the similarity structure of the feature vectors at multiple scales, and that this is especially true for the filterbank features. A more elaborated visualization method, based on a top-down generative process in the reverse direction of the classification network, was recently developed by Zeiler and Fergus (2013) for examining what features the deepconvolutional networks capture from the image data. The power of the deep networks is shown to be their ability to extract appropriate features and do discrimination jointly (LeCun, 2012).
As another way to concisely introduce the DNN, we can review the history of artificial neural networks using a "Hype Cycle", which is a graphic representation of the maturity, adoption and social application of specific technologies. The 2012 version of the Hype Cycles graph compiled by Gartner is shown in Figure 2.1. It intends to show how a technology or application will evolve over time (according to five phases: technology trigger, peak of inflated expectations, trough of disillusionment, slope of enlightenment, and plateau of production), and to provide a source of insight to manage its deployment.
Figure 2.1. Gartner Hyper Cycle graph representing five phases of a technology(http://en.wikipedia.org/wiki/Hype_cycle)
Applying the Gartner Hyper Cycle to the artificial neural network development, we created Figure
2.2 to align different generations of the neural network with the various phases designated in the Hype Cycle. The peak activities ("expectations" or "media hype" on the vertical axis) occurred in late 1980's and early 1990's, corresponding to the height of what is often referred to as the "second generation" of neural networks. The deep belief network (DBN) and a fast algorithm for training it were invented in 2006 (Hinton and Salakhudinov, 2006; Hinton et al., 2006). When the DBN was used to initialize the DNN, the learning became highly effective and this has inspired the subsequent fast growing research ("enlightenment" phase shown in Figure 2.2). Applications of the DBN and DNN to industry-scale speech feature extraction and speech recognition started in 2009 when leading academic and industrial researchers with both deep learning and speech expertise collaborated; see reviews in (Hinton et al., 2012; Deng et al., 2013b). This collaboration fast expanded the work of speech recognition using deep learning methods to increasingly larger successes (Yu et al., 2010c; Seide et al., 2011; Hinton et al., 2012; Deng et al., 2013a), many of which will be covered in the remainder of this book. The height of the "plateau of productivity" phase, not yet reached in our opinion, is expected to be higher than in the stereotypical curve(circled with a question mark in Figure 2.2), and is marked by the dashed line that moves straight up.
Figure 2.2: Applying Gartner Hyper Cycle graph to analyzing the history of artificial neural network technology (We thank our colleague John Platt during 2012 for bringing this type of "Hyper Cycle" graph to our attention for concisely analyzing the neural network history).
We show in Figure 2.3 the history of speech recognition, which has been compiled by NIST, organized by plotting the word error rate (WER) as a function of time for a number of increasingly difficult speech recognition tasks. Note all WER results were obtained using the GMM-HMM technology. When one particularly difficult task (Switchboard) is extracted from Figure 2.3, we see a flat curve over many years using the GMM-HMM technology but after the DNN technology is used the WER drops sharply (marked by the red star in Figure 2.4).
Figure 2.3: The famous NIST plot showing the historical speech recognition error rates achieved by the GMM-HMM approach for a number of increasingly difficult speech recognition tasks. Data source: http://itl.nist.gov/iad/mig/publications/ASRhistory/index.html
Figure 2.4. Extracting WERs of one task from Figure 2.3 and adding the significantly lower WER(marked by the star) achieved by the DNN technology approach.
In the next Chapter, an overview is provided on the various architectures of deep learning, followed by more detailed expositions of a few widely studied architectures and methods and by selected applications in signal and information processing including speech and audio, natural language, information retrieval, vision, and multi-modal processing.
CHAPTER 3
THREE CLASSES OF DEEP LEARNING
NETWORKS
3.1 A Three-Way Categorization
As described earlier, deep learning refers to a rather wide class of machine learning techniques and architectures, with the hallmark of using many layers of non-linear information processing that are hierarchical in nature. Depending on how the architectures and techniques are intended for use, e.g., synthesis/generation or recognition/classification, one can broadly categorize most of the work in this area into three major classes:
1) Deep networks for unsupervised or generative learning, which are intended to capture high-order correlation of the observed or visible data for pattern analysis or synthesis purposes when no information about target class labels is available. Unsupervised feature or representation learning in the literature refers to this category of the deep networks.
When used in the generative mode, may also be intended to characterize joint statistical distributions of the visible data and their associated classes when available and being treated as part of the visible data. In the latter case, the use of Bayes rule can turn this type of generative networks into a discriminative one for learning.
2) Deep networks for supervised learning, which are intended to directly provide discriminative power for pattern classification purposes, often by characterizing the posterior distributions of classes conditioned on the visible data. Target label data are always available in direct or indirect forms for such supervised learning. They are also called discriminative deep networks.
3) Hybrid deep networks, where the goal is discrimination which is assisted, often in a significant way, with the outcomes of generative or unsupervised deep networks. This can be accomplished by better optimization or/and regularization of the deep networks in category 2). The goal can also be accomplished when discriminative criteria for supervised learning are used to estimate the parameters in any of the deep generative or unsupervised deep networks in category 1) above.
Note the use of "hybrid" in 3) above is different from that used sometimes in the literature, which refers to the hybrid systems for speech recognition feeding the output probabilities of a neural network into an HMM (Bengio, 1991; Bengio et al., 1992; Bourlard and Morgan, 1993; Morgan, By the commonly adopted machine learning tradition (e.g., Chapter 28 in Murphy, 2012; Deng and Li, 2013), it may be natural to just classify deep learning techniques into deep discriminative models (e.g., deep neural networks or DNNs, recurrent neural networks or RNNs, convolutional neural networks or CNNs, etc.) and generative/unsupervised models (e.g., restricted Boltzmann machine or RBMs, deep belief networks or DBNs, deep Boltzmann machines (DBMs), regularized autoencoders, etc.). This two-way classification scheme, however, misses a key insight gained in deep learning research about how generative or unsupervised-learning models can greatly improve the training of DNNs and other deep discriminative or supervised-learning models via better regularization or optimization. Also, deep networks for unsupervised learning may not necessarily need to be probabilistic or be able to meaningfully sample from the model (e.g., traditional autoencoders, sparse coding networks, etc.). We note here that more recent studies have generalized the traditional denoising autoencoders so that they can be efficiently sampled from and thus have become generative models (Alain and Bengio, 2013; Bengio et al., 2013, 2013b).
Nevertheless, the traditional two-way classification indeed points to several key differences between deep networks for unsupervised and supervised learning. Compared between the two, deep supervised-learning models such as DNNs are usually more efficient to train and test, more flexible to construct, and more suitable for end-to-end learning of complex systems (e.g., no approximate inference and learning such as loopy belief propagation). On the other hand, the deep unsupervised-learning models, especially the probabilistic generative ones, are easier to interpret, easier to embed domain knowledge, easier to compose, and easier to handle uncertainty, but they are typically intractable in inference and learning for complex systems. These distinctions are retained also in the proposed three-way classification which is hence adopted throughout this book.
Below we review representative work in each of the above three categories, where several basic definitions are summarized in Table 3.1. Applications of these deep architectures, with varied ways of learning including supervised, unsupervised, or hybrid, are deferred to Chapters 7-11.
TABLE 3.1. BASIC DEEP LEARNING TERMINOLOGIES
Deep Learning: a class of machine learning techniques, where many layers of information processing stages in hierarchical architectures are exploited for unsupervised feature learning and for pattern analysis/classification. The essence of deep learning is to compute hierarchical features or representations of the observational data, where the higher-level features or factors are defined from lower-level ones. The family of deep learning methods have been growing increasingly richer, encompassing those of neural networks, hierarchical probabilistic models, and a variety of unsupervised and supervised feature learning algorithms.
Deep belief network (DBN): probabilistic generative models composed of multiple layers of stochastic, hidden variables. The top two layers have undirected, symmetric connections between them. The lower layers receive top-down, directed connections from the layer above.
Boltzmann machine (BM): a network of symmetrically connected, neuron-like units that make stochastic decisions about whether to be on or off.
Restricted Boltzmann machine (RBM): a special type of BM consisting of a layer of visible units and a layer of hidden units with no visible-visible or hidden-hidden connections.
Deep neural network (DNN): a multilayer perceptron with many hidden layers, whose weights are fully connected and are often initialized using either an unsupervised or a supervised pretraining technique. (In the literature prior to 2012, a DBN was often used incorrectly to mean a DNN.)
Deep autoencoder: a "discriminative" DNN whose output targets are the data input itself rather than class labels; hence an unsupervised learning model. When trained with a denoising criterion, a deep autoencoder is also a generative model and can be sampled from.
Distributed representation: an internal representation of the observed data in such a way that they are modeled as being explained by the interactions of many hidden factors. A particular factor learned from configurations of other factors can often generalize well to new configurations. Distributed representations naturally occur in a "connectionist" neural network, where a concept is represented by a pattern of activity across a number of many units and where at the same time a unit typically contributes to many concepts. One key advantage of such manyto-many correspondence is that they provide robustness in representing the internal structure of the data in terms of graceful degradation and damage resistance. Another key advantage is that they facilitate generalizations of concepts and relations, thus enabling reasoning abilities.
3.2 Deep Networks for Unsupervised or Generative
Learning
Unsupervised learning refers to no use of task specific supervision information (e.g., target class labels) in the learning process. Many deep networks in this category can be used to meaningfully generate samples by sampling from the networks, with examples of RBMs, DBNs, DBMs, and generalized denoising autoencoders (Bengio et al., 2013), and are thus generative models. Some networks in this category, however, cannot be easily sampled, with examples of sparse coding networks and the original forms of deep autoencoders, and are thus not generative in nature.
Among the various subclasses of generative or unsupervised deep networks, the energy-based deep models are the most common (e.g., Bengio at al., 2006; LeCun et al., 2007; Ngiam et al., 2011;
Bengio, 2009). The original form of the deep auto encoder (Hinton and Salakhutdinov, 2006;
Bengio at al., 2006; Deng et al., 2010), which we will give more detail about in Chapter 4, is a typical example of this unsupervised model category. Most other forms of deep autoencoders are also unsupervised in nature, but with quite different properties and implementations. Examples are transforming autoencoders (Hinton et al., 2011), predictive sparse coders and their stacked version, and de-noising autoencoders and their stacked versions (Vincent et al., 2010).
Specifically, in de-noising autoencoders, the input vectors are first corrupted by, for example, randomly selecting a percentage of the inputs and setting them to zeros or adding Gaussian noise to them. Then the parameters are adjusted for the hidden encoding nodes to reconstruct the original, uncorrupted input data using criteria such as mean square reconstruction error and KL divergence between the original inputs and the reconstructed inputs. The encoded representations transformed from the uncorrupted data are used as the inputs to the next level of the stacked de-noising autoencoder.
Another prominent type of deep unsupervised models with generative capability is the deep
Boltzmann machine or DBM (Salakhutdinov and Hinton, 2009, 2012; Srivastava and Salakhutdinov, 2012; Goodfellow et al., 2013). A DBM contains many layers of hidden variables, and has no connections between the variables within the same layer. This is a special case of the general Boltzmann machine (BM), which is a network of symmetrically connected units that are on or off based on a stochastic mechanism. While having a simple learning algorithm, the general
BMs are very complex to study and very slow to train. In a DBM, each layer captures complicated, higher-order correlations between the activities of hidden features in the layer below. DBMs have the potential of learning internal representations that become increasingly complex, highly desirable for solving object and speech recognition problems. Further, the high-level representations can be built from a large supply of unlabeled sensory inputs and very limited labeled data can then be used to only slightly fine-tune the model for a specific task at hand.
When the number of hidden layers of DBM is reduced to one, we have restricted Boltzmann machine (RBM). Like DBM, there are no hidden-to-hidden and no visible-to-visible connections in the RBM. The main virtue of RBM is that via composing many RBMs, many hidden layers can be learned efficiently using the feature activations of one RBM as the training data for the next.
Such composition leads to deep belief network (DBN), which we will describe in more detail, together with RBMs, in Chapter 5.
The standard DBN has been extended to the factored higher-order Boltzmann machine in its bottom layer, with strong results for phone recognition obtained (Dahl et. al., 2010). This model, called the mean-covariance RBM or mcRBM, recognizes the limitation of the standard RBM in its ability to represent the covariance structure of the data. However, it is difficult to train mcRBMs and to use them at the higher levels of the deep architecture. Further, the strong results published are not easy to reproduce. In the architecture described by Dahl et al. (2010), the mcRBM parameters in the full DBN are not fine-tuned using the discriminative information, which is used for fine tuning the higher layers of RBMs, due to the high computational cost.
Another representative deep generative network that can be used for unsupervised (as well as supervised) learning is the sum-product network or SPN (Poon and Domingo, 2011; Gens and Domingo, 2012). An SPN is a directed acyclic graph with the observed variables as leaves, and with sum and product operations as internal nodes in the deep network. The "sum" nodes give mixture models, and the "product" nodes build up the feature hierarchy. Properties of "completeness" and "consistency" constrain the SPN in a desirable way. The learning of SPNs is carried out using the EM algorithm together with back-propagation. The learning procedure starts with a dense SPN. It then finds an SPN structure by learning its weights, where zero weights indicate removed connections. The main difficulty in learning SPNs is that the learning signal (i.e., the gradient) quickly dilutes when it propagates to deep layers. Empirical solutions have been found to mitigate this difficulty as reported in (Poon and Domingo, 2011). It was pointed out in that early paper that despite the many desirable generative properties in the SPN, it is difficult to fine tune the parameters using the discriminative information, limiting its effectiveness in classification tasks. However, this difficulty has been overcome in the subsequent work reported in (Gens and Domingo, 2012), where an efficient backpropagation-style discriminative training algorithm for SPN was presented. Importantly, the standard gradient descent, based on the derivative of the conditional likelihood, suffers from the same gradient diffusion problem well known in the regular DNNs. The trick to alleviate this problem in learning SPNs is to replace the marginal inference with the most probable state of the hidden variables and to propagate gradients through this "hard" alignment only. Excellent results on small-scale image recognition tasks were reported by Gens and Domingo (2012).
Recurrent neural networks (RNNs) can be considered as another class of deep networks for unsupervised (as well as supervised) learning, where the depth can be as large as the length of the input data sequence. In the unsupervised learning mode, the RNN is used to predict the data sequence in the future using the previous data samples, and no additional class information is used for learning. The RNN is very powerful for modeling sequence data (e.g., speech or text), but until recently they had not been widely used partly because they are difficult to train to capture longterm dependencies, giving rise to gradient vanishing or gradient explosion problems. These problems can now be dealt with more easily (Bengio et al., 2013a; Pascanu et al., 2013; Chen and Deng, 2013). Recent advances in Hessian-free optimization (Martens, 2010) have also partially overcome this difficulty using approximated second-order information or stochastic curvature estimates. In the more recent work (Martens and Sutskever, 2011), RNNs that are trained with
Hessian-free optimization are used as a generative deep network in the character-level languagemodeling tasks, where gated connections are introduced to allow the current input characters to predict the transition from one latent state vector to the next. Such generative RNN models are demonstrated to be well capable of generating sequential text characters. More recently, Bengio et al. (2013) and Sutskever (2013) have explored variations of stochastic gradient descent optimization algorithms in training generative RNNs and shown that these algorithms can outperform Hessian-free optimization methods. Molotov et al. (2010) have reported excellent results on using RNNs for language modeling. More recently, Mesnil et al. (2013) and Yao et al.(2013) reported the success of RNNs in spoken language understanding. We will review this set of work in Chapter 8.
There has been a long history in speech recognition research where human speech production mechanisms are exploited to construct dynamic and deep structure in probabilistic generative models; for a comprehensive review, see the book by Deng (2006). Specifically, the early work described in (Deng 1992, 1993; Deng et al., 1994; Ostendorf et al., 1996, Deng and Sameti, 1996;
Deng and Aksmanovic, 1997) generalized and extended the conventional shallow and conditionally independent HMM structure by imposing dynamic constraints, in the form of polynomial trajectory, on the HMM parameters. A variant of this approach has been more recently developed using different learning techniques for time-varying HMM parameters and with the applications extended to speech recognition robustness (Yu and Deng, 2009; Yu et al., 2009a).
Similar trajectory HMMs also form the basis for parametric speech synthesis (Zen et al., 2011;
Zen et al., 2012; Ling et al., 2013; Shannon et al., 2013). Subsequent work added a new hidden layer into the dynamic model to explicitly account for the target-directed, articulatory-like properties in human speech generation (Deng and Ramsay, 1997; Deng, 1998; Bridle et al., 1998;
Deng, 1999; Picone et al., 1999; Deng, 2003; Minami et al., 2002; Deng and Huang, 2004; Deng and Ma, 2000; Ma and Deng, 2000, 2003, 2004). More efficient implementation of this deep architecture with hidden dynamics is achieved with non-recursive or finite impulse response (FIR) filters in more recent studies (Deng et. al., 2006, 2006a, Deng and Yu, 2007). The above deepstructured generative models of speech can be shown as special cases of the more general dynamic network model and even more general dynamic graphical models (Bilmes and Bartels, 2005;
Bilmes, 2010). The graphical models can comprise many hidden layers to characterize the complex relationship between the variables in speech generation. Armed with powerful graphical modeling tool, the deep architecture of speech has more recently been successfully applied to solve the very difficult problem of single-channel, multi-talker speech recognition, where the mixed speech is the visible variable while the un-mixed speech becomes represented in a new hidden layer in the deep generative architecture (Rennie et al., 2010; Wohlmayr et al., 2011). Deep generative graphical models are indeed a powerful tool in many applications due to their capability of embedding domain knowledge. However, they are often used with inappropriate approximations in inference, learning, prediction, and topology design, all arising from inherent intractability in these tasks for most real-world applications. This problem has been addressed in the recent work of Stoyanov et al. (2011), which provides an interesting direction for making deep generative graphical models potentially more useful in practice in the future. An even more drastic way to deal with this intractability was proposed recently by Bengio et al. (2013b), where the need to marginalize latent variables is avoided altogether.
The standard statistical methods used for large-scale speech recognition and understanding combine (shallow) hidden Markov models for speech acoustics with higher layers of structurerepresenting different levels of natural language hierarchy. This combined hierarchical model can be suitably regarded as a deep generative architecture, whose motivation and some technical detail may be found in Chapter 7 of the recent book (Kurzweil, 2012) on "Hierarchical HMM" or HHMM.
Related models with greater technical depth and mathematical treatment can be found in (Fine et al., 1998) for HHMM and (Oliver et al., 2004) for Layered HMM. These early deep models were formulated as directed graphical models, missing the key aspect of "distributed representation" embodied in the more recent deep generative networks of the DBN and DBM discussed earlier in this chapter. Filling in this missing aspect would help improve these generative models.
Finally, dynamic or temporally recursive generative models based on neural network architectures can be found in (Taylor et al., 2007) for human motion modeling, and in (Socher et al., 2011, 2012) for natural language and natural scene parsing. The latter model is particularly interesting because the learning algorithms are capable of automatically determining the optimal model structure. This contrasts with other deep architectures such as DBN where only the parameters are learned while the architectures need to be pre-defined. Specifically, as reported in (Socher et al., 2011), the recursive structure commonly found in natural scene images and in natural language sentences can be discovered using a max-margin structure prediction architecture. It is shown that the units contained in the images or sentences are identified, and the way in which these units interact with each other to form the whole is also identified.
3.3 Deep Networks for Supervised Learning
Many of the discriminative techniques for supervised learning in signal and information processing are shallow architectures such as HMMs (e.g., Juang et al., 1997; Chengalvarayan and Deng, 1998;
Povey and Woodland, 2002; Yu et al., 2007; He et al., 2008; Jiang and Li, 2010; Xiao and Deng, 2010; Gibson and Hain, 2010) and conditional random fields (CRFs) (e.g., Yang and Furui, 2009;
Yu et al., 2010; Hifny and Renals, 2009; Heintz et al., 2009; Zweig and Nguyen, 2009; Peng et al., 2009). A CRF is intrinsically a shallow discriminative architecture, characterized by the linear relationship between the input features and the transition features. The shallow nature of the CRF is made most clear by the equivalence established between the CRF and the discriminatively trained Gaussian models and HMMs (Heigold et al., 2011). More recently, deep-structured CRFs have been developed by stacking the output in each lower layer of the CRF, together with the original input data, onto its higher layer (Yu et al., 2010a). Various versions of deep-structured
CRFs are successfully applied to phone recognition (Yu and Deng, 2010), spoken language identification (Yu et al., 2010a), and natural language processing (Yu et al., 2010). However, at least for the phone recognition task, the performance of deep-structured CRFs, which are purely discriminative (non-generative), has not been able to match that of the hybrid approach involving
DBN, which we will take on shortly.
Morgan (2012) gives an excellent review on other major existing discriminative models in speech recognition based mainly on the traditional neural network or MLP architecture using backpropagation learning with random initialization. It argues for the importance of both the increased width of each layer of the neural networks and the increased depth. In particular, a class of deep neural network models forms the basis of the popular "tandem" approach (Morgan et al., 2005), where the output of the discriminatively learned neural network is treated as part of the observationvariable in HMMs. For some representative recent work in this area, see (Pinto et al., 2011;
Ketabdar and Bourlard, 2010).
In the most recent work of (Deng et. al, 2011; Deng et al., 2012a; Tur et al., 2012; Lena et al., 2012; Vinyals et al., 2012), a new deep learning architecture, sometimes called Deep Stacking
Network (DSN), together with its tensor variant (Hutchinson et al, 2012, 2013) and its kernel version (Deng et al., 2012), are developed that all focus on discrimination with scalable, parallelizable learning relying on little or no generative component. We will describe this type of discriminative deep architecture in detail in Chapter 6.
As discussed in the preceding section, recurrent neural networks (RNNs) have been used as a generative model; see also the neural predictive model (Deng et al., 1994a) with a similar
"generative" mechanism. RNNs can also be used as a discriminative model where the output is a label sequence associated with the input data sequence. Note that such discriminative RNNs or sequence models were applied to speech a long time ago with limited success. In (Bengio, 1991), an HMM was trained jointly with the neural networks, with a discriminative probabilistic training criterion. In (Robinson, 1994), a separate HMM was used to segment the sequence during training, and the HMM was also used to transform the RNN classification results into label sequences.
However, the use of the HMM for these purposes does not take advantage of the full potential of RNNs.
A set of new models and methods were proposed more recently in (Graves et al., 2006; Graves, 2012, Graves et al., 2013, 2013a) that enable the RNNs themselves to perform sequence classification while embedding the long-short-term memory into the model, removing the need for pre-segmenting the training data and for post-processing the outputs. Underlying this method is the idea of interpreting RNN outputs as the conditional distributions over all possible label sequences given the input sequences. Then, a differentiable objective function can be derived to optimize these conditional distributions over the correct label sequences, where the segmentation of the data is performed automatically by the algorithm. The effectiveness of this method has been demonstrated in handwriting recognition tasks and in a small speech task (Graves et al., 2013, 2013a) to be discussed in more detail in Chapter 7 of this book.
Another type of discriminative deep architecture is the convolutional neural network (CNN), in which each module consists of a convolutional layer and a pooling layer. These modules are often stacked up with one on top of another, or with a DNN on top of it, to form a deep model. The convolutional layer shares many weights, and the pooling layer subsamples the output of the convolutional layer and reduces the data rate from the layer below. The weight sharing in the convolutional layer, together with appropriately chosen pooling schemes, endows the CNN with some "invariance" properties (e.g., translation invariance). It has been argued that such limited
"invariance" or equi-variance is not adequate for complex pattern recognition tasks and more principled ways of handling a wider range of invariance may be needed (Hinton et al., 2011).
Nevertheless, CNNs have been found highly effective and been commonly used in computer vision and image recognition (Bengio and LeCun, 1995; LeCun et al., 1998; Ciresan et al., 2010, 2011, 2012, 2012a; Le et al., 2012; Dean et al., 2012; Krizhevsky et al., 2012, Zeiler, 2014). More recently, with appropriate changes from the CNN designed for image analysis to that taking intoaccount speech-specific properties, the CNN is also found effective for speech recognition (AbdelHamid et al., 2012, 2013, 2013a; Sainath et al., 2013; Deng et al., 2013). We will discuss such applications in more detail in Chapter 7 of this book.
It is useful to point out that the time-delay neural network (TDNN, Lang et al., 1990; Waibel et al., 1989) developed for early speech recognition is a special case and predecessor of the CNN when weight sharing is limited to one of the two dimensions, i.e., time dimension, and there is no pooling layer. It was not until recently that researchers have discovered that the time-dimension invariance is less important than the frequency-dimension invariance for speech recognition (Abdel-Hamid et al., 2012, 2013; Deng et al., 2013). A careful analysis on the underlying reasons is described in(Deng et al., 2013), together with a new strategy for designing the CNN's pooling layer demonstrated to be more effective than all previous CNNs in phone recognition.
It is also useful to point out that the model of hierarchical temporal memory (HTM, Hawkins and Blakeslee, 2004; Hawkins et al., 2010; George, 2008) is another variant and extension of the CNN.
The extension includes the following aspects: 1) Time or temporal dimension is introduced to serve as the "supervision" information for discrimination (even for static images); 2) Both bottom-up and top-down information flows are used, instead of just bottom-up in the CNN; and 3) A Bayesian probabilistic formalism is used for fusing information and for decision making.
Finally, the learning architecture developed for bottom-up, detection-based speech recognition proposed in (Lee, 2004) and developed further since 2004, notably in (Yu et al., 2012a; Siniscalchi et al., 2013, 2013a) using the DBN-DNN technique, can also be categorized in the discriminative or supervised-learning deep architecture category. There is no intent and mechanism in this architecture to characterize the joint probability of data and recognition targets of speech attributes and of the higher-level phone and words. The most current implementation of this approach is based on the DNN, or neural networks with many layers using back-propagation learning. One intermediate neural network layer in the implementation of this detection-based framework explicitly represents the speech attributes, which are simplified entities from the "atomic" units of speech developed in the early work of (Deng and Sun, 1994; Sun and Deng, 2002). The simplification lies in the removal of the temporally overlapping properties of the speech attributes or articulatory-like features. Embedding such more realistic properties in the future work is expected to improve the accuracy of speech recognition further.
3.4 Hybrid Deep Networks
The term "hybrid" for this third category refers to the deep architecture that either comprises or makes use of both generative and discriminative model components. In the existing hybrid architectures published in the literature, the generative component is mostly exploited to help with discrimination, which is the final goal of the hybrid architecture. How and why generative modeling can help with discrimination can be examined from two viewpoints (Erhan et al., 2010):

The optimization viewpoint where generative models trained in an unsupervised fashion can provide excellent initialization points in highly nonlinear parameter estimation problems(The commonly used term of "pre-training" in deep learning has been introduced for this reason); and/or

The regularization perspective where the unsupervised-learning models can effectively provide a prior on the set of functions representable by the model.
The study reported in (Erhan et al., 2010) provided an insightful analysis and experimental evidence supporting both of the viewpoints above.
The DBN, a generative, deep network for unsupervised learning discussed in Chapter 3.2, can be converted to and used as the initial model of a DNN for supervised learning with the same network structure, which is further discriminatively trained or fine-tuned using the target labels provided.
When the DBN is used in this way we consider this DBN-DNN model as a hybrid deep model, where the model trained using unsupervised data helps to make the discriminative model effective for supervised learning. We will review details of the discriminative DNN for supervised learning in the context of RBM/DBN generative, unsupervised pre-training in Chapter 5.
Another example of the hybrid deep network is developed in (Mohamed et al., 2010), where the DNN weights are also initialized from a generative DBN but are further fine-tuned with a sequence-level discriminative criterion, which is the conditional probability of the label sequence given the input feature sequence, instead of the frame-level criterion of cross-entropy commonly used. This can be viewed as a combination of the static DNN with the shallow discriminative architecture of CRF. It can be shown that such a DNN-CRF is equivalent to a hybrid deep architecture of DNN and HMM whose parameters are learned jointly using the full-sequence maximum mutual information (MMI) criterion between the entire label sequence and the input feature sequence. A closely related full-sequence training method designed and implemented for much larger tasks is carried out more recently with success for a shallow neural network(Kingsbury, 2009) and for a deep one (Kingsbury et al., 2012; Su et al., 2013). We note that the origin of the idea for joint training of the sequence model (e.g., the HMM) and of the neural network came from the early work of (Bengio, 1991; Bengio et al., 1992), where shallow neural networks were trained with small amounts of training data and with no generative pre-training.
Here, it is useful to point out a connection between the above pretraining/fine-tuning strategy associated with hybrid deep networks and the highly popular minimum phone error (MPE) training technique for the HMM (Povey and Woodland, 2002; and He et al., 2008 for an overview). To make MPE training effective, the parameters need to be initialized using an algorithm (e.g., BaumWelch algorithm) that optimizes a generative criterion (e.g., maximum likelihood). This type of methods, which uses maximum-likelihood trained parameters to assist in the discriminative HMM training can be viewed as a "hybrid" approach to train the shallow HMM model.
Along the line of using discriminative criteria to train parameters in generative models as in the above HMM training example, we here discuss the same method applied to learning other hybrid deep networks. In (Larochelle and Bengio, 2008), the generative model of RBM is learned using the discriminative criterion of posterior class-label probabilities. Here the label vector is concatenated with the input data vector to form the combined visible layer in the RBM. In thisway, RBM can serve as a stand-alone solution to classification problems and the authors derived a discriminative learning algorithm for RBM as a shallow generative model. In the more recent work by Ranzato et al. (2011), the deep generative model of DBN with gated Markov random field(MRF) at the lowest level is learned for feature extraction and then for recognition of difficult image classes including occlusions. The generative ability of the DBN facilitates the discovery of what information is captured and what is lost at each level of representation in the deep model, as demonstrated in (Ranzato et al., 2011). A related study on using the discriminative criterion of empirical risk to train deep graphical models can be found in (Stoyanov et al., 2011).
A further example of hybrid deep networks is the use of generative models of DBNs to pre-train deep convolutional neural networks (deep CNNs) (Lee et al., 2009, 2010, 2011). Like the fully connected DNN discussed earlier, pre-training also helps to improve the performance of deep
CNNs over random initialization. Pre-training DNNs or CNNs using a set of regularized deep autoencoders (Bengio et al., 2013a), including denoising autoencoders, contractive autoencoders, and sparse autoencoders, is also a similar example of the category of hybrid deep networks.
The final example given here for hybrid deep networks is based on the idea and work of (Ney, 1999; He and Deng, 2011), where one task of discrimination (e.g., speech recognition) produces the output (text) that serves as the input to the second task of discrimination (e.g., machine translation). The overall system, giving the functionality of speech translation – translating speech in one language into text in another language – is a two-stage deep architecture consisting of both generative and discriminative elements. Both models of speech recognition (e.g., HMM) and of machine translation (e.g., phrasal mapping and non-monotonic alignment) are generative in nature, but their parameters are all learned for discrimination of the ultimate translated text given the speech data. The framework described in (He and Deng, 2011) enables end-to-end performance optimization in the overall deep architecture using the unified learning framework initially published in (He et al., 2008). This hybrid deep learning approach can be applied to not only speech translation but also all speech-centric and possibly other information processing tasks such as speech information retrieval, speech understanding, cross-lingual speech/text understanding and retrieval, etc. (e.g., Yamin et al., 2008; Tur et al., 2012; He and Deng, 2012, 2013; Deng et al., 2012; Deng et al., 2013a; He et al., 2013).
In the next three chapters, we will elaborate on three prominent types of models for deep learning, one from each of the three classes reviewed in this chapter. These are chosen to serve the tutorial purpose, given their simplicity of the architectural and mathematical descriptions. The three architectures described in the following three chapters may not be interpreted as the most representative and influential work in each of the three classes.
CHAPTER 4
DEEP AUTOENCODERS --UNSUPERVISED LEARNING
This chapter and the next two will each select one prominent example deep network for each of the three categories outlined in Chapter 3. Here we begin with the category of the deep models designed mainly for unsupervised learning.
4.1 Introduction
The deep autoencoder is a special type of the DNN (with no class labels), whose output vectors have the same dimensionality as the input vectors. It is often used for learning a representation or effective encoding of the original data, in the form of input vectors, at hidden layers. Note that the autoencoder is a nonlinear feature extraction method without using class labels. As such, the features extracted aim at conserving and better representing information instead of performing classification tasks, although sometimes these two goals are correlated.
An autoencoder typically has an input layer which represents the original data or input feature vectors (e.g., pixels in image or spectra in speech), one or more hidden layers that represent the transformed feature, and an output layer which matches the input layer for reconstruction. When the number of hidden layers is greater than one, the autoencoder is considered to be deep. The dimension of the hidden layers can be either smaller (when the goal is feature compression) or larger (when the goal is mapping the feature to a higher-dimensional space) than the input dimension.
An autoencoder is often trained using one of the many back-propagation variants, typically the stochastic gradient descent method. Though often reasonably effective, there are fundamental problems when using back-propagation to train networks with many hidden layers. Once the errors get back-propagated to the first few layers, they become minuscule, and training becomes quite ineffective. Though more advanced back-propagation methods help with this problem to some degree, it still results in slow learning and poor solutions, especially with limited amounts of training data. As mentioned in the previous chapters, the problem can be alleviated by pre-training each layer as a simple autoencoder (Hinton et al, 2006; Bengio et al., 2006). This strategy has been applied to construct a deep autoencoder to map images to short binary code for fast, content-based image retrieval, to encode documents (called semantic hashing), and to encode spectrogram-like speech features which we review below.
4.2 Use of Deep Autoencoders to Extract Speech
Features
Here we review a set of work, some of which was published in (Deng et al., 2010), in developing an autoencoder for extracting binary speech codes using unlabeled speech data only. The discrete representations in terms of a binary code extracted by this model can be used in speech information retrieval or as bottleneck features for speech recognition.
A deep generative model of patches of spectrograms that contain 256 frequency bins and 1, 3, 9, or 13 frames is illustrated in Figure 4.1. An undirected graphical model called a GaussianBernoulli RBM is built that has one visible layer of linear variables with Gaussian noise and one hidden layer of 500 to 3000 binary latent variables. After learning the Gaussian- Bernoulli RBM, the activation probabilities of its hidden units are treated as the data for training another BernoulliBernoulli RBM. These two RBM's can then be composed to form a deep belief net (DBN) in which it is easy to infer the states of the second layer of binary hidden units from the input in a single forward pass. The DBN used in this work is illustrated on the left side of Figure 4.1, where the two RBMs are shown in separate boxes. (See more detailed discussions on RBM and DBN in Chapter 5).
Figure 4.1. The architecture of the deep autoencoder used in (Deng et al., 2010) for extracting binary speech codes from high-resolution spectrograms. [after (Deng et. al., 2010), @Elsevier]
The deep autoencoder with three hidden layers is formed by "unrolling" the DBN using its weight matrices. The lower layers of this deep autoencoder use the matrices to encode the input and the upper layers use the matrices in reverse order to decode the input. This deep autoencoder is then fine-tuned using error back-propagation to minimize the reconstruction error, as shown on the right side of Figure 4.1. After learning is complete, any variable-length spectrogram can be encoded and reconstructed as follows. First, N consecutive overlapping frames of 256-point log power spectra are each normalized to zero-mean and unit-variance across samples per feature to provide the input to the deep autoencoder. The first hidden layer then uses the logistic function to compute realvalued activations. These real values are fed to the next, coding layer to compute "codes". The real-valued activations of hidden units in the coding layer are quantized to be either zero or one with 0.5 as the threshold. These binary codes are then used to reconstruct the original spectrogram, where individual fixed-frame patches are reconstructed first using the two upper layers of network weights. Finally, the standard overlap-and-add technique in signal processing is used to reconstruct the full-length speech spectrogram from the outputs produced by applying the deep autoencoder to every possible window of N consecutive frames. We show some illustrative encoding and reconstruction examples below.
At the top of Figure 4.2 is the original, un-coded speech, followed by the speech utterances reconstructed from the binary codes (zero or one) at the 312 unit bottleneck code layer with encoding window lengths of N=1, 3, 9, and 13, respectively. The lower reconstruction errors for
N=9 and N=13 are clearly seen.
Figure 4.2. Top to Bottom: The original spectrogram; reconstructions using input window sizes of N= 1, 3, 9, and 13 while forcing the coding units to take values of zero or one (i.e., a binary code). [after (Deng et. al., 2010), @Elsevier]
Encoding error of the deep autoencoder is qualitatively examined in comparison with the more traditional codes via vector quantization (VQ). Figure 3 shows various aspects of the encoding errors. At the top is the original speech utterance's spectrogram. The next two spectrograms are the blurry reconstruction from the 312-bit VQ and the much more faithful reconstruction from the 312-bit deep autoencoder. Coding errors from both coders, plotted as a function of time, are shown below the spectrograms, demonstrating that the autoencoder (red curve) is producing lower errors than the VQ coder (blue curve) throughout the entire span of the utterance. The final two spectrograms show detailed coding error distributions over both time and frequency bins.
Figures 4.4 to 4.10 show additional examples (unpublished) for the original un-coded speech spectrograms and their reconstructions using the deep autoencoder. They give a diverse number of binary codes for either a single or three consecutive frames in the spectrogram samples.
Figure 4.3. Top to bottom: The original spectrogram from the test set; reconstruction from the 312-bit VQ coder; reconstruction from the 312-bit autoencoder; coding errors as a function of time for the VQ coder (blue) and autoencoder (red); spectrogram of the VQ coder residual; spectrogram of the deep autoencoder's residual. [after (Deng et. al., 2010), @Elsevier]
Figure 4.4. The original speech spectrogram and the reconstructed counterpart. A total of 312 binary codes are with one for each single frame.
Figure 4.5. Same as Figure 4.4 but with a different TIMIT speech utterance.
Figure 4.6. The original speech spectrogram and the reconstructed counterpart. A total of 936 binary codes are used for three adjacent frames.
Figure 4.7. Same as Figure 4.6 but with a different TIMIT speech utterance.
Figure 4.8. Same as Figure 4.6 but with yet another TIMIT speech utterance.
Figure 4.9. The original speech spectrogram and the reconstructed counterpart. A total of 2000 binary codes with one for each single frame.
Figure 4.10. Same as Figure 4.9 but with a different TIMIT speech utterance.
4.3 Stacked Denoising Autoencoders
In early years of autoencoder research, the encoding layer had smaller dimensions than the input layer. However, in some applications, it is desirable that the encoding layer is wider than the input layer, in which case techniques are needed to prevent the neural network from learning the trivial identity mapping function. One of the reasons for using a higher dimension in the hidden or encoding layers than the input layer is that it allows the autoencoder to capture a rich input distribution.
The trivial mapping problem discussed above can be prevented by methods such as using sparseness constraints, or using the "dropout" trick by randomly forcing certain values to be zero and thus introducing distortions at the input data (Vincent, et al., 2010; Vincent, 2011) or at the hidden layers (Hinton et al., 2012a). For example, in the stacked denoising autoencoder detailed in (Vincent, et al., 2010), random noises are added to the input data. This serves several purposes.
First, by forcing the output to match the original undistorted input data the model can avoid learning the trivial identity solution. Second, since the noises are added randomly, the model learned would be robust to the same kind of distortions in the test data. Third, since each distorted input sample is different, it greatly increases the training set size and thus can alleviate the overfitting problem.
It is interesting to note that when the encoding and decoding weights are forced to be the transpose of each other, such denoising autoencoder with a single sigmoidal hidden layer is strictly equivalent to a particular Gaussian RBM, but instead of training it by the technique of contrastive divergence (CD) or persistent CD, it is trained by a score matching principle, where the score is defined as the derivative of the log-density with respect to the input (Vincent, 2011). Furthermore, Alain and Bengio (2013) generalized this result to any parameterization of the encoder and decoder with squared reconstruction error and Gaussian corruption noise. They show that as the amount of noise approaches zero, such models estimate the true score of the underlying data generating distribution. Finally, Bengio et al (2013b) show that any denoising autoencoder is a consistent estimator of the underlying data generating distribution within some family of distributions. This is true for any parameterization of the autoencoder, for any type of information-destroying corruption process with no constraint on the noise level except being positive, and for any reconstruction loss expressed as a conditional log-likelihood. The consistency of the estimator is achieved by associating the denoising autoencoder with a Markov chain whose stationary distribution is the distribution estimated by the model, and this Markov chain can be used to sample from the denoising autoencoder.
4.4 Transforming Autoencoders
The deep autoencoder described above can extract faithful codes for feature vectors due to many layers of nonlinear processing. However, the code extracted in this way is transformation-variant.
In other words, the extracted code would change in ways chosen by the learner when the input feature vector is transformed. Sometimes, it is desirable to have the code change predictably to reflect the underlying transformation-invariant property of the perceived content. This is the goal of the transforming autoencoder proposed in (Hinton et al., 2011) for image recognition.
The building block of the transforming autoencoder is a "capsule", which is an independent subnetwork that extracts a single parameterized feature representing a single entity, be it visual or audio. A transforming autoencoder receives both an input vector and a target output vector, which is transformed from the input vector through a simple global transformation mechanism; e.g. translation of an image and frequency shift of speech (the latter due to the vocal tract length difference). An explicit representation of the global transformation is assumed known. The coding layer of the transforming autoencoder consists of the outputs of several capsules.
During the training phase, the different capsules learn to extract different entities in order to minimize the error between the final output and the target.
In addition to the deep autoencoder architectures described here, there are many other types of generative architectures in the literature, all characterized by the use of data alone (i.e., free of classification labels) to automatically derive higher-level features.
CHAPTER 5
PRE-TRAINED DEEP NEURAL
NETWORKS --- A HYBRID
In this chapter, we present the most widely used hybrid deep architecture – the pre-trained deep neural network (DNN), and discuss the related techniques and building blocks including the RBM and DBN. We discuss the DNN example here in the category of hybrid deep networks before the examples in the category of deep networks for supervised learning (Chapter 6). This is partly due to the natural flow from the unsupervised learning models to the DNN as a hybrid model. The discriminative nature of artificial neural networks for supervised learning has been widely known, and thus would not be required for understanding the hybrid nature of the DNN that uses unsupervised pre-training to facilitate the subsequent discriminative fine tuning.
Part of the review in this chapter is based on recent publications in (Hinton et al., 2012), (Yu and Deng, 2011), and (Dahl et al., 2012).
5.1 Restricted Boltzmann Machines
An RBM is a special type of Markov random field that has one layer of (typically Bernoulli) stochastic hidden units and one layer of (typically Bernoulli or Gaussian) stochastic visible or observable units. RBMs can be represented as bipartite graphs, where all visible units are connected to all hidden units, and there are no visible-visible or hidden-hidden connections.
In an RBM, the joint distribution p(𝐯, 𝐡; θ) over the visible units 𝐯 and hidden units 𝐡, given the model parameters θ, is defined in terms of an energy function E(𝐯, 𝐡; θ) of p(𝐯, 𝐡; θ) = 𝑒𝑥𝑝 (−E(𝐯, 𝐡; θ))
𝑍, where 𝑍 = ∑ ∑ 𝑒𝑥𝑝(−E(𝐯, 𝐡; θ)) 𝐡 𝐯 is a normalization factor or partition function, and the marginal probability that the model assigns to a visible vector 𝐯 is p(𝐯; θ) =
∑ 𝑒𝑥𝑝(−E(𝐯, 𝐡; θ)) 𝒉
𝑍
For a Bernoulli (visible)-Bernoulli (hidden) RBM, the energy function is defined as
E(𝐯, 𝐡; θ) = − ∑ ∑ 𝑤𝑖𝑗
𝐽 𝑗=1 𝑣𝑖ℎ𝑗
𝐼 𝑖=1
− ∑ 𝑏𝑖𝑣𝑖
𝐼 𝑖=1
− ∑ 𝑎𝑗ℎ𝑗
𝐽 𝑗=1, where 𝑤𝑖𝑗 represents the symmetric interaction term between visible unit 𝑣𝑖 and hidden unit ℎ𝑗, 𝑏𝑖 and 𝑎𝑗 the bias terms, and 𝐼 and 𝐽 are the numbers of visible and hidden units. The conditional probabilities can be efficiently calculated as 𝑝(ℎ𝑗 = 1|𝐯; θ) = 𝜎 (∑ 𝑤𝑖𝑗
𝐼 𝑖=1 𝑣𝑖 + 𝑎𝑗), 𝑝(𝑣𝑖 = 1|𝐡; θ) = 𝜎 (∑ 𝑤𝑖𝑗
𝐽 𝑗=1 ℎ𝑗 + 𝑏𝑖), where 𝜎(𝑥) = 1 (1 + 𝑒𝑥𝑝(−𝑥))
⁄
Similarly, for a Gaussian (visible)-Bernoulli (hidden) RBM, the energy is E(𝐯, 𝐡; θ) = − ∑ ∑ 𝑤𝑖𝑗
𝐽 𝑗=1 𝑣𝑖ℎ𝑗
𝐼 𝑖=1
− 1
2 ∑(𝑣𝑖 − 𝑏𝑖)2
𝐼 𝑖=1
− ∑ 𝑎𝑗ℎ𝑗
𝐽 𝑗=1, The corresponding conditional probabilities become 𝑝(ℎ𝑗 = 1|𝐯; θ) = 𝜎 (∑ 𝑤𝑖𝑗
𝐼 𝑖=1 𝑣𝑖 + 𝑎𝑗), 𝑝(𝑣𝑖|𝐡; θ) = 𝒩 (∑ 𝑤𝑖𝑗
𝐽 𝑗=1 ℎ𝑗 + 𝑏𝑖, 1), where 𝑣𝑖 takes real values and follows a Gaussian distribution with mean ∑ 𝑤𝑖𝑗
𝐽 𝑗=1 ℎ𝑗 + 𝑏𝑖 and variance one. Gaussian-Bernoulli RBMs can be used to convert real-valued stochastic variables to binary stochastic variables, which can then be further processed using the Bernoulli-Bernoulli
RBMs.
The above discussion used two of the most common conditional distributions for the visible data in the RBM – Gaussian (for continuous-valued data) and binomial (for binary data). More generaltypes of distributions in the RBM can also be used. See (Welling et al., 2005) for the use of general exponential-family distributions for this purpose.
Taking the gradient of the log likelihood log 𝑝(𝐯; θ) we can derive the update rule for the RBM weights as:
∆𝑤𝑖𝑗 = 𝐸𝑑𝑎𝑡𝑎(𝑣𝑖ℎ𝑗) − 𝐸𝑚𝑜𝑑𝑒𝑙(𝑣𝑖ℎ𝑗), where 𝐸𝑑𝑎𝑡𝑎(𝑣𝑖ℎ𝑗) is the expectation observed in the training set (with ℎ𝑗 sampled given 𝑣𝑖 according to the model), and 𝐸𝑚𝑜𝑑𝑒𝑙(𝑣𝑖ℎ𝑗) is that same expectation under the distribution defined by the model. Unfortunately, 𝐸𝑚𝑜𝑑𝑒𝑙(𝑣𝑖ℎ𝑗) is intractable to compute. The contrastive divergence (CD) approximation to the gradient was the first efficient method proposed to approximate this expected value, where 𝐸𝑚𝑜𝑑𝑒𝑙(𝑣𝑖ℎ𝑗) is replaced by running the Gibbs sampler initialized at the data for one or more steps. The steps in approximating 𝐸𝑚𝑜𝑑𝑒𝑙(𝑣𝑖ℎ𝑗) is summarized as follows:

Initialize 𝐯𝟎 at data

Sample 𝐡𝟎 ∼ 𝒑(𝐡|𝐯𝟎)

Sample 𝐯𝟏 ∼ 𝒑(𝐯|𝐡𝟎)

Sample 𝐡𝟏 ∼ 𝒑(𝐡|𝐯𝟏)
Here, (𝐯𝟏, 𝐡𝟏) is a sample from the model, as a very rough estimate of 𝐸𝑚𝑜𝑑𝑒𝑙(𝑣𝑖ℎ𝑗). The use of(𝐯𝟏, 𝐡𝟏) to approximate 𝐸𝑚𝑜𝑑𝑒𝑙(𝑣𝑖ℎ𝑗) gives rise to the algorithm of CD-1. The sampling process can be pictorially depicted in Figure 5.1 below.
Figure 5.1. A pictorial view of sampling from a RBM during RBM learning (courtesy of Geoff
Hinton).
Note that CD-k generalizes this to more steps of the Markov chain. There are other techniques for estimating the log-likelihood gradient of RBMs, in particular the stochastic maximum likelihood or persistent contrastive divergence (PCD) (Younes 1999; Tieleman, 2008). Both work better than
CD when using the RBM as a generative model.
Careful training of RBMs is essential to the success of applying RBM and related deep learning techniques to solve practical problems. See Technical Report (Hinton 2010) for a very useful practical guide for training RBMs.
The RBM discussed above is both a generative and an unsupervised model, which characterizes the input data distribution using hidden variables and there is no label information involved.
However, when the label information is available, it can be used together with the data to form the concatenated "data" set. Then the same CD learning can be applied to optimize the approximate
"generative" objective function related to data likelihood. Further, and more interestingly, a "discriminative" objective function can be defined in terms of conditional likelihood of labels.
This discriminative RBM can be used to "fine tune" RBM for classification tasks (Larochelle and Bengio, 2008).
Ranzato et al. (2007) proposed an unsupervised learning algorithm called Sparse Encoding
Symmetric Machine (SESM), which is quite similar to RBM. They both have a symmetric encoder and decoder, and a logistic non-linearity on the top of the encoder. The main difference is that whereas the RBM is trained using (very approximate) maximum likelihood, SESM is trained by simply minimizing the average energy plus an additional code sparsity term. SESM relies on the sparsity term to prevent flat energy surfaces, while RBM relies on an explicit contrastive term in the loss, an approximation of the log partition function. Another difference is in the coding strategy in that the code units are "noisy" and binary in the RBM, while they are quasi-binary and sparse in SESM.
5.2 Unsupervised Layer-wise Pre-training
Here we describe how to stack up RBMs just described to form a DBN as the basis for DNN's pretraining. Before delving into details, we first note that this procedure, proposed by Hinton and Salakhutdinov (2006) is a more general technique of unsupervised layer-wise pretraining. That is, not only RBMs can be stacked to form deep generative (or discriminative) networks, but other types of networks can also do the same, such as autoencoder variants as proposed by Bengio et al.
Stacking a number of the RBMs learned layer by layer from bottom up gives rise to a DBN, an example of which is shown in Figure 5.2. The stacking procedure is as follows. After learning a Gaussian-Bernoulli RBM (for applications with continuous features such as speech) or BernoulliBernoulli RBM (for applications with nominal or binary features such as black-white image or coded text), we treat the activation probabilities of its hidden units as the data for training the Bernoulli-Bernoulli RBM one layer up. The activation probabilities of the second-layer BernoulliBernoulli RBM are then used as the visible data input for the third-layer Bernoulli-Bernoulli RBM, and so on. Some theoretical justification of this efficient layer-by-layer greedy learning strategy is given in (Hinton et al., 2006), where it is shown that the stacking procedure above improves a variational lower bound on the likelihood of the training data under the composite model. That is, the greedy procedure above achieves approximate maximum likelihood learning. Note that this learning procedure is unsupervised and requires no class label.
Figure 5.2. An illustration of the DBN-DNN architecture.
When applied to classification tasks, the generative pre-training can be followed by or combined with other, typically discriminative, learning procedures that fine-tune all of the weights jointly to improve the performance of the network. This discriminative fine-tuning is performed by adding a final layer of variables that represent the desired outputs or labels provided in the training data.
Then, the back-propagation algorithm can be used to adjust or fine-tune the network weights in the same way as for the standard feed-forward neural network. What goes to the top, label layer of this DNN depends on the application. For speech recognition applications, the top layer, denoted by "l1, l2,… lj,… lL," in Figure 5.2, can represent either syllables, phones, sub-phones, phone states, or other speech units used in the HMM-based speech recognition system.
The generative pre-training described above has produced better phone and speech recognition results than random initialization on a wide variety of tasks, which will be surveyed in Chapter 7.
Further research has also shown the effectiveness of other pre-training strategies. As an example, greedy layer-by-layer training may be carried out with an additional discriminative term to the generative cost function at each level. And without generative pre-training, purely discriminative training of DNNs from random initial weights using the traditional stochastic gradient decent method has been shown to work very well when the scales of the initial weights are set carefully and the mini-batch sizes, which trade off noisy gradients with convergence speed, used instochastic gradient decent are adapted prudently (e.g., with an increasing size over training epochs).
Also, randomization order in creating mini-batches needs to be judiciously determined.
Importantly, it was found effective to learn a DNN by starting with a shallow neural network with a single hidden layer. Once this has been trained discriminatively (using early stops to avoid overfitting), a second hidden layer is inserted between the first hidden layer and the labeled softmax output units and the expanded deeper network is again trained discriminatively. This can be continued until the desired number of hidden layers is reached, after which a full backpropagation "fine tuning" is applied. This discriminative "pre-training" procedure is found to work well in practice (e.g., Seide et al., 2011; Yu et al., 2011), especially with a reasonably large amount of training data. When the amount of training data is increased even more, then some carefully designed random initialization methods can work well also without using the above pretraining schemes.
In any case, pre-training based on the use of RBMs to stack up in forming the DBN has been found to work well in most cases, regardless of a large or small amount of training data. It is useful to point out that there are other ways to perform pre-training in addition to the use of RBMs and DBNs. For example, denoising autoencoders have now been shown to be consistent estimators of the data generating distribution (Bengio et al., 2013b). Like RBMs, they are also shown to be generative models from which one can sample. Unlike RBMs, however, an unbiased estimator of the gradient of the training objective function can be obtained by the denoising autoencoders, avoiding the need for MCMC or variational approximations in the inner loop of training. Therefore, the greedy layer-wise pre-training may be performed as effectively by stacking the denoising autoencoders as by stacking the RBMs each as a single-layer learner.
Further, a general framework for layer-wise pre-training can be found in many deep learning papers; e.g., Section 2 of (Bengio, 2012). This includes, as a special case, the use of RBMs as the single-layer building block as discussed in this section. The more general framework can cover the RBM/DBN as well as any other unsupervised feature extractor. It can also cover the case of unsupervised pre-training of the representation only followed by a separate stage of learning a classifier on top of the unsupervised, pre-trained features (Lee et al., 2009, 2010, 2011).
5.3 Interfacing DNNs with HMMs
The pre-trained DNN as a prominent example of the hybrid deep networks discussed so far in this chapter is a static classifier with input vectors having a fixed dimensionality. However, many practical pattern recognition and information processing problems, including speech recognition, machine translation, natural language understanding, video processing and bio-information processing, require sequence recognition. In sequence recognition, sometimes called classification with structured input/output, the dimensionality of both inputs and outputs are variable.
Figure 5.3. Interface between DBN/DNN and HMM to form a DNN-HMM. This architecture, developed at Microsoft, has been successfully used in speech recognition experiments reported in(Dahl et al., 2011, 2012). [after (Dahl et. al., 2011, 2012), @IEEE]
The HMM, based on dynamic programing operations, is a convenient tool to help port the strength of a static classifier to handle dynamic or sequential patterns. Thus, it is natural to combine feedforward neural networks and HMMs to bridge the gap between the static and sequence pattern recognition, as was done in the early days of neural networks for speech recognition (Bengio, 1991;
Bengio et al., 1992; Bourlard and Morgan, 1993). A popular architecture to fulfill this role with the use of the DNN is shown in 5.3. This architecture has been successfully used in speech recognition experiments as reported in (Dahl et al., 2011, 2012).
It is important to note that the unique elasticity of temporal dynamics of speech as elaborated in(Deng et al., 1997; Bridle et al., 1998; Deng, 1998, 2006) would require temporally-correlated models more powerful than HMMs for the ultimate success of speech recognition. Integrating such dynamic models that have realistic co-articulatory properties with the DNN and possibly other deep learning models to form the coherent dynamic deep architecture is a challenging new research direction.
CHAPTER 6
DEEP STACKING NETWORKS AND
VARIANTS --- SUPERVISED LEARNING
6.1 Introduction
While the DNN just reviewed has been shown to be extremely powerful in connection with performing recognition and classification tasks including speech recognition and image classification, training a DNN has proven to be difficult computationally. In particular, conventional techniques for training DNNs at the fine tuning phase involve the utilization of a stochastic gradient descent learning algorithm, which is difficult to parallelize across machines.
This makes learning at large scale non-trivial. For example, it has been possible to use one single, very powerful GPU machine to train DNN-based speech recognizers with dozens to a few hundreds or thousands of hours of speech training data with remarkable results. It is less clear, however, to scale up this success with many thousands or more hours of training data. See (Dean et al., 2012) for recent work in this direction.
Here we describe a new deep learning architecture, the deep stacking network (DSN), which was originally designed with the learning scalability problem in mind. This chapter is based in part on the recent publications of (Deng and Yu, 2011; Deng et al., 2012a; Hutchinson et al., 2012, 2013) with expanded discussions.
The central idea of the DSN design relates to the concept of stacking, as proposed originally in(Wolpert, 1992), where simple modules of functions or classifiers are composed first and then they are "stacked" on top of each other in order to learn complex functions or classifiers. Various ways of implementing stacking operations have been developed in the past, typically making use of supervised information in the simple modules. The new features for the stacked classifier at a higher level of the stacking architecture often come from concatenation of the classifier output of a lower module and the raw input features. In (Cohen and de Carvalho, 2005), the simple module used for stacking was a conditional random field (CRF). This type of deep architecture was further developed with hidden states added for successful natural language and speech recognition applications where segmentation information in unknown in the training data (Yu et al., 2010).
Convolutional neural networks, as in (Jarrett, et al., 2009), can also be considered as a stacking architecture but the supervision information is typically not used until in the final stacking module.
The DSN architecture was originally presented in (Deng and Yu, 2011) and was referred as deep convex network or DCN to emphasize the convex nature of a major portion of the algorithm used for learning the network. The DSN makes use of supervision information for stacking each of the basic modules, which takes the simplified form of multilayer perceptron. In the basic module, the output units are linear and the hidden units are sigmoidal nonlinear. The linearity in the outputunits permits highly efficient, parallelizable, and closed-form estimation (a result of convex optimization) for the output network weights given the hidden units' activities. Due to the closedform constraints between the input and output weights, the input weights can also be elegantly estimated in an efficient, parallelizable, batch-mode manner, which we will describe in some detail in Section 6.3.
The name "convex" used in (Deng and Yu, 2011) accentuates the role of convex optimization in learning the output network weights given the hidden units' activities in each basic module. It also points to the importance of the closed-form constraints, derived from the convexity, between the input and output weights. Such constraints make the learning of the remaining network parameters(i.e., the input network weights) much easier than otherwise, enabling batch-mode learning of the DSN that can be distributed over CPU clusters. And in more recent publications, the DSN was used when the key operation of stacking is emphasized.
6.2 A Basic Architecture of the Deep Stacking
Network
A DSN, as shown in Figure 6.1, includes a variable number of layered modules, wherein each module is a specialized neural network consisting of a single hidden layer and two trainable sets of weights. In Figure 6.1, only four such modules are illustrated, where each module is shown with a separate color. In practice, up to a few hundreds of modules have been efficiently trained and used in image and speech classification experiments.
The lowest module in the DSN comprises a linear layer with a set of linear input units, a hidden non-linear layer with a set of non-linear units, and a second linear layer with a set of linear output units. A sigmoidal nonlinearity is typically used in the hidden layer. However, other nonlinearities can also be used. If the DSN is utilized in connection with recognizing an image, the input units can correspond to a number of pixels (or extracted features) in the image, and can be assigned values based at least in part upon intensity values, RGB values, or the like corresponding to the respective pixels. If the DSN is utilized in connection with speech recognition, the set of input units may correspond to samples of speech waveform, or the extracted features from speech waveforms, such as power spectra or cepstral coefficients. The output units in the linear output layer represent the targets of classification. For instance, if the DSN is configured to perform digit recognition, then the output units may be representative of the values 0, 1, 2, 3, and so forth up to
9 with a 0-1 coding scheme. If the DSN is configured to perform speech recognition, then the output units may be representative of phones, HMM states of phones, or context-dependent HMM states of phones.
The lower-layer weight matrix, which we denote by W, connects the linear input layer and the hidden nonlinear layer. The upper-layer weight matrix, which we denote by U, connects the nonlinear hidden layer with the linear output layer. The weight matrix U can be determined through a closed-form solution given the weight matrix W when the mean square error training criterion is used.
As indicated above, the DSN includes a set of serially connected, overlapping, and layered modules, wherein each module has the same architecture – a linear input layer followed by a nonlinear hidden layer, which is connected to a linear output layer. Note that the output units of a lower module are a subset of the input units of an adjacent higher module in the DSN. More specifically, in a second module that is directly above the lowest module in the DSN, the input units can include the output units of the lowest module and optionally the raw input feature.
This pattern of including output units in a lower module as a portion of the input units in an adjacent higher module and thereafter learning a weight matrix that describes connection weights between hidden units and linear output units via convex optimization can continue for many modules. A resultant learned DSN may then be deployed in connection with an automatic classification task such as frame-level speech phone or state classification. Connecting the DSN's output to an HMM or any dynamic programming device enables continuous speech recognition and other forms of sequential pattern recognition.
Figure 6.1. A DSN architecture using input-output stacking. Four modules are illustrated, each with a distinct color. Dashed lines denote copying layers. [after (Tur et. al., 2012), @IEEE]
6.3 A Method for Learning the DSN Weights
W2
U2
W1
U1
Wrand
W3
Wrand
U3
Wrand
W4
U4
Here, we provide some technical detail on how the use of linear output units in the DSN facilitates the learning of the DSN weights. A single module is used to illustrate the advantage for simplicity reasons. First, it is clear that the upper layer weight matrix U can be efficiently learned once the activity matrix H over all training samples in the hidden layer is known. Let's denote the training vectors by 𝑿 = [𝒙1, ⋯, 𝒙𝑖, ⋯, 𝒙𝑁], in which each vector is denoted by 𝒙𝑖 = [𝑥1𝑖, ⋯, 𝑥𝑗𝑖, ⋯, 𝑥𝐷𝑖]
𝑇 where D is the dimension of the input vector, which is a function of the block, and 𝑁 is the total number of training samples. Denote by 𝐿 the number of hidden units and by 𝐶 the dimension of the output vector. Then the output of a DSN block is 𝒚𝑖 = 𝑼𝑇𝒉𝑖, where 𝒉𝑖 = 𝜎(𝑾𝑇𝒙𝑖) is the hidden-layer vector for sample i, 𝑼 is an 𝐿 × 𝐶 weight matrix at the upper layer of a block. 𝑾 is a 𝐷 × 𝐿 weight matrix at the lower layer of a block, and σ(∙) is a sigmoid function. Bias terms are implicitly represented in the above formulation if 𝒙𝑖 and 𝒉𝑖 are augmented with ones.
Given target vectors in the full training set with a total of N samples, 𝑻 = [𝒕1, ⋯, 𝒕𝑖, ⋯, 𝒕𝑁], where each vector is 𝒕𝑖 = [𝑡1𝑖, ⋯, 𝑡𝑗𝑖, ⋯, 𝑡𝐶𝑖]
𝑇, the parameters 𝑼 and 𝑾 are learned so as to minimize the average of the total square error below:
E = 1
2 ∑ ||𝒚𝑖 − 𝑖 𝒕𝑖||2 = 1
2 Tr[(𝐘 − 𝐓)(𝐘 − 𝐓)T], where the output of the network is 𝒚𝑖 = 𝑼𝑇𝒉𝑖 = 𝑼𝑇𝜎(𝑾𝑇𝒙𝑖) = 𝐺𝑖(𝑼, 𝑾)which depends on both weight matrices, as in the standard neural net. Assuming 𝑯 =
[𝒉1, ⋯, 𝒉𝑖, ⋯, 𝒉𝑁] is known, or equivalently, 𝑾 is known. Then, setting the error derivative with respective to U to zero gives
𝑼 = (𝑯𝑯𝑻)−1𝑯𝑻𝑇 = F(𝑾), where 𝒉𝑖 = 𝜎(𝑾𝑇𝒙𝑖).
This provides an explicit constraint between 𝑼 and 𝑾which were treated independently in the conventional backpropagation algorithm.
Now, given the equality constraint 𝑼 = F(𝑾), let's use Lagrangian multiplier method to solve the optimization problem in learning 𝑾. Optimizing the Lagrangian:
𝐸 =
2 ∑ ||𝐺𝑖(𝑼, 𝑾) − 𝑖 𝒕𝑖||2 + 𝜆 ||U − F(𝑾)||we can derive batch-mode gradient descent learning algorithm where the gradient takes the following form (Deng and Yu, 2011; Yu and Deng, 2012):
𝜕𝐸
𝜕𝑾 = 𝟐𝑿 [𝑯𝑇 ∘ (𝟏 − 𝑯)𝑇 ∘ [𝑯†(𝑯𝑻𝑇)(𝑻𝑯†) − 𝑻𝑇(𝑻𝑯†)]]where 𝑯† = 𝑯𝑇(𝑯𝑯𝑇)−𝟏 is pseudo-inverse of 𝑯 and symbol ∘ denotes element-wise multiplication.
Compared with conventional backpropagation, the above method has less noise in gradient computation due to the exploitation of the explicit constraint 𝑼 = F(𝑾). As such, it was found experimentally that, unlike backpropagation, batch training is effective, which aids parallel learning of the DSN.
6.4 The Tensor Deep Stacking Network
The above DSN architecture has recently been generalized to its tensorized version, which we call the tensor DSN (TDSN) (Hutchinson et al., 2012, 2013). It has the same scalability as the DSN in terms of parallelizability in learning, but it generalizes the DSN by providing higher-order feature interactions missing in the DSN.
The architecture of the TDSN is similar to that of the DSN in the way that stacking operation is carried out. That is, modules of the TDSN are stacked up in a similar way to form a deep architecture. The differences between the TDSN and the DSN lie mainly in how each module is constructed. In the DSN, we have one set of hidden units forming a hidden layer, as denoted at the left panel of Figure 6.2. In contrast, each module of a TDSN contains two independent hidden layers, denoted as "Hidden 1" and "Hidden 2" in the middle and right panels of Figure 6.2. As a result of this difference, the upper-layer weights, denoted by "U" in Figure 6.2, changes from a matrix (a two dimensional array) in the DSN to a tensor (a three dimensional array) in the TDSN, shown as a cube labeled by "U" in the middle panel.
Figure 6.2. Comparisons of a single module of a DSN (left) and that of a tensor DSN (TDSN).
Two equivalent forms of a TDSN module are shown to the right. [after (Hutchinson et. al., 2012), @IEEE]
The tensor U has a three-way connection, one to the prediction layer and the remaining to the two separate hidden layers. An equivalent form of this TDSN module is shown in the right panel of Figure 6.2, where the implicit hidden layer is formed by expanding the two separate hidden layersinto their outer product. The resulting large vector contains all possible pair-wise products for the two sets of hidden-layer vectors. This turns tensor U into a matrix again whose dimensions are 1) size of the prediction layer; and 2) product of the two hidden layers' sizes. Such equivalence enables the same convex optimization for learning U developed for the DSN to be applied to learning tensor U. Importantly, higher-order hidden feature interactions are enabled in the TDSN via the outer product construction for the large, implicit hidden layer.
Stacking the TDSN modules to form a deep architecture pursues in a similar way to the DSN by concatenating various vectors. Two examples are shown in Figure 6.3 and Figure 6.4. Note stacking by concatenating hidden layers with input (Figure 6.4) would be difficult for the DSN since its hidden layer tends to be too large for practical purposes.
Figure 6.3. Stacking of TDSN modules by concatenating prediction vector with input vector. [after(Hutchinson et. al., 2012), @IEEE]
Figure 6.4. Stacking of TDSN modules by concatenating two hidden-layers' vectors with the input vector.
6.5 The Kernelized Deep Stacking Network
The DSN architecture has also recently been generalized to its kernelized version, which we call the kernel-DSN (K-DSN) (Deng et al., 2012; Huang et al, 2013). The motivation of the extension is to increase the size of the hidden units in each DSN module, yet without increasing the size of the free parameters to learn. This goal can be easily accomplished using the kernel trick, resulting in the K-DSN which we describe below.
In the DSN architecture reviewed above optimizing the weight matrix U given the hidden layers' outputs in each module is a convex optimization problem. However, the problem of optimizing weight matrix 𝑾 and thus the whole network is non-convex. In a recent extension of DSN, a tensor structure was imposed, shifting most of the non-convex learning burden for 𝑾 to the convex optimization of U (Hutchinson et al, 2012; 2013). In the new K-DSN extension, we completely eliminate non-convex learning for 𝑾 using the kernel trick.
To derive the K-DSN architecture and the associated learning algorithm, we first take the bottom module of DSN as an example and generalize the sigmoidal hidden layer 𝒉𝑖 = 𝜎(𝑾𝑇𝒙𝑖) in the DSN module into a generic nonlinear mapping function 𝑮(𝑿) from the raw input feature 𝑿, with high dimensionality in 𝑮(𝑿) (possibly infinite) determined only implicitly by a kernel function to be chosen. Second, we formulate the constrained optimization problem ofminimize
2 Tr[𝑬𝑬T] +
𝐶
2 𝑼T𝑼subject to 𝐓-𝑼𝑇𝑮(𝑿) = E
Third, we make use of dual representations of the above constrained optimization problem to obtain U = 𝐆T𝒂, where vector 𝒂 takes the following forma = (𝐶𝑰 + 𝑲)−1𝑻and 𝑲 = 𝑮(𝑿)𝑮T(𝑿) is a symmetric kernel matrix with elements Knm = gT(xn)𝑔(xm).
Finally, for each new input vector x in the test or dev set, we obtain the K-DSN (bottom) module's prediction asy(x) = UT𝒈(𝒙) = 𝐚T𝑮(𝑿) 𝒈(𝒙)= 𝒌T(𝒙)(𝐶 𝑰 + 𝑲)−1𝑻where the kernel vector 𝒌(𝒙) is so defined that its elements have values of 𝑘𝑛(𝒙) = 𝑘(𝒙𝑛, 𝒙) in which 𝒙𝑛 is a training sample and 𝒙 is the current test sample.
For l-th module in K-DCN where 𝑙 ≥ 2, the kernel matrix is modified to
𝑲 = 𝑮 ([𝑿| 𝒀(𝑙−1)| 𝒀(𝑙−2)|.. 𝒀(1)]) 𝑮T ([𝑿| 𝒀(𝑙−1)| 𝒀(𝑙−2)|.. 𝒀(1)]).
The key advantages of K-DSN can be analyzed as follows. First, unlike DSN which needs to compute hidden units' output, the K-DSN does not need to explicitly compute hidden units' output
𝑮(𝑿) or 𝑮([𝑿| 𝒀(𝑙−1)| 𝒀(𝑙−2)|.. 𝒀(1)]). When Gaussian kernels are used, kernel trick equivalently gives us an infinite number of hidden units without the need to compute them explicitly. Further, we no longer need to learn the lower-layer weight matrix 𝑾 in DSN as described in (Deng et al, 2012) and the kernel parameter (e.g., the single variance parameter 𝜎 in the Gaussian kernel) makes K-DSN much less subject to overfitting than DSN. Figure 6.5 illustrates the basic architecture of a K-DSN using the Gaussian kernel and using three modules.
Figure 6.5. An example architecture of the K-DSN with three modules each of which uses a Gaussian kernel with different kernel parameters. [after (Deng. al., 2012), @IEEE]
The entire K-DSN with Gaussian kernels is characterized by two sets of module-dependent hyperparameters: 𝜎(𝑙) and 𝐶(𝑙), the kernel smoothing parameter and regularization parameter, respectively. While both parameters are intuitive and their tuning (via line search or leave-one-out cross validation) is straightforward for a single bottom module, tuning the full network with all the modules is more difficult. For example, if the bottom module is tuned too well, then adding more modules would not benefit much. In contrast, when the lower modules are loosely tuned (i.e., relaxed from the results obtained from straightforward methods), the overall K-DSN often performs much better. The experimental results reported by Deng et al. (2012) are obtained using a set of empirically determined tuning schedules to adaptively regularize the K-DSN from bottom to top modules.
The K-DSN described here has a set of highly desirable properties from the machine learning and pattern recognition perspectives. It combines the power of deep learning and kernel learning in a principled way and unlike the basic DSN there is no longer non-convex optimization problem( ) = ( ( )) T( ( ))( ) = ( ) T( );(2) = ( (2)) T( (2))involved in training the K-DSN. The computation steps make the K-DSN easier to scale up for parallel computing in distributed servers than the DSN and tensor-DSN. There are many fewer parameters in the K-DSN to tune than in the DSN, T-DSN, and DNN, and there is no need for pretraining. It is found in the study of (Deng et al., 2012) that regularization plays a much more important role in the K-DSN than in the basic DSN and Tensor-DSN. Further, effective regularization schedules developed for learning the K-DSN weights can be motivated by intuitive insight from useful optimization tricks such as the heuristic in Rprop or resilient backpropagation algorithm (Riedmiller and Braun, 1993).
However, as inherent in any kernel method, the scalability becomes an issue also for the K-DSN as the training and testing samples become very large. A solution is provided in the study by Huang et al. (2013), based on the use of random Fourier features, which possess the strong theoretical property of approximating the Gaussian kernel while rendering efficient computation in both training and evaluation of the K-DSN with large training samples. It is empirically demonstrated that just like the conventional K-DSN exploiting rigorous Gaussian kernels, the use of random
Fourier features also enables successful stacking of kernel modules to form a deep architecture.
CHAPTER 7
SELECTED APPLICATIONS IN SPEECH
AND AUDIO PROCESSING
7.1 Acoustic Modeling for Speech Recognition
As discussed in Chapter 2, speech recognition is the very first successful application of deep learning methods at an industry scale. This success is a result of close academic-industrial collaboration, initiated at Microsoft Research, with the involved researchers identifying and acutely attending to the industrial need for large-scale deployment (Deng et al., 2009; Yu et al., 2010c; Seide et al., 2011; Hinton et al, 2012; Dahl et al., 2012; Deng et al., 2013b). It is also a result of carefully exploiting the strengths of the deep learning and the then-state-of-the-art speech recognition technology, including notably the highly efficient decoding techniques.
Speech recognition has long been dominated by the GMM-HMM method, with an underlying shallow or flat generative model of context-dependent GMMs and HMMs (e.g., Rabiner, 1989;
Juang et al., 1986; Deng et al., 1990, 1991). Neural networks once were a popular approach but had not been competitive with the GMM-HMM (Waibel et al., 1989; Bourlard and Morgan, 1993;
Deng et al., 1994; Morgan, 2012). Generative models with deep hidden dynamics likewise have also not been clearly competitive (e.g., Picone et al., 1999; Deng, 1998; Bridle et al. 1998; Deng et al., 2006).
Deep learning and the DNN started making their impact in speech recognition in 2010, after close collaborations between academic and industrial researchers; see reviews in (Hinton et al., 2012;
Deng et al., 2013c). The collaborative work started in phone recognition tasks (Mohamed et al., 2009, 2010, 2012; Deng et al., 2010, 2013; Sivaram and Hermansky, 2012; Graves et al., 2013, 2013a; Sainath et al., 2011, 2013), demonstrating the power of hybrid DNN architectures discussed in Chapter 5 and of subsequent new architectures with convolutional and recurrent structure. The work also showed the importance of raw speech features of spectrogram --- back from the longpopular MFCC features toward but not yet reaching the raw speech-waveform level (e.g., Sheikhzadeh and Deng, 1994; Jaitly and Hinton, 2011). The collaboration continued to large vocabulary tasks with more convincing, highly positive results (Yu et al., 2010c; Dahl et al., 2011, 2012; Seide et al., 2011; Kubo et al., 2012; Hinton et al., 2012; Kingsbury et al., 2012; Deng et al., 2013a, 2013b; Su et al., 2013; Yan et al., 2013; Liao et al., 2013). The success in large vocabulary speech recognition is in large part attributed to the use of a very large DNN output layer structured in the same way as the GMM-HMM speech units (senones), motivated initially by the speech researchers' desires to take advantage of the context-dependent phone modeling techniques that have been proven to work well in the GMM-HMM framework, and to keep the change of the already highly efficient decoder software's infrastructure developed for the GMM-HMM systems to a minimum. In the meantime, this body of work also demonstrated the possibility to reduce the need for the DBN-like pre-training in effective learning of DNNs when a large amount of labeleddata is available. A combination of three factors helped to quickly spread the success of deep learning in speech recognition to the entire speech industry and academia: 1) minimal decoder changes required to deploy the new DNN-based speech recognizer due to the use of senones as the DNN output; 2) significantly lowered errors compared with the then-state-of-the-art GMM-HMM systems; and 3) reduced system complexity empowered by the DNN's strong modeling power. By the ICASSP-2013 timeframe, at least 15 major speech recognition groups worldwide confirmed experimentally the success of DNNs with very large tasks and with the use of raw speech spectral features other than MFCCs. The most notable groups include major industrial speech labs worldwide: Microsoft (Seide et al, 2011; Chen et al., 2012; Deng et al., 2013b, 2013c; Yan et al.
2013; Yu et al., 2013b), IBM (Sainath et al., 2011, 2013, 2013b; Kingsbury et al., 2012; Saon et al., 2013), Google (Jaitly et al., 2012; Dean et al., 2012; Heigold et al., 2013; Liao et al., 2013), iFlyTek, and Baidu. Their results represent a new state-of-the-art in speech recognition widely deployed in these companies' voice products and services with extensive media coverage in recent years.
In the remainder of this chapter, we review a wide range of speech recognition work based on deep learning methods according to several major themes expressed in the section titles.
7.1.1 Back to primitive spectral features of speech
Deep learning, also referred as representation learning or (unsupervised) feature learning, sets an important goal of automatic discovery of powerful features from raw input data independent of application domains. For speech feature learning and for speech recognition, this goal is condensed to the use of primitive spectral or possibly waveform features. Over the past 30 years or so, largely
"hand-crafted" transformations of speech spectrogram have led to significant accuracy improvements in the GMM-based HMM systems, despite the known loss of information from the raw speech data. The most successful transformation is the non-adaptive cosine transform, which gave rise to Mel-frequency cepstral coefficients (MFCC) features. The cosine transform approximately de-correlates feature components, which is important for the use of GMMs with diagonal covariance matrices. However, when GMMs are replaced by deep learning models such as DNNs, deep belief nets (DBNs), or deep autoencoders, such de-correlation becomes irrelevant due to the very strength of the deep learning methods in modeling data correlation. As discussed in detail in Chapter 4, early work of (Deng et al., 2010) demonstrated this strength and in particular the benefit of spectrograms over MFCCs in effective coding of bottleneck speech features using autoencoders in an unsupervised manner.
The pipeline from speech waveforms (raw speech features) to MFCCs and their temporal differences goes through intermediate stages of log-spectra and then (Mel-warped) filter-banks, with learned parameters based on the data. An important character of deep learning is to move away from separate design of feature representations and of classifiers. This idea of jointly learning classifier and feature transformation for speech recognition was already explored in early studies on the GMM-HMM based systems; e.g., (Chengalvarayan and Deng, 1997; 1997a; Rathinavalu and Deng, 1997). However, greater speech recognition performance gain is obtained only recently in the recognizers empowered by deep learning methods. For example, Li et al., (2012) and Deng et al., (2013a) showed significantly lowered speech recognition errors using large-scale DNNswhen moving from the MFCC features back to more primitive (Mel-scaled) filter-bank features.
These results indicate that DNNs can learn a better transformation than the original fixed cosine transform from the Mel-scaled filter-bank features.
Compared with MFCCs, "raw" spectral features not only retain more information, but also enable the use of convolution and pooling operations to represent and handle some typical speech invariance and variability --- e.g., vocal tract length differences across speakers, distinct speaking styles causing formant undershoot or overshoot, etc. --- expressed explicitly in the frequency domain. For example, the convolutional neural network (CNN) can only be meaningfully and effectively applied to speech recognition (Abdel-Hamid et al., 2012; 2013, 2013a; Deng et al., 2013) when spectral features, instead of MFCC features, are used.
More recently, Sainath et al. (2013b) went one step further toward raw features by learning the parameters that define the filter-banks on power spectra. That is, rather than using Mel-warped filter-bank features as the input features as in (Abdel-Hamid et al., 2012; 2013; Li et al., 2012;
Chengalvarayan and Deng, 1997), the weights corresponding to the Mel-scale filters are only used to initialize the parameters, which are subsequently learned together with the rest of the deep network as the classifier. The overall architecture of the jointly learned feature generator and classifier is shown in Figure 7.1. Substantial speech recognition error reduction is reported in(Sainath et al., 2013b).
Figure 7.1. Illustration of the joint learning of filter parameters and the rest of the deep network.
Adopted from [after (Sainath et al., 2013b), @IEEE].
It has been shown that not only learning the spectral aspect of the features are beneficial for speech recognition, learning the temporal aspect of the features is also helpful (Siniscalchi et al. 2013).
Further, Yu et al. (2013a) carefully analyzed the properties of different layers in the DNN as the layer-wise extracted features starting from the lower raw filter-bank features. They found that the improved speech recognition accuracy achieved by the DNNs partially attributes to DNN's ability to extract discriminative internal representations that are robust to the many sources of variability in speech signals. They also show that these representations become increasingly insensitive to small perturbations in the input at higher layers, which helps to achieve better speech recognition accuracy.
To the extreme end, deep learning would promote to use the lowest level of raw features of speech, i.e., speech sound waveforms, for speech recognition, and learn the transformation automatically.
As an initial attempt toward this goal the study carried out by Jaitly and Hinton (2011) makes use of speech sound waves as the raw input feature to an RBM with a convolutional structure as the classifier. With the use of rectified linear units in the hidden layer (Glorot et al., 2011), it is possible, to a limited extent, to automatically normalize the amplitude variation in the waveform signal.
Although the final results are disappointing, the work shows that much work is needed along this direction. For example, just as demonstrated by Sainath et al. (2013b) that the use of raw spectra as features requires additional attention in normalization than MFCCs, the use of speech waveforms demands even more attention (e.g., Sheikhzadeh and Deng, 1994). This is true for both
GMM-based and deep learning based methods.
7.1.2 The DNN-HMM architecture vs. use of DNN-derived features
Another major theme in the recent studies reported in the literature on applying deep learning methods to speech recognition is two disparate ways of using the DNN: 1) Direct applications of the DNN-HMM architecture as discussed in Chapter 5.3 to perform speech recognition; and 2)
The use of DNNs to extract or derive features, which are then fed into a separate sequence classifier.
In the speech recognition literature (e.g., Bourlard and Morgan, 1993), a system, in which a neural network's output is directly used to estimate the emission probabilities of an HMM, is often called an ANN/HMM hybrid system. This should be distinguished from the use of "hybrid" in Chapter
5 and throughout this book, where a hybrid of unsupervised pre-training and of supervised fine tuning is exploited to learn the parameters of DNNs.
The DNN-HMM architecture as a recognizer
An early DNN-HMM architecture (Mohamed et al., 2009) was presented at the NIPS Workshop(Deng, Yu, Hinton, 2009), developed, analyzed, and assisted by University of Toronto and MSR speech researchers. In this work, a five-layer DNN (called the DBN in the paper) was used to replace the Gaussian mixture models in the GMM-HMM system, and the monophone state was used as the modeling unit. Although monophones are generally accepted as a weaker phonetic representation than triphones, the DNN-HMM approach with monophones was shown to achieve higher phone recognition accuracy than the state-of-the-art triphone GMM-HMM systems. Further, the DNN results were found to be slightly superior to the then-best-performing single system based on the generative hidden trajectory model (HTM) in the literature (Deng et al., 2006, 2007) evaluated on the same, commonly used TIMIT task by many speech researchers (e.g., Ostendorf et al., 1996; Deng et al., 2006; Sainath et al., 2011a). At MSR, Redmond, the error patterns produced by these two separate systems (the DNN vs. the HTM) were carefully analyzed and found to be very different, reflecting distinct core capabilities of the two approaches and igniting intensive further studies on the DNN-HMM approach described below.
MSR and University of Toronto researchers (Yu et al., 2010c; Dahl et al., 2011, 2012) extended the DNN-HMM system from the monophone phonetic representation of the DNN outputs to the triphone or context-dependent counterpart and from phone recognition to large vocabulary speech recognition. Experiments conducted at MSR on the 24-hr and 48-hr Bing mobile voice search datasets collected under the real usage scenario demonstrate that the context-dependent DNNHMM significantly outperforms the state-of-the-art HMM system. Three factors, in addition to the use of the DNN, contribute to the success: the use of triphones as the DNN modeling units, the use of the best available tri-phone GMM-HMM to generate the tri-phone state alignment, and the effective exploitation of a long window of input features. Experiments also indicate that the decoding time of a five-layer DNN-HMM is almost the same as that of the state-of-the-art triphone
GMM-HMM.
The success was quickly extended to large vocabulary speech recognition tasks with hundreds and even thousands of hours of training set and with thousands of tri-phone states, including the Switchboard and Broadcast News databases, and Google's voice search and YouTube tasks (Seide et al., 2011; Sainath et al., 2011; Jaitly et al., 2012; Hinton et al., 2012; Deng et al., 2013a; Sainath et al, 2013). For example, on the Switchboard benchmark, the context-dependent DNN-HMM(CD-DNN-HMM) is shown to cut error by one third compared to the state-of-the-art GMM-HMM system (Seide et al., 2011). As a summary, we show in Table 7.1 some quantitative recognition error rates produced by the DNN-HMM architecture in comparison with those by the previous state of the art systems based on the generative models. Note from sub-tables A to D, the training data are increased approximately one order of magnitude from one task to the next. Not only the computation scales up well (i.e., almost linearly) with the training size, but most importantly the relative error rate reduction increases substantially with increasing amounts of training data --from approximately 10% to 20%, and then to 30%. This set of results highlight the strongly desirable properties of the DNN-based methods, despite the conceptual simplicity of the overall
DNN-HMM architecture and some known weaknesses.
Table 7.1. Comparisons of the DNN-HMM architecture with the generative model (e.g., the GMMHMM) in terms of phone or word recognition error rates. From sub-tables A to D, the training data are increased approximately three orders of magnitudes.
The use of DNN-derived features in a separate recognizer
One clear weakness of the above DNN-HMM architecture for speech recognition is that much of the highly effective techniques for the GMM-HMM systems, including discriminative training (in both feature space and model space), unsupervised speaker adaptation, noise robustness, and scalable batch training tools for big training data, developed over the past 20 some years may not be directly applicable to the new systems although similar techniques have been recently developed for DNN-HMMs. To remedy this problem, the "tandem" approach, developed originally by Hermansky et al. (2000), has been adopted, where the output of the neural networks in the form of posterior probabilities of the phone classes, are used, often in conjunction with the acoustic features to form new augmented input features, in a separate GMM-HMM system.
This tandem approach is used by Vinyals and Ravuri (2011) where a DNN's outputs are extracted to serve as the features for mismatched noisy speech. It is reported that DNNs outperform the neural networks with a single hidden layer under the clean condition, but the gains slowly diminish as the noise level is increased. Furthermore, using MFCCs in conjunction with the posteriors computed from DNNs outperforms using the DNN features alone in low to moderate noise conditions with the tandem architecture. Comparisons of such tandem approach with the direct
DNN-HMM approach are made by Tüske et al. (2012) and Imseng et al. (2013).
An alternative way of extracting the DNN features is to use the "bottleneck" layer, which is narrower than other layers in the DNN, to restrict the capacity of the network. Then, such bottleneck features are fed to a GMM-HMM system, often in conjunction with the original acoustic features and some dimensionality reduction techniques. The bottleneck features derived from the DNN are believed to capture information complementary to conventional acoustic features derived from the short-time spectra of the input. A speech recognizer based on the above bottleneck feature approach is built by Yu and Seltzer (2011), with the overall architecture shown in Figure 7.2.
Several variants of the DNN-based bottleneck-feature approach have been explored; see details in(Bell, et al., 2013; Lal, et al., 2013; Sainath et al., 2012; Tüske et al., 2012; Plahl et al., 2010).
Figure 7.2. Illustration of the use of bottleneck (BN) features extracted from a DNN in a GMMHMM speech recognizer. [after (Yu and Seltzer, 2011), @IEEE].
Yet another method to derive the features from the DNN is to feed its top-most hidden layer as the new features for a separate speech recognizer. In (Yan et al., 2013), a GMM-HMM is used as such a recognizer, and the high-dimensional, DNN-derived features are subject to dimensionality reduction before feeding them into the recognizer. More recently, a recurrent neural network (RNN) is used as the "backend" recognizer receiving the high-dimensional, DNN-derived features as the input without dimensionality reduction (Chen and Deng, 2013; Deng and Chen, 2014). These studies also show that the use of the top-most hidden layer of the DNN as features is better than other hidden layers and also better than the output layer in terms of recognition accuracy for the RNN sequence classifier.
7.1.3 Noise robustness by deep learning
The study of noise robustness in speech recognition has a long history, mostly before the recent rise of deep learning. One major contributing factor to the often observed brittleness of speech recognition technology is the inability of the standard GMM-HMM-based acoustic model to accurately model noise-distorted speech test data that differs in character from the training data, which may or may not be distorted by noise. A wide range of noise-robust techniques developed over past 30 years can be analyzed and categorized using five different criteria: 1) feature-domain vs. model-domain processing, 2) the use of prior knowledge about the acoustic environmentdistortion, 3) the use of explicit environment-distortion models, 4) deterministic vs. uncertainty processing, and 5) the use of acoustic models trained jointly with the same feature enhancement or model adaptation process used in the testing stage. See a comprehensive review in (Li et al., 2014) and some additional review literature or original work in (Gales, 2011; Lu et al., 2013;
Yoshioka and Nakatani, 2013; Wang and Gales, 2012; Zhao and Juang, 2012; Hain et al., 2012; van Dalen, et al., 2011; Yu et al., 2009; Acero et al., 2000; Deng et al., 2000).
Many of the model-domain techniques developed for GMM-HMMs (e.g., model-domain noise robustness techniques surveyed by Li et al. (2014) and Gales (2011)), are not directly applicable to the new deep learning models for speech recognition. The feature-domain techniques, however, can be directly applied to the DNN system. A detailed investigation of the use of DNNs for noise robust speech recognition in the feature domain is reported by Seltzer et al. (2013), who apply the C-MMSE (Yu et al., 2008) feature enhancement algorithm on the input feature used in the DNN.
By processing both the training and testing data with the same algorithm, any consistent errors or artifacts introduced by the enhancement algorithm can be learned by the DNN-HMM recognizer.
This study also successfully explores the use of the noise aware training paradigm for training the DNN, where each observation is augmented with an estimate of the noise. Strong results are obtained on the Aurora4 task. More recently, Kashiwagi et al. (2013) applies the SPLICE feature enhancement technique (Deng et al., 2000, 2001) to a DNN speech recognizer. In that study the DNN's output layer is determined on the clean data instead of the noisy data as in the study by
Seltzer et al. (2013).
Besides DNN, other deep architectures have also been proposed to perform feature enhancement and noise-robust speech recognition. For example, Mass et al. (2012) applied a deep recurrent auto encoder neural network to remove noise in the input features for robust speech recognition. The model is trained on stereo (noisy and clean) speech features to predict clean features given noisy input, similar to the SPLICE setup but using a deep model instead of a GMM. Vinyals and Ravuri(2011) investigated the tandem approaches to noise-robust speech recognition, where DNNs are trained directly with noisy speech to generate posterior features.
7.1.4 Output representations in the DNN
Most deep learning methods for speech recognition and other information processing applications have focused on learning representations from input acoustic features without paying attention to output representations. The recent 2013 NIPS Workshop on Learning Output Representations(http://nips.cc/Conferences/2013/Program/event.php?ID=3714) was dedicated to bridging this gap.
For example, the Deep Visual-Semantic Embedding Model described in (Frome et al., 2013, to be discussed more in Chapter 11) exploits continuous-valued output representations obtained from the text embeddings to assist in the branch of the deep network for classifying images. For speech recognition, importance of designing effective linguistic representations for the output layers of deep networks is highlighted in (Deng, 2013).
Most current DNN systems use a high-dimensional output representation to match the context-dependent phonetic states in the HMMs. For this reason, the output layer evaluation can cost 1/3 of the total computation time. To improve the decoding speed, techniques such as low-rank approximation is typically applied to the output layer. In (Sainath et al., 2013c)and (Xue et al., 2013), the DNN with high-dimensional output layer was trained first. The singular value decomposition (SVD)-based dimension reduction technique was then performed on the large output-layer matrix. The resulting matrices are further combined and as the result the original large weight matrix is approximated by a product of two much smaller matrices. This technique in essence converts the original large output layer to two layers – a bottleneck linear layer and a nonlinear output layer --- both with smaller weight matrices. The converted DNN with reduced dimensionality in is further refined. The experimental results show that no speech recognition accuracy reduction was observed even when the size is cut to 1/3, while the run-time computation is significantly reduced.
The output representations for speech recognition can benefit from the structured design of the symbolic or phonological units of speech as presented in (Deng, 2013). The rich phonological structure of symbolic nature in human speech has been well known for many years. Likewise, it has also been well understood for a long time that the use of phonetic or its finer state sequences, even with contextual dependency, in engineering speech recognition systems, is inadequate in representing such rich structure (e.g., Deng and Erler, 1992; Ostendorf, 1999; Sun and Deng, 2002), and thus leaving a promising open direction to improve the speech recognition systems' performance. Basic theories about the internal structure of speech sounds and their relevance to speech recognition technology in terms of the specification, design, and learning of possible output representations of the underlying speech model for speech target sequences are surveyed in (Deng and O'Shaughnessy, 2003) and more recently in (Deng, 2013).
There has been a growing body of deep learning work in speech recognition with their focus placed on designing output representations related to linguistic structure. In (Wang and Sim, 2013; 2014), a limitation of the output representation design, based on the context-dependent phone units as proposed by Dahl et al. (2012), is recognized and a solution is offered. The root cause of this limitation is that all context-dependent phone states within a cluster created by the decision tree share the same set of parameters and this reduces its resolution power for fine-grained states during the decoding phase. The solution proposed formulates output representations of the contextdependent DNN as an instance of the canonical state modeling technique, making use of broad phonetic classes. First, triphones are clustered into multiple sets of shorter bi-phones using broad phone contexts. Then, the DNN is trained to discriminate the bi-phones within each set. Logistic regression is used to transform the canonical states into the detailed triphone state output probabilities. That is, the overall design of the output representation of the context-dependent DNN is hierarchical in nature, solving both the data sparseness and low-resolution problems at the same time.
Related work on designing the output linguistic representations for speech recognition can be found in (Ko and Mak, 2013) and in (McGraw et al., 2013). While the designs are in the context of GMM-HMM-based speech recognition systems, they both can be extended to deep learning models.
7.1.5 Adaptation of the DNN-based speech recognizers
The DNN-HMM is an advanced version of the artificial neural network and HMM hybrid system developed in 1990s, for which several adaptation techniques have been developed. Most of these techniques are based on linear transformation of the network weights of either input or output layers. Some initial work on DNN adaptation makes use of the same or related linear transformation methods (e.g., Yao et al., 2012; 2013a). However, compared with the earlier narrower and shallower neural network systems, the DNN-HMM has significantly more parameters due to wider and deeper hidden layers used and the much larger output layer designed to model context dependent phones and states. This difference casts additional challenges to adapting the DNN-HMM, especially when the adaptation data is small. Here we discuss three recent studies on overcoming such challenges in adapting the large-sized DNN weights in three distinct ways.
Yu et al. (2013b) proposed a regularized adaptation technique for DNNs. It adapts the DNN weights conservatively by forcing the distribution estimated from the adapted model to be close to that estimated from those before the adaptation. This constraint is realized by adding Kullback–
Leibler divergence (KLD) regularization to the adaptation criterion. This type of regularization is shown to be equivalent to a modification of the target distribution in the conventional backpropagation algorithm and thus the training of the DNN remains largely unchanged. The new target distribution is derived to be a linear interpolation of the distribution estimated from the model before adaptation and the ground truth alignment of the adaptation data. This interpolation prevents overtraining by keeping the adapted model from straying too far from the speakerindependent model. This type of adaptation differs from L2 regularization which constrains the model parameters themselves rather than the output probabilities.
In (Siniscalchi et al., 2013a), adaptation of the DNN is applied not on the conventional network weights by on the hidden activation functions. In this way, the main limitation of current adaptation techniques based on adaptable linear transformation of the network weights in either the input or the output layer is effectively overcome, since the new method only needs to adapt limited hidden activation function.
Most recently, Saon et al. (2013) explore a new and highly effective method in adapting DNNs for speech recognition. The method combines I-vector features with fMLLR (feature-domain MaxLikelihood Linear Regression) features as the input into a DNN. I-vectors or (speaker) identity vectors are commonly used for speaker verification and speaker recognition applications, as they encapsulate relevant information about a speaker's identity in a low-dimensional feature vector.
The fMLLR is an effective adaptation technique developed for GMM-HMM systems. Since Ivectors do not obey locality in frequency, they must be combined carefully with the fMLLR features that obey locality. The architecture of multi-scale CNN-DNN is shown to be effective for the combination of these two different types of features. During both training and decoding, the speaker-specific I-vector is appended to the frame-based fMLLR features.
7.1.6 Better architectures and nonlinear units
Over recent years, since the success of the (fully-connected) DNN-HMM hybrid system was demonstrated in (Mohamed et al., 2009, 2012; Deng et al., 2009; Yu et al., 2010; Dahl et al., 2011, 2012; Seide et al., 2011; Sainath et al., 2011, 2012; Hinton et al., 2012), many new architectures and nonlinear units have been proposed and evaluated for speech recognition. Here we provide an overview of this progress, extending the overview provided in (Deng et al., 2013b).
The tensor version of the DNN is reported by Yu et al. (2012c, 2013), which extends the conventional DNN by replacing one or more of its layers with a double-projection layer and a tensor layer. In the double-projection layer, each input vector is projected into two nonlinear subspaces. In the tensor layer, two subspace projections interact with each other and jointly predict the next layer in the overall deep architecture. An approach is developed to map the tensor layers to the conventional sigmoid layers so that the former can be treated and trained in a similar way to the latter. With this mapping the tensor version of the DNN can be treated as the DNN augmented with double-projection layers so that the backpropagation learning algorithm can be cleanly derived and relatively easily implemented.
A related architecture to the above is the tensor version of the DSN described in Chapter 6, also usefully applied to speech classification and recognition (Hutchinson et al., 2012, 2013). The same approach applies to mapping the tensor layers (i.e., the upper layer in each of the many modules in the DSN context) to the conventional sigmoid layers. Again, this mapping simplifies the training algorithm so that it becomes not so far apart from that for the DSN.
As discussed in Chapter 3.2, the concept of convolution in time was originated in the TDNN (timedelay neural network) as a shallow neural network (Lang et al., 1990; Waibel et al., 1989) developed during early days of speech recognition. Only recently and when deep architectures (e.g. deep Convolutional Neural Network or deep CNN) were used, it has been found that frequencydimension weight sharing is more effective for high-performance phone recognition, when the HMM is used to handle the time variability, than time-domain weight sharing as in the previous
TDNN in which the HMM was not used (Abdel-Hamid et al., 2012, 2013, 2013a; Deng et al., 2013). These studies also show that designing the pooling in the deep CNN to properly trade-off between invariance to vocal tract length and discrimination between speech sounds, together with a regularization technique of "dropout" (Hinton et al., 2012a), leads to even better phone recognition performance. This set of work further points to the direction of trading-off between trajectory discrimination and invariance expressed in the whole dynamic pattern of speech defined in mixed time and frequency domains using convolution and pooling. Moreover, the most recent studies reported in (Sainath et al., 2013, 2013a; 2013e) show that CNNs also benefit large vocabulary continuous speech recognition. They further demonstrate that multiple convolutional layers provide even more improvement when the convolutional layers use a large number of convolution kernels or feature maps. In particular, Sainath et al. (2013e) extensively explored many variants of the deep CNN. In combination with several novel methods the deep CNN is shown to produce state of the art results in a few large vocabulary speech recognition tasks.
In addition to the DNN, CNN, and DSN, as well as their tensor versions, other deep models have also been developed and reported in the literature for speech recognition. For example, the deepstructured CRF, which stacks many layers of CRFs, have been usefully applied to the task of language identification (Yu et al., 2010), phone recognition (Yu and Deng, 2010), sequential labeling in natural language processing (Yu et al., 2010a), and confidence calibration in speech recognition (Yu et al., 2010b). More recently, Demuynck and Triefenbach (2013) developed the deep GMM architecture, where the aspects of DNNs that lead to strong performance are extracted and applied to build hierarchical GMMs. They show that by going "deep and wide" and feeding windowed probabilities of a lower layer of GMMs to a higher layer of GMMs, the performance of the deep-GMM system can be made comparable to a DNN. One advantage of staying in the GMM space is that the decades of work in GMM adaptation and discriminative learning remains applicable.
Perhaps the most notable deep architecture among all is the recurrent neural network (RNN) as well as its stacked or deep version (Graves et al., 2013, 2013a; Hermans and Schrauwen, 2013).
While the RNN saw its early success in phone recognition (Robinson, 1994), it was not easy to duplicate due to the intricacy in training, let alone to scale up for larger speech recognition tasks.
Learning algorithms for the RNN have been dramatically improved since then, and much better results have been obtained recently using the RNN (Graves, et al, 2006; Maas et al., 2012; Chen and Deng, 2013), especially when the bi-directional LSTM (long short-term memory) is used(Graves et al., 2013, 2013a). The basic information flow in the bi-directional RNN and a cell of LSTM is shown in Figures 7.3 and 7.4 respectively.
Figure 7.3. Information flow in the bi-directional RNN, with both diagrammatic and mathematical descriptions. W's are weight matrices, not shown but can be easily inferred in the diagram. [after(Graves et al., 2013), @IEEE].
Figure 7.4. Information flow in an LSTM unit of the RNN, with both diagrammatic and mathematical descriptions. W's are weight matrices, not shown but can easily be inferred in the diagram. [after (Graves et al., 2013), @IEEE].
Learning the RNN parameters is known to be difficult due to vanishing or exploding gradients(Pascanu et al., 2013). Chen and Deng (2013) and Deng and Chen (2014) developed a primal-dual training method that formulates the learning of the RNN as a formal optimization problem, where cross entropy is maximized subject to the condition that the infinity norm of the recurrent matrix of the RNN is less than a fixed value to guarantee the stability of RNN dynamics. Experimental results on phone recognition demonstrate: 1) the primal-dual technique is highly effective in learning RNNs, with superior performance to the earlier heuristic method of truncating the size of the gradient; 2) The use of a DNN to compute high-level features of speech data to feed into the RNN gives much higher accuracy than without using the DNN; and 3) The accuracy drops progressively as the DNN features are extracted from higher to lower hidden layers of the DNN.
A special case of the RNN is reservoir models or echo state networks, where the output layers are fixed to be linear instead of nonlinear as in the regular RNN, and where the recurrent matrices are carefully designed but not learned. The input matrices are also fixed and not learned, due partly to the difficulty of learning. Only the weight matrices between the hidden and output layers are learned. Since the output layer is linear, the learning is very efficient and with global optimumachievable by a closed-form solution. But due to the fact that many parameters are not learned, the hidden layer needs to be very large in order to obtain good results. Triefenbach et al. (2013) applied such models to phone recognition, with reasonably good accuracy obtained.
Palangi et al. (2013a) presented an improved version of the reservoir model by learning both the input and recurrent matrices which were fixed in the previous model that makes use of the linear output (or readout) units to simplify the learning of only the output matrix in the RNN. Rather, a special technique is devised that takes advantage of the linearity in the output units in the reservoir model to learn the input and recurrent matrices. Compared with the backpropagation through time(BPTT) algorithm commonly used in learning the general RNNs, the proposed technique makes use of the linearity in the output units to provide constraints among various matrices in the RNN, enabling the computation of the gradients as the learning signal in an analytical form instead of by recursion as in the BPTT.
In addition to the recent innovations in better architectures of deep learning models for speech recognition reviewed above, there is also a growing body of work on developing and implementing better nonlinear units. Although sigmoidal and tanh functions are the most commonly used nonlinear types in DNNs their limitations are well known. For example, it is slow to learn the whole network due to weak gradients when the units are close to saturation in both directions.
Jaitly and Hinton (2011) appear to be the first to apply the rectified linear units (ReLU) in the DNNs to speech recognition to overcome the weakness of the sigmoidal units. ReLU refers to the units in a neural network that use the activation function of𝑓(𝑥) = max(0, 𝑥). Dahl et al. (2013) and Mass et al. (2013) successfully applied ReLU to large vocabulary speech recognition, with the best accuracy obtained when combining ReLU with the "Dropout" regularization technique.
Another new type of DNN units demonstrated more recently to be useful for speech recognition is the "maxout" units, which were used for forming the deep maxout network as described in (Miao et al., 2013). A deep maxout network consists of multiple layers which generate hidden activations via the maximum or "maxout" operation over a fixed number of weighted inputs called a "group".
This is the same operation as the max pooling used in the CNN as discussed earlier for both speech recognition and computer vision. The maximal value within each group is taken as the output from the previous layer. Most recently, Zheng et al. (2014) generalize the above "maxout" units to two new types. The "soft-maxout" type of units replace the original max operation with the soft-max function. The second, p-norm type of units used the nonlinearity of y = ||x||p. It is shown experimentally that the p-norm units with p=2 perform consistently better than the maxout, tanh, and ReLU units.
Finally, Srivastava et al. (2013) propose yet another new type of nonlinear units, called winnertake-all units. Here, local competition among neighboring neurons are incorporated into the otherwise regular feed-forward architecture, which is then trained via backpropagation with different gradients than the normal one. Winner-take-all is an interesting new form of nonlinearity, and it forms groups of (typically two) neurons where all the neurons in a group are made zerovalued except the one with the largest value. Experiments show that the network does not forget as much as networks with standard sigmoidal nonlinearity. This new type of nonlinear units are yet to be evaluated in speech recognition tasks.
7.1.7 Better optimization and regularization
Another area where significant advances are made recently in applying deep learning to acoustic model for speech recognition is on optimization criteria and methods, as well as on the related regularization techniques to help prevent overfitting during the deep network training.
One of the early studies on DNNs for speech recognition, conducted at Microsoft Research and reported in (Mohamed et al., 2010), first recognizes the mismatch between the desired error rate and the cross-entropy training criterion in the conventional DNN training. The solution is provided by replacing the frame-based, cross-entropy training criterion with the full-sequence-based maximum mutual information optimization objective. Equivalently, this amounts to putting the model of conditional random field (CRF) at the top of the DNN, replacing the original softmax layer which naturally leads to cross entropy. (Note the DNN was called the DBN in the paper).
This new sequential discriminative learning technique is developed to jointly optimize the DNN weights, CRF transition weights, and bi-phone language model. Importantly, the speech task is defined in TIMIT, with the use of a simple bi-phone-gram "language" model. The simplicity of the bi-gram language model enables the full-sequence training to carry out without the need to use lattices, drastically reducing the training complexity.
As another way to motivate the full-sequence training method of (Mohamed et al., 2010), we note that the earlier DNN phone recognition experiments made use of the standard frame-based objective function in static pattern classification, cross-entropy, to optimize the DNN weights. The transition parameters and language model scores were obtained from an HMM and were trained independently of the DNN weights. However, it has been known during the long history of the HMM research that sequence classification criteria can be very helpful in improving speech and phone recognition accuracy. This is because the sequence classification criteria are more directly correlated with the performance measure (e.g., the overall word or phone error rate) than framelevel criteria. More specifically, the use of frame-level cross entropy to train the DNN for phone sequence recognition does not explicitly take into account the fact that the neighboring frames have smaller distances between the assigned probability distributions over phone class labels. To overcome this deficiency, one can optimize the conditional probability of the whole sequence of labels, given the whole visible feature utterance or equivalent the hidden feature sequence extracted by DNN. To optimize the log conditional probability on the training data, the gradient can be taken over the activation parameters, transition parameters and lower-layer weights, and then pursue back-propagation of the error defined at the sentence level. We remark that in a much earlier study (LeCun et al., 1998), combining a neural network with a CRF-like structure was done, where the mathematical formulation appears to include CRFs as a special case. Also, the benefit of using the full-sequence classification criteria was shown earlier on shallow neural networks in (Kingsbury 2009; Prabhavalkar and Fosler-Lussier, 2010).
In implementing the above full-sequence learning algorithm for the DNN system as described in(Mohamed et al., 2010), the DNN weights are initialized using the frame-level cross entropy as the objective. The transition parameters are initialized from the combination of the HMM transition matrices and the "bi-phone language" model scores, and are then further optimized by tuning the transition features while fixing the DNN weights before the joint optimization. Usingjoint optimization with careful scheduling to reduce overfitting, it is shown that the full-sequence training outperforms the DNN trained with frame-level cross entropy by approximately 5% relative (Mohamed et al., 2010). Without the effort to reduce overfitting, it is found that the DNN trained with MMI is much more prone to overfitting than that trained with frame-level cross entropy. This is because the correlations across frames in speech tend to be different among the training, development, and test data. Importantly, such differences do not show when frame-based objective functions are used for training.
For large vocabulary speech recognition where more complex language models are in use, the optimization methods for full-sequence training of the DNN-HMM are much more sophisticated.
Kingsbury et al. (2012) reported the first success of such training using parallel, second-order, Hessian-free optimization techniques, which are carefully implemented for large vocabulary speech recognition. Sainath et al. (2013d) improved and speeded up the Hessian-free techniques by reducing the number of Krylov subspace solver iterations (Vinyals and Povey, 2012), which are used for implicit estimation of the Hessian. They also use sampling methods to decrease the amount of training data to speed up the training. While the batch-mode, second-order Hessian-free techniques prove successful for full-sequence training of large-scale DNN-HMM systems, the success of the first-order stochastic gradient descent methods is also reported recently (Su et al., 2013). It is found that heuristics are needed to handle the problem of lattice sparseness. That is, the DNN must be adjusted to the updated numerator lattices by additional iterations of frame-based cross-entropy training. Further, artificial silence arcs need to be added to the denominator lattices, or the maximum mutual information objective function needs to be smoothed with the frame-based cross entropy objective. The conclusion is that for large vocabulary speech recognition tasks with sparse lattices, the implementation of the sequence training requires much greater engineering skills than the small tasks such as reported in (Mohamed et al., 2010), although the objective function as well as the gradient derivation are essentially the same. Similar conclusions are reached by Vesely et al. (2013) when carrying out full-sequence training of DNN-HMMs for largevocabulary speech recognition. However, different heuristics from (Su et al., 2013) are shown to be effective in the training. Separately, Wiesler et al. (2013) investigated the Hessian-free optimization method for training the DNN with the cross-entropy objective and empirically analyzed the properties of the method. And finally, Dognin and Goel (2013) combined stochastic average gradient and Hessian-free optimization for sequence training of deep neural networks with success in that the training procedure converges in about half the time compared with the full
Hessian-free sequence training.
For large DNN-HMM systems with either frame-level or sequence-level optimization objectives, speeding up the training is essential to take advantage of large amounts of training data and of large model sizes. In addition to the methods described above, Dean et al. (2012) reported the use of the asynchronous stochastic gradient descent (ASGD) method, the adaptive gradient descent(Adagrad) method, and the large-scale limited-memory BFGS (L-BFGS) method for very large vocabulary speech recognition. Sainath et al. (2013) provided a review of a wide range of optimization methods for speeding up the training of DNN-based systems for large speech recognition tasks.
In addition to the advances described above focusing on optimization with the fully supervised learning paradigm, where all training data contain the label information, the semi-supervisedtraining paradigm is also exploited for learning DNN-HMM systems for speech recognition. Liao et al. (2013) reported the exploration of using semi-supervised training on the DNN-HMM system for the very challenging task of recognizing YouTube speech. The main technique is based on the use of "island of confidence" filtering heuristics to select useful training segments. Separately, semi-supervised training of DNNs is explored by Vesely et al. (2013), where self-training strategies are used as the basis for data selection using both the utterance-level and frame-level confidences. Frame-selection based on per-frame confidences derived from confusion in a lattice is found beneficial. Huang et al. (2013) reported another variant of semi-supervised training technique in which multi-system combination and confidence recalibration is applied to select the training data. Further, Thomas et al. (2013) overcome the problem of lacking sufficient training data for acoustic modeling in a number of low-resource scenarios. They make use of transcribed multilingual data and semi-supervised training to build the proposed feature front-ends for subsequent speech recognition.
Finally, we see important progress in deep learning based speech recognition in recent years with the introduction of new regularization methods based on "dropout" originally proposed by Hinton et al., (2012a). Overfitting is very common in DNN training and co-adaptation is prevalent within the DNN with multiple activations adapting together to explain input acoustic data. Dropout is a technique to limit co-adaptation. It operates as follows. On each training instance, each hidden unit is randomly omitted with a fixed probability (e.g., p=0.5). Then, decoding is done normally except with straightforward scaling of the DNN weights (by a factor of 1-p). Alternatively, the scaling of the DNN weights can be done during training [by a factor of 1/(1-p)] rather than in decoding.
The benefits of dropout regularization for training DNNs are to make a hidden unit in the DNN act strongly by itself without relying on others, and to serve a way to do model averaging of different networks. These benefits are most pronounced when the training data is limited, or when the DNN size is disproportionally large with respect to the size of the training data. Dahl et al.(2013) applied dropout in conjunction with the ReLU units and to only the top few layers of a fully-connected DNN. Seltzer and Yu (2013) applied it to noise robust speech recognition. Deng et al. (2013), on the other hand, applied dropout to all layers of a deep convolutional neural network, including both the top fully-connected DNN layers and the bottom locally-connected CNN layer and the pooling layer. It is found that the dropout rate need to be substantially smaller for the convolutional layer.
Subsequent work on applying dropout includes the study by Miao and Metze (2013), where DNNbased speech recognition is constrained by low resources with sparse training data. Most recently, Sainath et al. (2013e) combined dropout with a number of novel techniques described in this section (including the use of deep CNNs, Hessian-free sequence learning, the use of ReLU units, and the use of joint fMLLR and filterbank features, etc.) to obtain state of the art results on several large vocabulary speech recognition tasks.
As a summary, the initial success of deep learning methods for speech analysis and recognition reported around 2010 has come a long way over the past three years. An explosive growth in the work and publications on this topic has been observed, and huge excitement has been ignited within the speech recognition community. We expect that the growth in the research on deep learning based speech recognition will continue, at least in the near future. It is also fair to say that the continuing large-scale success of deep learning in speech recognition as surveyed in thischapter (up to the ASRU-2013 time frame) is a key stimulant to the large-scale exploration and applications of the deep learning methods to other areas, which we will survey in Chapters 8-11.
7.2 Speech Synthesis
In addition to speech recognition, the impact of deep learning has recently spread to speech synthesis, aimed to overcome the limitations of the conventional approach in statistical parametric synthesis based on Gaussian-HMM and decision-tree-based model clustering. The goal of speech synthesis is to generate speech sounds directly from text and possibly with additional information.
The first set of papers appeared at ICASSP, May 2013, where four different deep learning approaches are reported to improve the traditional HMM-based statistical parametric speech synthesis systems built based on "shallow" speech models, which we briefly review here after providing appropriate background information.
Statistical parametric speech synthesis emerged in the mid-1990s, and is currently the dominant technology in speech synthesis. See a recent overview in (Tokuda et al., 2013). In this approach, the relationship between texts and their acoustic realizations are modeled using a set of stochastic generative acoustic models. Decision tree-clustered context-dependent HMMs with a Gaussian distribution as the output of an HMM state are the most popular generative acoustic model used.
In such HMM-based speech synthesis systems, acoustic features including the spectra, excitation and segment durations of speech are modeled simultaneously within a unified context-dependent
HMM framework. At the synthesis time, a text analysis module extracts a sequence of contextual factors including phonetic, prosodic, linguistic, and grammatical descriptions from an input text to be synthesized. Given the sequence of contextual factors, a sentence-level context-dependent
HMM corresponding to the input text is composed, where its model parameters are determined by traversing the decision trees. The acoustic features are predicted so as to maximize their output probabilities from the sentence HMM under the constraints between static and dynamic features.
Finally, the predicted acoustic features are sent to a waveform synthesis module to reconstruct the speech waveforms. It has been known for many years that the speech sounds generated by this standard approach are often muffled compared with natural speech. The inadequacy of acoustic modeling based on the shallow-structured HMM is conjectured to be one of the reasons. Several very recent studies have adopted deep learning approaches to overcome such deficiency. One significant advantage of deep learning techniques is their strong ability to represent the intrinsic correlation or mapping relationship among the units of a high-dimensional stochastic vector using a generative (e.g., the RBM and DBN discussed in Chapter 3.2) or discriminative (e.g., the DNN discussed in Chapter 3.3) modeling framework. The deep learning techniques are thus expected to help the acoustic modeling aspect of speech synthesis in overcoming the limitations of the conventional shallow modeling approach.
A series of studies are carried out recently on ways of overcoming the above limitations using deep learning methods, inspired partly by the intrinsically hierarchical processes in human speech production and the successful applications of a number of deep learning methods in speech recognition as reviewed earlier in this chapter. In Ling et al. (2013, 2013a), the RBM and DBN as generative models are used to replace the traditional Gaussian models, achieving significant quality improvement, in both subjective and objective measures, of the synthesized voice. In the approach developed in (Kang et al., 2013), the DBN as a generative model is used to representjoint distribution of linguistic and acoustic features. Both the decision trees and Gaussian models are replaced by the DBN. The method is very similar to that used for generating digit images by the DBN, where the issue of temporal sequence modeling specific to speech (non-issue for image) is by-passed via the use of the relatively large, syllable-sized units in speech synthesis. On the other hand, in contrast to the generative deep models (RBMs and DBNs) exploited above, the study reported in (Zen et al., 2013) makes use of the discriminative model of the DNN to represent the conditional distribution of the acoustic features given the linguistic features. Finally, in(Fernandez et al., 2013), the discriminative model of the DNN is used as a feature extractor that summarizes high-level structure from the raw acoustic features. Such DNN features are then used as the input for the second stage for the prediction of prosodic contour targets from contextual features in the full speech synthesis system.
The application of deep learning to speech synthesis is in its infancy, and much more work is expected from that community in the near future.
7.3 Audio and Music Processing
Similar to speech recognition but to a less extent, in the area of audio and music processing, deep learning has also become of intense interest but only quite recently. As an example, the first major event of deep learning for speech recognition took place in 2009, followed by a series of events including a comprehensive tutorial on the topic at ICASSP-2012 and with the special issue at IEEE
Transactions on Audio, Speech, and Language Processing, the premier publication for speech recognition, in the same year. The first major event of deep learning for audio and music processing appears to be the special session at ICASSP-2014, titled Deep Learning for Music (Battenberg et al., 2014).
In the general field of audio and music processing, the impacted areas by deep learning include mainly music signal processing and music information retrieval (e.g., Bengio et al., 2013;
Humphrey et al., 2012, 2012a, 2013; Battenberg and Wessel, 2012; Schmidt and Kim, 2011;
Hamel and Eck, 2010). Deep learning presents a unique set of challenges in these areas. Music audio signals are time series where events are organized in musical time, rather than in real time, which changes as a function of rhythm and expression. The measured signals typically combine multiple voices that are synchronized in time and overlapping in frequency, mixing both shortterm and long-term temporal dependencies. The influencing factors include musical tradition, style, composer and interpretation. The high complexity and variety give rise to the signal representation problems well-suited to the high levels of abstraction afforded by the perceptually and biologically motivated processing techniques of deep learning.
In the early work on audio signals as reported by Lee et al. (2009) and their follow-up work, the convolutional structure is imposed on the RBM while building up a DBN. Convolution is made in time by sharing weights between hidden units in an attempt to detect the same "invariant" feature over different times. Then a max-pooling operation is performed where the maximal activations over small temporal neighborhoods of hidden units are obtained, inducing some local temporal invariance. The resulting convolutional DBN is applied to audio as well as speech data for anumber of tasks including music artist and genre classification, speaker identification, speaker gender classification, and phone classification, with promising results presented.
The RNN has also been recently applied to music processing applications (Bengio et al., 2013;
Boulanger-Lewandowski, et al., 2013), where the use of ReLU hidden units instead of logistic or tanh nonlinearities are explored in the RNN. As reviewed in Chapter 7.2, ReLU units compute y
= max(x, 0), and lead to sparser gradients, less diffusion of credit and blame in the RNN, and faster training. The RNN is applied to the task of automatic recognition of chords from audio music, an active area of research in music information retrieval. The motivation of using the RNN architecture is its power in modeling dynamical systems. The RNN incorporates an internal memory, or hidden state, represented by a self-connected hidden layer of neurons. This property makes them well suited to model temporal sequences, such as frames in a magnitude spectrogram or chord labels in a harmonic progression. When well trained, the RNN is endowed with the power to predict the output at the next time step given the previous ones. Experimental results show that the RNN-based automatic chord recognition system is competitive with existing state-of-the-art approaches (e.g., Oudre et al., 2011). The RNN is capable of learning basic musical properties such as temporal continuity, harmony and temporal dynamics. It can also efficiently search for the most musically plausible chord sequences when the audio signal is ambiguous, noisy or weakly discriminative.
A recent review article by Humphrey et al. (2013) provides a detailed analysis on content-based music informatics, and in particular on why the progress is decelerating throughout the field. The analysis concludes that hand-crafted feature design is sub-optimal and unsustainable, that the power of shallow architectures is fundamentally limited, and that short-time analysis cannot encode musically meaningful structure. These conclusions motivate the use of deep learning methods aimed at automatic feature learning. By embracing feature learning, it becomes possible to optimize a music retrieval system's internal feature representation or discovering it directly, since deep architectures are especially well-suited to characterize the hierarchical nature of music.
Finally, we review the very recent work by van den Oord, et al. (2013) on content-based music recommendation using deep learning methods. Automatic music recommendation has become an increasingly significant and useful technique in practice. Most recommender systems rely on collaborative filtering, suffering from the cold start problem where it fails when no usage data is available. Thus, collaborative filtering is not effective for recommending new and unpopular songs.
Deep learning methods power the latent factor model for recommendation, which predicts the latent factors from music audio when they cannot be obtained from usage data. A traditional approach using a bag-of-words representation of the audio signals is compared with deep CNNs with rigorous evaluation made. The results show highly sensible recommendations produced by the predicted latent factors using deep CNNs. The study demonstrates that a combination of convolutional neural networks and richer audio features lead to such promising results for contentbased music recommendation.
Like speech recognition and speech synthesis, much more work is expected from the music and audio signal processing community in the near future.
CHAPTER 8
SELECTED APPLICATIONS IN
LANGUAGE MODELING AND NATURAL
LANGUAGE PROCESSING
Research in language, document, and text processing has seen increasing popularity recently in the signal processing community, and has been designated as one of the main focus areas by the IEEE
Signal Processing Society's Speech and Language Processing Technical Committee. Applications of deep learning to this area started with language modeling (LM), where the goal is to provide a probability to any arbitrary sequence of words or other linguistic symbols (e.g., letters, characters, phones, etc.). Natural language processing (NLP) or computational linguistics also deals with sequences of words or other linguistic symbols, but the tasks are much more diverse (e.g., translation, parsing, text classification, etc.), not focusing on providing probabilities for linguistic symbols. The connection is that LM is often an important and very useful component of NLP systems. Applications to NLP is currently one of the most active areas in deep learning research, and deep learning is also considered as one promising direction by the NLP research community.
However, the intersection between the deep learning and NLP researchers is so far not nearly as large as that for the application areas of speech or vision. This is partly because the hard evidence for the superiority of deep learning over the current state of the art NLP methods has not been as strong as speech or visual object recognition.
8.1 Language Modeling
Language models (LMs) are crucial part of many successful applications, such as speech recognition, text information retrieval, statistical machine translation and other tasks of NLP.
Traditional techniques for estimating the parameters in LMs are based on N-gram counts. Despite known weaknesses of N-grams and huge efforts of research communities across many fields, Ngrams remained the state-of-the-art until neural network and deep learning based methods were shown to significantly lower the perplexity of LMs, one common (but not ultimate) measure of the LM quality, over several standard benchmark tasks (Mikolov, 2012; Mikolov et al., 2010, 2011).
Before we discuss neural network based LMs, we note the use of hierarchical Bayesian priors in building up deep and recursive structure for LMs (Huang and Renals, 2010). Specifically, PitmanYor process is exploited as the Bayesian prior, from which a deep (four layers) probabilistic generative model is built. It offers a principled approach to LM smoothing by incorporating the power-law distribution for natural language. As discussed in Chapter 3, this type of prior knowledge embedding is more readily achievable in the generative probabilistic modeling setup than in the discriminative neural network based setup. The reported results on LM perplexity reduction are not nearly as strong as that achieved by the neural network based LMs, which we discuss next.
There has been a long history (e.g., Bengio et al., 2001; 2003; Zamora et al., 2009) of using(shallow) feed-forward neural networks in LMs, called the NNLM. An LM is a function that captures the salient statistical characteristics of the distribution of sequences of words in natural language. It allows one to make probabilistic predictions of the next word given preceding ones.
An NNLM is one that exploits the neural network's ability to learn distributed representations in order to reduce the impact of the curse of dimensionality. The original NNLM, with a feed-forward neural network structure works as follows: the input of the N-gram NNLM is formed by using a fixed length history of N-1 words. Each of the previous N-1 words is encoded using the very sparse
1-of-V coding, where V is the size of the vocabulary. Then, this 1-of-V orthogonal representation of words is projected linearly to a lower dimensional space, using the projection matrix shared among words at different positions in the history. After the projection layer, a hidden layer with non-linear activation function, which is either a hyperbolic tangent or a logistic sigmoid, is used.
An output layer of the neural network then follows the hidden layer, with the number of output units equal to the size of the full vocabulary. After the network is trained, the output layer activations represent the "N-gram" LM's probability distribution.
The main advantage of NNLMs over the traditional counting-based N-gram LMs is that history is no longer seen as exact sequence of N-1 words, but rather as a projection of the entire history into some lower dimensional space. This leads to a reduction of the total number of parameters in the model that have to be trained, resulting in automatic clustering of similar histories. Compared with the class-based N-gram LMs, the NNLMs are different in that they project all words into the same low dimensional space, in which there can be many degrees of similarity between words. On the other hand, NNLMs have much larger computational complexity than N-gram LMs.
Let's look at the strengths of the NNLMs again from the viewpoint of distributed representations.
A distributed representation of a symbol is a vector of features which characterize the meaning of the symbol. Each element in the vector participates in representing the meaning. With an NNLM, one relies on the learning algorithm to discover meaningful, continuous-valued features. The basic idea is to learn to associate each word in the dictionary with a continuous-valued vector representation, which in the literature is called a word embedding, where each word corresponds to a point in a feature space. One can imagine that each dimension of that space corresponds to a semantic or grammatical characteristic of words. The hope is that functionally similar words get to be closer to each other in that space, at least along some directions. A sequence of words can thus be transformed into a sequence of these learned feature vectors. The neural network learns to map that sequence of feature vectors to the probability distribution over the next word in the sequence. The distributed representation approach to LMs has the advantage that it allows the model to generalize well to sequences that are not in the set of training word sequences, but that are similar in terms of their features, i.e., their distributed representation. Because neural networks tend to map nearby inputs to nearby outputs, the predictions corresponding to word sequences with similar features are mapped to similar predictions.
The above ideas of NNLMs have been implemented in various studies, some involving deep architectures. In (Mnih and Hinton, 2007), the temporally factored RBM was used for language modeling. Unlike the traditional N-gram model, the factored RBM uses distributed representationsnot only for context words but also for the words being predicted. This approach is generalized to deeper structures as reported in (Mnih and Hinton, 2008).
Subsequent work on NNLM with "deep" architectures can be found in (Le et al., 2010, 2011, 2013;
Mikolov et al., 2010; Mikolov et al., 2011; Mikolov, 2012). As an example, Le et al. (2013) describes an NNLM with structured output layer (SOUL-NNLM) where the processing depth in the LM is focused in the neural network's output representation. Figure 1 illustrates the SOULNNLM architecture with hierarchical structure in the output layers of the neural network, which shares the same architecture with the conventional NNLM up to the hidden layer. The hierarchical structure for the network's output vocabulary is in the form of a clustering tree, shown to the right of Figure 8.1, where each word belongs to only one class and ends in a single leaf node of the tree.
As a result of the hierarchical structure, the SOUL-NNLM enables the training of the NNLM with a full, very large vocabulary. This gives advantages over the traditional NNLM which requires shortlists of words in order to carry out the efficient computation in training.
Figure 8.1. The SOUL-NNLM architecture with hierarchical structure in the output layers of the neural network [after (Le et al., 2013), @IEEE].
As another example neural-network-based LMs, the work described in (Mikolov et al., 2010, 2011) and (Mikolov, 2012) makes use of RNNs to build large scale language models, called RNNLMs.
The main difference between the feed-forward and the recurrent architecture for LMs is different ways of representing the word history. For feed-forward NNLM, the history is still just previous several words. But for the RNNLM, an effective representation of history is learned from the data during training. The hidden layer of RNN represents all previous history and not just N-1 previous words, thus the model can theoretically represent long context patterns. A further important advantage of the RNNLM over the feed-forward counterpart is the possibility to represent more advanced patterns in the word sequence. For example, patterns that rely on words that could have occurred at variable positions in the history can be encoded much more efficiently with the recurrent architecture. That is, the RNNLM can simply remember some specific word in the stateof the hidden layer, while the feed-forward NNLM would need to use parameters for each specific position of the word in the history.
The RNNLM is trained using the algorithm of back-propagation through time; see details in(Mikolov, 2012), which provided Figure 8.2 to show during training how the RNN unfolds as a deep feed-forward network (with three time steps back in time).
Figure 8.2. During the training of RNNLMs, the RNN unfolds into a deep feed-forward network; based on Figure 3.2 of (Mikolov, 2012).
The training of the RNNLM achieves stability and fast convergence, helped by capping the growing gradient in training RNNs. Adaptation schemes for the RNNLM are also developed by sorting the training data with respect to their relevance and by training the model during processing of the test data. Empirical comparisons with other state-of-the-art counting-based N-gram LMs show much better performance of RNNLM in the perplexity measure, as reported in (Mikolov et al., 2010, 2011) and (Mikolov, 2012).
A separate work on applying RNN as an LM on the unit of characters instead of words can be found in (Sutskever et al., 2011; Hermans et al., 2013). Many interesting properties such as predicting long-term dependencies (e.g., making open and closing quotes in a paragraph) are demonstrated. However, the usefulness of characters instead of words as units in practical applications is not clear because the word is such a powerful representation for natural language.
Changing words to characters in LMs may limit most practical application scenarios and the training become more difficult. Word-level models currently remain superior.
In the most recent work, Mnih and Teh (2012) and Mnih and Kavukcuoglu (2013) have developed a fast and simple training algorithm for NNLMs. Despite their superior performance, NNLMs have been used less widely than standard N-gram LMs due to the much longer training time. The reported algorithm makes use of a method called noise-contrastive estimation or NCE (Gutmann and Hyvarinen, 2012) to achieve much faster training for NNLMs, with time complexity independent of the vocabulary size; hence a flat instead of tree-structured output layer in the NNLM is used. The idea behind NCE is to perform nonlinear logistic regression to discriminate between the observed data and some artificially generated noise. That is, to estimate parameters in a density model of observed data, we can learn to discriminate between samples from the data distribution and samples from a known noise distribution. As an important special case, NCE is particularly attractive for unnormalized distributions (i.e., free from partition functions in the denominator). In order to apply NCE to train NNLMs efficiently, Mnih and Teh (2012) and Mnih and Kavukcuoglu (2013) first formulate the learning problem as one which takes the objective function as the distribution of the word in terms of a scoring function. The NNLM then can be viewed as a way to quantify the compatibility between the word history and a candidate next word using the scoring function. The objective function for training the NNLM thus becomes exponentiation of the scoring function, normalized by the same constant over all possible words.
Removing the costly normalization factor, NCE is shown to speed up the NNLM training over an order of magnitude.
A similar concept to NCE is used in the recent work of (Mikolov et al., 2013), which is called negative sampling. This is applied to a simplified version of an NNLM, for the purpose of constructing word embedding instead of computing probabilities of word sequences. Word embedding is an important concept for NLP applications, which we discuss next.
8.2 Natural Language Processing
Machine learning has been a dominant tool in NLP for many years. However, the use of machine learning in NLP has been mostly limited to numerical optimization of weights for human designed representations and features from the text data. The goal of deep or representation learning is to automatically develop features or representations from the raw text material appropriate for a wide range of NLP tasks.
Recently, neural network based deep learning methods have been shown to perform well on various NLP tasks such as language modeling, machine translation, part-of-speech tagging, named entity recognition, sentiment analysis, and paraphrase detection. The most attractive aspect of deep learning methods is their ability to perform these tasks without external hand-designed resources or time-intensive feature engineering. To this end, deep learning develops and makes use an important concept called "embedding", which refers to the representation of symbolic information in natural language text at word-level, phrase-level, and even sentence-level in terms of continuous-valued vectors.
The early work highlighting the importance of word embedding came from (Collobert and Weston, 2008), (Turian et al., 2010), and (Collobert et al., 2011), although the original form came from(Bengio et al., 2000) as a side product of language modeling. Raw symbolic word representations are transformed from the sparse vectors via 1-of-V coding with a very high dimension (i.e., the vocabulary size V or its square or even its cubic) into low-dimensional, real-valued vectors via a neural network and then used for processing by subsequent neural network layers. The key advantage of using the continuous space to represent words (or phrases) is its distributed nature, which enables sharing or grouping the representations of words with a similar meaning. Such sharing is not possible in the original symbolic space, constructed by 1-of-V coding with a very high dimension, for representing words. Unsupervised learning is used where "context" of the word is used as the learning signal in neural networks. Excellent tutorials were recently given by
Socher et al. (2012-2013) to explain how the neural network is trained to perform word embedding.
More recent work proposes new ways of learning word embeddings that better capture the semantics of words by incorporating both local and global document contexts and better account for homonymy and polysemy by learning multiple embeddings per word (Huang et al., 2012).
Also, there is strong evidence that the use of RNNs can also provide empirically good performance in learning word embeddings (Mikolov, 2012). While the use of NNLMs, whose aim is to predict the future words in context, also induces word embeddings as its by-product, much simpler ways of achieving the embeddings are possible without the need to do word prediction. As shown by
Collobert and Weston (2008), the neural networks used for creating word embeddings need much smaller output units than the huge size typically required for NNLMs.
In the same early paper on word embedding, Collobert and Weston (2008) developed and employed a convolutional network as the common model to simultaneously solve a number of classic problems including part-of-speech tagging, chunking, named entity tagging, semantic role identification, and similar word identification. More recent work reported in (Collobert, 2011) further developed a fast, purely discriminative approach for parsing based on the deep recurrent convolutional architecture. Collobert et al., (2011) provide a comprehensive review on ways of applying unified neural network architectures and related deep learning algorithms to solve NLP problems from "scratch", meaning that no traditional NLP methods are used to extract features.
The theme of this line of work is to avoid task-specific, "man-made" feature engineering while providing versatility and unified features constructed automatically from deep learning applicable to all natural language processing tasks. The systems described in (Collobert et al., 2011) automatically learn internal representations or word embedding from vast amounts of mostly unlabeled training data while performing a wide range of NLP tasks.
The recent work by Mikolov et al. (2013a) derives word embeddings by simplifying the NNLM described in Section 8.1 of this chapter. It is found that the NNLM can be successfully trained in two steps. First, continuous word vectors are learned using a simple model which eliminates the nonlinearity in the upper neural network layer and share the projection layer for all words. And second, the N-gram NNLM is trained on top of the word vectors. So, after removing the second step in the NNLM, the simple model is used to learn word embeddings, where the simplicity allows the use of very large amount of data. This gives rise to a word embedding model called Continuous
Bag-of-Words Model (CBOW), as shown in Fig. 8.3a. Further, since the goal is no longer computing probabilities of word sequences as in LMs, the word embedding system here is made more effective by not only to predict the current word based on the context but also to performinverse prediction known as "Skip-gram" model, as shown in Fig. 8.3b. In the follow-up work(Mikolov et al., 2013) by the same authors, this word embedding system including the Skip-gram model is extended by a much faster learning method called negative sampling, similar to NCE discussed in Chapter 8.1.
Figure 8.3. The CBOW architecture (a) on the left, and the Skip-gram architecture (b) on the right.
[after (Mikolov et al., 2013a), @ICLR].
In parallel with the above development, Mnih and Kavukcuoglu (2013) demonstrate that NCE training of lightweight word embedding models is a highly efficient way of learning high-quality word representations, much like the somewhat earlier lightweight LMs developed by Mnih and Teh (2012) described in Section 8.1. Consequently, results that used to require very considerable hardware and software infrastructure can now be obtained on a single desktop with minimal programming effort and using less time and data. This most recent work also shows that for representation learning, only five noise samples in NCE can be sufficient for obtaining strong results for word embedding, much fewer than that required for LMs. The authors also used an
"inversed language model" for computing word embeddings, similar to the way in which the Skipgram model is used in (Mikolov et al., 2013).
Huang et al. (2012) recognized the limitation of the earlier work on word embeddings in that these models were built with only local context and one representation per word. They extended the local context models to one that can incorporate global context from full sentences or the entire document. This extended models accounts for homonymy and polysemy by learning multipleembeddings for each word. An illustration of this model is shown in Figure 8.4. In the earlier work by the same research group (Socher et al., 2011), a recursive neural network with local context was developed to build a deep architecture. The network, despite missing global context, was already shown to be capable of successful merging of natural language words based on the learned semantic transformations of their original features. This deep learning approach provided an excellent performance on natural language parsing. The same approach was also demonstrated to be reasonably successful in parsing natural scene images. In related studies, a similar recursive deep architecture is used for paraphrase detection (Socher et al., 2011a), and for predicting sentiment distributions from text (Socher et al., 2011b).
Figure 8.4. The extended word-embedding model using a recursive neural network that takes into account not only local context but also global context. The global context is extracted from the document and put in the form of a global semantic vector, as part of the input into the original word-embedding model with local context. Taken from Figure 1 of (Huang et al., 2012). [after(Huang et al., 2012), @ACL].
We now turn to selected applications of deep learning methods including the use of neural network architectures and word embeddings to practically useful NLP tasks. Machine translation is one of such tasks, pursued by NLP researchers for many years based typically on shallow statistical models. The work described in (Schwenk, et al., 2012) are perhaps the first comprehensive report on the successful application of neural-network-based language models with word embeddings, trained on a GPU, for large machine translation tasks. They address the problem of high computation complexity, and provide a solution that allows training 500 million words with 20 hours. Strong results are reported, with perplexity down from 71 to 60 in LMs and the corresponding BLEU score gained by 1.8 points using the neural-network-based language models with word embeddings compared with the best back-off LM.
A more recent study on applying deep learning methods to machine translation appears in (Gao et al., 2013), where the phrase-translation component, rather than the LM component in the machine translation system is replaced by the neural network models with semantic word embeddings. As shown in Figure 8.5 for the architecture of this approach, a pair of source (denoted by f) and target(denoted by e) phrases are projected into continuous-valued vector representations in a lowdimensional latent semantic space (denoted by the two y vectors).Then their translation score iscomputed by the distance between the pair in this new space. The projection is performed by two deep neural networks (not shown here) whose weights are learned on parallel training data. The learning is aimed to directly optimize the quality of end-to-end machine translation results.
Experimental evaluation has been performed on two standard Europarl translation tasks used by the NLP community, English-French and German-English. The results show that the new semantic-based phrase translation model significantly improves the performance of a state-of-theart phrase-based statistical machine translation system, leading to a gain close to1.0 BLEU point.
Figure 8.5. Illustration of the basic approach reported in (Gao et al., 2013) for machine translation. Parallel pairs of source (denoted by f) and target (denoted by e) phrases are projected into continuous-valued vector representations (denoted by the two y vectors), and their translation score is computed by the distance between the pair in this continuous space. The projection is performed by deep neural networks (denoted by the two arrows) whose weights are learned on parallel training data. [after (Gao et al., 2013), @NIPS].
A related approach to machine translation was developed by Schwenk (2012). The estimation of the translation model probabilities of a phrase-based machine translation system is carried out using neural networks. The translation probability of phrase pairs is learned using continuousspace representations induced by neural networks. A simplification is made that decomposes the translation probability of a phrase or a sentence to a product of n-gram probabilities as in a standard n-gram language model. No joint representations of a phrase in the source language and the translated version in the target language are exploited as in the approach reported by Gao et al.
Yet another deep learning approach to machine translation appeared in (Mikolov et al., 2013b). As in other approaches, a corpus of words in one language are compared with the same corpus of words translated into another, and words and phrases in such bilingual data that share similar statistical properties are considered equivalent. A new technique is proposed that automatically generates dictionaries and phrase tables that convert one language into another. It does not rely on versions of the same document in different languages. Instead, it uses data mining techniques to model the structure of a source language and then compares it to the structure of the targetlanguage. The technique is shown to translate missing word and phrase entries by learning language structures based on large monolingual data and mapping between languages from small bilingual data. It is based on vector-valued word embeddings as discussed earlier in this chapter and it learns a linear mapping between vector spaces of source and target languages.
An earlier study on applying deep learning techniques with DBNs was provided in (Deselaers et al., 2009) to attack a machine transliteration problem, a much easier task than machine translation.
This type of deep architectures and learning may be generalized to the more difficult machine translation problem but no follow-up work has been reported. As another early NLP application, Sarikaya et al. (2011) applied DNNs (called DBNs in the paper) to perform a natural language call–routing task. The DNNs use unsupervised learning to discover multiple layers of features that are then used to optimize discrimination. Unsupervised feature discovery is found to make DBNs far less prone to overfitting than the neural networks initialized with random weights.
Unsupervised learning also makes it easier to train neural networks with many hidden layers.
DBNs are found to produce better classification results than several other widely used learning techniques, e.g., maximum entropy and boosting based classifiers.
One most interesting NLP task recently tackled by deep learning methods is that of knowledge base (ontology) completion, which is instrumental in question-answering and many other NLP applications. An early work in this space came from (Bordes et al., 2011), where a process is introduced to automatically learn structured distributed embeddings of knowledge bases. The proposed representations in the continuous-valued vector space are compact and can be efficiently learned from large-scale data of entities and relations. A specialized neural network architecture, a generalization of "Siamese" network, is used. In the follow-up work that focuses on multirelational data (Bordes et al., 2013), the semantic matching energy model is proposed to learn vector representations for both entities and relations. More recent work (Socher et al., 2013) adopts an alternative approach, based on the use of neural tensor networks, to attack the problem of reasoning over a large joint knowledge graph for relation classification. The knowledge graph is represented as triples of a relation between two entities, and the authors aim to develop a neural network model suitable for inference over such relationships. The model they presented is a neural tensor network, with one layer only. The network is used to represent entities in a fixeddimensional vectors, which are created separately by averaging pre-trained word embedding vectors. It then learn the tensor with the newly added relationship element that describes the interactions among all the latent components in each of the relationships. The neural tensor network can be visualized in Figure 8.6, where each dashed box denotes one of the two slices of the tensor. Experimentally, the paper of (Socher et al., 2013) shows that this tensor model can effectively classify unseen relationships in WordNet and FreeBase.
Figure 8.6. Illustration of the neural tensor network described in (Socher et al., 2013), with two relationships shown as two slices in the tensor. The tensor is denoted by W[1:2]. The network contains a bilinear tensor layer that directly relates the two entity vectors (shown as e1 and e2 ) across three dimensions. Each dashed box denotes one of the two slices of the tensor. [after(Socher et al., 2013), @NIPS].
As the final example of deep learning applied successfully to NLP, we discuss here sentiment analysis applications based on recursive deep models published recently by Socher et al. (2013a).
Sentiment analysis is a task that is aimed to estimate the positive or negative opinion by an algorithm based on input text information. As we discussed earlier in this chapter, word embeddings in the semantic space achieved by neural network models have been very useful but it is difficult for them to express the meaning of longer phrases in a principled way. For sentiment analysis with the input data from typically many words and phrases, the embedding model requires the compositionality properties. To this end, Socher et al. (2013a) developed the recursive neural tensor network, where each layer is constructed similarly to that of the neural tensor network described in (Socher et al., 2013) with an illustration shown in Figure 8.6. The recursive construction of the full network exhibiting properties of compositionality follows that of (Socher et al., 2011) for the regular, non-tensor network. When trained on a carefully constructed sentiment analysis database, the recursive neural tensor network is shown to outperform all previous methods on several metrics. The new model pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag-of-features baselines.
CHAPTER 9
SELECTED APPLICATIONS IN
INFORMATION RETRIEVAL
9.1 A Brief Introduction to Information Retrieval
Information retrieval (IR) is a process whereby a user enters a query into the automated computer system that contains a collection of many documents with the goal of obtaining a set of most relevant documents. Queries are formal statements of information needs, such as search strings in web search engines. In IR, a query does not uniquely identify a single document in the collection.
Instead, several documents may match the query with different degrees of relevancy.
A document, sometimes called an object as a more general term which may include not only a text document but also an image, audio (music or speech), or video, is an entity that contains information and represented as an entry in a database. In this chapter, we limit the "object" to only text documents. User queries in IR are matched against the documents' representation stored in the database. Documents themselves often are not kept or stored directly in the IR system. Rather, they are represented in the system by metadata. Typical IR systems compute a numeric score on how well each document in the database matches the query, and rank the objects according to this value. The top-ranking documents from the system are then shown to the user. The process may then be iterated if the user wishes to refine the query.
Based partly on (Manning et al., 2009), common IR methods consist of several categories:
 Boolean retrieval, where a document either matches a query or it does not.
 Algebraic approaches to retrieval, where models are used to represent documents and queries as vectors, matrices, or tuples. The similarity of the query vector and document vector is represented as a scalar value. This value can be used to produce a list of documents that are rank-ordered for a query. Common models and methods include vector space model, topic-based vector space model, extended Boolean model, and latent semantic analysis.
 Probabilistic approaches to retrieval, where the process of IR is treated as a probabilistic inference. Similarities are computed as probabilities that a document is relevant for a given query, and the probability value is then used as the score in ranking documents. Common models and methods include binary Independence model, probabilistic relevance model with the BM25 relevance function, methods of inference with uncertainty, probabilistic, language modeling, and the technique of latent Dirichlet allocation.
 Feature-based approaches to retrieval, where documents are viewed as vectors of values of feature functions. Principled methods of "learning to rank" are devised to combine these features into a single relevance score. Feature functions are arbitrary functions of document and query, and as such Feature-based approaches can easily incorporate almost any other retrieval model as just yet another feature.
Deep learning applications to IR are rather recent. The approaches in the literature so far belong mostly to the category of feature-based approaches. The use of deep networks is mainly for extracting semantically meaningful features for subsequent document ranking stages. We will review selected studies in the recent literature in the remainder of this chapter below.
9.2 Semantic Hashing with Deep Autoencoders for
Document Indexing and Retrieval
Here we discuss the "semantic hashing" approach for the application of deep autoencoders to document indexing and retrieval as published in (Salakhutdinov and Hinton, 2007; Hinton and Salakhutdinov, 2010). It is shown that the hidden variables in the final layer of a DBN not only are easy to infer after using an approximation based on feed-forward propagation, but they also give a better representation of each document, based on the word-count features, than the widely used latent semantic analysis and the traditional TF-IDF approach for information retrieval. Using the compact code produced by deep autoencoders, documents are mapped to memory addresses in such a way that semantically similar text documents are located at nearby addresses to facilitate rapid document retrieval. The mapping from a word-count vector to its compact code is highly efficient, requiring only a matrix multiplication and a subsequent sigmoid function evaluation for each hidden layer in the encoder part of the network.
A deep generative model of DBN is exploited for the above purpose as discussed in (Hinton and Salakhutdinov, 2010). Briefly, the lowest layer of the DBN represents the word-count vector of a document and the top layer represents a learned binary code for that document. The top two layers of the DBN form an undirected associative memory and the remaining layers form a Bayesian(also called belief) network with directed, top-down connections. This DBN, composed of a set of stacked RBMs as we reviewed in Chapter 5, produces a feed-forward "encoder" network that converts word-count vectors to compact codes. By composing the RBMs in the opposite order, a "decoder" network is constructed that maps compact code vectors into reconstructed word-count vectors. Combining the encoder and decoder, one obtains a deep autoencoder (subject to further fine-tuning as discussed in Chapter 4) for document coding and subsequent retrieval.
After the deep model is trained, the retrieval process starts with mapping each query into a 128-bit binary code by performing a forward pass through the model with thresholding. Then the Hamming distance between the query binary code and all the documents' 128-bit binary codes, especially those of the "neighboring" documents defined in the semantic space, are computed extremely efficiently. The efficiency is accomplished by looking up the neighboring bit vectors in the hash table. The same idea as discussed here for coding text documents for information retrievalhas been explored for audio document retrieval and speech feature coding problems with some initial exploration reported in (Deng et al., 2010), discussed in Chapter 4 in detail.
9.3 Deep-Structured Semantic Modeling (DSSM) for
Document Retrieval
Here we discuss the more advanced and recent approach to large-scale document retrieval (Web search) based on a specialized deep architecture, called deep-structured semantic model or deep semantic similarity model (DSSM), as published in (Huang et al., 2013), and its convolutional version (C-DSSM), as published in (Shen et al., 2014).
Modern search engines retrieve Web documents mainly by matching keywords in documents with those in a search query. However, lexical matching can be inaccurate due to the fact that a concept is often expressed using different vocabularies and language styles in documents and queries.
Latent semantic models are able to map a query to its relevant documents at the semantic level where lexical-matching often fails (Manning et al., 2009). These models address the language discrepancy between Web documents and search queries by grouping different terms that occur in a similar context into the same semantic cluster. Thus, a query and a document, represented as two vectors in the lower-dimensional semantic space, can still have a high similarity even if they do not share any term. Probabilistic topic models such as probabilistic latent semantic models and latent Dirichlet allocation models have been proposed for semantic matching to partially overcome such difficulties. However, the improvement on IR tasks has not been as significant as originally expected because of two main factors: 1) most state-of-the-art latent semantic models are based on linear projection, and thus are inadequate in capturing effectively the complex semantic properties of documents; and 2) these models are often trained in an unsupervised manner using an objective function that is only loosely coupled with the evaluation metric for the retrieval task. In order to improve semantic matching for IR, two lines of research have been conducted to extend the above latent semantic models. The first is the semantic hashing approach reviewed in Section 9.1 above in this chapter based on the use of deep autoencoders (Salakhutdinov and Hinton, 2007; Hinton and Salakhutdinov, 2010). While the hierarchical semantic structure embedded in the query and the document can be extracted via deep learning, the deep learning approach used for their models still adopts an unsupervised learning method where the model parameters are optimized for the reconstruction of the documents rather than for differentiating the relevant documents from the irrelevant ones for a given query. As a result, the deep neural network models do not significantly outperform strong baseline IR models that are based on lexical matching. In the second line of research, click-through data, which consists of a list of queries and the corresponding clicked documents, is exploited for semantic modeling so as to bridge the language discrepancy between search queries and Web documents in recent studies (Gao et al., 2010, 2011). These models are trained on click-through data using objectives that tailor to the document ranking task. However, these click-through-based models are still linear, suffering from the issue of expressiveness. As a result, these models need to be combined with the keyword matching models (such as BM25) in order to obtain a significantly better performance than baselines.
The DSSM approach reported in (Huang et al., 2013) aims to combine the strengths of the above two lines of work while overcoming their weaknesses. It uses the DNN architecture to capture complex semantic properties of the query and the document, and to rank a set of documents for a given query. Briefly, a non-linear projection is performed first to map the query and the documents to a common semantic space. Then, the relevance of each document given the query is calculated as the cosine similarity between their vectors in that semantic space. The DNNs are trained using the click-through data such that the conditional likelihood of the clicked document given the query is maximized. Different from the previous latent semantic models that are learned in an unsupervised fashion, the DSSM is optimized directly for Web document ranking, and thus gives superior performance. Furthermore, to deal with large vocabularies in Web search applications, a new word hashing method is developed, through which the high-dimensional term vectors of queries or documents are projected to low-dimensional letter based n-gram vectors with little information loss.
Figure 9.1 illustrates the DNN part in the DSSM architecture. The DNN is used to map highdimensional sparse text features into low-dimensional dense features in a semantic space. The first hidden layer, with 30k units, accomplishes word hashing. The word-hashed features are then projected through multiple layers of non-linear projections. The final layer's neural activities in this DNN form the feature in the semantic space.
Figure 9.1. The DNN component of the DSSM architecture for computing semantic features. The DNN uses multiple layers to map high-dimensional sparse text features, for both Queries and Documents into low-dimensional dense features in a semantic space. [after (Huang et al., 2013), @CIKM]
To show the computational steps in the various layers of the DNN in Figure 9.1, we denote 𝑥 as the input term vector, 𝑦 as the output vector, 𝑙𝑖, 𝑖 = 1, …, 𝑁 − 1, as the intermediate hidden layers, 𝑊𝑖 as the i-th projection matrix, and 𝑏𝑖 as the 𝑖-th bias vector, we have
𝑙1 = 𝑊1𝑥, 𝑙𝑖 = 𝑓(𝑊𝑖𝑙𝑖−1 + 𝑏𝑖), 𝑖 > 1 𝑦 = 𝑓(𝑊𝑁𝑙𝑁−1 + 𝑏𝑁)where 𝑡𝑎𝑛ℎ function is used at the output layer and the hidden layers 𝑙𝑖, 𝑖 = 2, …, 𝑁 − 1:
𝑓(𝑥) = 1 − 𝑒−2𝑥
1 + 𝑒−2𝑥
The semantic relevance score between a query 𝑄 and a document 𝐷 can then be computed as the consine distance
𝑅(𝑄, 𝐷) = cosine(𝑦𝑄, 𝑦𝐷) = 𝑦𝑄
𝑇𝑦𝐷
‖𝑦𝑄‖‖𝑦𝐷‖where 𝑦𝑄 and 𝑦𝐷 are the concept vectors of the query and the document, respectively. In Web search, given the query, the documents can be sorted by their semantic relevance scores.
Learning of the DNN weights 𝑊𝑖 and 𝑏𝑖 shown in Figure 9.1 is an important contribution of the study of (Huang et al., 2013). Compared with the DNNs used in speech recognition where the targets or labels of the training data are readily available, the DNN in the DSSM does not have such label information well defined. That is, rather than using the common cross entropy or mean square errors as the training objective function, IR-centric loss functions need to be developed in order to train the DNN weights in the DSSM using the available data such as click-through logs.
The click-through logs consist of a list of queries and their clicked documents. A query is typically more relevant to the documents that are clicked on than those that are not. This weak supervision information can be exploited to train the DSSM. More specifically, the weight matrices in the DSSM, 𝑊𝑖, is learned to maximize the posterior probability of the clicked documents given the queries
𝑃(𝐷|𝑄) = exp(𝛾𝑅(𝑄, 𝐷))
∑ exp(𝛾𝑅(𝑄, 𝐷′))
𝐷′ 𝑫defined on the semantic relevance score 𝑅(𝑄, 𝐷) between the Query (Q) and the Document (D), where 𝛾 is a smoothing factor set empirically on a held-out data set, and 𝑫 denotes the set of candidate documents to be ranked. Ideally, 𝑫 should contain all possible documents, as in the maximum mutual information training for speech recognition where all possible negative candidates may be considered (He and Deng, 2008). However in this case 𝑫 is of Web scale and thus is intractable in practice. In the implementation of DSSM learning described in (Huang et al., 2013), a subset of the negative candidates are used, following the common practice adopted in MCE (Minimum Classification Error) training in speech recognition (Chengalvarayan and Deng, 1998; Yu and Deng, 2007; Yu et al., 2008; Fu et al., 2007). In other words, for each query and clicked-document pair, denoted by (𝑄, 𝐷+) where 𝑄 is a query and 𝐷+ is the clicked document, the set of D is approximated by including 𝐷+ and only four randomly selected unclickeddocuments, denote by {𝐷𝑗
−; 𝑗 = 1, …,4}. In the study reported in (Huang et al., 2013), no significant difference was found when different sampling strategies were used to select the unclicked documents.
With the above simplification the DSSM parameters are estimated to maximize the approximate likelihood of the clicked documents given the queries across the training set
𝐿(Λ) = log
∏
𝑃(𝐷+|𝑄)(𝑄,𝐷+,𝐷𝑗
−)where Λ denotes the parameter set of the DNN weights {𝑊𝑖} in the DSSM. In Figure 9.2, we show the overall DSSM architecture that contains several DNNs. All these DNNs share the same weights but take different documents (one positive and several negatives) as inputs when training the DSSM parameters. Details of the gradient computation of this approximate loss function with respect to the DNN weights tied across documents and queries can be found in (Huang et al., 2013) and are not elaborated here.
Figure 9.2. Architectural illustration of the DSSM for document retrieval (from Huang et al., 2013). All DNNs shown have shared weights. A set of n documents are shown here to illustrate the random negative sampling discussed in the text for simplifying the training procedure for the DSSM. [after (Huang et al., 2013), @CIKM]
Most recently, the DSSM described above has been extended to its convolutional version, or CDSSM (Shen et al., 2014). In the C-DSSM, semantically similar words within context are projected to vectors that are close to each other in the contextual feature space through a convolutional structure. The overall semantic meaning of a sentence is found to be determined by a few key words in the sentence, and thus the C-DSSM uses an additional max pooling layer to extract the most salient local features to form a fixed-length global feature vector. The global feature vectoris then fed to the remaining nonlinear DNN layer(s) to map it to a point in the shared semantic space.
The convolutional neural network component of the C-DSSM is shown in Figure 9.3, where a window size of three is illustrated for the convolutional layer. The overall C-DSSM architecture is similar to the DSSM architecture shown in Figure 9.2 except that the fully-connected DNNs are replaced by the convolutional neural networks with locally-connected tied weights and additional max-pooling layers. The model component shown in Figure 9.3 contains 1) a word hashing layer to transform words into letter-tri-gram count vectors in the same way as the DSSM; 2) a convolutional layer to extract local contextual features for each context window; 3) a max-pooling layer to extract and combine salient local contextual features to form a global feature vector; and 4) a semantic layer to represent the high-level semantic information of the input word sequence.
The main motivation for using the convolutional structure in the C-DSSM is its ability to map a variable-length word sequence to a low-dimensional vector in a latent semantic space. Unlike most previous models that treat a query or a document as a bag of words, a query or a document in the C-DSSM is viewed as a sequence of words with contextual structures. By using the convolutional structure, local contextual information at the word n-gram level is modeled first. Then, salient local features in a word sequence are combined to form a global feature vector. Finally, the high-level semantic information of the word sequence is extracted to form a global vector representation.
Like the DSSM just described, the C-DSSM is also trained on click-through data by maximizing the conditional likelihood of the clicked documents given a query using the back-propagation algorithm.
30k
30k
30k
30k
30k
300 max max... max
Word hashing layer: ft
Convolutional layer: ht
Max pooling layer: v
Semantic layer: y
<s> w1 w2 … wT <s>
Word sequence: xt
Word hashing matrix: Wf
Convolution matrix: Wc
Max pooling operation
Affine projection matrix: Ws
Figure 9.3. The convolutional neural network component of the C-DSSM, with the window size of three is illustrated for the convolutional layer. [after (Shen et al., 2014), @WWW].
9.4 Use of Deep Stacking Networks for Information
Retrieval
In parallel with the IR studies reviewed above, the deep stacking network (DSN) discussed in Chapter 6 has also been explored recently for IR with insightful results (Deng et al., 2013c). The experimental results suggest that the classification error rate using the binary decision of "relevant" vs. "non-relevant" from the DSN, which is closely correlated with the DSN training objective, is also generally correlated well with the NDCG (normalized discounted cumulative gain) as the most common IR quality measure. The exception is found in the region of high IR quality.
As described in Chapter 6, the simplicity of the DSN's training objective, the mean square error(MSE), drastically facilitates its successful applications to image recognition, speech recognition, and speech understanding. The MSE objective and classification error rate have been shown to be well correlated in these speech or image applications. For information retrieval (IR) applications, however, the inconsistency between the MSE objective and the desired objective (e.g., NDCG) is much greater than that for the above classification-focused applications. For example, the NDCG as a desirable IR objective function is a highly non-smooth function of the parameters to be learned, with a very different nature from the nonlinear relationship between MSE and classification error rate. Thus, it is of interest to understand to what extent the NDCG is reasonably well correlated with classification rate or MSE where the relevance level in IR is used as the DSN prediction target. Further, can the advantage of learning simplicity in the DSN be applied to improve IR quality measures such as the NDCG? Our experimental results presented in (Deng et al., 2013c) provide largely positive answers to both of the above questions. In addition, special care that need to be taken in implementing DSN learning algorithms when moving from classification to IR applications are addressed.
The IR task in the experiments of (Deng et al., 2013c) is the sponsored search related to ad placement. In addition to the organic web search results, commercial search engines also provide supplementary sponsored results in response to the user's query. The sponsored search results are selected from a database pooled by advertisers who bid to have their ads displayed on the search result pages. Given an input query, the search engine will retrieve relevant ads from the database, rank them, and display them at the proper place on the search result page; e.g., at the top or right hand side of the web search results. Finding relevant ads to a query is quite similar to common web search. For instance, although the documents come from a constrained database, the task resembles typical search ranking that targets on predicting document relevance to the input query.
The experiments conducted for this task are the first with the use of deep learning techniques(based on the DSN architecture) on the ad-related IR problem. The preliminary results from the experiments are the close correlation between the MSE as the DSN training objective with the NDCG as the IR quality measure over a wide NDCG range.
CHAPTER 10
SELECTED APPLICATIONS IN OBJECT
RECOGNITION AND COMPUTER VISION
Over the past two years or so, tremendous progress has been made in applying deep learning techniques to computer vision, especially in the field of object recognition. The success of deep learning in this area is now commonly accepted by the computer vision community. It is the second area in which the application of deep learning techniques is successful, following the speech recognition area as we reviewed and analyzed in Chapters 2 and 7.
Excellent surveys on the recent progress of deep learning for computer vision are available in the NIPS-2013 tutorial (https://nips.cc/Conferences/2013/Program/event.php?ID=4170 with video recording at http://research.microsoft.com/apps/video/default.aspx?id=206976&l=i ) and slides at http://cs.nyu.edu/~fergus/presentations/nips2013_final.pdf, and also in the CVPR-2012 tutorial(http://cs.nyu.edu/~fergus/tutorials/deep_learning_cvpr12). The reviews provided in this chapter below are based partly on these tutorials, in connection with the earlier deep learning material in this book. Another excellent source which this chapter draws upon is the most recent Ph.D. thesis on the topic of deep learning for computer vision (Zeiler, 2014).
Over many years, object recognition in computer vision has been relying on hand-designed features such as SIFT (Scale Invariant Feature Transform) and HOG (Histogram of Oriented
Gradients), akin to the reliance of speech recognition on hand-designed features such as MFCC and PLP. However, features like SIFT and HOG only capture low-level edge information. The design of features to effectively capture mid-level information such as edge intersections or highlevel representation such as object parts becomes much more difficult. Deep learning aims to overcome such challenges by automatically learning hierarchies of visual features in both unsupervised and supervised manners directly from data. The review below categorizes the many deep learning methods applied to computer vision into two classes: 1) unsupervised feature learning where the deep learning is used to extract features only, which may be subsequently fed to relatively simple machine learning algorithm for classification or other tasks; and 2) supervised learning methods where end-to-end learning is adopted to jointly optimize feature extractor and classifier components of the full system when large amounts of labeled training data are available.
10.1 Unsupervised or Generative Feature Learning
When labeled data are relatively scarce, unsupervised learning algorithms have been shown to learn useful visual feature hierarchies. In fact, prior to the demonstration of remarkable successes of CNN architectures with supervised learning in the 2012 ImageNet competition, much of the work in applying deep learning methods to computer vision had been on unsupervised feature learning. The original unsupervised deep autoencoder that exploits DBN pre-training wasdeveloped and demonstrated by Hinton and Salakhutdinov (2006) with success on the image recognition and dimensionality reduction (coding) tasks of MNIST with only 60,000 samples in the training set; see details of this task in http://yann.lecun.com/exdb/mnist/ and an analysis in(Deng, 2012). It is interesting to note that the gain of coding efficiency using the DBN-based autoencoder on the image data over the conventional method of principal component analysis as demonstrated in (Hinton and Salakhutdinov, 2006) is very similar to the gain reported in (Deng et al., 2010) and described in Chapter 4 of this book on the speech data over the traditional technique of vector quantization. Also, Nair and Hinton (2009) developed a modified DBN where the toplayer model uses a third-order Boltzmann machine. This type of DBN is applied to the NORB database – a three-dimensional object recognition task. An error rate close to the best published result on this task is reported. In particular, it is shown that the DBN substantially outperforms shallow models such as SVMs. In (Tang and Eliasmith, 2010), two strategies to improve the robustness of the DBN are developed. First, sparse connections in the first layer of the DBN are used as a way to regularize the model. Second, a probabilistic de-noising algorithm is developed.
Both techniques are shown to be effective in improving robustness against occlusion and random noise in a noisy image recognition task. DBNs have also been successfully applied to create compact but meaningful representations of images (Tarralba et al., 2008) for retrieval purposes.
On this large collection image retrieval task, deep learning approaches also produced strong results.
Further, the use of a temporally conditional DBN for video sequence and human motion synthesis were reported in (Taylor et al., 2007). The conditional RBM and DBN make the RBM and DBN weights associated with a fixed time window conditioned on the data from previous time steps.
The computational tool offered in this type of temporal DBN and the related recurrent networks may provide the opportunity to improve the DBN-HMMs towards efficient integration of temporal-centric human speech production mechanisms into DBN-based speech production model.
Deep learning methods have a rich family, including hierarchical probabilistic and generative models (neural networks or otherwise). One most recent example of this type developed and applied to facial expression datasets is the stochastic feed-forward neural networks that can be learned efficiently and that can induce a rich multiple-mode distribution in the output space not possible with the standard, deterministic neural networks (Tang and Salakhutdinov, 2013). In
Figure 10.1, we show the architecture of a typical stochastic feed-forward neural network with four hidden layers with mixed deterministic and stochastic neurons (left) used to model multimode distributions illustrated on the right. The stochastic network here is a deep, directed graphical model, where the generation process starts from input x, a neural face, and generates the output y, the facial expression. In face expression classification experiments, the learned unsupervised hidden features generated from this stochastic network are appended to the image pixels and helped to obtain superior accuracy to the baseline classifier based on the conditional RBM/DBN (Taylor et al., 2007).
Figure 10.1. Left: A typical architecture of the stochastic feed-forward neural network with four hidden layers. Right: Illustration of how the network can produce a distribution with two distinct modes and use them to represent two or more different facial expressions y given a neutral face x.
[after (Tang and Salakhutdinov, 2013), @NIPS].
Perhaps the most notable work in the category of unsupervised deep feature learning for computer vision (prior to the recent surge of the work on CNNs) is that of (Le et al., 2012), a nine-layer locally connected sparse autoencoder with pooling and local contrast normalization. The model has one billion connections, trained on the dataset with 10 million images downloaded from the Internet. The unsupervised feature learning methods allow the system to train a face detector without having to label images as containing a face or not. And the control experiments show that this feature detector is robust not only to translation but also to scaling and out-of-plane rotation.
Another set of popular studies on unsupervised deep feature learning for computer vision are based on deep sparse coding models (Yu et al., 2011; Lin et al., 2011). This type of deep models produced state-of-the-art accuracy results on the ImageNet object recognition tasks prior to the rise of the CNN architectures armed with supervised learning to perform joint feature learning and classification, which we turn to now.
10.2 Supervised Feature Learning and Classification
The origin of the applications of deep learning to object recognition tasks can be traced to the convolutional neural networks (CNNs) in the early 90s; see a comprehensive overview in (LeCun et al., 1998). The CNN-based architectures in the supervised learning mode have captured intense interest in computer vision since October 2012 shortly after the ImageNet competition results were released (http://www.image-net.org/challenges/LSVRC/2012/ ). This is mainly due to the huge recognition accuracy gain over competing approaches when large amounts of labeled data are available to efficiently train large CNNs using GPU-like high-performance computing platforms.
Just like DNN-based deep learning methods have outperformed previous state-of-the-art approaches in speech recognition in a series of benchmark tasks including phone recognition, large-vocabulary speech recognition, noise-robust speech recognition, and multi-lingual speech recognition, CNN-based deep learning methods have demonstrated the same in a set of computervision benchmark tasks including category-level object recognition, object detection, and semantic segmentation.
The basic architecture of the CNN described in (LeCun et al., 1998) is shown in Figure 10.1. To incorporate the relative invariance of the spatial relationship in typical image pixels with respect to the location, the CNN uses a convolutional layer with local receptive fields and with tied filter weights, much like 2-dimensional FIR filters in image processing. The output of the FIR filters is then passed through a nonlinear activation function to create activation maps, followed by another nonlinear pooling (labeled as "subsampling" in Figure 10.2) layer that reduces the data rate while providing invariance to slightly different input images. The output of the pooling layer are subject to a few fully-connected layers as in the DNN discussed in earlier chapters. The whole architecture above is also called the deep CNN in the literature.
Figure 10.2. The original convolutional neural network that is composed of multiple alternating convolution and pooling layers followed by fully connected layers. [after (LeCun, et al., 1998), @IEEE].
Deep models with convolution structure such as CNNs have been found effective and have been in use in computer vision and image recognition since 90's (Bengio and LeCun, 1995; LeCun et al., 1998; Jarrett et al., 2009; Kavukcuoglu et al., 2010; Ciresan et al., 2012; Krizhevsky et al., 2012). The most notable advance was achieved in the 2012 ImageNet LSVRC competition, in which the task is to train a model with 1.2 million high-resolution images to classify unseen images to one of the 1000 different image classes. On the test set consisting of 150k images, the deep
CNN approach described in (Krizhevsky et al., 2012) achieved the error rates considerably lower than the previous state-of-the-art. Very large deep-CNNs are used, consisting of 60 million weights, and 650,000 neurons, and five convolutional layers together with max-pooling layers. Additional two fully-connected layers as in the DNN described previously are used on top of the CNN layers.
Although all the above structures were developed separately in earlier work, their best combination accounted for major part of the success. See the overall architecture of the deep CNN system in Figure 10.3. Two additional factors contribute to the final success. The first is a powerful regularization technique called "dropout"; see details in (Hinton et al., 2012a) and a series of further analysis and improvement in (Baldi and Sadowski, 2013; McAllester, 2013; Frey and Ba, 2013; Wager et al., 2013). Applications of the same "dropout" techniques are also successful for some speech recognition tasks (Deng et al., 2013; Dahl et al., 2013). The second factor is the use of non-saturating neurons or rectified linear units (ReLU) that compute 𝑓(𝑥) = max(𝑥, 0), whichsignificantly speeds up the overall training process especially with efficient GPU implementation.
This deep-CNN system achieved a winning top-5 test error rate of 15.3% using extra training data from ImageNet Fall 2011 release, or 16.4% using only supplied training data in ImageNet-2012, significantly lower than 26.2% achieved by the second-best system which combines scores from many classifiers using a set of hand-crafted features such as SIFT and Fisher vectors. See details in http://www.image-net.org/challenges/LSVRC/2012/oxford_vgg.pdf about the best competing method. It is noted, however, that the Fisher-vector-encoding approach has recently been extended by Simonyan et al. (2013) via stacking in multiple layers to form deep Fisher networks, which achieve competitive results with deep CNNs at a smaller computational learning cost.
Figure 10.3. The architecture of the deep-CNN system which won the 2012 ImageNet competition by a large margin over the second-best system and the state of the art by 2012. [after (Krizhevsky et al., 2012), @NIPS].
The state of the art performance demonstrated in (Krizhevsky et al., 2012) using the deep-CNN approach is further improved by another significant margin during 2013, using a similar approach but with bigger models and larger amounts of training data. A summary of top-5 test error rates from 11 top-performing teams participating in the 2013 ImageNet ILSVRC competition is shown in Figure 10.4, with the best result of the 2012 competition shown to the right most as the baseline.
Here we see rapid error reduction on the same task from the lowest pre-2012 error rate of 26.2%(non-neural networks) to 15.3% in 2012 and further to 11.2% in 2013, both achieved with deepCNN technology. It is also interesting to observe that all major entries in the 2013 ImageNet
ILSVRC competition is based on deep learning approaches. For example, the Adobe system shown in Figure 10.4 is based on the deep-CNN reported in (Krizhevsky et al., 2012) including the use of dropout. The network architecture is modified to include more filters and connections. At test time, image saliency is used to obtain 9 crops from original images, which are combined with the standard five multiview crops. The NUS system uses a non-parametric, adaptive method to combine the outputs from multiple shallow and deep experts, including deep-CNN, kernel, and GMM methods. The VGG system is described in (Simonyan et al., 2013) and uses a combination of the deep Fisher vector network and the deep-CNN. The ZF system is based on a combination of a large CNN with a range of different architectures. The choice of architectures was assisted by visualization of model features using a deconvolutional network as described by Zeiler et al. (2011), Zeiler and Fergus (2013), and Zeiler (2014). The CognitiveVision system uses an image classification scheme based on a DNN architecture. The method is inspired by cognitive psychophysics about how the human vision system first learns to classify the basic-level categories and then learns to classify categories at the subordinate level for fine-grained object recognition.
Finally, the best-performing system called Clarifai in Figure 10.4 is based on a large and deep
CNN with dropout regularization. It augments the amount of training data by down-sampling images to 256 pixels. The system contains a total of 65M parameters. Multiple such models were averaged together to further boost performance. The main novelty is to use the visualization technique based on the deconvolutional networks as described in (Zeiler et. al, 2011; Zeiler, 2014) to identify what makes the deep model perform well, based on which a powerful deep architecture was chosen. See more details of these systems in http://www.imagenet.org/challenges/LSVRC/2013/results.php.
Figure 10.4. Summary results of ImageNet Large Scale Visual Recognition Challenge 2013(ILSVRC2013), representing the state-of-the-are performance of object recognition systems. Data source: http://www.image-net.org/challenges/LSVRC/2013/results.php
While the deep CNN has demonstrated remarkable classification performance on object recognition tasks, there has been no clear understanding of why they perform so well until recently.
Zeiler and Fergus (2013) conducted research to address just this issue, and then used the gained understanding to further improve the CNN systems, which yielded excellent performance as shown in Figure 10.4 with labels "ZF" and "Clarifai". A novel visualization technique is developed that gives insight into the function of intermediate feature layers of the deep CNN. The technique also sheds light onto the operation of the full network acting as a classifier. The visualization technique is based on a deconvolutional network, which maps the neural activities in intermediate layers ofthe original convolutional network back to the input pixel space. This allows the researchers to example what input pattern originally caused a given activation in the feature maps. Figure 10.5(the top portion) illustrates how a deconvolutional network is attached to each of its layers, thereby providing a closed loop back to image pixels as the input to the original CNN. The information flow in this closed loop is as follows. First, an input image is presented to the deep CNN in a feedforward manner so that the features at all layers are computed. To examine a given CNN activation, all other activations in the layer are set to zero and the feature maps are passed as input to the attached deconvolutional network's layer. Then, successive operations, opposite to the feedforward computation in the CNN, are carried out including unpooling, rectifying, and filtering.
This allows the reconstruction of the activity in the layer beneath that gave rise to the chosen activation. These operations are repeated until input layer is reached. During unpooling, noninvertibility of the max pooling operation in the CNN is resolved by an approximate inverse, where the locations of the maxima within each pooling region are recorded in a set of "switch" variables.
These switches are used to place the reconstructions from the layer above into appropriate locations, preserving the structure of the stimulus. This procedure is shown at the bottom portion of Figure
In addition to the deep-CNN architecture described above, the DNN architecture has also been shown to be highly successful in a number of computer vision tasks (Ciresan, et al., 2010, 2011, 2012, 2012a). We have not found in the literature on direct comparisons among the CNN, DNN, and other related architectures on the identical tasks.
Figure 10.5. The top portion shows how a deconvolutional network's layer (left) is attached to a corresponding CNN's layer (right). The deconvolutional network reconstructs an approximate version of the CNN features from the layer below. The bottom portion is an illustration of the unpooling operation in the deconvolutional network, where "Switches" are used to record the location of the local max in each pooling region during pooling in the CNN. [after (Zeiler and Fergus, 2013), @arXiv].
Finally, the most recent study on supervised learning for computer vision shows that the deep CNN architecture is not only successful for object/image classification discussed earlier in this section but also successful for objection detection in the whole images (Girshick et al., 2013). The detection task is substantially more complex than the classification task.
As a brief summary of this chapter, deep learning has made huge inroads into computer vision, soon after its success in speech recognition discussed in Chapter 7. So far, it is the supervised learning paradigm based on the deep CNN architecture and the related classification techniques that are making the greatest impact, showcased by the ImageNet competition results from 2012and 2013. These methods can be used for not only objection recognition but also many other computer vision tasks. There has been some debate as to the reasons for the success of these CNNbased deep learning methods, and about their limitations. Many questions are still open as to how these methods can be tailored to certain computer vision applications and how to scale up the models and training data. Finally, we discussed a number of studies on unsupervised and generative approaches of deep learning to computer vision and image modeling problems in the earlier part of this chapter. Their performance has not been competitive with the supervised learning approach on object recognition tasks with ample training data. To achieve long term and ultimate success in computer vision, it is likely that unsupervised learning will be needed. To this end, many open problems in unsupervised feature learning and deep learning need to be addressed and much more research need to be carried out.
CHAPTER 11
SELECTED APPLICATIONS IN MULTIMODAL AND MULTI-TASK LEARNING
Multi-task learning is a machine learning approach that learns to solve several related problems at the same time, using a shared representation. It can be regarded as one of the two major classes of transfer learning or learning with knowledge transfer, which focuses on generalizations across distributions, domains, or tasks. The other major class of transfer learning is adaptive learning, where knowledge transfer is carried out in a sequential manner, typically from a source task to a target task (Deng and Li, 2013). Multi-modal learning is a closely related concept to multi-task learning, where the learning domains or "tasks" cut across several modalities for human-computer interactions or other applications embracing a mixture of textual, audio/speech, touch, and visual information sources.
The essence of deep learning is to automate the process of discovering effective features or representations for any machine learning task, including automatically transferring knowledge from one task to another concurrently. Multi-task learning is often applied to conditions where no or very little training data are available for the target task domain, and hence is sometimes called zero-shot or one-shot learning. It is evident that difficult multi-task leaning naturally fits the paradigm of deep learning or representation learning where the shared representations and statistical strengths across tasks (e.g., those involving separate modalities of audio, image, touch, and text) is expected to greatly facilitate many machine learning scenarios under low- or zeroresource conditions. Before deep learning methods were adopted, there had been numerous efforts in multi-modal and multi-task learning. For example, a prototype called MiPad for multi-modal interactions involving capturing, leaning, coordinating, and rendering a mix of speech, touch, and visual information was developed and reported in (Huang et al., 2001; Deng et al., 2002). And in(Zheng et al., 2004; Subramanya et al., 2005), mixed sources of information from multiple-sensory microphones with separate bone-conductive and air-born paths were exploited to de-noise speech.
These early studies all used shallow models and learning methods and achieved worse than desired performance. With the advent of deep learning, it is hopeful that the difficult multi-modal learning problems can be solved with eventual success to enable a wide range of practical applications. In this chapter, we will review selected applications in this area, organized according to different combinations of more than one modalities or learning tasks. Much of the work reviewed here is on-going research, and readers should expect follow-up publications in the future.
11.1 Multi-Modalities: Text and Image
The underlying mechanism for potential effectiveness of multi-modal learning involving text and image is the common semantics associated with the text and image. The relationship between thetext and image may come, for example, from the text annotations of an image (as the training data for a multi-modal learning system). If the related text and image share the same representation in a common semantic space, the system can generalize to the unseen situation where either text or image is unavailable. It can thus be naturally used for zero-shot learning for image or text. In other words, multi-modality learning can use text information to help image/visual recognition, and vice versa. Exploiting text information for image/visual recognition constitutes most of the work done in this space, which we review in this section below.
The deep architecture, called DeViSE (Deep Visual-Semantic Embedding) and developed by
Frome et al. (2013), is a typical example of the multi-modal learning where text information is used to improve the image recognition system, especially for performing zero-shot learning. Image recognition systems are often limited in their ability to scale to large number of object categories, due in part to the increasing difficulty of acquiring sufficient training data with text labels as the number of image categories grows. The multi-modal DeViSE system is aimed to leverage text data to train the image models. The joint model is trained to identify image classes using both labeled image data and the semantic information learned from unannotated text. An illustration of the DeViSE architecture is shown in the center portion of Figure 10.1. It is initialized with the parameters pre-trained at the lower layers of two models: the deep-CNN for image classification in the left portion of the figure and the text embedding model in the right portion of the figure. The part of the deep CNN, labeled "core visual model" in Figure 10.1, is further learned to predict the target word-embedding vector using a projection layer labeled "transformation" and using a similarity metric. The loss function used in training adopts a combination of dot-product similarity and max-margin, hinge rank loss. The former is the un-normalized version of the cosine loss function used for training the DSSM model in (Huang et al., 2013) as described in Chapter 9.3.
The latter is similar to the earlier joint image-text model called WSABIE (Web Scale Annotation by Image Embedding developed by Weston, et al. (2010, 2011)). The results show that the information provided by text improves zero-shot image predictions, achieving good hit rates (close to 15%) across thousands of the labels never seen by the image model.
Figure 11.1. Illustration of the multi-modal DeViSE architecture. The left portion is an image recognition neural network with a softmax output layer. The right portion is a skip-gram text modelproviding word embedding vectors; see Chapter 8.2 and Figure 8.3 for details. The center is the joint deep image-text model of DeViSE, with the two Siamese branches initialized by the image and word embedding models below the softmax layers. The layer labeled "transformation" is responsible for mapping the outputs of the image (left) and text (right) branches into the same semantic space. [after (Frome, et al., 2013), @NIPS].
The earlier WSABIE system as described in (Weston, et al. 2010, 2011) adopted a shallow architecture and trained a joint embedding model of both images and labels. Rather than using deep architectures to derive the highly nonlinear image (as well as text-embedding) feature vectors as in DeViSE, the WSABIE uses simple image features and a linear mapping to arrive at the joint embedding space. Further, it uses an embedding vector for each possible label. Thus, unlike
DeViSE, WSABIE could not generalize to new classes.
It is also interesting to compare the DeViSE architecture of Figure 11.1 with the DSSM architecture of Figure 9.2 in Chapter 9. The branches of "Query" and "Documents" in DSSM are analogous to the branches of "image" and "text-label" in DeViSE. Both DeViSE and DSSM use the objective function related to cosine distance between two vectors for training the network weights in an end-to-end fashion. One key difference, however, is that the two sets of inputs to the DSSM are both text (i.e., "Query" and "Documents" designed for IR), and thus mapping "Query" and "Documents" to the same semantic space is conceptually more straightforward compared with the need in DeViSE for mapping from one modality (image) to another (text). Another key difference is that the generalization ability of DeViSE to unseen image classes comes from computing text embedding vectors for many unsupervised text sources (i.e., with no image counterparts) that would cover the text labels corresponding to the unseen classes. The generalization ability of the DSSM over unseen words, however, is derived from a special coding scheme for words in terms of their constituent letters.
The DeViSE architecture has inspired a more recent method, which maps images into the semantic embedding space via convex combination of embedding vectors for the text label and the image classes (Norouzi et al., 2013). Here is the main difference. DeViSE replaces the last, softmax layer of a CNN image classifier with a linear transformation layer. The new transformation layer is then trained together with the lower layers of the CNN. The method in (Norouzi et al., 2013) is much simpler --- keeping the softmax layer of the CNN while not training the CNN. For a test image, the CNN first produces top N-best candidates. Then, the convex combination of the corresponding
N embedding vectors in the semantic space is computed. This gives a deterministic transformation from the outputs of the softmax classifier into the embedding space. This simple multi-modal learning method is shown to work very well on the ImageNet zero-shot learning task.
Another thread of studies separate from but related to the above work on multi-modal learning involving text and image have centered on the use of multi-modal embeddings, where data from multiple sources with separate modalities of text and image are projected into the same vector space. For example, Socher and Fei-Fei (2010) project words and images into the same space using kernelized canonical correlation analysis. Socher et al. (2013b) map images to single-word vectors so that the constructed multi-modal system can classify images without seeing any examples of the class, i.e., zero-shot learning similar to the capability of DeViSE. The most recent work by
Socher et al. (2013c) extends their earlier work from single-word embeddings to those of phrasesand full-length sentences. The mechanism for mapping sentences instead of the earlier single words into the multi-modal embedding space is derived from the power of the recursive neural network described in Socher et al. (2013a) as summarized in Chapter 8.2, and its extension with dependency tree.
In addition to mapping text to image (or vice versa) into the same vector space or to creating the joint image/text embedding space, multi-modal learning for text and image can also be cast in the framework of language models. In (Kiros, et al., 2013), a model of natural language is made conditioned on other modalities such as image as the focus of the study. This type of multi-modal language model is used to 1) retrieve images given complex description queries, 2) retrieve phrase descriptions given image queries, and 3) generate text conditioned on images. Word representations and image features are jointly learned by training the multi-modal language model together with a convolutional network. An illustration of the multi-modal language model is shown in Figure 11.2.
Figure 11.2. A multi-modal language model (of the type of log-bilinear) which predicts a word conditioned not only on the previous words in the sentence but also on images. The model operates on word embedding vectors. [after (Kiros et al., 2013), @NIPS].
11.2 Multi-Modalities: Speech and Image
Ngiam et al. (2011) propose and evaluate an application of deep networks to learn features over audio/speech and image/video modalities. They demonstrate cross-modality feature learning, where better features for one modality (e.g., image) is learned when multiple modalities (e.g., speech and image) are present at feature learning time. A bi-modal deep autoencoder architecture for separate audio/speech and video/image input channels are shown in Figure 11.3. The essence of this architecture is to use a shared, middle layer to represent both types of modalities. This is a straightforward generalization from the single-modal deep autoencoder for speech shown in Figure
4.1 of Chapter 4 to bi-modal counterpart. The authors further show how to learn a shared audio and video representation, and evaluate it on a fixed task, where the classifier is trained with audioonly data but tested with video-only data and vice versa. The work concludes that deep learningarchitectures are generally effective in learning multimodal features from unlabeled data and in improving single modality features through cross modality information transfer. One exception is the cross-modality setting using the CUAVE dataset. The results presented in (Ngiam et al., 2011) show that learning video features with both video and audio outperforms that with only video data.
However, the same paper also shows that a model of (Papandreou, 2009) in which a sophisticated signal processing technique for extracting visual features, together with the uncertaintycompensation method developed originally from robust speech recognition (Deng et al., 2005), gives the best classification accuracy in the cross-modal learning task, beating the features derived from the generative deep architecture designed for this task.
Figure 11.3. The architecture of a deep denoising autoencoder for multi-modal audio/speech and visual features. [after (Ngiam et al., 2011), @ICML].
While the deep generative architecture for multimodal learning described in (Ngiam et al., 2011) is based on non-probabilistic autoencoder neural nets, a probabilistic version based on deep
Boltzmann machine (DBM) has appeared more recently for the same multimodal application. In(Srivastava and Salakhutdinov, 2012), a DBM is used to extract a unified representation integrating separate modalities, useful for both classification and information retrieval tasks. Rather than using the "bottleneck" layers in the deep autoencoder to represent multimodal inputs, here a probability density is defined on the joint space of multimodal inputs, and states of suitably defined latent variables are used for the representation. The advantage of this probabilistic formulation, possibly lacking in the traditional deep autoencoder, is that the missing modality's information can be filled in naturally by sampling from its conditional distribution. More recent work on autoencoders(Bengio et al., 2013, 2013b) shows the capability of generalized denoising autoencoders in carrying out sampling, thus they may overcome the earlier problem of filling-in the missing modality's information. For the bi-modal data consisting of image and text, the multimodal DBMwas shown to slightly outperform the traditional version of the deep multimodal autoencoder as well as multimodal DBN in classification and information retrieval tasks. No results on the comparisons with the generalized version of deep autoencoders has been reported but may appear soon.
The several architectures discussed so far in this chapter for multi-modal processing and learning can be regarded as special cases of more general multi-task learning and transfer learning (Caruana, 1997; Bengio et al., 2013). Transfer learning, encompassing both adaptive and multi-task learning, refers to the ability of a learning architecture and technique to exploit common hidden explanatory factors among different learning tasks. Such exploitation permits sharing of aspects of diverse types of input data sets, thus allowing the possibility of transferring knowledge across seemingly different learning tasks. As argued in (Bengio et al., 2013), the learning architecture shown in Figure 11.4 and the associated learning algorithms have an advantage for such tasks because they learn representations that capture underlying factors, a subset of which may be relevant for each particular task. We will discuss a number of such multi-task learning applications in the remainder of this chapter that are confined with a single modality of speech, natural language processing, or image domain.
Figure 11.4. A DNN architecture for multitask learning that is aimed to discover hidden explanatory factors shared among three tasks A, B, and C. [after (Bengio, 2013), @IEEE].
11.3 Multi-Task Learning within the Speech, NLP or Image Domain
Within the speech domain, one most interesting application of multi-task learning is multi-lingual or cross-lingual speech recognition, where speech recognition for different languages is considered as different tasks. Various approaches have been taken to attack this rather challenging acoustic modeling problem for speech recognition, where the difficulty lies in the lack of transcribed speech data due to economic considerations in developing speech recognition systems for all languages in the world. Cross-language data sharing and data weighing are common and useful approaches for the GMM-HMM system (Lin et al., 2009). Another successful approach for the GMM-HMMis to map pronunciation units across languages either via knowledge-based or data-driven methods(Yu et al., 2009b). But they are much inferior to the DNN-HMM approach which we now summarize.
In recent papers of (Huang et al., 2013; Deng et al., 2013a) and (Heigold et al., 2013), two research groups independently developed closely related DNN architectures with multi-task learning capabilities for multilingual speech recognition. See Figure 11.5 for an illustration of this type of architecture. The idea behind these architectures is that the hidden layers in the DNN, when learned appropriately, serve as increasingly complex feature transformations sharing common hidden factors across the acoustic data in different languages. The final softmax layer representing a loglinear classifier makes use of the most abstract feature vectors represented in the top-most hidden layer. While the log-linear classifier is necessarily separate for different languages, the feature transformations can be shared across languages. Excellent multilingual speech recognition results are reported, far exceeding the earlier results using the GMM-HMM based approaches (e.g., Lin et al., 2009; Yu et al., 2009b). The implication of this set of work is significant and far reaching.
It points to the possibility of quickly building a high-performance DNN-based system for a new language from an existing multilingual DNN. This huge benefit would require only a small amount of training data from the target language, although having more data would further improve the performance. This multitask learning approach can reduce the need for the unsupervised pretraining stage, and can train the DNN with much fewer epochs. Extension of this set of work would be to efficiently build a language-universal speech recognition system. Such a system cannot only recognize many languages and improve the accuracy for each individual language, but also expand the languages supported by simply stacking softmax layers on the DNN for new languages.
Figure 11.5. A DNN architecture for multilingual speech recognition. [after (Huang et al., 2013), @IEEE].
A closely related DNN architecture, as shown in Figure 11.6, with multitask learning capabilities was also recently applied to another acoustic modeling problem --- learning joint representations for two separate sets of acoustic data (Li et al., 2012; Deng et al., 2013a). The set that consists of the speech data with 16kHz sampling rate is of wideband and high quality, which is often collected from increasingly popular smart phones under the voice search scenario. Another, narrowband data set has a lower sampling rate of 8kHz, often collected using the telephony speech recognition systems.
Figure 11.6. A DNN architecture for speech recognition trained with mixed-bandwidth acoustic data with 16-kHz and 8-kHz sampling rates; [after (Li et al., 2012), @IEEE].
As a final example of multi-task learning within the speech domain, let us consider phone recognition and word recognition as separate "tasks". That is, phone recognition results are used not for producing text outputs but for language-type identification or for spoken document retrieval.
Then, the use of pronunciation dictionary in almost all speech systems can be considered as multitask learning that share the tasks of phone recognition and word recognition. More advanced frameworks in speech recognition have pushed this direction further by advocating the use of even finer units of speech than phones to bridge the raw acoustic information of speech to semantic content of speech via a hierarchy of linguistic structure. These atomic speech units include "speech attributes" in the detection-based and knowledge-rich modeling framework for speech recognition, whose accuracy has been significantly boosted recently by the use of deep learning methods (Yu et al., 2012a; Siniscalchi et al., 2013, 2013a).
Within the natural language processing domain, the best known example of multi-task learning is the comprehensive studies reported in (Collobert and Weston, 2008; Collobert et al., 2011), where a range of separate "tasks" of part-of-speech tagging, chunking, named entity tagging, semantic role identification, and similar-word identification in natural language processing are attackedusing a common representation of words and a unified deep learning approach. A summary of these studies can be found in Chapter 8.2.
Finally, within the domain of image/vision as a single modality, deep learning has also been found effective in multi-task learning. Srivastava and Salakhutdinov (2013) present a multi-task learning approach based on hierarchical Bayesian priors in a DNN system applied to various image classification data sets. The priors are combined with a DNN, which improves discriminative learning by encouraging information sharing among tasks and by discovering similar classes among which knowledge is transferred. More specifically, methods are developed to jointly learn to classify images and a hierarchy of classes, such that "poor classes", for which there are relatively few training examples, can benefit from similar "rich classes", for which more training examples are available. This work can be considered as an excellent instance of learning output representations, in addition to learning input representation of the DNN as the focus of nearly all deep learning work reported in the literature.
As another example of multi-task learning within the single-modality domain of image, Ciresan et al. (2012b) applied the architecture of deep CNNs to character recognition tasks for Latin and for
Chinese. The deep CNNs trained on Chinese characters are shown to be easily capable of recognizing uppercase Latin letters. Further, learning Chinese characters is accelerated by first pretraining a CNN on a small subset of all classes and then continuing to train on all classes.
CHAPTER 12
EPILOGUES
This book first presented a brief history of deep learning (focusing on speech recognition) and developed a categorization scheme to analyze the existing deep networks in the literature into unsupervised (many of which are generative), supervised, and hybrid classes. The deep autoencoder, the DSN (as well as many of its variants), and the DBN-DNN or pre-trained DNN architectures, one in each of the three classes, are discussed and analyzed in detail, as they appear to be popular and promising approaches based on the authors' personal research experiences.
Applications of deep learning in five broad areas of information processing are also reviewed, including speech and audio (Chapter 7), natural language modeling and processing (Chapter 8), information retrieval (Chapter 9), object recognition and computer vision (Chapter 10), and multimodal and multi-task learning (Chapter 11). There are other interesting yet non-mainstream applications of deep learning, which are not covered in this book. For interested readers, please consult recent papers on the applications of deep learning to optimal control in (Levine, 2013), to reinforcement learning in (Mnih, et al, 2013), to malware classification in (Dahl et al., 2013a), to compressed sensing in (Palangi et al., 2013), to recognition confidence prediction in (Huang et al., 2013a), to acoustic-articulatory inversion mapping in (Uria et al., 2011), to emotion recognition from video in (Kahou et al., 2013), to emotion recognition from speech in (Li et al., 2013; Le and Mower, 2013), to spoken language understanding in (Mesnil et al., 2013; Yao et al., 2013; Tur et al., 2012), to speaker recognition in (Vasilakakis et al., 2013; Stafylakis et al., 2012), to languagetype recognition in (Diez, 2013), to dialogue state tracking for spoken dialogue systems in(Henderson et al., 2013; Deng et al., 2013a), to automatic voice activity detection in (Zhang and Wu, 2013), to speech enhancement in (Xu et al., 2014), to voice conversion in (Nakashika et al., 2013), and to single-channel source separation in (Grais et al., 2013; Weng et al.. 2014).
The literature on deep learning is vast, mostly coming from the machine learning community. The signal processing community embraced deep learning only within the past four years or so (starting around end of 2009) and the momentum is growing fast ever since. This book is written mainly from the signal processing perspective. Beyond just surveying the existing deep learning work, a classificatory scheme based on the architectures and on the nature of the learning algorithms is developed, and an analysis and discussion with concrete examples are presented. This will hopefully provide insight for readers to better understand the capability of the various deep learning systems discussed in the book, the connection among different but similar deep learning methods, and how to design proper deep learning algorithms under different circumstances.
Throughout this review, the important message is conveyed that building and learning deep hierarchies of features are highly desirable. We have discussed the difficulty of learning parameters in all layers of deep networks in one shot due to optimization difficulties that need to be better understood. The unsupervised pre-training method in the hybrid architecture of the DBNDNN, which we reviewed in detail in Chapter 5, appears to have offered a useful, albeit empirical, solution to poor local optima in optimization and to regularization for the deep model containingmassive parameters even though a solid theoretical foundation is still lacking. The effectiveness of the pre-training method, which was one factor stimulating the interest in deep learning by the signal processing community in 2009 via collaborations between academic and industrial researchers, is most prominent when the supervised training data are limited.
Deep learning is an emerging technology. Despite the empirical promising results reported so far, much more work needs to be carried out. Importantly, it has not been the experience of deep learning researchers that a single deep learning technique can be successful for all classification tasks. For example, while the popular learning strategy of generative pre-training followed by discriminative fine-tuning seems to work well empirically for many tasks, it failed to work for some other tasks that have been explored (e.g., language identification or speaker recognition; unpublished by the authors of this book). For these tasks, the features extracted at the generative pre-training phase seem to describe the underlying speech variations well but do not contain sufficient information to distinguish between different languages. A learning strategy that can extract discriminative yet also invariant features is expected to provide better solutions. This idea has also been called "disentangling" and is developed further in (Bengio et al., 2013a). Further, extracting discriminative features may greatly reduce the model size needed in many of the current deep learning systems. Domain knowledge such as what kind of invariance is useful for a specific task in hand (e.g., vision, speech, or natural language) and what kind of regularization in terms of parameter constraints is key to the success of applying deep learning methods. Moreover, new types of DNN architectures and learning beyond the several popular ones discussed in this book are currently under active development by the deep learning research community (e.g., Bengio et al., 2013a; Deng et al., 2013b), holding the promise to improve the performance of deep learning models in more challenging applications in signal processing and in artificial intelligence.
Recent published work showed that there is vast room to improve the current optimization techniques for learning deep architectures (Martens, 2010; Le et al., 2011; Martens and Sutskever, 2011; Dean et al., 2012; Sutskever, 2013; Sainath et al., 2013; Wright et al., 2013). To what extent pre-training is essential to learning the full set of parameters in deep architectures is currently under investigation, especially when very large amounts of labeled training data are available, reducing or even obliterating the need for model regularization. Some preliminary results have been discussed in this book and in (Ciresan et al., 2010; Yu et al. 2010; Seide et al. 2011; Hinton et al., 2012).
In recent years, machine learning is becoming increasingly dependent on large-scale data sets. For instance, many of the recent successes of deep learning as discussed in this book have relied on the access to massive data sets and massive computing power. It would become increasingly difficult to explore the new algorithmic space without the access to large, real-world data sets and without the related engineering expertise. How well deep learning algorithms behave would depend heavily on the amount of data and computing power available. As we showed with speech recognition examples, a deep learning algorithm that appears to be performing not so remarkably on small data sets can begin to perform considerably better when these limitations are removed, one of main reasons for the recent resurgence in neural network research. As an example, the DBN pre-training that ignited a new era of (deep) machine learning research appears unnecessary if enough data and computing power are used.
As a consequence, effective and scalable parallel algorithms are critical for training deep models with large data sets, as in many common information processing applications such as speech recognition and machine translation. The popular mini-batch stochastic gradient technique is known to be difficult to parallelize over computers. The common practice nowadays is to use
GPGPUs to speed up the learning process, although recent advance in developing asynchronous stochastic gradient descent learning has shown promises by using large-scale CPU clusters (e.g.
Le et al., 2012; Dean et al., 2012) and GPU clusters (Coates et al., 2013). In this interesting computing architecture, many different replicas of the DNN compute gradients on different subsets of the training data in parallel. These gradients are communicated to a central parameter server that updates the shared weights. Even though each replica typically computes gradients using parameter values not immediately updated, stochastic gradient descent is robust to the slight errors this has introduced. To make deep learning techniques scalable to very large training data, theoretically sound parallel learning and optimization algorithms together with novel architectures need to be further developed (e.g., Bottou and LeCun, 2004; Chen et al., 2012; Seide et al., 2014;
Dean et al., 2012; Hutchinson et al., 2013; Sutskever, 2013; Bengio et al., 2013). Optimization methods specific to speech recognition problems may need to be taken into account in order to push speech recognition advances to the next level (e.g., Wright et al., 2013; Cardinal et al., 2013;
Heigold et al., 2013a).
One major barrier to the application of DNNs and related deep models is that it currently requires considerable skill and experience to choose sensible values for hyper-parameters such as the learning rate schedule, the strength of the regularizer, the number of layers and the number of units per layer, etc. Sensible values for one hyper-parameter may depend on the values chosen for other hyper-parameters and hyper-parameter tuning in DNNs is especially expensive. Some interesting methods for solving the problem have been developed recently, including random sampling(Bergstra et al., 2012) and Bayesian optimization procedure (Snoek et al., 2012). Further research is needed in this important area.
This book, mainly in Chapters 8 and 11 on natural language and multi-modal applications, has touched on some recent work on using deep learning methods to do reasoning, moving beyond the topic of more straightforward pattern recognition using supervised, unsupervised or hybrid learning methods to which much of this book has been devoted to. In principle, since deep networks are naturally equipped with distributed representations (rf. Table 3.1) using their layerwise collections of units for coding relations and coding entities, concepts, events, topics, etc., they can potentially perform powerful reasoning over structures, as argued in various historical publications as well as recent essays (e.g., Hinton, 1990; Smolensky, 1990; Pollack, 1990; Plate, 1995; Prince and Smolensky, 1997; Bottou, 2013). While initial explorations on this capability of deep networks have recently appeared in the literature, as reviewed in Chapters 8 and 11, much research is needed. If successful, this new type of deep learning "machine" will open up many novel and exciting applications in applied artificial intelligence as a "thinking brain". We expect growing work of deep learning in this area, full of new challenges, in the future.
Further, solid theoretical foundations of deep learning need to be established in a myriad of aspects.
As an example, the success of deep learning in unsupervised learning has not been demonstrated as much as for supervised learning; yet the essence and major motivation of deep learning lie right in unsupervised learning for automatically discovering data representation. The issues involveappropriate objectives for learning effective feature representations and the right deep learning architectures/algorithms for distributed representations to effectively disentangle the hidden explanatory factors of variation in the data. Unfortunately, a majority of the successful deep learning techniques have so far dealt with unstructured or "flat" classification problems. For example, although speech recognition is a sequential classification problem by nature, in the most successful and large-scale systems, a separate HMM is used to handle the sequence structure and the DNN is only used to produce the frame-level, unstructured posterior distributions. Recent proposals have called for and investigated moving beyond the "flat" representations and incorporating structures in both the deep learning architectures and input and output representations (Socher, 2012; Deng, 2013; Srivastava and Salakhutdinov, 2013; Graves et al., Finally, deep learning researchers have been advised by neuroscientists to seriously consider a broader set of issues and learning architectures so as to gain insight into biologically plausible representations in the brain that may be useful for practical applications (e.g., Olshausen, 2012).
How can computational neuroscience models about hierarchical brain structure help improve engineering deep learning architectures? How may the biologically feasible learning styles in the brain (e.g., Hinton, 2003; Xie and Seung, 2003) help design more effective and more robust deep learning algorithms? All these issues and those discussed in this chapter will need intensive research in order to further push the frontier of deep learning.
BIBLIOGRAPHY
Abdel-Hamid, O., Mohamed, A., Jiang, H., and G. Penn, "Applying convolutional neural networks concepts to hybrid NN-HMM model for speech recognition," Proc. ICASSP, 2012.
Abdel-Hamid, O., Deng, L., and Yu. D. "Exploring convolutional neural network structures and optimization for speech recognition," Interspeech, 2013.
Abdel-Hamid, O., Deng, L., Yu. D., Jiang, H. "Deep segmental neural networks for speech recognition," Proc. Interspeech, 2013a.
Acero, A., Deng, L., Kristjansson, T., and Zhang, J. "HMM adaptation using vector Taylor series for noisy speech recognition," Proc. Interspeech, 2000.
Alain, G. and Bengio, Y. "What Regularized Autoencoders Learn from the Data Generating
Distribution," Proc. International Conference on Learning Representations (ICLR), 2013.
Anthes, G. "Deep learning comes of age," Communications of the ACM, Vol. 56 No. 6, pp. 1315, June 2013.
Arel, I., Rose, C., and Karnowski, T. "Deep Machine Learning - A New Frontier in Artificial
Intelligence," IEEE Computational Intelligence Mag., vol. 5, pp. 13-18, November, 2010.
Aslan, O., Cheng, H., Schuurmans, D., and Zhang, X. "Convex two-layer modeling," Proc. NIPS, Ba, J. and Frey, B. "Adaptive dropout for training deep neural networks," Proc. NIPS, 2013.
Baker, J., Deng, L., Glass, J., Khudanpur, S., Lee, C.-H., Morgan, N., and O'Shaughnessy, D.
"Research developments and directions in speech recognition and understanding," IEEE Sig.
Proc. Mag., vol. 26, no. 3, May 2009, pp. 75-80.
Baker, J., Deng, L., Glass, J., Khudanpur, S., Lee, C.-H., Morgan, N., and O'Shaughnessy, D.
"Updated MINS report on speech recognition and understanding," IEEE Sig. Proc. Mag., vol.
26, no. 4, July 2009a.
Baldi, P. and Sadowski, P. "Understanding Dropout," Proc. NIPS, 2013.
Battenberg, E., Schmidt, E., and Bello, J. Deep learning for music, special session at ICASSP(http://www.icassp2014.org/special_sections.html#SS8), 2014.
Batternberg, E. and Wessel, D. "Analyzing drum patterns using conditional deep belief networks,"
Proc. ISMIR, 2012.
Bell, P., Swietojanski, P., and Renals, S. "Multi-level adaptive networks in tandem and hybrid
ASR systems", Proc. ICASSP, 2013.
Bengio, Y., Yao, L., Alain, G., and Vincent, P. "Generalized denoising autoencoders as generative models," Proc. NIPS, 2013.
Bengio, Y. "Deep learning of representations: Looking forward," in: Statistical Language and Speech Processing, pp. 1--37, Springer, 2013.
Bengio, Y., Boulanger, N., and Pascanu, R. "Advances in optimizing recurrent networks," Proc.
ICASSP, 2013.
Bengio, Y., Courville, A., and Vincent, P. "Representation learning: A review and new perspectives," IEEE Trans. PAMI, vol. 38, pp. 1798-1828, 2013a.
Bengio, Y., Thibodeau-Laufer, E., and Yosinski, J. "Deep generative stochastic networks trainable by backprop," arXiv 1306:1091, 2013b.
Bengio, Y. "Deep Learning of Representations for Unsupervised and Transfer Learning," JMLR
Workshop and Conference Proceedings, vol. 27, pp. 17-37, 2012.
Bengio, Y. "Learning deep architectures for AI," in Foundations and Trends in Machine Learning, Vol. 2, No. 1, 2009, pp. 1-127.
Bengio, Y., Lamblin, P., Popovici, D., and Larochelle, H. "Greedy Layer-Wise Training of Deep
Networks," Proc. NIPS, 2006.
Bengio, Y., Ducharme, R., Vincent, P., and Jauvin, C. "A Neural Probabilistic Language Model,"
Journal of Machine Learning Research, vol. 3, pp. 1137-1155, 2003.
Bengio, Y., Ducharme, R., Vincent, P. and Jauvin, C. "A neural probabilistic language model,"
Proc. NIPS, 2000.
Bengio, Y., De Mori, R., Flammia, G., and Kompe, R. "Global Optimization of a Neural NetworkHidden Markov Model Hybrid," IEEE Transactions on Neural Networks, vol. 3, pp. 252-259, Bengio, Y. Artificial Neural Networks and Their Application to Sequence Recognition, Ph.D.
Thesis, McGill University, Montreal, Canada, 1991.
Bergstra, J. and Bengio, Y. "Random search for hyper-parameter optimization," J. Machine
Learning Research," Vol. 3, pp. 281-305, 2012.
Bottou, L. and LeCun. Y. "Large scale online learning," Proc. NIPS, 2004.
Bilmes, J. "Dynamic graphical models," IEEE Signal Processing Mag., vol. 33, pp. 29–42, 2010.
Bilmes, J. and Bartels, C. "Graphical model architectures for speech recognition," IEEE Signal
Processing Mag., vol. 22, pp. 89–100, 2005.
Bordes, A., Weston, J., Collobert, R., and Bengio, Y. "Learning Structured Embeddings of Knowledge Bases," Proc. AAAI, 2011.
Bordes, A., Weston, J., Collobert, R., and Bengio, Y. "Learning Structured Embeddings of Knowledge Bases," Proc. AAAI, 2011.
Bordes, A., Glorot, X., Weston, J., and Bengio, Y. "A semantic matching energy function for learning with multi-relational data --- Application to word-sense disambiguation," Machine
Learning, May, 2013.
Boulanger-Lewandowski, N., Bengio, Y., and Vincent, P. "Audio Chord Recognition with
Recurrent Neural Networks," Proc. ISMIR, 2013.
Bourlard, H. and Morgan, N., Connectionist Speech Recognition: A Hybrid Approach, Norwell, MA: Kluwer, 1993.
Bottou, L. "From machine learning to machine reasoning: an essay," Journal of Machine Learning
Research, Vol. 14, pp. 3207-3260, 2013.
Bouvrie, J. "Hierarchical Learning: Theory with Applications in Speech and Vision," Ph.D. thesis, MIT, 2009.
Bridle, J., Deng, L., Picone, J., Richards, H., Ma, J., Kamm, T., Schuster, M., Pike, S., and Reagan, R. "An investigation of segmental hidden dynamic models of speech coarticulation for automatic speech recognition," Final Report for 1998 Workshop on Language Engineering, CLSP, Johns Hopkins, 1998.
Cardinal, P.; Dumouchel, P.; Boulianne, G., "Large Vocabulary Speech Recognition on Parallel
Architectures," IEEE Transactions on Audio, Speech, and Language Processing, vol.21, no.11, pp.2290,2300, Nov. 2013.
Caruana, R. "Multitask Learning," Machine Learning, Vol. 28, pp. 41-75, 1997.
Chen, J. and Deng, L. "A Primal-Dual Method for Training Recurrent Neural Networks
Constrained by the Echo-State Property", arXiv:1311.6091, pp. 1-16, 2013.
Chen, X., Eversole, A., Li, G., Yu, D. and Seide, F., "Pipelined Back-Propagation for ContextDependent Deep Neural Networks", Proc. Interspeech 2012.
Chengalvarayan, R. and Deng, L. "Speech Trajectory Discrimination using the Minimum
Classification Error Learning," IEEE Transactions on Speech and Audio Processing, vol. 6, no. 6, pp. 505-515, 1998.
Chengalvarayan R. and Deng, L. "HMM-based speech recognition using state-dependent, discriminatively derived transforms on Mel-warped DFT features," IEEE Transactions on
Speech and Audio Processing, pp. 243-256, 1997.
Chengalvarayan R. and Deng, L. "Use of generalized dynamic feature parameters for speech recognition," IEEE Transactions on Speech and Audio Processing, pp. 232-242, 1997a.
Cho, Y. and Saul, L. "Kernel methods for deep learning," Proc. NIPS, pp. 342–350, 2009.
Ciresan, D., Meier, U., and Schmidhuber, J. "Multi-column deep neural networks for image classification." Proc. CVPR, 2012.
Ciresan, D., Giusti, A., Gambardella, L., and Schmidhuber, J. "Deep neural networks segment neuronal membranes in electron microscopy images," Proc. NIPS, 2012a.
Ciresan, D. C., Meier, U., & Schmidhuber, J. "Transfer learning for Latin and Chinese characters with deep neural networks," Proc. IJCNN, 2012b.
Ciresan, D., Meier, U., Masci, J., and Schmidhuber, J. "A committee of neural networks for traffic sign classification," Proc. IJCNN, 2011.
Ciresan, D., Meier, U., Gambardella, L., and Schmidhuber, J. "Deep, Big, Simple Neural Nets for
Handwritten Digit Recognition," Neural Computation, December 2010.
Coates, A., Huval, B., Wang, T., Wu, D., Ng, A., and Catanzaro, B. "Deep Learning with COTS
HPC," Proc. ICML, 2013.
Cohen, W. and R. V. de Carvalho. "Stacked sequential learning," Proc. IJCAI, pp. 671–676, 2005.
Collobert, R. "Deep learning for efficient discriminative parsing," Proc. AISTATS, 2011.
Collobert, R. and Weston J. "A unified architecture for natural language processing: Deep neural networks with multitask learning," Proc. ICML, 2008.
Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., and Kuksa, P. "Natural language processing (almost) from scratch," J. Machine Learning Research, Vo. 12, pp. 2493Dahl, G, Sainath, T., and Hinton, G. "Improving deep neural networks for LVCSR using rectified linear units and dropout," Proc. ICASSP, 2013.
Dahl, G., Stokes, J., Deng, L., and Yu, D. "Large-Scale Malware Classification Using Random
Projections and Neural Networks," Proc. ICASSP, 2013a.
Dahl, G., Yu, D., Deng, L., and Acero, A. "Context-dependent, pre-trained deep neural networks for large vocabulary speech recognition," IEEE Trans. Audio, Speech, & Language Proc., Vol.
20 (1), pp. 30-42, January 2012.
Dahl, G., Yu, D., Deng, L., and Acero, A. "Context-dependent DBN-HMMs in large vocabulary continuous speech recognition," Proc. ICASSP, 2011.
Dahl, G., Ranzato, M., Mohamed, A. and Hinton, G. "Phone recognition with the mean-covariance restricted Boltzmann machine," Proc. NIPS, vol. 23, 2010, 469-477.
Dean, J., Corrado, G., R. Monga, K. Chen, M. Devin, Q. Le, M. Mao, M. Ranzato, A. Senior, P.
Tucker, Yang, K., and Ng, A. "Large Scale Distributed Deep Networks," Proc. NIPS, 2012.
Demuynck, K. and Triefenbach, F. "Porting concepts from DNNs back to GMMs," Proc. ASRU
Deng, L. and Chen, J. "Sequence Classification Using the High-Level Features Extracted from
Deep Neural Networks," Proc. ICASSP, 2014.
Deng, L. "Design and Learning of Output Representations for Speech Recognition," NIPS
Workshop on Learning Output Representations, December 2013.
Deng, L. "A tutorial survey of architectures, algorithms, and applications for deep learning,"
APSIPA Transactions on Signal and Information Processing, 2013.
Deng, L. and Li, X. "Machine learning paradigms in speech recognition: An overview," IEEE
Trans. Audio, Speech, & Language, vol. 21, pp. 1060 – 1089, May 2013.
Deng, L., Abdel-Hamid, O., and Yu, D. "A deep convolutional neural network using heterogeneous pooling for trading acoustic invariance with phonetic confusion," Proc.
ICASSP, 2013.
Deng, L., Li, J., Huang, K., Yao, D. Yu, F. Seide, M. Seltzer, G. Zweig, X. He, J. Williams, Y.
Gong, and A. Acero. "Recent advances in deep learning for speech research at Microsoft,"
Proc. ICASSP, 2013a.
Deng, L., Hinton, G., and Kingsbury, B. "New types of deep neural network learning for speech recognition and related applications: An overview," Proc. ICASSP, 2013b.
Deng, L., He, X., and Gao, J. "Deep stacking networks for information retrieval," Proc. ICASSP, 2013c.
Deng, L. "The MNIST database of handwritten digit images for machine learning research," IEEE
Signal Processing Magazine, no. 141-142, November 2012.
Deng, L., Tur, G, He, X, and Hakkani-Tur, D. "Use of kernel deep convex networks and end-toend learning for spoken language understanding," Proc. IEEE Workshop on Spoken Language
Technologies, December 2012.
Deng, L., Yu, D., and Platt, J. "Scalable stacking and learning for building deep architectures,"
Proc. ICASSP, 2012a.
Deng, L., Hutchinson, B., and Yu, D. "Parallel training of deep stacking networks," Proc.
Interspeech, 2012b.
Deng, L. "An Overview of Deep-Structured Learning for Information Processing," Proceedings of Asian-Pacific Signal & Information Processing Annual Summit and Conference (APSIPAASC), October 2011.
Deng, L. and Yu, D. "Deep Convex Network: A scalable architecture for speech pattern classification," Proc. Interspeech, 2011.
Deng, L., Seltzer, M., Yu, D., Acero, A., Mohamed, A., and Hinton, G. "Binary coding of speech spectrograms using a deep autoencoder," Proc. Interspeech, 2010.
Deng, L., Yu, D., and Hinton, G. "Deep Learning for Speech Recognition and Related
Applications" NIPS Workshop, 2009.
Deng, L. and Yu, D. "Use of differential cepstra as acoustic features in hidden trajectory modeling for phonetic recognition," Proc. ICASSP, 2007.
Deng, L. Dynamic Speech Models – Theory, Algorithm, and Application, Morgan & Claypool, December 2006.
Deng, L., Yu, D. and Acero, A. "Structured speech modeling," IEEE Trans. on Audio, Speech and Language Processing, vol. 14, no. 5, pp. 1492-1504, September 2006
Deng, L., Yu, D. and Acero, A. "A bidirectional target filtering model of speech coarticulation:
Two-stage implementation for phonetic recognition," IEEE Transactions on Audio and Speech
Processing, vol. 14, no. 1, pp. 256-265, January 2006a.
Deng, L., Wu, J., Droppo, J., and Acero, A. "Dynamic Compensation of HMM Variances Using the Feature Enhancement Uncertainty Computed From a Parametric Model of Speech
Distortion," IEEE Transactions on Speech and Audio Processing, vol. 13, no. 3, pp. 412–421, Deng, L. and Huang, X.D. "Challenges in Adopting Speech Recognition, Communications of the ACM, vol. 47, no. 1, pp. 11-13, January 2004.
Deng, L. and O'Shaughnessy, D. SPEECH PROCESSING – A Dynamic and OptimizationOriented Approach, Marcel Dekker, 2003.
Deng, L. "Switching dynamic system models for speech articulation and acoustics," in Mathematical Foundations of Speech and Language Processing, pp. 115–134. SpringerVerlag, New York, 2003.
Deng, L., Wang, K., Acero, A., Hon, Droppo, J., Boulis, C., Wang, Y., Jacoby, D., Mahajan, M., Chelba, C., and Huang, X. "Distributed speech processing in MiPad's multimodal user interface," IEEE Transactions on Speech and Audio Processing, vol. 10, no. 8, pp. 605–619, Deng, L., Acero, A., Jiang, L., Droppo, J., and Huang, X. "High performance robust speech recognition using stereo training data," Proc. ICASSP, 2001.
Deng, L. and Ma, J. "Spontaneous speech recognition using a statistical coarticulatory model for the vocal tract resonance dynamics," J. Acoust. Soc. Am., vol. 108, pp. 3036-3048, 2000.
Deng, L. "Computational Models for Speech Production," in Computational Models of Speech
Pattern Processing, pp. 199-213, Springer Verlag, 1999.
Deng, L. "A dynamic, feature-based approach to the interface between phonology and phonetics for speech modeling and recognition,' Speech Communication, vol. 24, no. 4, pp. 299-323, Deng L. and Aksmanovic, M. "Speaker-independent phonetic classification using hidden Markov models with state-conditioned mixtures of trend functions," IEEE Trans. Speech and Audio
Processing, vol. 5, pp. 319-324, 1997.
Deng, L., Ramsay, G., and Sun, D. "Production models as a structural basis for automatic speech recognition," Speech Communication, vol. 33, no. 2-3, pp. 93–111, Aug 1997.
Deng, L. and Sameti, H. "Transitional speech units and their representation by regressive Markov states: Applications to speech recognition," IEEE Transactions on speech and audio processing, vol. 4, no. 4, pp. 301–306, July 1996.
Deng, L., Aksmanovic, M., Sun, D., and Wu, J. "Speech recognition using hidden Markov models with polynomial regression functions as nonstationary states," IEEE Transactions on Speech and Audio Processing, vol. 2, no. 4, pp. 507-520, 1994.
Deng L. and Sun, D. "A statistical approach to automatic speech recognition using the atomic speech units constructed from overlapping articulatory features," Journal of the Acoustical
Society of America, vol. 85, no. 5, pp. 2702-2719, 1994.
Deng, L., Hassanein, K., and Elmasry, M. "Analysis of correlation structure for a neural predictive model with application to speech recognition," Neural Networks, vol. 7, no. 2, pp. 331-339, 1994a.
Deng, L. "A stochastic model of speech incorporating hierarchical nonstationarity," IEEE
Transactions on Speech and Audio Processing, vol. 1, no. 4, pp. 471-475, 1993.
Deng, L. "A generalized hidden Markov model with state-conditioned trend functions of time for the speech signal," Signal Processing, vol. 27, no. 1, pp. 65–78, 1992.
Deng. L. and Erler, K. "Structural design of a hidden Markov model based speech recognizer using multi-valued phonetic features: Comparison with segmental speech units," Journal of the Acoustical Society of America, vol. 92, no. 6, pp. 3058-3067, 1992.
Deng, L. Lennig, M. Gupta, V., Seitz, F., and Mermelstein, P., and Kenny, P. "Phonemic hidden
Markov models with continuous mixture output densities for large vocabulary word recognition," IEEE Transactions on Signal Processing, vol. 39, no. 7, pp. 1677-1681, 1991.
Deng, L. Lennig, M., Seitz, F., and Mermelstein, P. "Large vocabulary word recognition using context-dependent allophonic hidden Markov models," Computer Speech and Language, vol.
4, no. 4, pp. 345-357, 1990.
Deselaers, T., Hasan, S., Bender, O. and Ney, H. "A deep learning approach to machine transliteration," Proc. 4th Workshop on Statistical Machine Translation, pp. 233–241, Athens, Greece, March 2009.
Diez, A. "Automatic language recognition using deep neural networks," Thesis, Universidad
Autonoma de Madrid, SPAIN, September 2013.
Dognin, P. and Goel, V. "Combining stochastic average gradient and Hessian-free optimiztion for sequence training of deep neural networks," Proc. ASRU, 2013.
Erhan, D., Bengio, Y., Courvelle, A., Manzagol, P., Vencent, P., and Bengio, S. "Why does unsupervised pre-training help deep learning?" J. Machine Learning Research, pp. 201-208, Fernandez, R., Rendel, A., Ramabhadran, B., and Hoory, R. "F0 contour prediction with a deep belief network-Gaussian process hybrid Model," Proc. ICASSP, pp. 6885-6889, 2013.
Fine, S., Singer, Y. and Tishby, N. "The hierarchical hidden Markov model: Analysis and applications," Machine Learning, vol. 32, p. 41-62, 1998.
Frome, A., Corrado, G., Shlens, J., Bengio, S., Dean, J., Ranzato, M., and Mikolov, T. "DeViSE:
A Deep Visual-Semantic Embedding Model," Proc. NIPS, 2013.
Fu, Q., He, X., and Deng, L. "Phone-Discriminating Minimum Classification Error (P-MCE)
Training for Phonetic Recognition," Proc. Interspeech, 2007.
Gales, M. "Model-based approaches to handling uncertainty," in Robust Speech Recognition of Uncertain or Missing Data: Theory and Application, pp. 101–125. Springer, 2011.
Gao, J., He, X., Yih, W. and Deng, L. "Learning semantic representations for the phrase translation model," Proc. NIPS Workshop on Deep Learning, December, 2013.
Gao, J., He, X., Yih, W. and Deng, L. "Learning Semantic Representations for the Phrase
Translation Model," MSR-TR-2013-88, September 2013.
Gao, J., Toutanova, K., Yih., W-T. "Clickthrough-based latent semantic models for web search,"
Proc. SIGIR, 2011.
Gao, J., He, X., and Nie, J-Y. "Clickthrough-based translation models for web search: From word models to phrase models," Proc. CIKM, 2010.
Gens, R. and Domingo, P. "Discriminative learning of sum-product networks," NIPS, 2012.
George, D. "How the Brain Might Work: A Hierarchical and Temporal Model for Learning and Recognition," Ph.D. thesis, Stanford University, 2008.
Gibson, M. and Hain, T. "Error approximation and minimum phone error acoustic model estimation," IEEE Trans. Audio, Speech, and Language Proc., vol. 18, no. 6, August 2010, pp.
1269-1279.
Girshick, R., Donahue, J., Darrell, T., and Malik, J. "Rich feature hierarchies for accurate object detection and semantic segmentation," arXiv:1311.2524v1, 2013.
Glorot, X., Bordes, A., and Bengio, Y. "Deep sparse rectifier neural networks," Proc. AISTAT, April 2011.
Glorot, X. and Bengio, Y. "Understanding the difficulty of training deep feed-forward neural networks" Proc. AISTAT, 2010.
Goodfellow, I., Mirza, M., Courville, A., and Bengio, Y. "Multi-Prediction Deep Boltzmann
Machines," Proc. NIPS, 2013.
Grais, E., Sen, M., and Erdogan, H. "Deep neural networks for single channel source separation," arXiv:1311.2746v1, 2013.
Graves, A., Fernandez, S., Gomez, F., and Schmidhuber, J. "Connectionist temporal classification:
Labeling unsegmented sequence data with recurrent neural networks," Proc. ICML, 2006.
Graves, A. "Sequence Transduction with Recurrent Neural Networks," Representation Learning
Workshop, ICML 2012.
Graves, A., Mohamed, A., and Hinton, G. "Speech recognition with deep recurrent neural networks," Proc. ICASSP, 2013.
Graves, A., Jaitly, N., and Mahamed, A. "Hybrid speech recognition with deep bidirectional
LSTM," Proc. ASRU, 2013a.
Gutmann, M. and Hyvarinen, A. "Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics," Journal of Machine Learning Research, vol. 13, pp. 307–361, 2012.
Hain, T., Burget, L., Dines, J., Garner, P., Grezl, F., Hannani, A., Huijbregts, M., Karafiat, M., Lincoln, M., and Wan, V. "Transcribing meetings with the AMIDA systems," IEEE
Transactions on Audio, Speech, and Language Processing, vol. 20, pp. 486-498, 2012.
Hamel, P. and Eck, D. "Learning Features from Music Audio with Deep Belief Networks," Proc.
ISMIR, 2010.
Hawkins, J. and Blakeslee, S. On Intelligence: How a New Understanding of the Brain will lead to the Creation of Truly Intelligent Machines, Times Books, New York, 2004.
Hawkins, G., Ahmad, S. and Dubinsky, D. "Hierarchical Temporal Memory Including HTM
Cortical Learning Algorithms," Numenta Tech. Report, December 10, 2010.
He, X., Deng, L., Chou, W. "Discriminative learning in sequential pattern recognition – A unifying review for optimization-oriented speech recognition," IEEE Sig. Proc. Mag., vol. 25, 2008, pp.
14-36.
He, X. and Deng, L. "Speech recognition, machine translation, and speech translation – A unifying discriminative framework," IEEE Sig. Proc. Magazine, Vol. 28, November, 2011.
He, X. and Deng, L. "Optimization in speech-centric information processing: Criteria and techniques," Proc. ICASSP, 2012.
He, X. and Deng, L. "Speech-centric information processing: An optimization-oriented approach,"
Proc. of the IEEE, 2013.
Heigold, G., Vanhoucke, V., Senior, A. Nguyen, P., Ranzato, M., Devin, M., and Dean, J.
"Multilingual acoustic models using distributed deep neural networks," Proc. ICASSP, 2013.
Heigold, G., Ney, H., and Schluter, R., "Investigations on an EM-Style Optimization Algorithm for Discriminative Training of HMMs," IEEE Transactions on Audio, Speech, and Language
Processing, vol.21, no.12, pp. 2616-2626, Dec. 2013a.
Heigold, G., Ney, H., Lehnen, P., Gass, T., Schluter, R. "Equivalence of generative and log-liner models," IEEE Trans. Audio, Speech, and Language Proc., vol. 19, no. 5, February 2011, pp.
1138-1148.
Heintz, I., Fosler-Lussier, E., and Brew, C. "Discriminative input stream combination for conditional random field phone recognition," IEEE Trans. Audio, Speech, and Language Proc., vol. 17, no. 8, Nov. 2009, pp. 1533-1546.
Henderson, M., Thomson, B., and Young, S. "Deep Neural Network Approach for the Dialog State
Tracking Challenge," Proc. SIGDIAL, 2013.
Hermans, M. and Schrauwen, B. "Training and Analysing Deep Recurrent Neural Networks,"
Proc. NIPS, 2013.
Hermansky, H., Ellis, D., and Sharma, S. "Tandem connectionist feature extraction for conventional HMM systems", Proc. ICASSP, 2000.
Hifny, Y. and Renals, S. "Speech recognition using augmented conditional random fields," IEEE
Trans. Audio, Speech, and Language Proc., vol. 17, no. 2, February 2009, pp. 354-365.
Hinton, G. and Salakhutdinov, R. "Discovering binary codes for documents by learning deep generative models," Topics in Cognitive Science, pp. 1-18, 2010.
Hinton, G., Deng, L., Yu, D., Dahl, G., Mohamed, A., Jaitly, N., Senior, A., Vanhoucke, V., Nguyen, P., Sainath, T., and Kingsbury, B., "Deep Neural Networks for Acoustic Modeling in Speech Recognition," IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82-97, November
Hinton, G., Srivastava, N., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. "Improving neural networks by preventing co-adaptation of feature detectors," arXiv: 1207.0580v1, 2012a.
Hinton, G. "A better way to learn features," Communications of the ACM," Vol. 54, No. 10, October, 2011.
Hinton, G., Krizhevsky, A., and Wang, S. "Transforming autoencoders," Proc. Intern. Conf.
Artificial Neural Networks, 2011.
Hinton, G. "A practical guide to training restricted Boltzmann machines," UTML Tech Report
2010-003, Univ. Toronto, August 2010.
Hinton, G., Osindero, S., and Teh, Y. "A fast learning algorithm for deep belief nets," Neural
Computation, vol. 18, pp. 1527-1554, 2006.
Hinton, G. and Salakhutdinov, R. "Reducing the dimensionality of data with neural networks,"
Science, vol. 313. no. 5786, pp. 504 - 507, July 2006.
Hinton, G. "The ups and downs of Hebb synapses," Canadian Psychology, vol. 44, pp 10-13, Hinton, G. "Mapping part-whole hierarchies into connectionist networks," Artificial Intelligence, Vol. 46, pp. 47-75, 1990.
Hinton, G. "Preface to the special issue on connectionist symbol processing," Artificial
Intelligence, Vol. 46, pp. 1-4, 1990a.
Hochreiter, S., and Schmidhuber, J. "Long Short-Term Memory." Neural Computation, vol. 9, pp.
1735-1780, 1997.
Huang, J., Li, J., Deng, L., and Yu, D. "Cross-language knowledge transfer using multilingual deep neural networks with shared hidden layers," Proc. ICASSP, 2013.
Huang, P., Kumar, K., Liu, C., Gong, Y., and Deng, L. "Predicting speech recognition confidence using deep learning with word identity and score features," Proc. ICASSP, 2013a.
Huang, P., Deng, L., Hasegawa-Johnson, M., and He, X. "Random Features for Kernel Deep
Convex Network," Proc. ICASSP, 2013.
Huang, S. and Renals, S. "Hierarchical Bayesian language models for conversational speech recognition," IEEE Trans. Audio, Speech, and Language Proc., vol. 18, no. 8, November 2010, pp. 1941-1954.
Huang, E., Socher, R., Manning, C, and Ng, A. "Improving word representations via global context and multiple word prototypes," Proc. ACL, 2012.
Huang, P., He, X., Gao, J., Deng, L., Acero, A., and Heck, L. "Learning Deep Structured Semantic
Models for Web Search using Clickthrough Data," ACM Intern. Conf. Information and Knowledge Management (CIKM), 2013.
Huang, X., Acero, A., Chelba, C., Deng, L., Droppo, J., Duchene, D., Goodman, J., and Hon, H.
"MiPad: A multimodal interaction prototype," Proc. ICASSP, 2001.
Huang, Y., Yu, D., Gong, Y., and Liu, C. "Semi-Supervised GMM and DNN Acoustic Model
Training with Multi-system Combination and Confidence Re-calibration", Proc. Interspeech
2013, pp. 2360-2364.
Humphrey, E., Bello, J., and LeCun, Y. "Feature learning and deep architectures: New directions for music informatics," Journal of Intelligent Information Systems, 2013.
Humphrey, E., Bello, J., and LeCun, Y. "Moving beyond feature design: Deep architectures and automatic feature learning in music informatics," Proc. ISMIR, 2012.
Humphrey, E. and Bello, J. "Rethinking automatic chord recognition with convolutional neural networks," Proc. ICMLA, 2012a.
Hutchinson, B., Deng, L., and Yu, D. "A deep architecture with bilinear modeling of hidden representations: Applications to phonetic recognition," Proc. ICASSP, 2012.
Hutchinson, B., Deng, L., and Yu, D. "Tensor deep stacking networks," IEEE Trans. Pattern
Analysis and Machine Intelligence, vol. 35, pp. 1944 – 1957, 2013.
Imseng, D., Motlicek, P., Garner, P., and Bourlard, H. "Impact of deep MLP architecture on different modeling techniques for under-resourced speech recognition," Proc. ASRU, 2013.
Jaitly, N. and Hinton, G. "Learning a better representation of speech sound waves using restricted
Boltzmann machines," Proc. ICASSP, 2011.
Jaitly, N., Nguyen, P., and Vanhoucke, V. "Application of pre-trained deep neural networks to large vocabulary speech recognition," Proc. Interspeech, 2012.
Jarrett, K., Kavukcuoglu, K. and LeCun, Y. "What is the best multistage architecture for object recognition?" Proc. Intl. Conf. Computer Vision, pp. 2146–2153, 2009.
Jiang, H. and Li, X. "Parameter estimation of statistical models using convex optimization: An advanced method of discriminative training for speech and language processing," IEEE Signal
Processing Magazine, vol. 27, no. 3, pp. 115–127, 2010.
Juang, B.-H., Chou, W., and Lee, C.-H. "Minimum classification error rate methods for speech recognition," IEEE Trans. On Speech and Audio Processing, vol. 5, pp. 257–265, 1997.
Juang, B., Levinson, S. and Sondhi, M. "Maximum likelihood estimation for multivariate mixture observations of Markov chains," IEEE Trans. Inform. Theory, vol. 32, pp. 307–309, 1986.
Kahou, S. et al. "Combining modality specific deep neural networks for emotion recognition in video," Proc. ICMI, 2013.
Kang, S., Qian, X., and Meng, H. "Multi-distribution deep belief network for speech synthesis,"
Proc. ICASSP, 2013, pp. 8012-8016.
Kashiwagi, Y., Saito, D., Minematsu, N., and Hirose, K. "Discriminative piecewise linear transformation based on deep learning for noise robust automatic speech recognition," Proc.
ASRU, 2013.
Kavukcuoglu, K., Sermanet, P., Boureau, Y., Gregor, K., Mathieu M., and LeCun, Y. "Learning
Convolutional Feature Hierarchies for Visual Recognition," Proc. NIPS, 2010.
Ketabdar, H. and Bourlard, H. "Enhanced phone posteriors for improving speech recognition systems," IEEE Trans. Audio, Speech, and Language Proc., vol. 18, no. 6, August 2010, pp.
1094-1106.
Kingsbury, B. "Lattice-based optimization of sequence classification criteria for neural-network acoustic modeling," Proc. ICASSP, 2009.
Kingsbury, B., Sainath, T., and Soltau, H. "Scalable minimum Bayes risk training of deep neural network acoustic models using distributed Hessian-free optimization," Proc. Interspeech, Kiros, R., Zemel, R., and Salakhutdinov, R. "Multimodal Neural Language Models," Proc. NIPS
Deep Learning Workshop, 2013.
Ko, T. and Mak, B. "Eigentriphones for Context-Dependent Acoustic Modeling," IEEE
Transactions on Audio, Speech, and Language Processing, vol.21, no. 6, pp. 1285-1294, 2013.
Krizhevsky, A., Sutskever, I. and Hinton, G. "ImageNet classification with deep convolutional neural Networks," Proc. NIPS 2012.
Kubo, Y., Hori, T., and Nakamura, A. "Integrating deep neural networks into structural classification approach based on weighted finite-state transducers," Proc. Interspeech, 2012.
Kurzweil R. How to Create a Mind. Viking Books, Dec., 2012.
Lal, P.; King, S. "Cross-Lingual Automatic Speech Recognition Using Tandem Features," IEEE
Transactions on Audio, Speech, and Language Processing, vol.21, no.12, pp. 2506-2515, Dec.
Lang, K., Waibel, A., and Hinton, G. "A time-delay neural network architecture for isolated word recognition," Neural Networks, Vol. 3(1), pp. 23-43, 1990.
Larochelle, H. and Bengio, Y. "Classification using discriminative restricted Boltzmann machines," Proc. ICML, 2008.
Le, H., Oparin, I., Allauzen, A., Gauvain, J.-L., and Yvon, F. "Structured Output Layer Neural
Network Language Models for Speech Recognition," IEEE Transactions on Audio, Speech, and Language Processing, vol.21, no.1, pp.197-206, Jan. 2013.
Le, H., Oparin, I., Allauzen, A., Gauvain, J., and Yvon, F. "Structured output layer neural network language model," Proc. ICASSP, 2011.
Le, H., Allauzen, A., Wisniewski, G., and Yvon, F. "Training continuous space language models:
Some practical issues," Proc. EMNLP, 2010, pp. 778–788.
Le, D. and Mower P. "Emotion recognition from spontaneous speech using Hidden Markov models with deep belief networks," Proc. ASRU, 2013.
Le, Q., Ranzato, M., Monga, R., Devin, M., Corrado, G., Chen, K., Dean, J., Ng, A. "Building
High-Level Features Using Large Scale Unsupervised Learning," Proc. ICML 2012.
Le, Q., Ngiam, J., Coates, A., Lahiri, A., Prochnow, B., and Ng, A. "On optimization methods for deep learning," Proc. ICML, 2011.
LeCun, Y. "Learning invariant feature hierarchies," Proc. ECCV, 2012.
LeCun, Y., Chopra S., Ranzato, M., and Huang, F. "Energy-based models in document recognition and computer vision," Proc. Intern. Conf. Document Analysis and Recognition (ICDAR), LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. "Gradient-based learning applied to document recognition," Proceedings of the IEEE, Vol. 86, pp. 2278-2324, 1998.
LeCun, Y. and Bengio, Y. "Convolutional networks for images, speech, and time series," in The Handbook of Brain Theory and Neural Networks (M. Arbib, ed.), pp. 255- 258, Cambridge, Massachusetts: MIT Press, 1995.
Lee, C.-H. "From knowledge-ignorant to knowledge-rich modeling: A new speech research paradigm for next-generation automatic speech recognition," Proc. ICSLP, 2004, p. 109-111.
Lee, H., Grosse, R., Ranganath, R., and Ng, A. "Unsupervised learning of hierarchical representations with convolutional deep belief networks," Communications of the ACM," Vol.
54, No. 10, October, 2011, pp. 95-103.
Lee, H., Grosse, R., Ranganath, R., and Ng, A. "Convolutional Deep Belief Networks for Scalable
Unsupervised Learning of Hierarchical Representations," Proc. ICML, 2009.
Lee, H., Largman, Y., Pham, P., Ng, A. "Unsupervised feature learning for audio classification using convolutional deep belief networks," Proc. NIPS, 2010.
Lena, P., Nagata, K., and Baldi, P. "Deep spatiotemporal architectures and learning for protein structure prediction," Proc. NIPS, 2012.
Levine, S. "Exploring deep and recurrent architectures for optimal control", arXiv:1311.1761v1.
Li, L., Zhao, Y., Jiang, D., Zhang, Y., etc. "Hybrid Deep Neural Network--Hidden Markov Model(DNN-HMM) Based Speech Emotion Recognition," Proc. Conf. Affective Computing and Intelligent Interaction (ACII), pp.312-317, Sept. 2013.
Li, J., Deng, L., Gong, Y., and Haeb-Umbach, R. "An Overview of Noise-Robust Automatic
Speech Recognition," IEEE/ACM Transactions on Audio, Speech, and Language Processing, pp. 1-33, 2014.
Li, J., Yu, D., Huang, J., and Gong, Y. "Improving wideband speech recognition using mixedbandwidth training data in CD-DNN-HMM," Proc. IEEE SLT 2012.
Liao, H., McDermott, E., and Senior, A. "Large scale deep neural network acoustic modeling with semi-supervised training data for YouTube video transcription," Proc. ASRU, 2013.
Lin, H., Deng, L., Yu, D., Gong, Y., Acero, A., and C-H Lee, "A study on multilingual acoustic modeling for large vocabulary ASR." Proc. ICASSP, 2009.
Lin, Y., Lv, F., Zhu, S., Yang, M., Cour, T, Yu, K., Cao, L., and Huang, T. "Large-scale Image
Classification: Fast Feature Extraction and SVM Training," Proc. CVPR, 2011.
Ling, Z., Richmond, K., and Yamagishi, J. "Articulatory control of HMM-based parametric speech synthesis using feature-space-switched multiple regression," IEEE Trans. Audio, Speech, and Language Proc., Vol. 21, Jan, 2013.
Ling, Z., Deng, L. and Yu, D. "Modeling spectral envelopes using restricted Boltzmann machines for statistical parametric speech synthesis," in ICASSP, 2013, pp. 7825–7829.
Ling, Z., Deng, L. and Yu, D. "Modeling spectral envelopes using restricted Boltzmann machines and deep belief networks for statistical parametric speech synthesis," IEEE Trans. Audio
Speech Lang. Process., vol. 21, no. 10. pp. 2129-2139, 2013a.
Lu, L., Chin, K., Ghoshal, A., and Renals, S. "Joint uncertainty decoding for noise robust subspace
Gaussian mixture models," IEEE Trans. Audio, Speech, and Language Processing, vol. 21, no.
9, pp. 1791–1804, 2013.
Ma, J. and Deng, L. "Target-Directed Mixture Dynamic Models for Spontaneous Speech
Recognition," IEEE Trans. Speech and Audio Processing, vol. 12, no. 1, pp. 47-58, 2004.
Ma, J. and Deng, L. "Efficient Decoding Strategies for Conversational Speech Recognition Using a Constrained Nonlinear State-Space Model," IEEE Trans. Speech and Audio Processing, vol.
11, no. 6, pp. 590-602, 2003.
Ma, J. and Deng, L. "A Path-Stack Algorithm for Optimizing Dynamic Regimes in a Statistical
Hidden Dynamical Model of Speech," Computer, Speech and Language, 2000.
Manning, C., Raghavan, P., and Schütze, H. Introduction to Information Retrieval, Cambridge
University Press. 2009.
Maas, A., Hannun, A., and Ng, A. "Rectifier nonlinearities improve neural network acoustic models," ICML Workshop on Deep Learning for Audio, Speech, and Language Processing, Maas, A., Le, Q., O'Neil, T., Vinyals, O., Nguyen, P., and Ng, P. "Recurrent Neural Networks for
Noise Reduction in Robust ASR," Proc. Interspeech, 2012.
Markoff, J. "Scientists See Promise in Deep-Learning Programs," New York Times, Nov 24, 2012.
Martens, J. "Deep learning with Hessian-free optimization," Proc. ICML, 2010.
Martens, J. and Sutskever, I. "Learning recurrent neural networks with Hessian-free optimization,"
Proc. ICML, 2011.
McAllester, D. "A PAC-Bayesian Tutorial with a Dropout Bound," ArXive1307.2118, July, 2013.
McGraw, I.; Badr, I.; Glass, J.R., "Learning Lexicons From Speech Using a Pronunciation Mixture
Model," IEEE Transactions on Audio, Speech, and Language Processing, vol.21, no.2, pp.357,366, Feb. 2013.
Mesnil, G., He, X., Deng, L. and Bengio, Y. "Investigation of Recurrent-Neural-Network
Architectures and Learning Methods for Spoken Language Understanding," Proc. Interspeech
Miao, Y. and Metze, F. "Improving Low-Resource CD-DNN-HMM using Dropout and Multilingual DNN Training," Proc. Interspeech, 2013.
Miao, Y., Rawat, S., and Metze, F. "Deep maxout networks for low resource speech recognition,"
Proc. ASRU 2013.
Mikolov, T., Sutskever, I., Chen, K., Corrado, G., and Dean, J. "Distributed Representations ofWords and Phrases and their Compositionality," Proc. NIPS, 2013.
Mikolov, T., Chen, K., Corrado, G., and Dean, J. "Efficient estimation of word representations in vector space," Proc. ICLR, 2013a.
Mikolov, T., Le, Q., and Sutskever, I. "Exploiting Similarities among Languages for Machine
Translation," arXiv:1309.4168v1, 2013b.
Mikolov, T. "Statistical Language Models based on Neural Networks," PhD thesis, Brno
University of Technology, 2012.
Mikolov, T., Deoras, A., Povey, D., Burget, L., and Cernocky, J. "Strategies for training large scale neural network language models," Proc. IEEE ASRU, 2011.
Mikolov, T., Karafiat, M., Burget, L., Cernocky, J., and Khudanpur, S. "Recurrent neural network based language model," Proc. ICASSP, 2010, 1045–1048.
Minami, Y., McDermott, E. Nakamura, A. and Katagiri, S. "A recognition method with parametric trajectory synthesized using direct relations between static and dynamic feature vector time series," Proc. ICASSP, pp. 957-960, 2002.
Mnih, V., Kavukcuoglu, K. Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M. "Playing Arari with deep reinforcement learning," NIPS Deep Learning Workshop, 2013, also arXiv:1312.5602v1.
Mnih, A. and Kavukcuoglu, K. "Learning word embeddings efficiently with noise-contrastive estimation," Proc. NIPS, 2013.
Mnih, A. and Teh, W.-T. "A fast and simple algorithm for training neural probabilistic language
Models," Proc. ICML, pp. 1751–1758, 2012.
Mnih, A. and Hinton G. "A scalable hierarchical distributed language model" Proc. NIPS, 2008, pp. 1081-1088.
Mnih, A. and Hinton G. "Three new graphical models for statistical language modeling," Proc.
ICML, 2007, pp. 641-648.
Mohamed, A., Dahl, G. and Hinton, G. "Acoustic modeling using deep belief networks", IEEE
Trans. Audio, Speech, & Language Proc. Vol. 20 (1), January 2012.
Mohamed, A., Hinton, G., and Penn, G., "Understanding how deep belief networks perform acoustic modelling," Proc. ICASSP, 2012a.
Mohamed, A., Yu, D., and Deng, L. "Investigation of full-sequence training of deep belief networks for speech recognition," Proc. Interspeech, 2010.
Mohamed, A., Dahl, G., and Hinton, G. "Deep belief networks for phone recognition," in Proc.
NIPS Workshop Deep Learning for Speech Recognition and Related Applications, 2009.
Morgan, N. "Deep and Wide: Multiple Layers in Automatic Speech Recognition," IEEE Trans.
Audio, Speech, & Language Proc. Vol. 20 (1), January 2012.
Morgan, N., Q. Zhu, A. Stolcke, K. Sonmez, S. Sivadas, T. Shinozaki, M. Ostendorf, P. Jain, H.
Hermansky, D. Ellis, G. Doddington, B. Chen, O. Cretin, H. Bourlard,, and M. Athineos, "Pushing the envelope - aside [speech recognition]," IEEE Signal Processing Magazine, vol.
22, no. 5, pp. 81–88, Sep 2005.
Murphy, K. Machine Learning – A Probabilistic Perspective, the MIT Press, 2012.
Nair, V. and Hinton, G. "3-d object recognition with deep belief nets," Proc. NIPS, 2009.
Nakashika, T., Takashima, R., Takiguchi, T., and Ariki, Y. "Voice conversion in high-order eigen space using deep belief nets," Proc. Interspeech, 2013.
Ney, H. "Speech translation: Coupling of recognition and translation," Proc. ICASSP, 1999.
Ngiam, J., Khosla, A., Kim, M., Nam, J., Lee, H., and Ng, A. "Multimodal deep learning," Proc.
ICML, 2011.
Ngiam, J., Chen, Z., Koh, P., and Ng, A. "Learning deep energy models," Proc. ICML, 2011.
Norouzi, M., Mikolov, T., Bengio, S., Shlens, J., Frome, A., Corrado, G. and Dean, J. "Zero-shot learning by convex combination of semantic embeddings," arXiv:1312.5650v2, 2013.
Oliver, N., Garg, A., and Horvitz, E. "Layered Representations for Learning and Inferring Office
Activity from Multiple Sensory Channels," Computer Vision and Image Understanding," vol.
96, pp. 163-180, 2004.
Olshausen, B. "Can 'Deep Learning' offer deep insights about Visual Representation?" NIPS
Workshop on Deep Learning and Unsupervised Feature Learning, 2012.
Ostendorf, M. "Moving beyond the 'beads-on-a-string'model of speech," Proc. ASRU, 1999.
Ostendorf, M., Digalakis, V., and O. Kimball, "From HMMs to segment models: A unified view of stochastic modeling for speech recognition," IEEE Trans. Speech and Audio Proc., vol. 4, no. 5, September 1996.
Oudre, L., Fevotte, C., and Grenier, Y. "Probabilistic Template-Based Chord Recognition," IEEE
Transactions on Audio, Speech, and Language Processing,, vol.19, no.8, pp.2249-2259, Nov.
Palangi, H., Ward, R., Deng, L. "Using deep stacking network to improve structured compressive sensing with multiple measurement vectors," Proc. ICASSP, 2013.
Palangi, H., Deng, L. and Ward, R. "Learning Input and Recurrent Weight Matrices in Echo State
Networks," NIPS Deep Learning Workshop, December 2013a.
Papandreou, G., Katsamanis, A., Pitsikalis, V., and Maragos, P. "Adaptive multimodal fusion by uncertainty compensation with application to audiovisual speech recognition," IEEE Trans.
Audio, Speech, and Lang. Processing, Vol.17, pp. 423-435, 2009.
Pascanu, R., Mikolov, T., and Bengio, Y. "On the difficulty of training recurrent neural networks,"
Proc. ICML, 2013.
Peng, J., Bo, L., and Xu, J. "Conditional neural fields," Proc. NIPS, 2009.
Picone, P., S. Pike, R. Regan, T. Kamm, J. bridle, L. Deng, Z. Ma, H. Richards, and M. Schuster, "Initial evaluation of hidden dynamic models on conversational speech," Proc. ICASSP, 1999.
Pinto, J., Garimella, S., Magimai-Doss, M., Hermansky, H., and Bourlard, H. "Analysis of MLPbased hierarchical phone posterior probability estimators," IEEE Trans. Audio, Speech, and Language Proc., vol. 19, no. 2, Feb. 2011.
Plahl, C., Schlüter, R., and Ney, H. "Hierarchical Bottleneck Features for LVCSR," Proc.
Interspeech, 2010.
Plate, T. "Holographic reduced representations," IEEE Transactions on Neural Networks, Vol. 6, No. 3, pp. 623-641, May 1995.
Poggio. T. "How the Brain Might Work: The Role of Information and Learning in Understanding and Replicating Intelligence," In: Information: Science and Technology for the New Century, Editors: G. Jacovitt, A. Pettorossi, R. Consolo and V. Senni, Lateran University Press, pp. 45Pollack, J. "Recursive Distributed Representations," Artificial Intelligence, vol. 46, pp. 77-105, Poon, H. and Domingos, P. "Sum-product networks: A new deep architecture," Proc. UAI, 2011.
Povey, D. and Woodland, P. "Minimum phone error and I-smoothing for improved discriminative training," Proc. ICASSP, 2002.
Prabhavalkar, R. and Fosler-Lussier, E. "Backpropagation training for multilayer conditional random field based phone recognition", Proc. ICASSP, 2010.
Prince, A. and Smolensky, P. "Optimality: From neural networks to universal grammar," Science, vol. 275, pp. 1604-1610, 1997.
Rabiner, L. "A tutorial on hidden Markov models and selected applications in speech recognition,"
Proceedings of the IEEE, pp. 257-286, 1989.
Ranzato, M., Susskind, J., Mnih, V., and Hinton, G. "On deep generative models with applications to recognition," Proc. CVPR, 2011.
Ranzato, M., Chopra, S. and LeCun, Y., and Huang, F.-J. "Energy-based models in document recognition and computer vision," Proc. International Conference on Document Analysis and Recognition (ICDAR), 2007.
Ranzato, M., Boureau, Y., and LeCun, Y. "Sparse Feature Learning for Deep Belief Networks,"
Proc. NIPS, 2007.
Ranzato, M., Poultney, C., Chopra, S., and LeCun, Y. "Efficient Learning of Sparse
Representations with an Energy-Based Model," Proc. NIPS, 2006.
Rathinavalu C. and Deng, L. "Construction of state-dependent dynamic parameters by maximum likelihood: Applications to speech recognition," Signal Processing, vol. 55, no. 2, pp. 149-165, Rennie, S., Hershey, H., and Olsen, P. "Single-channel multi-talker speech recognition —
Graphical modeling approaches," IEEE Signal Processing Mag., vol. 33, pp. 66–80, 2010.
Riedmiller, M. and Braun, H. "A Direct Adaptive Method for Faster Backpropagation Learning:
The RPROP Algorithm," Proc. IEEE International Conf. Neural Networks, 1993.
Rifai, S., Vincent, P., X. Muller, X. Glorot, and Y. Bengio, "Contractive autoencoders: Explicit invariance during feature extraction," Proc. ICML, 2011, pp. 833-840.
Robinson, A. "An application of recurrent nets to phone probability estimation," IEEE Trans.
Neural Networks, Vol. 5, pp. 298-305, 1994.
Sainath, T., Kingsbury, B., Soltau, H., and Ramabhadran, B. "Optimization Techniques to Improve
Training Speed of Deep Neural Networks for Large Speech Tasks," IEEE Transactions on
Audio, Speech, and Language Processing, vol.21, no.11, pp.2267-2276, Nov. 2013.
Sainath, T., Mohamed, A., Kingsbury, B., and Ramabhadran, B. "Convolutional neural networks for LVCSR," Proc. ICASSP, 2013a.
Sainath, T., Kingsbury, B., Mohamed, A., and Ramabhadran, B. "Learning filter banks within a deep neural network framework," Proc. ASRU, 2013b.
Sainath, T., Kingsbury, B., Sindhwani, Arisoy, E., and Ramabhadran, B. "Low-rank matrix factorization for deep neural network training with high-dimensional output targets," Proc.
ICASSP, 2013c.
Sainath, T., Horesh, L., Kingsbury, B., Aravkin, A., and B. Ramabhadran. "Accelerating Hessianfree optimization for deep neural networks by implicit pre-conditioning and sampling," arXiv:
1309.1508v3, 2013d.
Sainath, T., Kingsbury, Mohamed, A., Dahl, G., Saon, G., Soltau, H., Beran, T., Aravkin, A., and B. Ramabhadran. "Improvements to deep convolutional neural networks for LVCSR," Proc.
ASRU, 2013e.
Sainath, T., Kingsbury, B., and Ramabhadran, B. "Autoencoder Bottleneck Features Using Deep
Belief Networks," Proc. ICASSP, 2012.
Sainath, T., Kingsbury, B., Ramabhadran, B., Novak, P., and Mohamed, A. "Making deep belief networks effective for large vocabulary continuous speech recognition," Proc. ASRU, 2011.
Sainath, T., Ramabhadran, B., Picheny, M., Nahamoo, D., and Kanevsky, D., "Exemplar-Based
Sparse Representation Features: From TIMIT to LVCSR," IEEE Transactions on Speech and Audio Processing, November 2011a.
Salakhutdinov R. and Hinton, G. "Semantic hashing," Proc. SIGIR Workshop on Information
Retrieval and Applications of Graphical Models, 2007.
Salakhutdinov R. and Hinton, G. "Deep Boltzmann machines," Proc. AISTATS, 2009.
Salakhutdinov R. and Hinton, G. "A better way to pretrain deep Boltzmann machines," Proc.
NIPS, 2012.
Saon, G., Soltau, H., Nahamoo, D., and Picheny, M. "Speaker adaptation of neural network acoustic models using i-vecors," Proc. ASRU, 2013.
Sarikaya, R., Hinton, G., Ramabhadran, B. "Deep belief nets for natural language call-routing,"
Proc. ICASSP, pp. 5680-5683, 2011.
Schmidt, E. and Kim, Y. "Learning emotion-based acoustic features with deep belief networks,"
Proc. IEEE Applications of Signal Processing to Audio and Acoustics, 2011.
Schwenk, H. "Continuous space translation models for phrase-based statistical machine translation," Proc.
COLING, 2012.
Schwenk, H., Rousseau, A., and Mohammed A. "Large, pruned or continuous space language models on a GPU for statistical machine translation," NAACL-HLT 2012 Workshop on the future of language modeling for HLT, pp. 11-19.
Seide, F., Li, G., Chen, X., and Yu, D. "Feature engineering in context-dependent deep neural networks for conversational speech transcription," Proc. ASRU 2011, pp. 24-29.
Seide, F., Li, G., and Yu, D. "Conversational Speech Transcription Using Context-Dependent
Deep Neural Networks," Proc. Interspeech, 2011, pp. 437-440.
Seide, F., Fu, H., Droppo, J., Li, G., Yu, D. "On Parallelizability of Stochastic Gradient Descent for Speech DNNs," Proc. ICASSP, 2014.
Seltzer, M., Yu, D. and Wang, E. "An Investigation of Deep Neural Networks for Noise Robust
Speech Recognition," Proc. ICASSP, 2013.
Shannon, M., Zen, H., and Byrne W. "Autoregressive models for statistical parametric speech synthesis," IEEE Trans. Audio, Speech, Language Proc., Vol. 21, No. 3, 2013, pp. 587-597.
Sheikhzadeh, H. and Deng, L. "Waveform-based speech recognition using hidden filter models:
Parameter selection and sensitivity to power normalization," IEEE Trans. on Speech and Audio
Processing, Vol. 2, pp. 80-91, 1994.
Shen, Y., He, X., Gao, J., Deng, L., and Mesnel, G. "Learning semantic representations using convolutional neural networks for Web search," Proc. WWW, 2014.
Simonyan, K., Vedaldi, A., and Zisserman, A. "Deep Fisher networks for large-scale image classification," Proc. NIPS, 2013.
Siniscalchi, M., Yu, D., Deng, L., and Lee, C.-H. "Exploiting deep neural networks for detectionbased speech recognition," Neurocomputing, Vol 106, 148-157, 2013.
Siniscalchi, M., Li, J., and Lee, C. "Hermitian Polynomial for Speaker Adaptation of Connectionist
Speech Recognition Systems," IEEE Transactions on Audio, Speech, and Language
Processing, Vol. 21, No. 10, pp. 2152-2161, 2013a.
Siniscalchi, M., Yu, D., Deng, L., and Lee, C.-H. "Speech Recognition Using Long-Span
Temporal Patterns in a Deep Network Model," IEEE Signal Processing Letters, vol. 20, no. 3, pp. 201-204, March 2013a.
Siniscalchi, M., Svendsen, T., and Lee, C.-H. "A bottom-up modular search approach to large vocabulary continuous speech recognition," IEEE Trans. Audio, Speech, Language Proc., Vol.
21, 2013a.
Sivaram G. and Hermansky, H. "Sparse multilayer perceptron for phoneme recognition," IEEE
Trans. Audio, Speech, & Language Proc. Vol. 20 (1), January 2012.
Smolensky, P. "Tensor Product Variable Binding and the Representation of Symbolic Structures in Connectionist Systems," Artificial Intelligence," Vol. 46, pp. 159-216, 1990.
Snoek, J., Larochelle, H., and Adams, R. "Practical Bayesian Optimization of Machine Learning
Algorithms," Proc. NIPS, 2012.
Socher, R., Chen, D., Manning, C., and Ng, A. "Reasoning With Neural Tensor Networks for
Knowledge Base Completion," Proc. NIPS, 2013.
Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C., Ng A., and Potts. C. "Recursive Deep
Models for Semantic Compositionality Over a Sentiment Treebank," Proc. EMNLP, 2013a.
Socher, R., Ganjoo, M, Sridhar, H., Bastani, O., Manning, C. and Ng, A. "Zero-shot learning through cross-modal transfer," Proc. NIPS, 2013b.
Socher, R., Le, Q., Manning, C. and Ng, A. "Grounded Compositional Semantics for Finding and Describing Images with Sentences," NIPS Deep Learning Workshop, 2013c.
Socher, R., Bengio, Y., and Manning, C. "Deep learning for NLP," Tutorial at ACL, 2012, and NAACL, 2013: http://www.socher.org/index.php/DeepLearningTutorial
Socher, R. "New Directions in Deep Learning: Structured Models, Tasks, and Datasets," NIPS
Workshop on Deep Learning and Unsupervised Feature Learning, 2012.
Socher, R., Lin, C., Ng, A., and Manning, C. "Learning continuous phrase representations and syntactic parsing with recursive neural networks," Proc. ICML, 2011.
Socher, R., Pennington, J., Huang, E., Ng, A., and Manning, C. "Semi-Supervised Recursive
Autoencoders for Predicting Sentiment Distributions," Proc. EMNLP, 2011a.
Socher, R., Pennington, J., Huang, E., Ng, A., and Manning, C. "Dynamic Pooling and Unfolding
Recursive Autoencoders for Paraphrase Detection, Proc. NIPS 2011b.
Socher, R. and Fei-Fei, L. "Connecting modalities: Semi-supervised segmentation and annotation of images using unaligned text corpora," Proc. CVPR, 2010.
Stoyanov, V., Ropson, A. and Eisner, J. "Empirical Risk Minimization of Graphical Model
Parameters Given Approximate Inference, Decoding, and Model Structure," Proc. AISTAT, Srivastava, N. and Salakhutdinov, R. "Discriminative Transfer Learning with Tree-based Priors,"
Proc. NIPS, 2013.
Srivastava, N. and Salakhutdinov R. "Multimodal learning with deep Boltzmann machines," Proc.
NIPS, 2012.
Stafylakis, T., Kenny, P., Senoussaoui, M., and Dumouchel, P. "Preliminary investigation of Boltzmann Machine classifiers for speaker recognition," Proc. Odyssey 2012, pp. 109–116, Su, H., Li, G., Yu, D., and Seide, F. "Error Back Propagation For Sequence Training Of ContextDependent Deep Networks For Conversational Speech Transcription," Proc. ICASSP, 2013.
Subramanya, A., Deng, L., Liu, Z. and Zhang, Z. "Multi-sensory speech processing: Incorporating automatically extracted hidden dynamic information," Proc. IEEE Intern. Conf. Multimedia &
Expo (ICME), Amsterdam, July 2005.
Sun, J. and Deng, L. "An overlapping-feature based phonological model incorporating linguistic constraints: Applications to speech recognition," J. Acoust. Society of America, vol. 111, no.
2, pp. 1086-1101, 2002.
Sutskever. I. "Training Recurrent Neural Networks," Ph.D. Thesis, University of Toronto, 2013.
Sutskever, I., Martens J., and Hinton, G. "Generating text with recurrent neural networks," Proc.
ICML, 2011.
Taylor, G., Hinton, G. E., and Roweis, S. "Modeling human motion using binary latent variables."
Proc. NIPS, 2007.
Tang, Y. and Eliasmith, C. "Deep networks for robust visual recognition," Proc. ICML, 2010.
Tarralba, A, Fergus R, and Weiss, Y. "Small codes and large image databases for recognition,"
Proc. CVPR, 2008.
Thomas, S., Seltzer, M., Church, K., and Hermansky, H. "Deep neural network features and seisupervised training for low resource speech recognition," Proc. Interspeech, 2013.
Tieleman, T. "Training Restricted Boltzmann Machines using Approximations to the Likelihood
Gradient," Proc. ICML, 2008.
Tokuda, K., Nankaku, Y., Toda, T. Zen, H., Yamagishi, H., and Oura, K. "Speech synthesis based on hidden Markov models," Prooceedings of the IEEE, vol. 101, no. 5, pp. 1234–1252, 2013.
Triefenbach, F.; Jalalvand, A.; Demuynck, K.; Martens, J.-P., "Acoustic Modeling With
Hierarchical Reservoirs," IEEE Transactions on Audio, Speech, and Language Processing, vol.21, no.11, pp.2439,2450, Nov. 2013.
Tur, G., Deng, L., Hakkani-Tür, D., and X. He. "Towards deep understanding: Deep convex networks for semantic utterance classification," Proc. ICASSP, 2012.
Turian, J., Ratinov, L., and Bengio, Y. "Word representations: A simple and general method for semi-supervised learning," Proc. ACL, 2010.
Tüske, Z., Sundermeyer, M., Schlüter, R., and Ney. H. "Context-Dependent MLPs for LVCSR:
TANDEM, Hybrid or Both?" Proc. Interspeech, 2012.
Uria, B., Renals, S., and Richmond, K. "A deep neural network for acoustic-articulatory speech inversion," NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2011. van Dalen, R. and Gales, M. "Extended VTS for noise-robust speech recognition," IEEE Trans.
Audio, Speech, and Language Processing, vol. 19, no. 4, pp. 733–743, 2011.van den Oord, A., Dieleman, S., Schrauwen, B. "Deep content-based music recommendation,"
Proc. NIPS, 2013.
Vasilakakis,V., Cumani, S., and Laface, P. "Speaker recognition by means of Deep Belief
Networks," Proc. Biometric Technologies in Forensic Science, 2013.
Vesely, K., Hannemann, M., and Burget, L. "Semi-supervised training of deep neural networks,"
Proc. ASRU, 2013.
Vesely, K., Ghoshal, A., Burget, L., and Povey, D. "Sequence-discriminative training of deep neural networks", Proc. Interspeech, 2013a.
Vincent, P. "A connection between score matching and denoising autoencoder", Neural
Computation, Vol. 23, No. 7, pp. 1661-1674, 2011.
Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., and Manzagol, P. "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion," J. Machine Learning Research, Vol. 11, 2010, pp. 3371-3408.
Vinyals, O., & Povey, D. "Krylov Subspace Descent for Deep Learning," Proc. AISTAT, 2012.
Vinyals, O., Jia, Y., Deng, L., and Darrell, T. "Learning with recursive perceptual representations,"
Proc. NIPS, 2012.
Vinyals O., and Ravuri, S. "Comparing multilayer perceptron to deep belief network tandem features for robust ASR," Proc. ICASSP, 2011.
Wager, S., Wang, S., and Liang, P. "Dropout Training as Adaptive Regularization," Proc. NIPS, Waibel, A., Hanazawa, T., Hinton, G., Shikano, K., and Lang, K. "Phoneme recognition using time-delay neural networks," IEEE Trans. ASSP, vol. 37, pp. 328-339, 1989.
Welling, M., Rosen-Zvi, M., and Hinton, G. "Exponential family harmoniums with an application to information retrieval," Proc. NIPS, Vol. 20, 2005.
Weng, C., Yu, D., Seltzer, M., and Droppo, J. "Single-channel Mixed Speech Recognition Using
Deep Neural Networks", Proc. ICASSP 2014.
Wang, G. and Sim, K. "Context-dependent modelling of deep neural network using logistic regression," Proc. ASRU, 2013.
Wang, G. and Sim, K. "Regression-based context-dependent modeling of deep neural networks for speech recognition," IEEE/ACM Trans. Audio, Speech, and Language Processing, 2014.
Weston, J., Bengio, S., and Usunier, N. "Wsabie: Scaling up to large vocabulary image annotation," Proc. IJCAI, 2011.
Weston, J., Bengio, S., and Usunier, N. "Large scale image annotation: learning to rank with joint word-image embeddings," Machine Learning, vol. 81(1), pp. 21–35, 2010.
Wiesler, S., Li, J., and Xue, J. "Investigations on Hessian-Free Optimization for Cross-Entropy
Training of Deep Neural Networks," Proc. Interspeech, 2013.
Wohlmayr, M., Stark, M., Pernkopf, F. "A probabilistic interaction model for multi-pitch tracking with factorial hidden Markov model," IEEE Trans. Audio, Speech, and Language Proc., vol.
19, no. 4, May. 2011.
Wolpert, D. "Stacked generalization," Neural Networks, vol. 5, no. 2, pp. 241-259, 1992.
Wright, S.J.; Kanevsky, D.; Deng, L.; He, X.; Heigold, G.; Li, H., "Optimization Algorithms and Applications for Speech and Language Processing," IEEE Transactions on Audio, Speech, and Language Processing, vol.21, no.11, pp.2231-2243, Nov. 2013.
Xiao, L. and Deng, L. "A geometric perspective of large-margin training of Gaussian models,"
IEEE Signal Processing Magazine, vol. 27, no. 6, pp. 118-123, IEEE, November 2010.
Xie, X. and Seung, S. "Equivalence of backpropagation and contrastive Hebbian learning in a layered network," Neural computation, Vol. 15, pp. 441-454, 2003.
Xu, Y., Du, J., Dai, L., and Lee, C. "An experimental study on speech enhancement based on deep neural networks," IEEE Signal Processing Letters, vol. 21, no. 1, pp. 65–68, 2014.
Xue, J., Li, J., and Gong, Y. "Restructuring of Deep Neural Network Acoustic Models with
Singular Value Decomposition," Proc. Interspeech, 2013.
Yamin, S., Deng, L., Wang, Y., and Acero, A. "An integrative and discriminative technique for spoken utterance classification," IEEE Trans. Audio, Speech, and Language Proc., Vol 16, 1207-1214, 2008.
Yan, Z., Huo Q., Xu, J. "A Scalable Approach to Using DNN-Derived Features in GMM-HMM
Based Acoustic Modeling For LVCSR," Proc. Interspeech, 2013.
Yang, D., Furui, S. "Combining a two-step CRF model and a joint source channel model for machine transliteration," Proc. ACL, 2010, pp. 275-280.
Yao, K., Zweig, G., Hwang, M., Shi, Y., and Yu, D. "Recurrent Neural Networks for Language
Understanding," Proc. Interspeech, 2013.
Yao, K., Yu, D., Deng, L., and Gong, Y. "A Fast Maximum Likelihood Nonlinear Feature
Transformation Method for GMM-HMM Speaker Adaptation," Neurocomputing, 2013a.
Yao, K., Yu, D., Seide, F., Su, H., Deng, L., and Gong, Y. "Adaptation of context-dependent deep neural networks for automatic speech recognition," Proc. ICASSP, 2012.
Yoshioka, T. and Nakatani, T. "Noise model transfer: Novel approach to robustness against nonstationary noise," IEEE Trans. Audio, Speech, and Language Processing, vol. 21, no. 10, pp. 2182–2192, 2013.
Younes, L. "On the convergence of Markovian stochastic algorithms with rapidly decreasing ergodicity rates," Stochastics and Stochastic Reports, vol. 65(3), pp. 177-228, 1999.
Yu, K., Gales, M., and Woodland, P. "Unsupervised adaptation with discriminative mapping transforms," IEEE Trans. Audio, Speech, and Language Processing, vol. 17, no.4, pp. 714–
Yu, D., Deng, L., and Seide, F. "The Deep Tensor Neural Network with Applications to Large
Vocabulary Speech Recognition," IEEE Transactions on Audio, Speech, and Language
Processing, vol. 21, no. 2, pp. 388-396, 2013.
Yu, D., Seltzer, M., Li, J., Huang, J.-T., and Seide, F. "Feature Learning in Deep Neural Networks
- Studies on Speech Recognition," Proc. ICLR, 2013a.
Yu, D., Yao, K., Su, H., Li, G., and Seide, F. "KL-Divergence Regularized Deep Neural Network
Adaptation For Improved Large Vocabulary Speech Recognition," Proc. ICASSP 2013b.
Yu, D. and Deng, L. "Efficient and effective algorithms for training single-hidden-layer neural networks," Pattern Recognition Letters, Vol. 33, 554-558, 2012.
Yu, D., Seide, F., Li, G., Deng, L. "Exploiting sparseness in deep neural networks for large vocabulary speech recognition," Proc. ICASSP 2012.
Yu, D., Siniscalchi, S., Deng, L., and Lee, C. "Boosting attribute and phone estimation accuracies with deep neural networks for detection-based speech recognition", Proc. ICASSP 2012a.
Yu, D., Chen, X., and Deng, L., "Factorized deep neural networks for adaptive speech recognition," International Workshop on Statistical Machine Learning for Speech Processing, March 2012b.
Yu, D., Deng, L. and Seide, F. "Large Vocabulary Speech Recognition Using Deep Tensor Neural
Networks", Proc. Interspeech 2012c.
Yu, D. and Seltzer, M. "Improved bottleneck features using pre-trained deep neural networks,"
Proc. Interspeech 2011.
Yu, D. and Deng, L. "Deep learning and its applications to signal and information processing,"
IEEE Signal Processing Magazine, January 2011, pp. 145-154.
Yu, D. and Deng, L. "Accelerated parallelizable neural networks learning algorithms for speech recognition," Proc. Interspeech 2011.
Yu, D., Deng, L., Li, G., and F. Seide. "Discriminative pretraining of deep neural networks," U.S.
Patent Filing, Nov. 2011.
Yu, D. and Deng, L. "Deep-structured hidden conditional random fields for phonetic recognition,"
Proc. Interspeech, Sept. 2010.
Yu, D., Wang, S., Karam, Z., Deng, L. "Language recognition using deep-structured conditional random fields," Proc. ICASSP, 2010, pp. 5030-5033.
Yu, D., Wang, S., Deng, L., "Sequential labeling using deep-structured conditional random fields", J. of Selected Topics in Signal Processing, vol.4, pp. 965 – 973, 2010a.
Yu, D., Li, J.-Y., and Deng, L. "Calibration of confidence measures in speech recognition," IEEE
Trans. Audio, Speech and Language, vol 19, 2461–2473, 2010b.
Yu, D., Deng, L., and Dahl, G.E., "Roles of Pre-Training and Fine-Tuning in Context-Dependent
DBN-HMMs for Real-World Speech Recognition," NIPS 2010 Workshop on Deep Learning and Unsupervised Feature Learning, Dec. 2010c.
Yu, D., Deng, D., Wang, S., "Learning in the Deep-Structured Conditional Random Fields," NIPS
2009 Workshop on Deep Learning for Speech Recognition and Related Applications, 2009.
Yu, D, Deng, L., Gong, Y. and Acero, A. "A novel framework and training algorithm for variableparameter hidden Markov models," IEEE Transactions on Audio, Speech and Language
Processing, vol. 17, no. 7, pp. 1348-1360, 2009a.
Yu, D., Deng, L., Liu, P., Wu, J., Gong, Y., and Acero, A. "Cross-lingual speech recognition under runtime resource constraints," Proc. ICASSP, 2009b.
Yu, D. and Deng, L. "Solving nonlinear estimation problems using Splines," IEEE Signal
Processing Magazine, vol. 26, no. 4, pp. 86-90, July 2009.
Yu, D., Deng, L., Droppo, J., Wu, J., Gong, Y., Acero, A. "Robust speech recognition using cepstral minimum-mean-square-error noise suppressor," IEEE Trans. Audio, Speech, and Language Processing, vol. 16, no. 5, July 2008.
Yu, D., Deng, L., He, X., and Acero, X. "Large-Margin Minimum Classification Error Training for Large-Scale Speech Recognition Tasks," Proc. ICASSP, 2007.
Yu, D., Deng, L., He, X., and Acero, A. "Large-Margin Minimum Classification Error Training:
A Theoretical Risk Minimization Perspective," Computer Speech and Language, vol. 22, no.
4, pp. 415-429, October 2008.
Yu, D. and Deng, L. "Large-Margin Discriminative Training of Hidden Markov Models for
Speech Recognition," Proc. ICASSP, 2007.
Yu, K., Lin, Y., and Lafferty, H. "Learning Image Representations from the Pixel Level via
Hierarchical Sparse Coding," Proc. CVPR, 2011.
Zamora-Martínez, F., Castro-Bleda, M., España-Boquera, S. "Fast evaluation of connectionist language models," Intern. Conf. Artificial Neural Networks, 2009, pp. 144-151.
Zeiler, M. Hierarchical Convolutional Deep Learning in Computer Vision, Ph.D. Thesis, New
York University, January 2014.
Zeiler, M. and Fergus, R. "Visualizing and understanding convolutional networks," arXiv:1311.2901, pp. 1-11, 2013.
Zeiler M. and Fergus. R. "Stochastic pooling for regularization of deep convolutional neural networks," Proc. ICLR, 2013.
Zeiler, M., Taylor, G., and Fergus, R. "Adaptive deconvolutional networks for mid and high level feature learning," Proc. ICCV, 2011.
Zen, H., Senior, A., and Schuster, M. "Statistical parametric speech synthesis using deep neural networks," Proc. ICASSP, pp. 7962-7966, 2013.
Zen, H. Gales, M. J. F. Nankaku, Y. Tokuda, K. "Product of experts for statistical parametric speech synthesis," IEEE Trans. Audio, Speech, and Language Proc., vol. 20, no. 3, March, 2012, pp. 794-805.
Zen, H., Nankaku, Y., and Tokuda, K. "Continuous stochastic feature mapping based on trajectory
HMMs," IEEE Trans. Audio, Speech, and Language Proc., vol. 19, no. 2, Feb. 2011, pp. 417Zhang, X. and Wu, J. "Deep belief networks based voice activity detection," IEEE Transactions on Audio, Speech, and Language Processing, vol. 21, no. 4, pp. 697–710, 2013.
Zhang, X., Trmal, J., Povey, D., and Khudanpur, S. "Improving deep neural network acoustic models using generalized maxout networks," Proc. ICASSP, 2014.
Zhang, Z., Liu, Z., Sinclair, M., Acero, A., Deng, L., Droppo, J., Huang, X., and Zheng, Y. "Multisensory microphones for robust speech detection, enhancement and recognition." Proc.
ICASSP, 2004.
Zhao, Y. and Juang, B. "Nonlinear compensation using the Gauss-Newton method for noise-robust speech recognition," IEEE Trans. Audio, Speech, and Language Processing, vol. 20, no. 8, pp.
2191–2206, 2012.
Zou, W., Socher, R., Cer, D., and Manning, C. "Bilingual Word Embeddings for Phrase-Based
Machine Translation," Proc. EMNLP, 2013.
Zweig, G. and Nguyen, P. "A segmental CRF approach to large vocabulary continuous speech recognition," Proc. ASRU, 2009.Large Scale Distributed Deep Networks
Jeffrey Dean, Greg S. Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc V. Le, Mark Z. Mao, Marc'Aurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang, Andrew Y. Ng
{jeff, gcorrado}@google.com
Google Inc., Mountain View, CA
Abstract
Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance. In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores. We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models. Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii)
Sandblaster, a framework that supports a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS. Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training. We have successfully used our system to train a deep network 30x larger than previously reported in the literature, and achieves state-of-the-art performance on
ImageNet, a visual object recognition task with 16 million images and 21k categories. We show that these same techniques dramatically accelerate the training of a more modestly- sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm.
Introduction
Deep learning and unsupervised feature learning have shown great promise in many practical applications. State-of-the-art performance has been reported in several domains, ranging from speech recognition, visual object recognition, to text processing.
It has also been observed that increasing the scale of deep learning, with respect to the number of training examples, the number of model parameters, or both, can drastically improve ultimate classification accuracy. These results have led to a surge of interest in scaling up the training and inference algorithms used for these models and in improving applicable optimization procedures. The use of GPUs is a significant advance in recent years that makes the training of modestly sized deep networks practical. A known limitation of the GPU approach is that the training speed-up is small when the model does not fit in GPU memory (typically less than
6 gigabytes). To use a GPU effectively, researchers often reduce the size of the data or parameters so that CPU-to-GPU transfers are not a significant bottleneck. While data and parameter reduction work well for small problems (e.g. acoustic modeling for speech recognition), they are less attractive for problems with a large number of examples and dimensions (e.g., high-resolution images).
In this paper, we describe an alternative approach: using large-scale clusters of machines to distribute training and inference in deep networks. We have developed a software framework called DistBelief that enables model parallelism within a machine (via multithreading) and across machines (via
1 message passing), with the details of parallelism, synchronization and communication managed by the framework. In addition to supporting model parallelism, the DistBelief framework also supports data parallelism, where multiple replicas of a model are used to optimize a single objective. Within this framework, we have designed and implemented two novel methods for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure which leverages adaptive learning rates and supports a large number of model replicas, and (ii) Sandblaster
L-BFGS, a distributed implementation of L-BFGS that uses both data and model parallelism.1 Both
Downpour SGD and Sandblaster L-BFGS enjoy significant speed gains compared to more conventional implementations of SGD and L-BFGS.
Our experiments reveal several surprising results about large-scale nonconvex optimization. Firstly, asynchronous SGD, rarely applied to nonconvex problems, works very well for training deep networks, particularly when combined with Adagrad adaptive learning rates. Secondly, we show that given sufficient resources, L-BFGS is competitive with or faster than many variants of SGD.
With regard to specific applications in deep learning, we report two main findings: that our distributed optimization approach can both greatly accelerate the training of modestly sized models, and that it can also train models that are larger than could be contemplated otherwise. To illustrate the first point, we show that we can use a cluster of machines to train a modestly sized speech model to the same classification accuracy in less than 1/10th the time required on a GPU. To illustrate the second point, we trained a large neural network of more than 1 billion parameters and used this network to drastically improve on state-of-the-art performance on the ImageNet dataset, one of the largest datasets in computer vision.
Previous work
In recent years commercial and academic machine learning data sets have grown at an unprecedented pace. In response, a great many authors have explored scaling up machine learning algorithms through parallelization and distribution. Much of this research has focused on linear, convex models, where distributed gradient computation is the natural first step.
Within this area, some groups have relaxed synchronization requirements, exploring delayed gradient updates for convex problems. In parallel, other groups working on problems with sparse gradients (problems where only a tiny fraction of the coordinates of the gradient vector are non-zero for any given training example) have explored lock-less asynchronous stochastic gradient descent on shared-memory architectures (i.e. single machines). We are interested in an approach that captures the best of both worlds, allowing the use of a cluster of machines asynchronously computing gradients, but without requiring that the problem be either convex or sparse.
In the context of deep learning, most work has focused on training relatively small models on a single machine (e.g., Theano ). Suggestions for scaling up deep learning include the use of a farm of GPUs to train a collection of many small models and subsequently averaging their predictions, or modifying standard deep networks to make them inherently more parallelizable. Our focus is scaling deep learning techniques in the direction of training very large models, those with a few billion parameters, but without introducing restrictions on the form of the model. In special cases where one layer dominates computation, some authors have considered distributing computation in that one layer and replicating computation in the remaining layers. But in the general case where many layers of the model are computationally intensive, full model parallelism in a spirit similar to is required. To be successful, however, we believe that model parallelism must be combined with clever distributed optimization techniques that leverage data parallelism.
We considered a number of existing large-scale computational tools for application to our problem, MapReduce and GraphLab being notable examples. We concluded that MapReduce, designed for parallel data processing, was ill-suited for the iterative computations inherent in deep network training; whereas GraphLab, designed for general (unstructured) graph computations, would not exploit computing efficiencies available in the structured graphs typically found in deep networks.
1We implemented L-BFGS within the Sandblaster framework, but the general approach is also suitable for a variety of other batch optimization methods.
Machine 1
Machine 2
Machine 3
Machine 4
Figure 1: An example of model parallelism in DistBelief. A five layer deep neural network with local connectivity is shown here, partitioned across four machines (blue rectangles). Only those nodes with edges that cross partition boundaries (thick lines) will need to have their state transmitted between machines. Even in cases where a node has multiple edges crossing a partition boundary, its state is only sent to the machine on the other side of that boundary once. Within each partition, computation for individual nodes will the parallelized across all available CPU cores.
Model parallelism
To facilitate the training of very large deep networks, we have developed a software framework, DistBelief, that supports distributed computation in neural networks and layered graphical models.
The user defines the computation that takes place at each node in each layer of the model, and the messages that should be passed during the upward and downward phases of computation.2 For large models, the user may partition the model across several machines (Figure 1), so that responsibility for the computation for different nodes is assigned to different machines. The framework automatically parallelizes computation in each machine using all available cores, and manages communication, synchronization and data transfer between machines during both training and inference.
The performance benefits of distributing a deep network across multiple machines depends on the connectivity structure and computational needs of the model. Models with a large number of parameters or high computational demands typically benefit from access to more CPUs and memory, up to the point where communication costs dominate. We have successfully run large models with up to 144 partitions in the DistBelief framework with significant speedups, while more modestly sized models show decent speedups for up to 8 or 16 partitions. (See Section 5, under the heading Model
Parallelism Benchmarks, for experimental results.) Obviously, models with local connectivity structures tend to be more amenable to extensive distribution than fully-connected structures, given their lower communication requirements. The typical cause of less-than-ideal speedups is variance in processing times across the different machines, leading to many machines waiting for the single slowest machine to finish a given phase of computation. Nonetheless, for our largest models, we can efficiently use 32 machines where each machine achieves an average CPU utilization of 16 cores, for a total of 512 CPU cores training a single large neural network. When combined with the distributed optimization algorithms described in the next section, which utilize multiple replicas of the entire neural network, it is possible to use tens of thousands of CPU cores for training a single model, leading to significant reductions in overall training times.
Distributed optimization algorithms
Parallelizing computation within the DistBelief framework allows us to instantiate and run neural networks considerably larger than have been previously reported. But in order to train such large models in a reasonable amount of time, we need to parallelize computation not only within a single
2In the case of a neural network 'upward' and 'downward' might equally well be called 'feedforward' and 'backprop', while for a Hidden Markov Model, they might be more familiar as 'forward' and 'backward'.
Parameter Server
Model
Replicas
Data
Shards w' = w - ηΔw w
Δw
Parameter Server
Model
Replicas
Data
Coordinator(small messages)
Figure 2: Left: Downpour SGD. Model replicas asynchronously fetch parameters w and push gradients ∆w to the parameter server. Right: Sandblaster L-BFGS. A single 'coordinator' sends small messages to replicas and the parameter server to orchestrate batch optimization. instance of the model, but to distribute training across multiple model instances. In this section we describe this second level of parallelism, where we employ a set of DistBelief model instances, or replicas, to simultaneously solve a single optimization problem.
We present a comparison of two large-scale distributed optimization procedures: Downpour SGD, an online method, and Sandblaster L-BFGS, a batch method. Both methods leverage the concept of a centralized sharded parameter server, which model replicas use to share their parameters. Both methods take advantage of the distributed computation DistBelief allows within each individual replica. But most importantly, both methods are designed to tolerate variance in the processing speed of different model replicas, and even the wholesale failure of model replicas which may be taken offline or restarted at random.
In a sense, these two optimization algorithms implement an intelligent version of data parallelism.
Both approaches allow us to simultaneously process distinct training examples in each of the many model replicas, and periodically combine their results to optimize our objective function.
Downpour SGD
Stochastic gradient descent (SGD) is perhaps the most commonly used optimization procedure for training deep neural networks. Unfortunately, the traditional formulation of SGD is inherently sequential, making it impractical to apply to very large data sets where the time required to move through the data in an entirely serial fashion is prohibitive.
To apply SGD to large data sets, we introduce Downpour SGD, a variant of asynchronous stochastic gradient descent that uses multiple replicas of a single DistBelief model. The basic approach is as follows: We divide the training data into a number of subsets and run a copy of the model on each of these subsets. The models communicate updates through a centralized parameter server, which keeps the current state of all parameters for the model, sharded across many machines (e.g., if we have 10 parameter server shards, each shard is responsible for storing and applying updates to 1/10th of the model parameters) (Figure 2). This approach is asynchronous in two distinct aspects: the model replicas run independently of each other, and the parameter server shards also run independently of one another.
In the simplest implementation, before processing each mini-batch, a model replica asks the parameter server service for an updated copy of its model parameters. Because DistBelief models are themselves partitioned across multiple machines, each machine needs to communicate with just the subset of parameter server shards that hold the model parameters relevant to its partition. After receiving an updated copy of its parameters, the DistBelief model replica processes a mini-batch of data to compute a parameter gradient, and sends the gradient to the parameter server, which then applies the gradient to the current value of the model parameters.
It is possible to reduce the communication overhead of Downpour SGD by limiting each model replica to request updated parameters only every nfetch steps and send updated gradient values only every npush steps (where nfetch might not be equal to npush). In fact, the process of fetching
4 parameters, pushing gradients, and processing training data can be carried out in three only weakly synchronized threads (see the Appendix for pseudocode). In the experiments reported below we fixed nfetch = npush = 1 for simplicity and ease of comparison to traditional SGD.
Downpour SGD is more robust to machines failures than standard (synchronous) SGD. For synchronous SGD, if one machine fails, the entire training process is delayed; whereas for asynchronous
SGD, if one machine in a model replica fails, the other model replicas continue processing their training data and updating the model parameters via the parameter servers. On the other hand, the multiple forms of asynchronous processing in Downpour SGD introduce a great deal of additional stochasticity in the optimization procedure. Most obviously, a model replica is almost certainly computing its gradients based on a set of parameters that are slightly out of date, in that some other model replica will likely have updated the parameters on the parameter server in the meantime. But there are several other sources of stochasticity beyond this: Because the parameter server shards act independently, there is no guarantee that at any given moment the parameters on each shard of the parameter server have undergone the same number of updates, or that the updates were applied in the same order. Moreover, because the model replicas are permitted to fetch parameters and push gradients in separate threads, there may be additional subtle inconsistencies in the timestamps of parameters. There is little theoretical grounding for the safety of these operations for nonconvex problems, but in practice we found relaxing consistency requirements to be remarkably effective.
One technique that we have found to greatly increase the robustness of Downpour SGD is the use of the Adagrad adaptive learning rate procedure. Rather than using a single fixed learning rate on the parameter sever (η in Figure 2), Adagrad uses a separate adaptive learning rate for each parameter. Let ηi,K be the learning rate of the i-th parameter at iteration K and ∆wi,K its gradient, then we set: ηi,K = γ/
��K j=1 ∆wi,j2. Because these learning rates are computed only from the summed squared gradients of each parameter, Adagrad is easily implemented locally within each parameter server shard. The value of γ, the constant scaling factor for all learning rates, is generally larger (perhaps by an order of magnitude) than the best fixed learning rate used without Adagrad.
The use of Adagrad extends the maximum number of model replicas that can productively work simultaneously, and combined with a practice of "warmstarting" model training with only a single model replica before unleashing the other replicas, it has virtually eliminated stability concerns in training deep networks using Downpour SGD (see results in Section 5).
Sandblaster L-BFGS
Batch methods have been shown to work well in training small deep networks. To apply these methods to large models and large datasets, we introduce the Sandblaster batch optimization framework and discuss an implementation of L-BFGS using this framework.
A key idea in Sandblaster is distributed parameter storage and manipulation. The core of the optimization algorithm (e.g L-BFGS) resides in a coordinator process (Figure 2), which does not have direct access to the model parameters. Instead, the coordinator issues commands drawn from a small set of operations (e.g., dot product, scaling, coefficient-wise addition, multiplication) that can be performed by each parameter server shard independently, with the results being stored locally on the same shard. Additional information, e.g the history cache for L-BFGS, is also stored on the parameter server shard on which it was computed. This allows running large models (billions of parameters) without incurring the overhead of sending all the parameters and gradients to a single central server. (See the Appendix for pseudocode.)
In typical parallelized implementations of L-BFGS, data is distributed to many machines and each machine is responsible for computing the gradient on a specific subset of data examples. The gradients are sent back to a central server (or aggregated via a tree ). Many such methods wait for the slowest machine, and therefore do not scale well to large shared clusters. To account for this problem, we employ the following load balancing scheme: The coordinator assigns each of the N model replicas a small portion of work, much smaller than 1/Nth of the total size of a batch, and assigns replicas new portions whenever they are free. With this approach, faster model replicas do more work than slower replicas. To further manage slow model replicas at the end of a batch, the coordinator schedules multiple copies of the outstanding portions and uses the result from whichever model replica finishes first. This scheme is similar to the use of "backup tasks" in the MapReduce framework. Prefetching of data, along with supporting data affinity by assigning sequential
5 portions of data to the same worker makes data access a non-issue. In contrast with Downpour
SGD, which requires relatively high frequency, high bandwidth parameter synchronization with the parameter server, Sandblaster workers only fetch parameters at the beginning of each batch (when they have been updated by the coordinator), and only send the gradients every few completed portions (to protect against replica failures and restarts).
Experiments
We evaluated our optimization algorithms by applying them to training models for two different deep learning problems: object recognition in still images and acoustic processing for speech recognition.
The speech recognition task was to classify the central region (or frame) in a short snippet of audio as one of several thousand acoustic states. We used a deep network with five layers: four hidden layer with sigmoidal activations and 2560 nodes each, and a softmax output layer with 8192 nodes. The input representation was 11 consecutive overlapping 25 ms frames of speech, each represented by
40 log-energy values. The network was fully-connected layer-to-layer, for a total of approximately
42 million model parameters. We trained on a data set of 1.1 billion weakly labeled examples, and evaluated on a hold out test set. See for similar deep network configurations and training procedures.
For visual object recognition we trained a larger neural network with locally-connected receptive fields on the ImageNet data set of 16 million images, each of which we scaled to 100x100 pixels.
The network had three stages, each composed of filtering, pooling and local contrast normalization, where each node in the filtering layer was connected to a 10x10 patch in the layer below. Our infrastructure allows many nodes to connect to the same input patch, and we ran experiments varying the number of identically connected nodes from 8 to 36. The output layer consisted of 21 thousand one-vs-all logistic classifier nodes, one for each of the ImageNet object categories. See for similar deep network configurations and training procedures.
Model parallelism benchmarks:
To explore the scaling behavior of DistBelief model parallelism(Section 3), we measured the mean time to process a single mini-batch for simple SGD training as a function of the number of partitions (machines) used in a single model instance. In Figure 3 we quantify the impact of parallelizing across N machines by reporting the average training speed-up: the ratio of the time taken using only a single machine to the time taken using N. Speedups for inference steps in these models are similar and are not shown here.
The moderately sized speech model runs fastest on 8 machines, computing 2.2× faster than using a single machine. (Models were configured to use no more than 20 cores per machine.) Partitioning
Machines per model instance
Training speed�up
Speech: 42M parameters
Images: 80M parameters
Images: 330M parameters
Images: 1.7B parameters
Figure 3: Training speed-up for four different deep networks as a function of machines allocated to a single DistBelief model instance. Models with more parameters benefit more from the use of additional machines than do models with fewer parameters.
Time (hours)
Average Frame Accuracy (%)
Accuracy on Training Set
SGD 
DownpourSGD 
DownpourSGD w/Adagrad
Sandblaster L−BFGS 
Time (hours)
Average Frame Accuracy (%)
Accuracy on Test Set
SGD 
GPU 
DownpourSGD 
DownpourSGD w/Adagrad
DownpourSGD w/Adagrad
Sandblaster L−BFGS 
Figure 4: Left: Training accuracy (on a portion of the training set) for different optimization methods. Right: Classification accuracy on the hold out test set as a function of training time. Downpour and Sandblaster experiments initialized using the same ∼10 hour warmstart of simple SGD. the model on more than 8 machines actually slows training, as network overhead starts to dominate in the fully-connected network structure and there is less work for each machine to perform with more partitions.
In contrast, the much larger, locally-connected image models can benefit from using many more machines per model replica. The largest model, with 1.7 billion parameters benefits the most, giving a speedup of more than 12× using 81 machines. For these large models using more machines continues to increase speed, but with diminishing returns.
Optimization method comparisons:
To evaluate the proposed distributed optimization procedures, we ran the speech model described above in a variety of configurations. We consider two baseline optimization procedures: training a DistBelief model (on 8 partitions) using conventional(single replica) SGD, and training the identical model on a GPU using CUDA. The three distributed optimization methods we compare to these baseline methods are: Downpour SGD with a fixed learning rate, Downpour SGD with Adagrad learning rates, and Sandblaster L-BFGS.
Figure 4 shows classification performance as a function of training time for each of these methods on both the training and test sets. Our goal is to obtain the maximum test set accuracy in the minimum amount of training time, regardless of resource requirements. Conventional single replica
SGD (black curves) is the slowest to train. Downpour SGD with 20 model replicas (blue curves) shows a significant improvement. Downpour SGD with 20 replicas plus Adagrad (orange curve) is modestly faster. Sandblaster L-BFGS using 2000 model replicas (green curves) is considerably faster yet again. The fastest, however, is Downpour SGD plus Adagrad with 200 model replicas (red curves). Given access to sufficient CPU resourses, both Sandblaster L-BFGS and Downpour SGD with Adagrad can train models substantially faster than a high performance GPU.
Though we did not confine the above experiments to a fixed resource budget, it is interesting to consider how the various methods trade off resource consumption for performance. We analyze this by arbitrarily choosing a fixed test set accuracy (16%), and measuring the time each method took to reach that accuracy as a function of machines and utilized CPU cores, Figure 5. One of the four points on each traces corresponds to a training configuration shown in Figure 4, the other three points are alternative configurations.
In this plot, points closer to the origin are preferable in that they take less time while using fewer resources. In this regard Downpour SGD using Adagrad appears to be the best trade-off: For any fixed budget of machines or cores, Downpour SGD with Adagrad takes less time to reach the accuracy target than either Downpour SGD with a fixed learning rate or Sandblaster L-BFGS. For any allotted training time to reach the accuracy target, Downpour SGD with Adagrad used few resources than
Sandblaster L-BFGS, and in many cases Downpour SGD with a fixed learning rate could not even reach the target within the deadline. The Sandblaster L-BFGS system does show promise in terms
Machines
Time (hours)
Time to 16% accuracy
Downpour SGD
Downpour SGD w/Adagrad
Sandblaster L−BFGS
GPU
Cores
Time (hours)
Time to 16% accuracy
Downpour SGD
Downpour SGD w/Adagrad
Sandblaster L−BFGS
GPU (CUDA cores)
Figure 5: Time to reach a fixed accuracy (16%) for different optimization strategies as a function of number of the machines (left) and cores (right). of its scaling with additional cores, suggesting that it may ultimately produce the fastest training times if used with an extremely large resource budget (e.g., 30k cores).
Application to ImageNet:
The previous experiments demonstrate that our techniques can accelerate the training of neural networks with tens of millions of parameters. However, the more significant advantage of our cluster-based approach to distributed optimization is its ability to scale to models that are much larger than can be comfortably fit on single machine, let alone a single GPU.
As a first step toward exploring the capabilities of very large neural networks, we used Downpour
SGD to train the 1.7 billion parameter image model described above on the ImageNet object classification task. As detailed in, this network achieved a cross-validated classification accuracy of over 15%, a relative improvement over 60% from the best performance we are aware of on the 21k category ImageNet classification task.
Conclusions
In this paper we introduced DistBelief, a framework for parallel distributed training of deep networks. Within this framework, we discovered several effective distributed optimization strategies.
We found that Downpour SGD, a highly asynchronous variant of SGD works surprisingly well for training nonconvex deep learning models. Sandblaster L-BFGS, a distributed implementation of L-BFGS, can be competitive with SGD, and its more efficient use of network bandwidth enables it to scale to a larger number of concurrent cores for training a single model. That said, the combination of Downpour SGD with the Adagrad adaptive learning rate procedure emerges as the clearly dominant method when working with a computational budget of 2000 CPU cores or less.
Adagrad was not originally designed to be used with asynchronous SGD, and neither method is typically applied to nonconvex problems. It is surprising, therefore, that they work so well together, and on highly nonlinear deep networks. We conjecture that Adagrad automatically stabilizes volatile parameters in the face of the flurry of asynchronous updates, and naturally adjusts learning rates to the demands of different layers in the deep network.
Our experiments show that our new large-scale training methods can use a cluster of machines to train even modestly sized deep networks significantly faster than a GPU, and without the GPU's limitation on the maximum size of the model. To demonstrate the value of being able to train larger models, we have trained a model with over 1 billion parameters to achieve better than state-of-the-art performance on the ImageNet object recognition challenge.
Acknowledgments
The authors would like to thank Samy Bengio, Tom Dean, John Duchi, Yuval Netzer, Patrick Nguyen, Yoram
Singer, Sebastian Thrun, and Vincent Vanhoucke for their indispensable advice, support, and comments.
References
 G. Dahl, D. Yu, L. Deng, and A. Acero. Context-dependent pre-trained deep neural networks for large vocabulary speech recognition. IEEE Transactions on Audio, Speech, and Language Processing, 2012.
 G. Hinton, L. Deng, D. Yu, G. Dahl, A. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. Sainath, and B. Kingsbury. Deep neural networks for acoustic modeling in speech recognition. IEEE
Signal Processing Magazine, 2012.
 D. C. Ciresan, U. Meier, L. M. Gambardella, and J. Schmidhuber. Deep big simple neural nets excel on handwritten digit recognition. CoRR, 2010.
 A. Coates, H. Lee, and A. Y. Ng. An analysis of single-layer networks in unsupervised feature learning.
In AISTATS 14, 2011.
 Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137–1155, 2003.
 R. Collobert and J. Weston. A unified architecture for natural language processing: Deep neural networks with multitask learning. In ICML, 2008.
 Q.V. Le, J. Ngiam, A. Coates, A. Lahiri, B. Prochnow, and A.Y. Ng. On optimization methods for deep learning. In ICML, 2011.
 R. Raina, A. Madhavan, and A. Y. Ng. Large-scale deep unsupervised learning using graphics processors.
In ICML, 2009.
 J. Martens. Deep learning via hessian-free optimization. In ICML, 2010.
 J. C. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12:2121–2159, 2011.
 Q. Shi, J. Petterson, G. Dror, J. Langford, A. Smola, A. Strehl, and V. Vishwanathan. Hash kernels. In
AISTATS, 2009.
 J. Langford, A. Smola, and M. Zinkevich. Slow learners are fast. In NIPS, 2009.
 G. Mann, R. McDonald, M. Mohri, N. Silberman, and D. Walker. Efficient large-scale distributed training of conditional maximum entropy models. In NIPS, 2009.
 R. McDonald, K. Hall, and G. Mann. Distributed training strategies for the structured perceptron. In
NAACL, 2010.
 M. Zinkevich, M. Weimer, A. Smola, and L. Li. Parallelized stochastic gradient descent. In NIPS, 2010.
 A. Agarwal, O. Chapelle, M. Dudik, and J. Langford. A reliable effective terascale linear learning system.
In AISTATS, 2011.
 A. Agarwal and J. Duchi. Distributed delayed stochastic optimization. In NIPS, 2011.
 F. Niu, B. Retcht, C. Re, and S. J. Wright. Hogwild! A lock-free approach to parallelizing stochastic gradient descent. In NIPS, 2011.
 J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu, G. Desjardins, J. Turian, D. Warde-Farley, and Y. Bengio. Theano: a CPU and GPU math expression compiler. In SciPy, 2010.
 D. Ciresan, U. Meier, and J. Schmidhuber. Multi-column deep neural networks for image classification.
Technical report, IDSIA, 2012.
 L. Deng, D. Yu, and J. Platt. Scalable stacking and learning for building deep architectures. In ICASSP, A. Krizhevsky. Learning multiple layers of features from tiny images. Technical report, U. Toronto, 2009.
 J. Dean and S. Ghemawat. Map-Reduce: simplified data processing on large clusters. CACM, 2008.
 Y. Low, J. Gonzalez, A. Kyrola, D. Bickson, C. Guestrin, and J. Hellerstein. Distributed GraphLab: A framework for machine learning in the cloud. In VLDB, 2012.
 L. Bottou. Stochastic gradient learning in neural networks. In Proceedings of Neuro-Nˆımes 91, 1991.
 Y. LeCun, L. Bottou, G. Orr, and K. Muller. Efficient backprop. In Neural Networks: Tricks of the trade.
Springer, 1998.
 V. Vanhoucke, A. Senior, and M. Z. Mao. Improving the speed of neural networks on cpus. In Deep
Learning and Unsupervised Feature Learning Workshop, NIPS 2011, 2011.
 J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical
Image Database. In CVPR, 2009.
 Q.V. Le, M.A. Ranzato, R. Monga, M. Devin, K. Chen, G.S. Corrado, J. Dean, and A.Y. Ng. Building high-level features using large scale unsupervised learning. In ICML, 2012.
Large Scale Distributed Deep Networks: Appendix
Jeffrey Dean, Greg S. Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc V. Le, Mark Z. Mao, Marc'Aurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang, Andrew Y. Ng
{jeff, gcorrado}@google.com
Google Inc., Mountain View, CA
Appendix
For completeness, here we provide pseudocode for the model replica (client) side of Downpour SGD(Algorithm 0.1), and Sandblaster L-BFGS (Algorithm 0.2).
Algorithm 1.1: DOWNPOURSGDCLIENT(α, nfetch, npush) procedure STARTASYNCHRONOUSLYFETCHINGPARAMETERS(parameters) parameters ← GETPARAMETERSFROMPARAMSERVER() procedure STARTASYNCHRONOUSLYPUSHINGGRADIENTS(accruedgradients)
SENDGRADIENTSTOPARAMSERVER(accruedgradients) accruedgradients ← 0 main global parameters, accruedgradients step ← 0 accruedgradients ← 0 while true do
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
� if (step mod nfetch) == 0 then STARTASYNCHRONOUSLYFETCHINGPARAMETERS(parameters) data ← GETNEXTMINIBATCH() gradient ← COMPUTEGRADIENT(parameters, data) accruedgradients ← accruedgradients + gradient parameters ← parameters − α ∗ gradient if (step mod npush) == 0 then STARTASYNCHRONOUSLYPUSHINGGRADIENTS(accruedgradients) step ← step + 1
Sandblaster is a framework for distributed batch optimization procedures. An essential concept in Sandblaster is decomposing operations into local computation on the DistBelief parameter server.
By way of example, suppose we have 1 billion parameters and 10 parameter server shards, so that each shard has 1/10 of the parameters. It is possible to decompose L-BFGS into a sequence of scalar-vector products (α ×x) and vector-vector inner products (xT y), where each vector is 1 billion dimensional. If one shard is always responsible for the first 1/10 of every vector used internally in L-BFGS, and a second shard is always responsible for the second 1/10 of every vector, and so on up to the final shard always being responsible for the final 1/10 of every vector, it is possible to show that these scalar-vector and vector-vector operations can all be done in a distributed fashion with very little communication, so that any intermediate vector-valued results are automatically stored in the same distributed fashion, and any intermediate scalar-valued result is communicated to all the shards.
Algorithm 1.2: SANDBLASTERLBFGS() procedure REPLICA.PROCESSPORTION(portion) if (!hasParametersForStep) then parameters ← GETPARAMETERSFROMPARAMSERVER() data ← GETDATAPORTION(portion) gradient ← COMPUTEGRADIENT(parameters, data) localAccruedGradients ← localAccruedGradients + gradient procedure PARAMETERSERVER.PERFORMOPERATION(operation)
PerformOperation main step ← 0 while true do
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
� comment: PS: ParameterServer
PS.accruedgradients ← 0 while (batchProcessed < batchSize) do
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
� for all (modelReplicas)comment: Loop is parallel and asynchronous
�
�
�
�
�
�
�
�
�
�
�
�
� if (modelReplicaAvailable) then
�
REPLICA.PROCESSPORTION(modelReplica) batchProcessed ← batchProcessed + portion if (modelReplicaWorkDone and timeToSendGradients) then
�
SENDGRADIENTS(modelReplica)
PS.accruedGradients ← PS.accruedGradients + gradient
COMPUTELBFGSDIRECTION(PS.Gradients, PS.History, PS.Direction)
LINESEARCH(PS.Parameters, PS.Direction)
PS.UPDATEPARAMETERS(PS.parameters, PS.accruedGradients) step ← step + 1Journal of Machine Learning Research 15 (2014) 1929-1958
Submitted 11/13; Published 6/14
Dropout: A Simple Way to Prevent Neural Networks from
Overfitting
Nitish Srivastava nitish@cs.toronto.edu
Geoffrey Hinton hinton@cs.toronto.edu
Alex Krizhevsky kriz@cs.toronto.edu
Ilya Sutskever ilya@cs.toronto.edu
Ruslan Salakhutdinov rsalakhu@cs.toronto.edu
Department of Computer Science
University of Toronto
10 Kings College Road, Rm 3302
Toronto, Ontario, M5S 3G4, Canada.
Editor: Yoshua Bengio
Abstract
Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem.
The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.
Keywords: neural networks, regularization, model combination, deep learning
1. Introduction
Deep neural networks contain multiple non-linear hidden layers and this makes them very expressive models that can learn very complicated relationships between their inputs and outputs.
With limited training data, however, many of these complicated relationships will be the result of sampling noise, so they will exist in the training set but not in real test data even if it is drawn from the same distribution. This leads to overfitting and many methods have been developed for reducing it. These include stopping the training as soon as performance on a validation set starts to get worse, introducing weight penalties of various kinds such as L1 and L2 regularization and soft weight sharing (Nowlan and Hinton, 1992).
With unlimited computation, the best way to "regularize" a fixed-sized model is to average the predictions of all possible settings of the parameters, weighting each setting by c⃝2014 Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever and Ruslan Salakhutdinov.
Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov(a) Standard Neural Net(b) After applying dropout.
Figure 1: Dropout Neural Net Model. Left: A standard neural net with 2 hidden layers. Right:
An example of a thinned net produced by applying dropout to the network on the left.
Crossed units have been dropped. its posterior probability given the training data. This can sometimes be approximated quite well for simple or small models (Xiong et al., 2011; Salakhutdinov and Mnih, 2008), but we would like to approach the performance of the Bayesian gold standard using considerably less computation. We propose to do this by approximating an equally weighted geometric mean of the predictions of an exponential number of learned models that share parameters.
Model combination nearly always improves the performance of machine learning methods. With large neural networks, however, the obvious idea of averaging the outputs of many separately trained nets is prohibitively expensive. Combining several models is most helpful when the individual models are different from each other and in order to make neural net models different, they should either have different architectures or be trained on different data. Training many different architectures is hard because finding optimal hyperparameters for each architecture is a daunting task and training each large network requires a lot of computation. Moreover, large networks normally require large amounts of training data and there may not be enough data available to train different networks on different subsets of the data. Even if one was able to train many different large networks, using them all at test time is infeasible in applications where it is important to respond quickly.
Dropout is a technique that addresses both these issues. It prevents overfitting and provides a way of approximately combining exponentially many different neural network architectures efficiently.
The term "dropout" refers to dropping out units (hidden and visible) in a neural network. By dropping a unit out, we mean temporarily removing it from the network, along with all its incoming and outgoing connections, as shown in Figure 1.
The choice of which units to drop is random. In the simplest case, each unit is retained with a fixed probability p independent of other units, where p can be chosen using a validation set or can simply be set at 0.5, which seems to be close to optimal for a wide range of networks and tasks. For the input units, however, the optimal probability of retention is usually closer to 1 than to 0.5.
Dropout
Present with probability p w(a) At training time
Always present pw(b) At test time
Figure 2: Left: A unit at training time that is present with probability p and is connected to units in the next layer with weights w. Right: At test time, the unit is always present and the weights are multiplied by p. The output at test time is same as the expected output at training time.
Applying dropout to a neural network amounts to sampling a "thinned" network from it. The thinned network consists of all the units that survived dropout (Figure 1b). A neural net with n units, can be seen as a collection of 2n possible thinned neural networks.
These networks all share weights so that the total number of parameters is still O(n2), or less. For each presentation of each training case, a new thinned network is sampled and trained. So training a neural network with dropout can be seen as training a collection of 2n thinned networks with extensive weight sharing, where each thinned network gets trained very rarely, if at all.
At test time, it is not feasible to explicitly average the predictions from exponentially many thinned models. However, a very simple approximate averaging method works well in practice. The idea is to use a single neural net at test time without dropout. The weights of this network are scaled-down versions of the trained weights. If a unit is retained with probability p during training, the outgoing weights of that unit are multiplied by p at test time as shown in Figure 2. This ensures that for any hidden unit the expected output (under the distribution used to drop units at training time) is the same as the actual output at test time. By doing this scaling, 2n networks with shared weights can be combined into a single neural network to be used at test time. We found that training a network with dropout and using this approximate averaging method at test time leads to significantly lower generalization error on a wide variety of classification problems compared to training with other regularization methods.
The idea of dropout is not limited to feed-forward neural nets. It can be more generally applied to graphical models such as Boltzmann Machines.
In this paper, we introduce the dropout Restricted Boltzmann Machine model and compare it to standard Restricted
Boltzmann Machines (RBM). Our experiments show that dropout RBMs are better than standard RBMs in certain respects.
This paper is structured as follows. Section 2 describes the motivation for this idea.
Section 3 describes relevant previous work. Section 4 formally describes the dropout model.
Section 5 gives an algorithm for training dropout networks. In Section 6, we present our experimental results where we apply dropout to problems in different domains and compare it with other forms of regularization and model combination. Section 7 analyzes the effect of dropout on different properties of a neural network and describes how dropout interacts with the network's hyperparameters. Section 8 describes the Dropout RBM model. In Section 9 we explore the idea of marginalizing dropout. In Appendix A we present a practical guide
Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov for training dropout nets. This includes a detailed analysis of the practical considerations involved in choosing hyperparameters when training dropout networks.
2. Motivation
A motivation for dropout comes from a theory of the role of sex in evolution (Livnat et al., 2010). Sexual reproduction involves taking half the genes of one parent and half of the other, adding a very small amount of random mutation, and combining them to produce an offspring. The asexual alternative is to create an offspring with a slightly mutated copy of the parent's genes. It seems plausible that asexual reproduction should be a better way to optimize individual fitness because a good set of genes that have come to work well together can be passed on directly to the offspring. On the other hand, sexual reproduction is likely to break up these co-adapted sets of genes, especially if these sets are large and, intuitively, this should decrease the fitness of organisms that have already evolved complicated coadaptations. However, sexual reproduction is the way most advanced organisms evolved.
One possible explanation for the superiority of sexual reproduction is that, over the long term, the criterion for natural selection may not be individual fitness but rather mix-ability of genes. The ability of a set of genes to be able to work well with another random set of genes makes them more robust. Since a gene cannot rely on a large set of partners to be present at all times, it must learn to do something useful on its own or in collaboration with a small number of other genes. According to this theory, the role of sexual reproduction is not just to allow useful new genes to spread throughout the population, but also to facilitate this process by reducing complex co-adaptations that would reduce the chance of a new gene improving the fitness of an individual. Similarly, each hidden unit in a neural network trained with dropout must learn to work with a randomly chosen sample of other units. This should make each hidden unit more robust and drive it towards creating useful features on its own without relying on other hidden units to correct its mistakes. However, the hidden units within a layer will still learn to do different things from each other. One might imagine that the net would become robust against dropout by making many copies of each hidden unit, but this is a poor solution for exactly the same reason as replica codes are a poor way to deal with a noisy channel.
A closely related, but slightly different motivation for dropout comes from thinking about successful conspiracies.
Ten conspiracies each involving five people is probably a better way to create havoc than one big conspiracy that requires fifty people to all play their parts correctly. If conditions do not change and there is plenty of time for rehearsal, a big conspiracy can work well, but with non-stationary conditions, the smaller the conspiracy the greater its chance of still working. Complex co-adaptations can be trained to work well on a training set, but on novel test data they are far more likely to fail than multiple simpler co-adaptations that achieve the same thing.
3. Related Work
Dropout can be interpreted as a way of regularizing a neural network by adding noise to its hidden units. The idea of adding noise to the states of units has previously been used in the context of Denoising Autoencoders (DAEs) by Vincent et al. (2008, 2010) where noise
Dropout is added to the input units of an autoencoder and the network is trained to reconstruct the noise-free input. Our work extends this idea by showing that dropout can be effectively applied in the hidden layers as well and that it can be interpreted as a form of model averaging.
We also show that adding noise is not only useful for unsupervised feature learning but can also be extended to supervised learning problems. In fact, our method can be applied to other neuron-based architectures, for example, Boltzmann Machines. While
5% noise typically works best for DAEs, we found that our weight scaling procedure applied at test time enables us to use much higher noise levels. Dropping out 20% of the input units and 50% of the hidden units was often found to be optimal.
Since dropout can be seen as a stochastic regularization technique, it is natural to consider its deterministic counterpart which is obtained by marginalizing out the noise. In this paper, we show that, in simple cases, dropout can be analytically marginalized out to obtain deterministic regularization methods.
Recently, van der Maaten et al. (2013) also explored deterministic regularizers corresponding to different exponential-family noise distributions, including dropout (which they refer to as "blankout noise"). However, they apply noise to the inputs and only explore models with no hidden layers. Wang and Manning(2013) proposed a method for speeding up dropout by marginalizing dropout noise. Chen et al. (2012) explored marginalization in the context of denoising autoencoders.
In dropout, we minimize the loss function stochastically under a noise distribution.
This can be seen as minimizing an expected loss function. Previous work of Globerson and Roweis (2006); Dekel et al. (2010) explored an alternate setting where the loss is minimized when an adversary gets to pick which units to drop. Here, instead of a noise distribution, the maximum number of units that can be dropped is fixed. However, this work also does not explore models with hidden units.
4. Model Description
This section describes the dropout neural network model. Consider a neural network with
L hidden layers. Let l ∈ {1,..., L} index the hidden layers of the network. Let z(l) denote the vector of inputs into layer l, y(l) denote the vector of outputs from layer l (y(0) = x is the input). W (l) and b(l) are the weights and biases at layer l. The feed-forward operation of a standard neural network (Figure 3a) can be described as (for l ∈ {0,..., L − 1} and any hidden unit i) z(l+1) i
= w(l+1) i yl + b(l+1) i, y(l+1) i
= f(z(l+1) i
), where f is any activation function, for example, f(x) = 1/ (1 + exp(−x)).
With dropout, the feed-forward operation becomes (Figure 3b) r(l) j
∼
Bernoulli(p), �y(l)
= r(l) ∗ y(l), z(l+1) i
= w(l+1) i
�yl + b(l+1) i, y(l+1) i
= f(z(l+1) i
Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov
+1 y(l)
1 y(l)
2 y(l)
3 z(l+1) i y(l+1) i w(l+1) i b(l+1) i f(a) Standard network
�y(l)
�y(l)
�y(l)
3 z(l+1) i y(l+1) i y(l)
1 y(l)
2 y(l)
3 r(l)
1 r(l)
2 r(l)
3 w(l+1) i b(l+1) i f(b) Dropout network
Figure 3: Comparison of the basic operations of a standard and dropout network.
Here ∗ denotes an element-wise product. For any layer l, r(l) is a vector of independent
Bernoulli random variables each of which has probability p of being 1.
This vector is sampled and multiplied element-wise with the outputs of that layer, y(l), to create the thinned outputs �y(l). The thinned outputs are then used as input to the next layer. This process is applied at each layer. This amounts to sampling a sub-network from a larger network. For learning, the derivatives of the loss function are backpropagated through the sub-network. At test time, the weights are scaled as W (l) test = pW (l) as shown in Figure 2.
The resulting neural network is used without dropout.
5. Learning Dropout Nets
This section describes a procedure for training dropout neural nets.
5.1 Backpropagation
Dropout neural networks can be trained using stochastic gradient descent in a manner similar to standard neural nets. The only difference is that for each training case in a mini-batch, we sample a thinned network by dropping out units. Forward and backpropagation for that training case are done only on this thinned network. The gradients for each parameter are averaged over the training cases in each mini-batch. Any training case which does not use a parameter contributes a gradient of zero for that parameter. Many methods have been used to improve stochastic gradient descent such as momentum, annealed learning rates and L2 weight decay. Those were found to be useful for dropout neural networks as well.
One particular form of regularization was found to be especially useful for dropout— constraining the norm of the incoming weight vector at each hidden unit to be upper bounded by a fixed constant c. In other words, if w represents the vector of weights incident on any hidden unit, the neural network was optimized under the constraint ||w||2 ≤ c. This constraint was imposed during optimization by projecting w onto the surface of a ball of radius c, whenever w went out of it. This is also called max-norm regularization since it implies that the maximum value that the norm of any weight can take is c. The constant
Dropout c is a tunable hyperparameter, which is determined using a validation set.
Max-norm regularization has been previously used in the context of collaborative filtering (Srebro and Shraibman, 2005).
It typically improves the performance of stochastic gradient descent training of deep neural nets, even when no dropout is used.
Although dropout alone gives significant improvements, using dropout along with maxnorm regularization, large decaying learning rates and high momentum provides a significant boost over just using dropout. A possible justification is that constraining weight vectors to lie inside a ball of fixed radius makes it possible to use a huge learning rate without the possibility of weights blowing up. The noise provided by dropout then allows the optimization process to explore different regions of the weight space that would have otherwise been difficult to reach. As the learning rate decays, the optimization takes shorter steps, thereby doing less exploration and eventually settles into a minimum.
5.2 Unsupervised Pretraining
Neural networks can be pretrained using stacks of RBMs (Hinton and Salakhutdinov, 2006), autoencoders (Vincent et al., 2010) or Deep Boltzmann Machines (Salakhutdinov and Hinton, 2009). Pretraining is an effective way of making use of unlabeled data. Pretraining followed by finetuning with backpropagation has been shown to give significant performance boosts over finetuning from random initializations in certain cases.
Dropout can be applied to finetune nets that have been pretrained using these techniques. The pretraining procedure stays the same. The weights obtained from pretraining should be scaled up by a factor of 1/p. This makes sure that for each unit, the expected output from it under random dropout will be the same as the output during pretraining.
We were initially concerned that the stochastic nature of dropout might wipe out the information in the pretrained weights. This did happen when the learning rates used during finetuning were comparable to the best learning rates for randomly initialized nets. However, when the learning rates were chosen to be smaller, the information in the pretrained weights seemed to be retained and we were able to get improvements in terms of the final generalization error compared to not using dropout when finetuning.
6. Experimental Results
We trained dropout neural networks for classification problems on data sets in different domains. We found that dropout improved generalization performance on all data sets compared to neural networks that did not use dropout. Table 1 gives a brief description of the data sets. The data sets are
• MNIST : A standard toy data set of handwritten digits.
• TIMIT : A standard speech benchmark for clean speech recognition.
• CIFAR-10 and CIFAR-100 : Tiny natural images (Krizhevsky, 2009).
• Street View House Numbers data set (SVHN) : Images of house numbers collected by
Google Street View (Netzer et al., 2011).
• ImageNet : A large collection of natural images.
• Reuters-RCV1 : A collection of Reuters newswire articles.
Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov
• Alternative Splicing data set: RNA features for predicting alternative gene splicing(Xiong et al., 2011).
We chose a diverse set of data sets to demonstrate that dropout is a general technique for improving neural nets and is not specific to any particular application domain. In this section, we present some key results that show the effectiveness of dropout. A more detailed description of all the experiments and data sets is provided in Appendix B.
Data Set
Domain
Dimensionality
Training Set
Test Set
MNIST
Vision
784 (28 × 28 grayscale)
60K
10K
SVHN
Vision
3072 (32 × 32 color)
600K
26K
CIFAR-10/100
Vision
3072 (32 × 32 color)
60K
10K
ImageNet (ILSVRC-2012)
Vision
65536 (256 × 256 color)
1.2M
150K
TIMIT
Speech
2520 (120-dim, 21 frames)
1.1M frames
58K frames
Reuters-RCV1
Text
200K
200K
Alternative Splicing
Genetics
Table 1: Overview of the data sets used in this paper.
6.1 Results on Image Data Sets
We used five image data sets to evaluate dropout—MNIST, SVHN, CIFAR-10, CIFAR-100 and ImageNet. These data sets include different image types and training set sizes. Models which achieve state-of-the-art results on all of these data sets use dropout.
6.1.1 MNIST
Method
Unit
Type
Architecture
Error
Standard Neural Net (Simard et al., 2003)
Logistic
2 layers, 800 units
SVM Gaussian kernel
NA
NA
Dropout NN
Logistic
3 layers, 1024 units
Dropout NN
ReLU
3 layers, 1024 units
Dropout NN + max-norm constraint
ReLU
3 layers, 1024 units
Dropout NN + max-norm constraint
ReLU
3 layers, 2048 units
Dropout NN + max-norm constraint
ReLU
2 layers, 4096 units
Dropout NN + max-norm constraint
ReLU
2 layers, 8192 units
Dropout NN + max-norm constraint (Goodfellow et al., 2013)
Maxout
2 layers, (5 × 240) units
DBN + finetuning (Hinton and Salakhutdinov, 2006)
Logistic
500-500-2000
DBM + finetuning (Salakhutdinov and Hinton, 2009)
Logistic
500-500-2000
DBN + dropout finetuning
Logistic
500-500-2000
DBM + dropout finetuning
Logistic
500-500-2000
Table 2: Comparison of different models on MNIST.
The MNIST data set consists of 28 × 28 pixel handwritten digit images. The task is to classify the images into 10 digit classes. Table 2 compares the performance of dropout with other techniques. The best performing neural networks for the permutation invariant
Dropout setting that do not use dropout or unsupervised pretraining achieve an error of about
1.60% (Simard et al., 2003). With dropout the error reduces to 1.35%. Replacing logistic units with rectified linear units (ReLUs) (Jarrett et al., 2009) further reduces the error to
1.25%. Adding max-norm regularization again reduces it to 1.06%. Increasing the size of the network leads to better results. A neural net with 2 layers and 8192 units per layer gets down to 0.95% error. Note that this network has more than 65 million parameters and is being trained on a data set of size 60,000. Training a network of this size to give good generalization error is very hard with standard regularization methods and early stopping.
Dropout, on the other hand, prevents overfitting, even in this case. It does not even need early stopping. Goodfellow et al. (2013) showed that results can be further improved to
0.94% by replacing ReLU units with maxout units. All dropout nets use p = 0.5 for hidden units and p = 0.8 for input units. More experimental details can be found in Appendix B.1.
Dropout nets pretrained with stacks of RBMs and Deep Boltzmann Machines also give improvements as shown in Table 2. DBM—pretrained dropout nets achieve a test error of 0.79% which is the best performance ever reported for the permutation invariant setting.
We note that it possible to obtain better results by using 2-D spatial information and augmenting the training set with distorted versions of images from the standard training set. We demonstrate the effectiveness of dropout in that setting on more interesting data sets.
Number of weight updates
Classification Error %
With dropout
Without dropout
@
R
@
@
R
Figure 4: Test error for different architectures with and without dropout.
The networks have 2 to 4 hidden layers each with 1024 to 2048 units.
In order to test the robustness of dropout, classification experiments were done with networks of many different architectures keeping all hyperparameters, including p, fixed.
Figure 4 shows the test error rates obtained for these different architectures as training progresses.
The same architectures trained with and without dropout have drastically different test errors as seen as by the two separate clusters of trajectories. Dropout gives a huge improvement across all architectures, without using hyperparameters that were tuned specifically for each architecture.
6.1.2 Street View House Numbers
The Street View House Numbers (SVHN)
Data Set (Netzer et al., 2011) consists of color images of house numbers collected by
Google Street View. Figure 5a shows some examples of images from this data set. The part of the data set that we use in our experiments consists of 32 × 32 color images roughly centered on a digit in a house number. The task is to identify that digit.
For this data set, we applied dropout to convolutional neural networks (LeCun et al., 1989). The best architecture that we found has three convolutional layers followed by 2 fully connected hidden layers. All hidden units were ReLUs. Each convolutional layer was
Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov
Method
Error %
Binary Features (WDCH) (Netzer et al., 2011)
HOG (Netzer et al., 2011)
Stacked Sparse Autoencoders (Netzer et al., 2011)
KMeans (Netzer et al., 2011)
Multi-stage Conv Net with average pooling (Sermanet et al., 2012)
Multi-stage Conv Net + L2 pooling (Sermanet et al., 2012)
Multi-stage Conv Net + L4 pooling + padding (Sermanet et al., 2012)
Conv Net + max-pooling
Conv Net + max pooling + dropout in fully connected layers
Conv Net + stochastic pooling (Zeiler and Fergus, 2013)
Conv Net + max pooling + dropout in all layers
Conv Net + maxout (Goodfellow et al., 2013)
Human Performance
Table 3: Results on the Street View House Numbers data set. followed by a max-pooling layer. Appendix B.2 describes the architecture in more detail.
Dropout was applied to all the layers of the network with the probability of retaining a hidden unit being p = (0.9, 0.75, 0.75, 0.5, 0.5, 0.5) for the different layers of the network (going from input to convolutional layers to fully connected layers). Max-norm regularization was used for weights in both convolutional and fully connected layers. Table 3 compares the results obtained by different methods. We find that convolutional nets outperform other methods. The best performing convolutional nets that do not use dropout achieve an error rate of 3.95%. Adding dropout only to the fully connected layers reduces the error to 3.02%.
Adding dropout to the convolutional layers as well further reduces the error to 2.55%. Even more gains can be obtained by using maxout units.
The additional gain in performance obtained by adding dropout in the convolutional layers (3.02% to 2.55%) is worth noting. One may have presumed that since the convolutional layers don't have a lot of parameters, overfitting is not a problem and therefore dropout would not have much effect. However, dropout in the lower layers still helps because it provides noisy inputs for the higher fully connected layers which prevents them from overfitting.
6.1.3 CIFAR-10 and CIFAR-100
The CIFAR-10 and CIFAR-100 data sets consist of 32 × 32 color images drawn from 10 and 100 categories respectively. Figure 5b shows some examples of images from this data set. A detailed description of the data sets, input preprocessing, network architectures and other experimental details is given in Appendix B.3. Table 4 shows the error rate obtained by different methods on these data sets. Without any data augmentation, Snoek et al.(2012) used Bayesian hyperparameter optimization to obtained an error rate of 14.98% on
CIFAR-10. Using dropout in the fully connected layers reduces that to 14.32% and adding dropout in every layer further reduces the error to 12.61%. Goodfellow et al. (2013) showed that the error is further reduced to 11.68% by replacing ReLU units with maxout units. On
CIFAR-100, dropout reduces the error from 43.48% to 37.20% which is a huge improvement.
No data augmentation was used for either data set (apart from the input dropout).
Dropout(a) Street View House Numbers (SVHN)(b) CIFAR-10
Figure 5: Samples from image data sets. Each row corresponds to a different category.
Method
CIFAR-10
CIFAR-100
Conv Net + max pooling (hand tuned)
Conv Net + stochastic pooling (Zeiler and Fergus, 2013)
Conv Net + max pooling (Snoek et al., 2012)Conv Net + max pooling + dropout fully connected layers
Conv Net + max pooling + dropout in all layers
Conv Net + maxout (Goodfellow et al., 2013)
Table 4: Error rates on CIFAR-10 and CIFAR-100.
6.1.4 ImageNet
ImageNet is a data set of over 15 million labeled high-resolution images belonging to roughly
22,000 categories. Starting in 2010, as part of the Pascal Visual Object Challenge, an annual competition called the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) has been held. A subset of ImageNet with roughly 1000 images in each of 1000 categories is used in this challenge. Since the number of categories is rather large, it is conventional to report two error rates: top-1 and top-5, where the top-5 error rate is the fraction of test images for which the correct label is not among the five labels considered most probable by the model. Figure 6 shows some predictions made by our model on a few test images.
ILSVRC-2010 is the only version of ILSVRC for which the test set labels are available, so most of our experiments were performed on this data set. Table 5 compares the performance of different methods. Convolutional nets with dropout outperform other methods by a large margin. The architecture and implementation details are described in detail in Krizhevsky et al. (2012).
Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov
Figure 6: Some ImageNet test cases with the 4 most probable labels as predicted by our model.
The length of the horizontal bars is proportional to the probability assigned to the labels by the model. Pink indicates ground truth.
Model
Top-1
Top-5
Sparse Coding (Lin et al., 2010)
SIFT + Fisher Vectors (Sanchez and Perronnin, 2011)
Conv Net + dropout (Krizhevsky et al., 2012)
Table 5: Results on the ILSVRC-2010 test set.
Model
Top-1(val)
Top-5(val)
Top-5(test)
SVM on Fisher Vectors of Dense SIFT and Color StatisticsAvg of classifiers over FVs of SIFT, LBP, GIST and CSIFTConv Net + dropout (Krizhevsky et al., 2012)Avg of 5 Conv Nets + dropout (Krizhevsky et al., 2012)
Table 6: Results on the ILSVRC-2012 validation/test set.
Our model based on convolutional nets and dropout won the ILSVRC-2012 competition.
Since the labels for the test set are not available, we report our results on the test set for the final submission and include the validation set results for different variations of our model. Table 6 shows the results from the competition. While the best methods based on standard vision features achieve a top-5 error rate of about 26%, convolutional nets with dropout achieve a test error of about 16% which is a staggering difference. Figure 6 shows some examples of predictions made by our model. We can see that the model makes very reasonable predictions, even when its best guess is not correct.
6.2 Results on TIMIT
Next, we applied dropout to a speech recognition task. We use the TIMIT data set which consists of recordings from 680 speakers covering 8 major dialects of American English reading ten phonetically-rich sentences in a controlled noise-free environment.
Dropout neural networks were trained on windows of 21 log-filter bank frames to predict the label of the central frame.
No speaker dependent operations were performed.
Appendix B.4 describes the data preprocessing and training details. Table 7 compares dropout neural
Dropout nets with other models. A 6-layer net gives a phone error rate of 23.4%. Dropout further improves it to 21.8%. We also trained dropout nets starting from pretrained weights. A
4-layer net pretrained with a stack of RBMs get a phone error rate of 22.7%. With dropout, this reduces to 19.7%. Similarly, for an 8-layer net the error reduces from 20.5% to 19.7%.
Method
Phone Error Rate%
NN (6 layers) (Mohamed et al., 2010)
Dropout NN (6 layers)
DBN-pretrained NN (4 layers)
DBN-pretrained NN (6 layers) (Mohamed et al., 2010)
DBN-pretrained NN (8 layers) (Mohamed et al., 2010)
20.7 mcRBM-DBN-pretrained NN (5 layers) (Dahl et al., 2010)
DBN-pretrained NN (4 layers) + dropout
DBN-pretrained NN (8 layers) + dropout
Table 7: Phone error rate on the TIMIT core test set.
6.3 Results on a Text Data Set
To test the usefulness of dropout in the text domain, we used dropout networks to train a document classifier. We used a subset of the Reuters-RCV1 data set which is a collection of over 800,000 newswire articles from Reuters. These articles cover a variety of topics. The task is to take a bag of words representation of a document and classify it into 50 disjoint topics. Appendix B.5 describes the setup in more detail. Our best neural net which did not use dropout obtained an error rate of 31.05%. Adding dropout reduced the error to
29.62%. We found that the improvement was much smaller compared to that for the vision and speech data sets.
6.4 Comparison with Bayesian Neural Networks
Dropout can be seen as a way of doing an equally-weighted averaging of exponentially many models with shared weights. On the other hand, Bayesian neural networks (Neal, 1996) are the proper way of doing model averaging over the space of neural network structures and parameters.
In dropout, each model is weighted equally, whereas in a Bayesian neural network each model is weighted taking into account the prior and how well the model fits the data, which is the more correct approach. Bayesian neural nets are extremely useful for solving problems in domains where data is scarce such as medical diagnosis, genetics, drug discovery and other computational biology applications. However, Bayesian neural nets are slow to train and difficult to scale to very large network sizes. Besides, it is expensive to get predictions from many large nets at test time. On the other hand, dropout neural nets are much faster to train and use at test time. In this section, we report experiments that compare Bayesian neural nets with dropout neural nets on a small data set where Bayesian neural networks are known to perform well and obtain state-of-the-art results. The aim is to analyze how much does dropout lose compared to Bayesian neural nets.
The data set that we use (Xiong et al., 2011) comes from the domain of genetics. The task is to predict the occurrence of alternative splicing based on RNA features. Alternative splicing is a significant cause of cellular diversity in mammalian tissues. Predicting the Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov
Method
Code Quality (bits)
Neural Network (early stopping) (Xiong et al., 2011)
Regression, PCA (Xiong et al., 2011)
SVM, PCA (Xiong et al., 2011)
Neural Network with dropout
Bayesian Neural Network (Xiong et al., 2011)
Table 8: Results on the Alternative Splicing Data Set. occurrence of alternate splicing in certain tissues under different conditions is important for understanding many human diseases. Given the RNA features, the task is to predict the probability of three splicing related events that biologists care about. The evaluation metric is Code Quality which is a measure of the negative KL divergence between the target and the predicted probability distributions (higher is better). Appendix B.6 includes a detailed description of the data set and this performance metric.
Table 8 summarizes the performance of different models on this data set. Xiong et al.(2011) used Bayesian neural nets for this task. As expected, we found that Bayesian neural nets perform better than dropout. However, we see that dropout improves significantly upon the performance of standard neural nets and outperforms all other methods. The challenge in this data set is to prevent overfitting since the size of the training set is small.
One way to prevent overfitting is to reduce the input dimensionality using PCA. Thereafter, standard techniques such as SVMs or logistic regression can be used. However, with dropout we were able to prevent overfitting without the need to do dimensionality reduction. The dropout nets are very large (1000s of hidden units) compared to a few tens of units in the Bayesian network. This shows that dropout has a strong regularizing effect.
6.5 Comparison with Standard Regularizers
Several regularization methods have been proposed for preventing overfitting in neural networks. These include L2 weight decay (more generally Tikhonov regularization (Tikhonov, 1943)), lasso (Tibshirani, 1996), KL-sparsity and max-norm regularization. Dropout can be seen as another way of regularizing neural networks. In this section we compare dropout with some of these regularization methods using the MNIST data set.
The same network architecture (784-1024-1024-2048-10) with ReLUs was trained using stochastic gradient descent with different regularizations. Table 9 shows the results.
The values of different hyperparameters associated with each kind of regularization (decay constants, target sparsity, dropout rate, max-norm upper bound) were obtained using a validation set. We found that dropout combined with max-norm regularization gives the lowest generalization error.
7. Salient Features
The experiments described in the previous section provide strong evidence that dropout is a useful technique for improving neural networks. In this section, we closely examine how dropout affects a neural network. We analyze the effect of dropout on the quality of features produced. We see how dropout affects the sparsity of hidden unit activations. We
Dropout
Method
Test Classification error %
L2
L2 + L1 applied towards the end of training
L2 + KL-sparsity
Max-norm
Dropout + L2
Dropout + Max-norm
Table 9: Comparison of different regularization methods on MNIST. also see how the advantages obtained from dropout vary with the probability of retaining units, size of the network and the size of the training set. These observations give some insight into why dropout works so well.
7.1 Effect on Features(a) Without dropout(b) Dropout with p = 0.5.
Figure 7: Features learned on MNIST with one hidden layer autoencoders having 256 rectified linear units.
In a standard neural network, the derivative received by each parameter tells it how it should change so the final loss function is reduced, given what all other units are doing.
Therefore, units may change in a way that they fix up the mistakes of the other units.
This may lead to complex co-adaptations. This in turn leads to overfitting because these co-adaptations do not generalize to unseen data. We hypothesize that for each hidden unit, dropout prevents co-adaptation by making the presence of other hidden units unreliable.
Therefore, a hidden unit cannot rely on other specific units to correct its mistakes. It must perform well in a wide variety of different contexts provided by the other hidden units. To observe this effect directly, we look at the first level features learned by neural networks trained on visual tasks with and without dropout.
Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov
Figure 7a shows features learned by an autoencoder on MNIST with a single hidden layer of 256 rectified linear units without dropout. Figure 7b shows the features learned by an identical autoencoder which used dropout in the hidden layer with p = 0.5. Both autoencoders had similar test reconstruction errors. However, it is apparent that the features shown in Figure 7a have co-adapted in order to produce good reconstructions. Each hidden unit on its own does not seem to be detecting a meaningful feature. On the other hand, in Figure 7b, the hidden units seem to detect edges, strokes and spots in different parts of the image. This shows that dropout does break up co-adaptations, which is probably the main reason why it leads to lower generalization errors.
7.2 Effect on Sparsity(a) Without dropout(b) Dropout with p = 0.5.
Figure 8: Effect of dropout on sparsity. ReLUs were used for both models. Left: The histogram of mean activations shows that most units have a mean activation of about 2.0. The histogram of activations shows a huge mode away from zero. Clearly, a large fraction of units have high activation. Right: The histogram of mean activations shows that most units have a smaller mean mean activation of about 0.7. The histogram of activations shows a sharp peak at zero. Very few units have high activation.
We found that as a side-effect of doing dropout, the activations of the hidden units become sparse, even when no sparsity inducing regularizers are present. Thus, dropout automatically leads to sparse representations. To observe this effect, we take the autoencoders trained in the previous section and look at the sparsity of hidden unit activations on a random mini-batch taken from the test set. Figure 8a and Figure 8b compare the sparsity for the two models. In a good sparse model, there should only be a few highly activated units for any data case. Moreover, the average activation of any unit across data cases should be low. To assess both of these qualities, we plot two histograms for each model. For each model, the histogram on the left shows the distribution of mean activations of hidden units across the minibatch. The histogram on the right shows the distribution of activations of the hidden units.
Comparing the histograms of activations we can see that fewer hidden units have high activations in Figure 8b compared to Figure 8a, as seen by the significant mass away from
Dropout zero for the net that does not use dropout. The mean activations are also smaller for the dropout net. The overall mean activation of hidden units is close to 2.0 for the autoencoder without dropout but drops to around 0.7 when dropout is used.
7.3 Effect of Dropout Rate
Dropout has a tunable hyperparameter p (the probability of retaining a unit in the network).
In this section, we explore the effect of varying this hyperparameter. The comparison is done in two situations.
1. The number of hidden units is held constant.
2. The number of hidden units is changed so that the expected number of hidden units that will be retained after dropout is held constant.
In the first case, we train the same network architecture with different amounts of dropout. We use a 784-2048-2048-2048-10 architecture. No input dropout was used. Figure 9a shows the test error obtained as a function of p. If the architecture is held constant, having a small p means very few units will turn on during training. It can be seen that this has led to underfitting since the training error is also high. We see that as p increases, the error goes down. It becomes flat when 0.4 ≤ p ≤ 0.8 and then increases as p becomes close to 1.
Probability of retaining a unit (p)
Classification Error %
Test Error
Training Error(a) Keeping n fixed.
Probability of retaining a unit (p)
Classification Error %
Test Error
Training Error(b) Keeping pn fixed.
Figure 9: Effect of changing dropout rates on MNIST.
Another interesting setting is the second case in which the quantity pn is held constant where n is the number of hidden units in any particular layer. This means that networks that have small p will have a large number of hidden units.
Therefore, after applying dropout, the expected number of units that are present will be the same across different architectures. However, the test networks will be of different sizes. In our experiments, we set pn = 256 for the first two hidden layers and pn = 512 for the last hidden layer.
Figure 9b shows the test error obtained as a function of p. We notice that the magnitude of errors for small values of p has reduced by a lot compared to Figure 9a (for p = 0.1 it fell from 2.7% to 1.7%). Values of p that are close to 0.6 seem to perform best for this choice of pn but our usual default value of 0.5 is close to optimal.
Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov
7.4 Effect of Data Set Size
One test of a good regularizer is that it should make it possible to get good generalization error from models with a large number of parameters trained on small data sets. This section explores the effect of changing the data set size when dropout is used with feedforward networks. Huge neural networks trained in the standard way overfit massively on small data sets. To see if dropout can help, we run classification experiments on MNIST and vary the amount of data given to the network.
Dataset size
Classification Error %
With dropout
Without dropout
Figure 10: Effect of varying data set size.
The results of these experiments are shown in Figure 10. The network was given data sets of size 100, 500, 1K, 5K, 10K and 50K chosen randomly from the MNIST training set.
The same network architecture (784-1024-1024-2048-10) was used for all data sets. Dropout with p = 0.5 was performed at all the hidden layers and p = 0.8 at the input layer. It can be observed that for extremely small data sets (100, 500) dropout does not give any improvements.
The model has enough parameters that it can overfit on the training data, even with all the noise coming from dropout. As the size of the data set is increased, the gain from doing dropout increases up to a point and then declines. This suggests that for any given architecture and dropout rate, there is a "sweet spot" corresponding to some amount of data that is large enough to not be memorized in spite of the noise but not so large that overfitting is not a problem anyways.
7.5 Monte-Carlo Model Averaging vs. Weight Scaling
Number of samples used for Monte-Carlo averaging (k)
Test Classification error %
Monte-Carlo Model Averaging
Approximate averaging by weight scaling
Figure 11: Monte-Carlo model averaging vs. weight scaling.
The efficient test time procedure that we propose is to do an approximate model combination by scaling down the weights of the trained neural network. An expensive but more correct way of averaging the models is to sample k neural nets using dropout for each test case and average their predictions.
As k → ∞, this Monte-Carlo model average gets close to the true model average. It is interesting to see empirically how many samples k are needed to match the performance of the approximate averaging method. By computing the error for different values of k we can see how quickly the error rate of the finite-sample average approaches the error rate of the true model average.
Dropout
We again use the MNIST data set and do classification by averaging the predictions of k randomly sampled neural networks. Figure 11 shows the test error rate obtained for different values of k. This is compared with the error obtained using the weight scaling method (shown as a horizontal line). It can be seen that around k = 50, the Monte-Carlo method becomes as good as the approximate method. Thereafter, the Monte-Carlo method is slightly better than the approximate method but well within one standard deviation of it. This suggests that the weight scaling method is a fairly good approximation of the true model average.
8. Dropout Restricted Boltzmann Machines
Besides feed-forward neural networks, dropout can also be applied to Restricted Boltzmann
Machines (RBM). In this section, we formally describe this model and show some results to illustrate its key properties.
8.1 Model Description
Consider an RBM with visible units v ∈ {0, 1}D and hidden units h ∈ {0, 1}F. It defines the following probability distribution
P(h, v; θ) =
Z(θ) exp(v⊤Wh + a⊤h + b⊤v).
Where θ = {W, a, b} represents the model parameters and Z is the partition function.
Dropout RBMs are RBMs augmented with a vector of binary random variables r ∈
{0, 1}F.
Each random variable rj takes the value 1 with probability p, independent of others. If rj takes the value 1, the hidden unit hj is retained, otherwise it is dropped from the model. The joint distribution defined by a Dropout RBM can be expressed as
P(r, h, v; p, θ)
=
P(r; p)P(h, v|r; θ), P(r; p)
=
F
� j=1 prj(1 − p)1−rj, P(h, v|r; θ)
=
Z′(θ, r) exp(v⊤Wh + a⊤h + b⊤v)
F
� j=1 g(hj, rj), g(hj, rj)
=
1(rj = 1) + 1(rj = 0)1(hj = 0).
Z′(θ, r) is the normalization constant. g(hj, rj) imposes the constraint that if rj = 0, hj must be 0. The distribution over h, conditioned on v and r is factorial
P(h|r, v)
=
F
� j=1
P(hj|rj, v), P(hj = 1|rj, v)
=
1(rj = 1)σ
� bj +
� i
Wijvi
�
Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov(a) Without dropout(b) Dropout with p = 0.5.
Figure 12: Features learned on MNIST by 256 hidden unit RBMs. The features are ordered by L2 norm.
The distribution over v conditioned on h is same as that of an RBM
P(v|h)
=
D
� i=1
P(vi|h), P(vi = 1|h)
= σ
�
�ai +
� j
Wijhj
�
�.
Conditioned on r, the distribution over {v, h} is same as the distribution that an RBM would impose, except that the units for which rj = 0 are dropped from h. Therefore, the Dropout RBM model can be seen as a mixture of exponentially many RBMs with shared weights each using a different subset of h.
8.2 Learning Dropout RBMs
Learning algorithms developed for RBMs such as Contrastive Divergence (Hinton et al., 2006) can be directly applied for learning Dropout RBMs. The only difference is that r is first sampled and only the hidden units that are retained are used for training. Similar to dropout neural networks, a different r is sampled for each training case in every minibatch.
In our experiments, we use CD-1 for training dropout RBMs.
8.3 Effect on Features
Dropout in feed-forward networks improved the quality of features by reducing co-adaptations.
This section explores whether this effect transfers to Dropout RBMs as well.
Figure 12a shows features learned by a binary RBM with 256 hidden units. Figure 12b shows features learned by a dropout RBM with the same number of hidden units. Features
Dropout(a) Without dropout(b) Dropout with p = 0.5.
Figure 13: Effect of dropout on sparsity. Left: The activation histogram shows that a large number of units have activations away from zero. Right: A large number of units have activations close to zero and very few units have high activation. learned by the dropout RBM appear qualitatively different in the sense that they seem to capture features that are coarser compared to the sharply defined stroke-like features in the standard RBM. There seem to be very few dead units in the dropout RBM relative to the standard RBM.
8.4 Effect on Sparsity
Next, we investigate the effect of dropout RBM training on sparsity of the hidden unit activations. Figure 13a shows the histograms of hidden unit activations and their means on a test mini-batch after training an RBM. Figure 13b shows the same for dropout RBMs.
The histograms clearly indicate that the dropout RBMs learn much sparser representations than standard RBMs even when no additional sparsity inducing regularizer is present.
9. Marginalizing Dropout
Dropout can be seen as a way of adding noise to the states of hidden units in a neural network. In this section, we explore the class of models that arise as a result of marginalizing this noise. These models can be seen as deterministic versions of dropout. In contrast to standard ("Monte-Carlo") dropout, these models do not need random bits and it is possible to get gradients for the marginalized loss functions. In this section, we briefly explore these models.
Deterministic algorithms have been proposed that try to learn models that are robust to feature deletion at test time (Globerson and Roweis, 2006). Marginalization in the context of denoising autoencoders has been explored previously (Chen et al., 2012). The marginalization of dropout noise in the context of linear regression was discussed in Srivastava (2013).
Wang and Manning (2013) further explored the idea of marginalizing dropout to speed-up training. van der Maaten et al. (2013) investigated different input noise distributions and Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov the regularizers obtained by marginalizing this noise. Wager et al. (2013) describes how dropout can be seen as an adaptive regularizer.
9.1 Linear Regression
First we explore a very simple case of applying dropout to the classical problem of linear regression. Let X ∈ RN×D be a data matrix of N data points. y ∈ RN be a vector of targets. Linear regression tries to find a w ∈ RD that minimizes
||y − Xw||2.
When the input X is dropped out such that any input dimension is retained with probability p, the input can be expressed as R∗X where R ∈ {0, 1}N×D is a random matrix with Rij ∼ Bernoulli(p) and ∗ denotes an element-wise product. Marginalizing the noise, the objective function becomes minimize w
ER∼Bernoulli(p)
�
||y − (R ∗ X)w||2�
This reduces to minimize w
||y − pXw||2 + p(1 − p)||Γw||2, where Γ = (diag(X⊤X))1/2.
Therefore, dropout with linear regression is equivalent, in expectation, to ridge regression with a particular form for Γ. This form of Γ essentially scales the weight cost for weight wi by the standard deviation of the ith dimension of the data. If a particular data dimension varies a lot, the regularizer tries to squeeze its weight more.
Another interesting way to look at this objective is to absorb the factor of p into w.
This leads to the following form minimize w
||y − X �w||2 + 1 − p p
||Γ�w||2, where �w = pw. This makes the dependence of the regularization constant on p explicit.
For p close to 1, all the inputs are retained and the regularization constant is small. As more dropout is done (by decreasing p), the regularization constant grows larger.
9.2 Logistic Regression and Deep Networks
For logistic regression and deep neural nets, it is hard to obtain a closed form marginalized model. However, Wang and Manning (2013) showed that in the context of dropout applied to logistic regression, the corresponding marginalized model can be trained approximately.
Under reasonable assumptions, the distributions over the inputs to the logistic unit and over the gradients of the marginalized model are Gaussian. Their means and variances can be computed efficiently. This approximate marginalization outperforms Monte-Carlo dropout in terms of training time and generalization performance.
However, the assumptions involved in this technique become successively weaker as more layers are added. Therefore, the results are not directly applicable to deep networks.
Dropout
Data Set
Architecture
Bernoulli dropout
Gaussian dropout
MNIST
2 layers, 1024 units each
1.08 ± 0.04
0.95 ± 0.04
CIFAR-10
3 conv + 2 fully connected layers
12.6 ± 0.1
12.5 ± 0.1
Table 10: Comparison of classification error % with Bernoulli and Gaussian dropout. For MNIST, the Bernoulli model uses p = 0.5 for the hidden units and p = 0.8 for the input units.
For CIFAR-10, we use p = (0.9, 0.75, 0.75, 0.5, 0.5, 0.5) going from the input layer to the top. The value of σ for the Gaussian dropout models was set to be
�
1−p p. Results were averaged over 10 different random seeds.
10. Multiplicative Gaussian Noise
Dropout involves multiplying hidden activations by Bernoulli distributed random variables which take the value 1 with probability p and 0 otherwise. This idea can be generalized by multiplying the activations with random variables drawn from other distributions. We recently discovered that multiplying by a random variable drawn from N(1, 1) works just as well, or perhaps better than using Bernoulli noise. This new form of dropout amounts to adding a Gaussian distributed random variable with zero mean and standard deviation equal to the activation of the unit.
That is, each hidden activation hi is perturbed to hi + hir where r ∼ N(0, 1), or equivalently hir′ where r′ ∼ N(1, 1). We can generalize this to r′ ∼ N(1, σ2) where σ becomes an additional hyperparameter to tune, just like p was in the standard (Bernoulli) dropout. The expected value of the activations remains unchanged, therefore no weight scaling is required at test time.
In this paper, we described dropout as a method where we retain units with probability p at training time and scale down the weights by multiplying them by a factor of p at test time.
Another way to achieve the same effect is to scale up the retained activations by multiplying by 1/p at training time and not modifying the weights at test time. These methods are equivalent with appropriate scaling of the learning rate and weight initializations at each layer.
Therefore, dropout can be seen as multiplying hi by a Bernoulli random variable rb that takes the value 1/p with probability p and 0 otherwise. E[rb] = 1 and V ar[rb] = (1 − p)/p.
For the Gaussian multiplicative noise, if we set σ2 = (1 − p)/p, we end up multiplying hi by a random variable rg, where E[rg] = 1 and V ar[rg] = (1 − p)/p. Therefore, both forms of dropout can be set up so that the random variable being multiplied by has the same mean and variance. However, given these first and second order moments, rg has the highest entropy and rb has the lowest. Both these extremes work well, although preliminary experimental results shown in Table 10 suggest that the high entropy case might work slightly better. For each layer, the value of σ in the Gaussian model was set to be
�
1−p p using the p from the corresponding layer in the Bernoulli model.
11. Conclusion
Dropout is a technique for improving neural networks by reducing overfitting. Standard backpropagation learning builds up brittle co-adaptations that work for the training data but do not generalize to unseen data. Random dropout breaks up these co-adaptations by
Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov making the presence of any particular hidden unit unreliable. This technique was found to improve the performance of neural nets in a wide variety of application domains including object classification, digit recognition, speech recognition, document classification and analysis of computational biology data. This suggests that dropout is a general technique and is not specific to any domain. Methods that use dropout achieve state-of-the-art results on SVHN, ImageNet, CIFAR-100 and MNIST. Dropout considerably improved the performance of standard neural nets on other data sets as well.
This idea can be extended to Restricted Boltzmann Machines and other graphical models. The central idea of dropout is to take a large model that overfits easily and repeatedly sample and train smaller sub-models from it. RBMs easily fit into this framework. We developed Dropout RBMs and empirically showed that they have certain desirable properties.
One of the drawbacks of dropout is that it increases training time. A dropout network typically takes 2-3 times longer to train than a standard neural network of the same architecture. A major cause of this increase is that the parameter updates are very noisy.
Each training case effectively tries to train a different random architecture. Therefore, the gradients that are being computed are not gradients of the final architecture that will be used at test time. Therefore, it is not surprising that training takes a long time. However, it is likely that this stochasticity prevents overfitting. This creates a trade-off between overfitting and training time. With more training time, one can use high dropout and suffer less overfitting. However, one way to obtain some of the benefits of dropout without stochasticity is to marginalize the noise to obtain a regularizer that does the same thing as the dropout procedure, in expectation. We showed that for linear regression this regularizer is a modified form of L2 regularization. For more complicated models, it is not obvious how to obtain an equivalent regularizer. Speeding up dropout is an interesting direction for future work.
Acknowledgments
This research was supported by OGS, NSERC and an Early Researcher Award.
Appendix A. A Practical Guide for Training Dropout Networks
Neural networks are infamous for requiring extensive hyperparameter tuning.
Dropout networks are no exception. In this section, we describe heuristics that might be useful for applying dropout.
A.1 Network Size
It is to be expected that dropping units will reduce the capacity of a neural network. If n is the number of hidden units in any layer and p is the probability of retaining a unit, then instead of n hidden units, only pn units will be present after dropout, in expectation.
Moreover, this set of pn units will be different each time and the units are not allowed to build co-adaptations freely. Therefore, if an n-sized layer is optimal for a standard neural net on any given task, a good dropout net should have at least n/p units. We found this to be a useful heuristic for setting the number of hidden units in both convolutional and fully connected networks.
Dropout
A.2 Learning Rate and Momentum
Dropout introduces a significant amount of noise in the gradients compared to standard stochastic gradient descent. Therefore, a lot of gradients tend to cancel each other. In order to make up for this, a dropout net should typically use 10-100 times the learning rate that was optimal for a standard neural net. Another way to reduce the effect the noise is to use a high momentum. While momentum values of 0.9 are common for standard nets, with dropout we found that values around 0.95 to 0.99 work quite a lot better. Using high learning rate and/or momentum significantly speed up learning.
A.3 Max-norm Regularization
Though large momentum and learning rate speed up learning, they sometimes cause the network weights to grow very large. To prevent this, we can use max-norm regularization.
This constrains the norm of the vector of incoming weights at each hidden unit to be bound by a constant c. Typical values of c range from 3 to 4.
A.4 Dropout Rate
Dropout introduces an extra hyperparameter—the probability of retaining a unit p. This hyperparameter controls the intensity of dropout. p = 1, implies no dropout and low values of p mean more dropout. Typical values of p for hidden units are in the range 0.5 to 0.8.
For input layers, the choice depends on the kind of input. For real-valued inputs (image patches or speech frames), a typical value is 0.8. For hidden layers, the choice of p is coupled with the choice of number of hidden units n. Smaller p requires big n which slows down the training and leads to underfitting. Large p may not produce enough dropout to prevent overfitting.
Appendix B. Detailed Description of Experiments and Data Sets
This section describes the network architectures and training details for the experimental results reported in this paper. The code for reproducing these results can be obtained from http://www.cs.toronto.edu/~nitish/dropout. The implementation is GPU-based. We used the excellent CUDA libraries—cudamat (Mnih, 2009) and cuda-convnet (Krizhevsky et al., 2012) to implement our networks.
B.1 MNIST
The MNIST data set consists of 60,000 training and 10,000 test examples each representing a 28×28 digit image. We held out 10,000 random training images for validation. Hyperparameters were tuned on the validation set such that the best validation error was produced after 1 million weight updates. The validation set was then combined with the training set and training was done for 1 million weight updates. This net was used to evaluate the performance on the test set. This way of using the validation set was chosen because we found that it was easy to set up hyperparameters so that early stopping was not required at all.
Therefore, once the hyperparameters were fixed, it made sense to combine the validation and training sets and train for a very long time.
Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov
The architectures shown in Figure 4 include all combinations of 2, 3, and 4 layer networks with 1024 and 2048 units in each layer. Thus, there are six architectures in all. For all the architectures (including the ones reported in Table 2), we used p = 0.5 in all hidden layers and p = 0.8 in the input layer. A final momentum of 0.95 and weight constraints with c = 2 was used in all the layers.
To test the limits of dropout's regularization power, we also experimented with 2 and 3 layer nets having 4096 and 8192 units. 2 layer nets gave improvements as shown in Table 2.
However, the three layer nets performed slightly worse than 2 layer ones with the same level of dropout. When we increased dropout, performance improved but not enough to outperform the 2 layer nets.
B.2 SVHN
The SVHN data set consists of approximately 600,000 training images and 26,000 test images. The training set consists of two parts—A standard labeled training set and another set of labeled examples that are easy. A validation set was constructed by taking examples from both the parts. Two-thirds of it were taken from the standard set (400 per class) and one-third from the extra set (200 per class), a total of 6000 samples. This same process is used by Sermanet et al. (2012). The inputs were RGB pixels normalized to have zero mean and unit variance. Other preprocessing techniques such as global or local contrast normalization or ZCA whitening did not give any noticeable improvements.
The best architecture that we found uses three convolutional layers each followed by a max-pooling layer. The convolutional layers have 96, 128 and 256 filters respectively.
Each convolutional layer has a 5 × 5 receptive field applied with a stride of 1 pixel. Each max pooling layer pools 3 × 3 regions at strides of 2 pixels. The convolutional layers are followed by two fully connected hidden layers having 2048 units each. All units use the rectified linear activation function. Dropout was applied to all the layers of the network with the probability of retaining the unit being p = (0.9, 0.75, 0.75, 0.5, 0.5, 0.5) for the different layers of the network (going from input to convolutional layers to fully connected layers). In addition, the max-norm constraint with c = 4 was used for all the weights. A momentum of 0.95 was used in all the layers. These hyperparameters were tuned using a validation set. Since the training set was quite large, we did not combine the validation set with the training set for final training. We reported test error of the model that had smallest validation error.
B.3 CIFAR-10 and CIFAR-100
The CIFAR-10 and CIFAR-100 data sets consists of 50,000 training and 10,000 test images each. They have 10 and 100 image categories respectively. These are 32 × 32 color images.
We used 5,000 of the training images for validation. We followed the procedure similar to MNIST, where we found the best hyperparameters using the validation set and then combined it with the training set. The images were preprocessed by doing global contrast normalization in each color channel followed by ZCA whitening. Global contrast normalization means that for image and each color channel in that image, we compute the mean of the pixel intensities and subtract it from the channel. ZCA whitening means that we mean center the data, rotate it onto its principle components, normalize each component
Dropout and then rotate it back. The network architecture and dropout rates are same as that for
SVHN, except the learning rates for the input layer which had to be set to smaller values.
B.4 TIMIT
The open source Kaldi toolkit (Povey et al., 2011) was used to preprocess the data into logfilter banks. A monophone system was trained to do a forced alignment and to get labels for speech frames. Dropout neural networks were trained on windows of 21 consecutive frames to predict the label of the central frame. No speaker dependent operations were performed.
The inputs were mean centered and normalized to have unit variance.
We used probability of retention p = 0.8 in the input layers and 0.5 in the hidden layers.
Max-norm constraint with c = 4 was used in all the layers. A momentum of 0.95 with a high learning rate of 0.1 was used. The learning rate was decayed as ϵ0(1 + t/T)−1. For
DBN pretraining, we trained RBMs using CD-1. The variance of each input unit for the Gaussian RBM was fixed to 1. For finetuning the DBN with dropout, we found that in order to get the best results it was important to use a smaller learning rate (about 0.01).
Adding max-norm constraints did not give any improvements.
B.5 Reuters
The Reuters RCV1 corpus contains more than 800,000 documents categorized into 103 classes. These classes are arranged in a tree hierarchy. We created a subset of this data set consisting of 402,738 articles and a vocabulary of 2000 words comprising of 50 categories in which each document belongs to exactly one class. The data was split into equal sized training and test sets. We tried many network architectures and found that dropout gave improvements in classification accuracy over all of them. However, the improvement was not as significant as that for the image and speech data sets. This might be explained by the fact that this data set is quite big (more than 200,000 training examples) and overfitting is not a very serious problem.
B.6 Alternative Splicing
The alternative splicing data set consists of data for 3665 cassette exons, 1014 RNA features and 4 tissue types derived from 27 mouse tissues. For each input, the target consists of 4 softmax units (one for tissue type). Each softmax unit has 3 states (inc, exc, nc) which are of the biological importance. For each softmax unit, the aim is to predict a distribution over these 3 states that matches the observed distribution from wet lab experiments as closely as possible. The evaluation metric is Code Quality which is defined as
|data points|
� i=1
� t∈tissue types
� s∈{inc, exc, nc} ps i,t log(qs t (ri)
¯ps
), where, ps i,t is the target probability for state s and tissue type t in input i; qs t (ri) is the predicted probability for state s in tissue type t for input ri and ¯ps is the average of ps i,t over i and t.
A two layer dropout network with 1024 units in each layer was trained on this data set.
A value of p = 0.5 was used for the hidden layer and p = 0.7 for the input layer. Max-norm regularization with high decaying learning rates was used. Results were averaged across the same 5 folds used by Xiong et al. (2011).
Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov
References
M. Chen, Z. Xu, K. Weinberger, and F. Sha.
Marginalized denoising autoencoders for domain adaptation.
In Proceedings of the 29th International Conference on Machine
Learning, pages 767–774. ACM, 2012.
G. E. Dahl, M. Ranzato, A. Mohamed, and G. E. Hinton. Phone recognition with the meancovariance restricted Boltzmann machine. In Advances in Neural Information Processing
Systems 23, pages 469–477, 2010.
O. Dekel, O. Shamir, and L. Xiao. Learning to classify with missing and corrupted features.
Machine Learning, 81(2):149–178, 2010.
A. Globerson and S. Roweis. Nightmare at test time: robust learning by feature deletion. In
Proceedings of the 23rd International Conference on Machine Learning, pages 353–360.
ACM, 2006.
I. J. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, and Y. Bengio. Maxout networks.
In Proceedings of the 30th International Conference on Machine Learning, pages 1319–
1327. ACM, 2013.
G. Hinton and R. Salakhutdinov. Reducing the dimensionality of data with neural networks.
Science, 313(5786):504 – 507, 2006.
G. E. Hinton, S. Osindero, and Y. Teh. A fast learning algorithm for deep belief nets.
Neural Computation, 18:1527–1554, 2006.
K. Jarrett, K. Kavukcuoglu, M. Ranzato, and Y. LeCun. What is the best multi-stage architecture for object recognition?
In Proceedings of the International Conference on
Computer Vision (ICCV'09). IEEE, 2009.
A. Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009.
A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems 25, pages
1106–1114, 2012.
Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D.
Jackel. Backpropagation applied to handwritten zip code recognition. Neural Computation, 1(4):541–551, 1989.
Y. Lin, F. Lv, S. Zhu, M. Yang, T. Cour, K. Yu, L. Cao, Z. Li, M.-H. Tsai, X. Zhou, T. Huang, and T. Zhang. Imagenet classification: fast descriptor coding and large-scale svm training. Large scale visual recognition challenge, 2010.
A. Livnat, C. Papadimitriou, N. Pippenger, and M. W. Feldman.
Sex, mixability, and modularity. Proceedings of the National Academy of Sciences, 107(4):1452–1457, 2010.
V. Mnih. CUDAMat: a CUDA-based matrix class for Python. Technical Report UTML
TR 2009-004, Department of Computer Science, University of Toronto, November 2009.
Dropout
A. Mohamed, G. E. Dahl, and G. E. Hinton. Acoustic modeling using deep belief networks.
IEEE Transactions on Audio, Speech, and Language Processing, 2010.
R. M. Neal. Bayesian Learning for Neural Networks. Springer-Verlag New York, Inc., 1996.
Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading digits in natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning 2011, 2011.
S. J. Nowlan and G. E. Hinton. Simplifying neural networks by soft weight-sharing. Neural
Computation, 4(4), 1992.
D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. Hannemann, P. Motlicek, Y. Qian, P. Schwarz, J. Silovsky, G. Stemmer, and K. Vesely. The Kaldi
Speech Recognition Toolkit. In IEEE 2011 Workshop on Automatic Speech Recognition and Understanding. IEEE Signal Processing Society, 2011.
R. Salakhutdinov and G. Hinton. Deep Boltzmann machines. In Proceedings of the International Conference on Artificial Intelligence and Statistics, volume 5, pages 448–455, R. Salakhutdinov and A. Mnih. Bayesian probabilistic matrix factorization using Markov chain Monte Carlo.
In Proceedings of the 25th International Conference on Machine
Learning. ACM, 2008.
J. Sanchez and F. Perronnin. High-dimensional signature compression for large-scale image classification.
In Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition, pages 1665–1672, 2011.
P. Sermanet, S. Chintala, and Y. LeCun. Convolutional neural networks applied to house numbers digit classification. In International Conference on Pattern Recognition (ICPR
P. Simard, D. Steinkraus, and J. Platt. Best practices for convolutional neural networks applied to visual document analysis. In Proceedings of the Seventh International Conference on Document Analysis and Recognition, volume 2, pages 958–962, 2003.
J. Snoek, H. Larochelle, and R. Adams. Practical Bayesian optimization of machine learning algorithms. In Advances in Neural Information Processing Systems 25, pages 2960–2968, N. Srebro and A. Shraibman. Rank, trace-norm and max-norm. In Proceedings of the 18th annual conference on Learning Theory, COLT'05, pages 545–560. Springer-Verlag, 2005.
N. Srivastava. Improving Neural Networks with Dropout. Master's thesis, University of Toronto, January 2013.
R. Tibshirani.
Regression shrinkage and selection via the lasso.
Journal of the Royal
Statistical Society. Series B. Methodological, 58(1):267–288, 1996.
Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov
A. N. Tikhonov. On the stability of inverse problems. Doklady Akademii Nauk SSSR, 39(5):
195–198, 1943.
L. van der Maaten, M. Chen, S. Tyree, and K. Q. Weinberger. Learning with marginalized corrupted features.
In Proceedings of the 30th International Conference on Machine
Learning, pages 410–418. ACM, 2013.
P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol. Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th International Conference on Machine Learning, pages 1096–1103. ACM, 2008.
P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. In Proceedings of the 27th International Conference on Machine Learning, pages
3371–3408. ACM, 2010.
S. Wager, S. Wang, and P. Liang. Dropout training as adaptive regularization. In Advances in Neural Information Processing Systems 26, pages 351–359, 2013.
S. Wang and C. D. Manning. Fast dropout training. In Proceedings of the 30th International
Conference on Machine Learning, pages 118–126. ACM, 2013.
H. Y. Xiong, Y. Barash, and B. J. Frey. Bayesian prediction of tissue-regulated splicing using RNA sequence and cellular context. Bioinformatics, 27(18):2554–2562, 2011.
M. D. Zeiler and R. Fergus. Stochastic pooling for regularization of deep convolutional neural networks. CoRR, abs/1301.3557, 2013.Fast Inference in Sparse Coding Algorithms with Applications to Object Recognition
Koray Kavukcuoglu
Marc'Aurelio Ranzato
Yann LeCun
Department of Computer Science
Courant Institute of Mathematical Sciences
New York University, New York, NY 10003
{koray,ranzato,yann}@cs.nyu.edu
December 4, 2008
Computational and Biological Learning Laboratory
Technical Report
CBLL-TR-2008-12-01†
Abstract
Adaptive sparse coding methods learn a possibly overcomplete set of basis functions, such that natural image patches can be reconstructed by linearly combining a small subset of these bases.
The applicability of these methods to visual object recognition tasks has been limited because of the prohibitive cost of the optimization algorithms required to compute the sparse representation. In this work we propose a simple and efficient algorithm to learn basis functions. After training, this model also provides a fast and smooth approximator to the optimal representation, achieving even better accuracy than exact sparse coding algorithms on visual object recognition tasks.
Introduction
Object recognition is one of the most challenging tasks in computer vision. Most methods for visual recognition rely on handcrafted features to represent images.
It has been shown that making these representations adaptive to image data can improve performance on vision tasks as demonstrated in in a supervised
†Presented at OPT 2008 Optimization for Machine Learning Workshop, Neural Information Processing Systems, 2008
INTRODUCTION
2 learning framework and in using unsupervised learning. In particular, learning sparse representations can be advantageous since features are more likely to be linearly separable in a high-dimensional space and they are more robust to noise. Many sparse coding algorithms have been shown to learn good local feature extractors for natural images. However, application of these methods to vision problems has been limited due to prohibitive cost of calculating sparse representations for a given image.
In this work, we propose an algorithm named Predictive Sparse Decomposition (PSD) that can simultaneously learn an overcomplete linear basis set, and produce a smooth and easy-to-compute approximator that predicts the optimal sparse representation. Experiments demonstrate that the predictor is over 100 times faster than the fastest sparse optimization algorithm, and yet produces features that yield better recognition accuracy on visual object recognition tasks than the optimal representations produced through optimization.
1.1 Sparse Coding Algorithms
Finding a representation Z ∈ Rm for a given signal Y ∈ Rn by linear combination of an overcomplete set of basis vectors, columns of matrix B ∈ Rn×m with m > n, has infinitely many solutions. In optimal sparse coding, the problem is formulated as: min ||Z||0 s.t. Y = BZ(1) where the ℓ0 "norm" is defined as the number of non-zero elements in a given vector. Unfortunately, the solution to this problem requires a combinatorial search, intractable in high-dimensional spaces. Matching Pursuit methods offer a greedy approximation to this problem.
Another way to approximate this problem is to make a convex relaxation by turning the ℓ0 norm into an ℓ1 norm.
This problem, dubbed Basis Pursuit in the signal processing community, has been shown to give the same solution to eq. (1), provided that the solution is sparse enough. Furthermore, the problem can be written as an unconstrained optimization problem:
L(Y, Z; B) = 1
2||Y − BZ||2
2 + λ||Z||1
This particular formulation, called Basis Pursuit Denoising, can be seen as minimizing an objective that penalizes the reconstruction error using a linear basis set and the sparsity of the corresponding representation. Many recent works have focused on efficiently solving the problem in eq. (2).
Yet, inference requires running some sort of iterative minimization algorithm that is always computationally expensive.
Additionally, some algorithms are also able to learn the set of basis functions.
The learning procedure finds the B matrix that minimizes the same loss of eq. (2). The columns of B are constrained to have unit norm in order to prevent trivial solutions where the loss is minimized by scaling down the coefficients
THE ALGORITHM
3 while scaling up the bases. Learning proceeds by alternating the optimization over Z to infer the representation for a given set of bases B, and the minimization over B for the given set of optimal Z found at the previous step.
Loosely speaking, basis functions learned on natural images under sparsity constraints are localized oriented edge detectors reminiscent of Gabor wavelets.
The Algorithm
In order to make inference efficient, we train a non-linear regressor that maps input patches Y to sparse representations Z. We consider the following nonlinear mapping:
F(Y ; G, W, D) = G tanh(WY + D)(3) where W ∈ Rm×n is a filter matrix, D ∈ Rm is a vector of biases, tanh is the hyperbolic tangent non-linearity, and G ∈ Rm×m is a diagonal matrix of gain coefficients allowing the outputs of F to compensate for the scaling of the input, given that the reconstruction performed by B uses bases with unit norm.
Let Pf collectively denote the parameters that are learned in this predictor, Pf = {G, W, D}. The goal of the algorithm is to make the prediction of the regressor, F(Y ; Pf) as close as possible to the optimal set of coefficients: Z∗ = arg minZ L(Y, Z; B) in eq. (2). This optimization can be carried out separately after the problem in eq. (2) has been solved. However, training becomes much faster by jointly optimizing the Pf and the set of bases B all together. This is achieved by adding another term to the loss function in eq. (2), enforcing the representation Z to be as close as possible to the feed-forward prediction
F(Y ; Pf):
L(Y, Z; B, Pf) = ∥Y − BZ∥2
2 + λ∥Z∥1 + α∥Z − F(Y ; Pf)∥2
Minimizing this loss with respect to Z produces a representation that simultaneously reconstructs the patch, is sparse, and is not too different from the predicted representation. If multiple solutions to the original loss (without the prediction term) exist, minimizing this compound loss will drive the system towards producing basis functions and optimal representations that are easily predictable. After training, the function F(Y ; Pf) will provide good and smooth approximations to the optimal sparse representations. Note that, a linear mapping would not be able to produce sparse representations using an overcomplete set because of the non-orthogonality of the filters, therefore a non-linear mapping is required.
2.1 Learning
The goal of learning is to find the optimal value of the basis functions B, as well as the value of the parameters in the regressor Pf. Learning proceeds by an on-line block coordinate gradient descent algorithm, alternating the following two steps for each training sample Y :
THE ALGORITHM
1. keeping the parameters Pf and B constant, minimize L(Y, Z; B, Pf) of eq. (4) with respect to Z, starting from the initial value provided by the regressor F(Y ; Pf). In our experiments we use gradient descent, but any other optimization method can be used;
2. using the optimal value of the coefficients Z provided by the previous step, update the parameters Pf and B by one step of stochastic gradient descent; The update is: U ← U − η ∂L
∂U, where U collectively denotes
{Pf, B} and η is the step size. The columns of B are then re-scaled to unit norm.
Interestingly, we recover different algorithms depending on the value of the parameter α:
• α = 0. The loss of eq. (4) reduces to the one in eq. (2). The learning algorithm becomes similar to Olshausen and Field's sparse coding algorithm. The regressor is trained separately from the set of basis functions
B.
• α ∈ (0, +∞). The parameters are updated taking into account also the constraint on the representation, using the same principle employed by
SESM training, for instance.
• α → +∞. The additional constraint on the representation (the third term in eq. (4)) becomes an equality, i.e. Z = F(Y ; Pf), and the model becomes similar to an auto-encoder neural network with a sparsity regularization term acting on the internal representation Z instead of a regularization acting on the parameters Pf and B.
In this paper, we always set α = 1. However, sec. 3 shows that training the regressor after training the set of bases B yields similar performance in terms of recognition accuracy. When the regressor is trained afterwards, the approximate representation is usually less sparse and the overall training time increases considerably. Finally, additional experiments not reported here show that training the system as an auto-encoder (α → +∞) provides a very fast and efficient algorithm that can produce good representations when the dimensionality of the representation is not much greater than the input dimensionality, i.e. m ≃ n.
When the sparse representation is highly overcomplete the block-coordinate descent algorithm with α ∈ (0, +∞) provides better features.
2.2 Inference
Once the parameters are learned, inferring the representation Z can be done in two ways.
Optimal inference consists of setting the representation to Z∗ = arg minz L, where L is defined in eq. (4), by running an iterative gradient descent algorithm involving two possibly large matrix-vector multiplications at each iteration (one for computing the value of the objective, and one for computing the derivatives
EXPERIMENTS
5 through B).
Approximate inference, on the other hand sets the representation to the value produced by F(Y ; Pf) as given in eq. (3), involving only a forward propagation through the regressor, i.e. a single matrix-vector multiplication.
Experiments
First, we demonstrate that the proposed algorithm (PSD) is able to produce good features for recognition by comparing to other unsupervised feature extraction algorithms, Principal Components Analysis (PCA), Restricted Boltzman
Machine (RBM), and Sparse Encoding Symmetric Machine (SESM).
Then, we compare the recognition accuracy and inference time of PSD feedforward approximation to feature sign algorithm, on the Caltech 101 dataset.
Finally we investigate the stability of representations under naturally changing inputs.
3.1 Comparison against PCA, RBM and SESM on the MNIST
The MNIST dataset has a training set with 60,000 handwritten digits of size
28x28 pixels, and a test set with 10,000 digits.
Each image is preprocessed by normalizing the pixel values so that their standard deviation is equal to
1. In this experiment the sparse representation has 256 units. This internal representation is used as a global feature vector and fed to a linear regularized logistic regression classifier. Fig. 1 shows the comparison between PSD (using feed-forward approximate codes) and, PCA, SESM, and RBM. Even though PSD provides the worst reconstruction error, it can achieve the best recognition accuracy on the test set under different number of training samples per class.
−10
RMSE
ERROR RATE %
10 samples
RMSE
ERROR RATE %
100 samples
RMSE
ERROR RATE %
1000 samples
RAW: train
RAW: test
PCA: train
PCA: test
SESM: train
SESM: test
RBM: train
RBM: test
PSD train
PSD test
Figure 1: Classification error on MNIST as a function of reconstruction error using raw pixel values and, PCA, RBM, SESM and PSD features. Left-to-Right
: 10-100-1000 samples per class are used for training a linear classifier on the features. The unsupervised algorithms were trained on the first 20,000 training samples of the MNIST dataset.
EXPERIMENTS
Table 1: Comparison between representations produced by FS and PSD. In order to compute the SNR, the noise is defined as (Signal − Approximation).
Comparison (Signal / Approximation)
Signal to Noise Ratio (SNR)
1. PSD Optimal / PSD Predictor
2. FS / PSD Optimal
3. FS / PSD Predictor
4. FS / Regressor
3.2 Comparison with Exact Algorithms
In order to quantify how well our jointly trained predictor given in eq. (3) approximates the optimal representations obtained by minimizing the loss in eq. (4) and the optimal representations that are produced by an exact algorithm minimizing eq. (2) such as feature sign (FS), we measure the average signal to noise ratio1 (SNR) over a test dataset of 20,000 natural image patches of size
9x9. The data set of images was constructed by randomly picking 9x9 patches from the images of the Berkeley dataset converted to gray-scale values, and these patches were normalized to have zero mean and unit standard deviation.
The algorithms were trained to learn sparse codes with 64 units2.
We compare representations obtained by "PSD Predictor" using the approximate inference, "PSD Optimal" using the optimal inference, "FS" minimizing eq. (2) with, and "Regressor" that is separately trained to approximate the exact optimal codes produced by FS. The results given in table 1 show that
PSD direct predictor achieves about the same SNR on the true optimal sparse representations produced by FS, as the Regressor that was trained to predict these representations.
Despite the lack of absolute precision in predicting the exact optimal sparse codes, PSD predictor achieves even better performance in recognition.
The
Caltech 101 dataset is pre-processed in the following way: 1) each image is converted to gray-scale, 2) it is down-sampled so that the longest side is 151 pixels, 3) the mean is subtracted and each pixel is divided by the image standard deviation, 4) the image is locally normalized by subtracting the weighted local mean from each pixel and dividing it by the weighted norm if this is larger than 1 with weights forming a 9x9 Gaussian window centered on each pixel, and 5) the image is 0-padded to 143x143 pixels. 64 feature detectors (either produced by FS or PSD predictor) were plugged into an image classification system that A) used the sparse coding algorithms convolutionally to produce
64 feature maps of size 128x128 for each pre-processed image, B) applied an absolute value rectification, C) computed an average down-sampling to a spatial resolution of 30x30 and D) used a linear SVM classifier to recognize the object
1SNR = 10log10(σ2 signal/σ2 noise)
2Principal Component Analysis shows that the effective dimensionality of 9x9 natural image patches is about 47 since the first 47 principal components capture the 95% of the variance in the data.
Hence, a 64-dimensional feature vector is actually an overcomplete representation for these 9x9 image patches.
EXPERIMENTS
Figure 2: a) 256 basis functions of size 12x12 learned by PSD, trained on the Berkeley dataset. Each 12x12 block is a column of matrix B in eq. (4), i.e. a basis function. b) Object recognition architecture: linear adaptive filter bank, followed by abs rectification, average down-sampling and linear SVM classifier.
Figure 3: a) Speed up for inferring the sparse representation achieved by PSD predictor over FS for a code with 64 units.
The feed-forward extraction is more than 100 times faster. b) Recognition accuracy versus measured sparsity(average ℓ1 norm of the representation) of PSD predictor compared to the to the representation of FS algorithm. A difference within 1% is not statistically significant. c) Recognition accuracy as a function of number of basis functions. in the image (see fig. 2(b)). Using this system with 30 training images per class we can achieve 53% accuracy on Caltech 101 dataset.
Since FS finds exact sparse codes, its representations are generally sparser than those found by PSD predictor trained with the same value of sparsity penalty λ. Hence, we compare the recognition accuracy against the measured sparsity level of the representation as shown in fig. 3(b). PSD is not only able to achieve better accuracy than exact sparse coding algorithms, but also, it does it much more efficiently. Fig. 3(a) demonstrates that our feed-forward predictor extracts features more than 100 times faster than feature sign.
In fact, the speed up is over 800 when the sparsity is set to the value that gives the highest accuracy shown in fig. 3(b).
Finally, we observe that these sparse coding algorithms are somewhat inefficient when applied convolutionally. Many feature detectors are the translated versions of each other as shown in fig. 2(a). Hence, the resulting feature maps are highly redundant. This might explain why the recognition accuracy tends to saturate when the number of filters is increased as shown in fig. 3(c).
SUMMARY AND FUTURE WORK
3.3 Stability
In order to quantify the stability of PSD and FS, we investigate their behavior under naturally changing input signals. For this purpose, we train a basis set with 128 elements, each of size 9x9, using the PSD algorithm on the Berkeley dataset. This basis set is then used with FS on the standard "foreman" test video together with the PSD Predictor. We extract 784 uniformly distributed patches from each frame with a total of 400 frames.
P(−|+) 0.00
P(+|−) 0.00
P(−|0) 0.01
P(+|0) 0.01
P(0|−) 0.40
P(0|+) 0.41
P(+|+) 0.59
P(−|−) 0.60
P(0|0) 0.99
Feature Sign
P(−|+) 0.00
P(+|−) 0.00
P(−|0) 0.00
P(+|0) 0.00
P(0|−) 0.06
P(0|+) 0.05
P(+|+) 0.95
P(−|−) 0.94
P(0|0) 1.00
PSD
P(−|+) 0.00
P(+|−) 0.01
P(−|0) 0.00
P(+|0) 0.01
P(0|−) 0.45
P(0|+) 0.41
P(+|+) 0.59
P(−|−) 0.54
P(0|0) 0.98
PSD Random
Figure 4: Conditional probabilities for sign transitions between two consecutive frames. For instance, P(−|+) shows the conditional probability of a unit being negative given that it was positive in the previous frame. The figure on the right is used as baseline, showing the conditional probabilities computed on pairs of random frames.
For each patch, a 128 dimensional representation is calculated using both FS and the PSD predictor. The stability is measured by the number of times a unit of the representation changes its sign, either negative, zero or positive, between two consecutive frames. Since the PSD predictor does not generate exact zero values, we threhsold its output units in such a way that the average number of zero units equals the one produced by FS (roughly, only the 4% of the units are non-zero). The transition probabilities are given in Figure 4. It can be seen from this figure that the PSD predictor generates a more stable representation of slowly varying natural frames compared to the representation produced by the exact optimization algorithm.
Summary and Future Work
Sparse coding algorithms can be used as pre-processor in many vision applications and, in particular, to extract features in object recognition systems. To the best of our knowledge, no sparse coding algorithm is computationally efficient because inference involves some sort of iterative optimization. We showed that sparse codes can actually be approximated by a feed-forward regressor without compromising the recognition accuracy, but making the recognition process very fast and suitable for use in real-time systems. We proposed a very simple algorithm to train such a regressor.
In the future, we plan to train the model convolutionally in order to make the sparse representation more efficient, and to build hierarchical deep models by sequentially replicating the model on the representation produced by the previous stage as successfully proposed in.
REFERENCES
References
 Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, November
 M. Elad and M. Aharon. Image denoising via learned dictionaries and sparse representation. In CVPR, 2006.
 M. Ranzato, F.J. Huang, Y. Boureau, and Y. LeCun. Unsupervised learning of invariant feature hierarchies with applications to object recognition. In CVPR, B. A. Olshausen and D. J. Field. Sparse coding with an overcomplete basis set: a strategy employed by v1? Vision Research, 37:3311–3325, 1997.
 M. Aharon, M. Elad, and A.M. Bruckstein. K-svd and its non-negative variant for dictionary design. volume 5914, 2005.
 J. Mairal, F. Bach, J. Ponce, G. Sapiro, and A. Zisserman. Discriminative learned dictionaries for local image analysis. In CVPR, 2008.
 H. Lee, A. Battle, R. Raina, and A.Y. Ng. Efficient sparse coding algorithms. In
NIPS, 2006.
 M. Ranzato, C. Poultney, S. Chopra, and Y. LeCun. Efficient learning of sparse representations with an energy-based model. In NIPS 2006. MIT Press, 2006.
 S Mallat and Z Zhang. Matching pursuits with time-frequency dictionaries. IEEE
Transactions on Signal Processing, 41(12):3397:3415, 1993.
 S.S. Chen, D.L. Donoho, and M.A. Saunders. Atomic decomposition by basis pursuit. SIAM Journal on Scientific Computing, 20(1):33–61, 1999.
 DL Donoho and M Elad. Optimally sparse representation in general (nonorthogonal) dictionaries via ℓ1 minimization. Proc Natl Acad Sci U S A, 100(5):2197–
2202, 2003 Mar 4.
 B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani.
Least angle regression, J.F. Murray and K. Kreutz-Delgado.
Learning sparse overcomplete codes for images. The Journal of VLSI Signal Processing, 45:97–110, 2008.
 C.J. Rozell, D.H. Johnson, Baraniuk R.G., and B.A. Olshausen. Sparse coding via thresholding and local competition in neural circuits. Neural Computation, M. Ranzato, Y. Boureau, and Y. LeCun. Sparse feature learning for deep belief networks. In NIPS, 2007.
 G.E. Hinton. Training products of experts by minimizing contrastive divergence.
Neural Computation, 14:1771–1800, 2002.
 L. Fei-Fei, R. Fergus, and P. Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In CVPR Workshop, 2004.
 G.E. Hinton and R. R Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504–507, 2006.
 http://yann.lecun.com/exdb/mnist/.
 http://www.cs.berkeley.edu/projects/vision/grouping/segbench/.Going Deeper with Convolutions
Christian Szegedy1, Wei Liu2, Yangqing Jia1, Pierre Sermanet1, Scott Reed3, Dragomir Anguelov1, Dumitru Erhan1, Vincent Vanhoucke1, Andrew Rabinovich4
1Google Inc. 2University of North Carolina, Chapel Hill
3University of Michigan, Ann Arbor 4Magic Leap Inc.
1{szegedy,jiayq,sermanet,dragomir,dumitru,vanhoucke}@google.com
2wliu@cs.unc.edu, 3reedscott@umich.edu, 4arabinovich@magicleap.com
Abstract
We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014(ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called
GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.
1. Introduction
In the last three years, our object classification and detection capabilities have dramatically improved due to advances in deep learning and convolutional networks.
One encouraging news is that most of this progress is not just the result of more powerful hardware, larger datasets and bigger models, but mainly a consequence of new ideas, algorithms and improved network architectures. No new data sources were used, for example, by the top entries in the ILSVRC 2014 competition besides the classification dataset of the same competition for detection purposes. Our
GoogLeNet submission to ILSVRC 2014 actually uses 12 times fewer parameters than the winning architecture of Krizhevsky et al from two years ago, while being significantly more accurate. On the object detection front, the biggest gains have not come from naive application of bigger and bigger deep networks, but from the synergy of deep architectures and classical computer vision, like the R-CNN algorithm by Girshick et al.
Another notable factor is that with the ongoing traction of mobile and embedded computing, the efficiency of our algorithms – especially their power and memory use – gains importance. It is noteworthy that the considerations leading to the design of the deep architecture presented in this paper included this factor rather than having a sheer fixation on accuracy numbers. For most of the experiments, the models were designed to keep a computational budget of 1.5 billion multiply-adds at inference time, so that the they do not end up to be a purely academic curiosity, but could be put to real world use, even on large datasets, at a reasonable cost.
In this paper, we will focus on an efficient deep neural network architecture for computer vision, codenamed Inception, which derives its name from the Network in network paper by Lin et al in conjunction with the famous
"we need to go deeper" internet meme. In our case, the word "deep" is used in two different meanings: first of all, in the sense that we introduce a new level of organization in the form of the "Inception module" and also in the more direct sense of increased network depth. In general, one can view the Inception model as a logical culmination of while taking inspiration and guidance from the theoretical work by Arora et al. The benefits of the architecture are experimentally verified on the ILSVRC 2014 classification and detection challenges, where it significantly outperforms the current state of the art.
2. Related Work
Starting with LeNet-5, convolutional neural networks (CNN) have typically had a standard structure – stacked convolutional layers (optionally followed by con1 trast normalization and max-pooling) are followed by one or more fully-connected layers. Variants of this basic design are prevalent in the image classification literature and have yielded the best results to-date on MNIST, CIFAR and most notably on the ImageNet classification challenge.
For larger datasets such as Imagenet, the recent trend has been to increase the number of layers
 and layer size, while using dropout to address the problem of overfitting.
Despite concerns that max-pooling layers result in loss of accurate spatial information, the same convolutional network architecture as
 has also been successfully employed for localization, object detection and human pose estimation.
Inspired by a neuroscience model of the primate visual cortex, Serre et al. used a series of fixed Gabor filters of different sizes to handle multiple scales. We use a similar strategy here. However, contrary to the fixed 2-layer deep model of, all filters in the Inception architecture are learned. Furthermore, Inception layers are repeated many times, leading to a 22-layer deep model in the case of the GoogLeNet model.
Network-in-Network is an approach proposed by Lin et al. in order to increase the representational power of neural networks. In their model, additional 1 × 1 convolutional layers are added to the network, increasing its depth.
We use this approach heavily in our architecture. However, in our setting, 1 × 1 convolutions have dual purpose: most critically, they are used mainly as dimension reduction modules to remove computational bottlenecks, that would otherwise limit the size of our networks. This allows for not just increasing the depth, but also the width of our networks without a significant performance penalty.
Finally, the current state of the art for object detection is the Regions with Convolutional Neural Networks (R-CNN) method by Girshick et al.. R-CNN decomposes the overall detection problem into two subproblems: utilizing lowlevel cues such as color and texture in order to generate object location proposals in a category-agnostic fashion and using CNN classifiers to identify object categories at those locations. Such a two stage approach leverages the accuracy of bounding box segmentation with low-level cues, as well as the highly powerful classification power of state-ofthe-art CNNs. We adopted a similar pipeline in our detection submissions, but have explored enhancements in both stages, such as multi-box prediction for higher object bounding box recall, and ensemble approaches for better categorization of bounding box proposals.
3. Motivation and High Level Considerations
The most straightforward way of improving the performance of deep neural networks is by increasing their size.
This includes both increasing the depth – the number of netFigure 1:
Two distinct classes from the 1000 classes of the ILSVRC 2014 classification challenge. Domain knowledge is required to distinguish between these classes. work levels – as well as its width: the number of units at each level. This is an easy and safe way of training higher quality models, especially given the availability of a large amount of labeled training data. However, this simple solution comes with two major drawbacks.
Bigger size typically means a larger number of parameters, which makes the enlarged network more prone to overfitting, especially if the number of labeled examples in the training set is limited. This is a major bottleneck as strongly labeled datasets are laborious and expensive to obtain, often requiring expert human raters to distinguish between various fine-grained visual categories such as those in ImageNet(even in the 1000-class ILSVRC subset) as shown in Figure 1.
The other drawback of uniformly increased network size is the dramatically increased use of computational resources.
For example, in a deep vision network, if two convolutional layers are chained, any uniform increase in the number of their filters results in a quadratic increase of computation. If the added capacity is used inefficiently (for example, if most weights end up to be close to zero), then much of the computation is wasted. As the computational budget is always finite, an efficient distribution of computing resources is preferred to an indiscriminate increase of size, even when the main objective is to increase the quality of performance.
A fundamental way of solving both of these issues would be to introduce sparsity and replace the fully connected layers by the sparse ones, even inside the convolutions. Besides mimicking biological systems, this would also have the advantage of firmer theoretical underpinnings due to the groundbreaking work of Arora et al.. Their main result states that if the probability distribution of the dataset is representable by a large, very sparse deep neural network, then the optimal network topology can be constructed layer after layer by analyzing the correlation statistics of the preceding layer activations and clustering neurons with highly correlated outputs. Although the strict mathematical proof requires very strong conditions, the fact that this statement resonates with the well known Hebbian principle – neurons that fire together, wire together – suggests that the underlying idea is applicable even under less strict conditions, in practice.
Unfortunately, today's computing infrastructures are very inefficient when it comes to numerical calculation on non-uniform sparse data structures. Even if the number of arithmetic operations is reduced by 100×, the overhead of lookups and cache misses would dominate: switching to sparse matrices might not pay off. The gap is widened yet further by the use of steadily improving and highly tuned numerical libraries that allow for extremely fast dense matrix multiplication, exploiting the minute details of the underlying CPU or GPU hardware. Also, non-uniform sparse models require more sophisticated engineering and computing infrastructure. Most current vision oriented machine learning systems utilize sparsity in the spatial domain just by the virtue of employing convolutions. However, convolutions are implemented as collections of dense connections to the patches in the earlier layer. ConvNets have traditionally used random and sparse connection tables in the feature dimensions since in order to break the symmetry and improve learning, yet the trend changed back to full connections with in order to further optimize parallel computation. Current state-of-the-art architectures for computer vision have uniform structure. The large number of filters and greater batch size allows for the efficient use of dense computation.
This raises the question of whether there is any hope for a next, intermediate step: an architecture that makes use of filter-level sparsity, as suggested by the theory, but exploits our current hardware by utilizing computations on dense matrices. The vast literature on sparse matrix computations (e.g. ) suggests that clustering sparse matrices into relatively dense submatrices tends to give competitive performance for sparse matrix multiplication. It does not seem far-fetched to think that similar methods would be utilized for the automated construction of non-uniform deeplearning architectures in the near future.
The Inception architecture started out as a case study for assessing the hypothetical output of a sophisticated network topology construction algorithm that tries to approximate a sparse structure implied by for vision networks and covering the hypothesized outcome by dense, readily available components. Despite being a highly speculative undertaking, modest gains were observed early on when compared with reference networks based on. With a bit of tuning the gap widened and Inception proved to be especially useful in the context of localization and object detection as the base network for and. Interestingly, while most of the original architectural choices have been questioned and tested thoroughly in separation, they turned out to be close to optimal locally. One must be cautious though: although the Inception architecture has become a success for computer vision, it is still questionable whether this can be attributed to the guiding principles that have lead to its construction. Making sure of this would require a much more thorough analysis and verification.
4. Architectural Details
The main idea of the Inception architecture is to consider how an optimal local sparse structure of a convolutional vision network can be approximated and covered by readily available dense components. Note that assuming translation invariance means that our network will be built from convolutional building blocks. All we need is to find the optimal local construction and to repeat it spatially. Arora et al. suggests a layer-by layer construction where one should analyze the correlation statistics of the last layer and cluster them into groups of units with high correlation. These clusters form the units of the next layer and are connected to the units in the previous layer. We assume that each unit from an earlier layer corresponds to some region of the input image and these units are grouped into filter banks. In the lower layers (the ones close to the input) correlated units would concentrate in local regions. Thus, we would end up with a lot of clusters concentrated in a single region and they can be covered by a layer of 1×1 convolutions in the next layer, as suggested in. However, one can also expect that there will be a smaller number of more spatially spread out clusters that can be covered by convolutions over larger patches, and there will be a decreasing number of patches over larger and larger regions. In order to avoid patch-alignment issues, current incarnations of the Inception architecture are restricted to filter sizes 1×1, 3×3 and 5×5; this decision was based more on convenience rather than necessity. It also means that the suggested architecture is a combination of all those layers with their output filter banks concatenated into a single output vector forming the input of the next stage. Additionally, since pooling operations have been essential for the success of current convolutional networks, it suggests that adding an alternative parallel pooling path in each such stage should have additional beneficial effect, too (see Figure 2(a)).
As these "Inception modules" are stacked on top of each other, their output correlation statistics are bound to vary: as features of higher abstraction are captured by higher layers, their spatial concentration is expected to decrease. This suggests that the ratio of 3×3 and 5×5 convolutions should increase as we move to higher layers.
One big problem with the above modules, at least in this na¨ıve form, is that even a modest number of 5×5 convolutions can be prohibitively expensive on top of a convolutional layer with a large number of filters. This problem becomes even more pronounced once pooling units are added to the mix: the number of output filters equals to the num1x1 convolutions
3x3 convolutions
5x5 convolutions
Filter concatenation
Previous layer
3x3 max pooling(a) Inception module, na¨ıve version
1x1 convolutions
3x3 convolutions
5x5 convolutions
Filter concatenation
Previous layer
3x3 max pooling
1x1 convolutions
1x1 convolutions
1x1 convolutions(b) Inception module with dimensionality reduction
Figure 2: Inception module ber of filters in the previous stage. The merging of output of the pooling layer with outputs of the convolutional layers would lead to an inevitable increase in the number of outputs from stage to stage. While this architecture might cover the optimal sparse structure, it would do it very inefficiently, leading to a computational blow up within a few stages.
This leads to the second idea of the Inception architecture: judiciously reducing dimension wherever the computational requirements would increase too much otherwise.
This is based on the success of embeddings: even low dimensional embeddings might contain a lot of information about a relatively large image patch.
However, embeddings represent information in a dense, compressed form and compressed information is harder to process. The representation should be kept sparse at most places (as required by the conditions of ) and compress the signals only whenever they have to be aggregated en masse. That is, 1×1 convolutions are used to compute reductions before the expensive 3×3 and 5×5 convolutions. Besides being used as reductions, they also include the use of rectified linear activation making them dual-purpose. The final result is depicted in Figure 2(b).
In general, an Inception network is a network consisting of modules of the above type stacked upon each other, with occasional max-pooling layers with stride 2 to halve the resolution of the grid. For technical reasons (memory efficiency during training), it seemed beneficial to start using Inception modules only at higher layers while keeping the lower layers in traditional convolutional fashion. This is not strictly necessary, simply reflecting some infrastructural inefficiencies in our current implementation.
A useful aspect of this architecture is that it allows for increasing the number of units at each stage significantly without an uncontrolled blow-up in computational complexity at later stages. This is achieved by the ubiquitous use of dimensionality reduction prior to expensive convolutions with larger patch sizes. Furthermore, the design follows the practical intuition that visual information should be processed at various scales and then aggregated so that the next stage can abstract features from the different scales simultaneously.
The improved use of computational resources allows for increasing both the width of each stage as well as the number of stages without getting into computational difficulties.
One can utilize the Inception architecture to create slightly inferior, but computationally cheaper versions of it.
We have found that all the available knobs and levers allow for a controlled balancing of computational resources resulting in networks that are 3 − 10× faster than similarly performing networks with non-Inception architecture, however this requires careful manual design at this point.
5. GoogLeNet
By the"GoogLeNet" name we refer to the particular incarnation of the Inception architecture used in our submission for the ILSVRC 2014 competition. We also used one deeper and wider Inception network with slightly superior quality, but adding it to the ensemble seemed to improve the results only marginally. We omit the details of that network, as empirical evidence suggests that the influence of the exact architectural parameters is relatively minor. Table 1 illustrates the most common instance of Inception used in the competition. This network (trained with different imagepatch sampling methods) was used for 6 out of the 7 models in our ensemble.
All the convolutions, including those inside the Inception modules, use rectified linear activation. The size of the receptive field in our network is 224×224 in the RGB color space with zero mean. "#3×3 reduce" and "#5×5 reduce" stands for the number of 1×1 filters in the reduction layer used before the 3×3 and 5×5 convolutions. One can see the number of 1×1 filters in the projection layer after the built-in max-pooling in the pool proj column. All these reduction/projection layers use rectified linear activation as well.
The network was designed with computational efficiency and practicality in mind, so that inference can be run on individual devices including even those with limited computational resources, especially with low-memory footprint. type patch size/ stride output size depth
#1×1
#3×3 reduce
#3×3
#5×5 reduce
#5×5 pool proj params ops convolution
7×7/2
112×112×64
2.7K
34M max pool
3×3/2
56×56×64
0 convolution
3×3/1
56×56×192
112K
360M max pool
3×3/2
28×28×192
0 inception (3a)
28×28×256
159K
128M inception (3b)
28×28×480
380K
304M max pool
3×3/2
14×14×480
0 inception (4a)
14×14×512
364K
73M inception (4b)
14×14×512
437K
88M inception (4c)
14×14×512
463K
100M inception (4d)
14×14×528
580K
119M inception (4e)
14×14×832
840K
170M max pool
3×3/2
7×7×832
0 inception (5a)
7×7×832
1072K
54M inception (5b)
7×7×1024
1388K
71M avg pool
7×7/1
1×1×1024
0 dropout (40%)
1×1×1024
0 linear
1×1×1000
1000K
1M softmax
1×1×1000
Table 1: GoogLeNet incarnation of the Inception architecture.
The network is 22 layers deep when counting only layers with parameters (or 27 layers if we also count pooling). The overall number of layers (independent building blocks) used for the construction of the network is about 100. The exact number depends on how layers are counted by the machine learning infrastructure. The use of average pooling before the classifier is based on, although our implementation has an additional linear layer. The linear layer enables us to easily adapt our networks to other label sets, however it is used mostly for convenience and we do not expect it to have a major effect. We found that a move from fully connected layers to average pooling improved the top-1 accuracy by about 0.6%, however the use of dropout remained essential even after removing the fully connected layers.
Given relatively large depth of the network, the ability to propagate gradients back through all the layers in an effective manner was a concern. The strong performance of shallower networks on this task suggests that the features produced by the layers in the middle of the network should be very discriminative. By adding auxiliary classifiers connected to these intermediate layers, discrimination in the lower stages in the classifier was expected. This was thought to combat the vanishing gradient problem while providing regularization.
These classifiers take the form of smaller convolutional networks put on top of the output of the Inception (4a) and (4d) modules. During training, their loss gets added to the total loss of the network with a discount weight (the losses of the auxiliary classifiers were weighted by 0.3). At inference time, these auxiliary networks are discarded. Later control experiments have shown that the effect of the auxiliary networks is relatively minor (around 0.5%) and that it required only one of them to achieve the same effect.
The exact structure of the extra network on the side, including the auxiliary classifier, is as follows:
• An average pooling layer with 5×5 filter size and stride 3, resulting in an 4×4×512 output for the (4a), and 4×4×528 for the (4d) stage.
• A 1×1 convolution with 128 filters for dimension reduction and rectified linear activation.
• A fully connected layer with 1024 units and rectified linear activation.
• A dropout layer with 70% ratio of dropped outputs.
• A linear layer with softmax loss as the classifier (predicting the same 1000 classes as the main classifier, but removed at inference time).
A schematic view of the resulting network is depicted in Figure 3.
6. Training Methodology
GoogLeNet networks were trained using the DistBelief distributed machine learning system using modest amount of model and data-parallelism. Although we used a CPU based implementation only, a rough estimate suggests that the GoogLeNet network could be trained to convergence using few high-end GPUs within a week, the main limitation being the memory usage. Our training used asynchronous stochastic gradient descent with 0.9 momentum, fixed learning rate schedule (decreasing the learning rate by 4% every 8 epochs). Polyak averaging was used to create the final model used at inference time.
Image sampling methods have changed substantially over the months leading to the competition, and already converged models were trained on with other options, sometimes in conjunction with changed hyperparameters, such as dropout and the learning rate. Therefore, it is hard to give a definitive guidance to the most effective single way to train these networks. To complicate matters further, some of the models were mainly trained on smaller relative crops, others on larger ones, inspired by. Still, one prescription that was verified to work very well after the competition, includes sampling of various sized patches of the image whose size is distributed evenly between 8% and 100% of the image area with aspect ratio constrained to the interval [ 3. Also, we found that the photometric distortions of Andrew Howard were useful to combat overfitting to the imaging conditions of training data.
ILSVRC
Classification
Challenge
Setup and Results
The ILSVRC 2014 classification challenge involves the task of classifying the image into one of 1000 leaf-node categories in the Imagenet hierarchy. There are about 1.2 million images for training, 50,000 for validation and 100,000 images for testing.
Each image is associated with one ground truth category, and performance is measured based on the highest scoring classifier predictions.
Two numbers are usually reported: the top-1 accuracy rate, which compares the ground truth against the first predicted class, and the top-5 error rate, which compares the ground truth against the first 5 predicted classes: an image is deemed correctly classified if the ground truth is among the top-5, regardless of its rank in them. The challenge uses the top-5 error rate for ranking purposes. input
Conv
7x7+2(S)
MaxPool
3x3+2(S)
LocalRespNorm
Conv
1x1+1(V)
Conv
3x3+1(S)
LocalRespNorm
MaxPool
3x3+2(S)
Conv
1x1+1(S)
Conv
1x1+1(S)
Conv
1x1+1(S)
MaxPool
3x3+1(S)
DepthConcat
Conv
3x3+1(S)
Conv
5x5+1(S)
Conv
1x1+1(S)
Conv
1x1+1(S)
Conv
1x1+1(S)
Conv
1x1+1(S)
MaxPool
3x3+1(S)
DepthConcat
Conv
3x3+1(S)
Conv
5x5+1(S)
Conv
1x1+1(S)
MaxPool
3x3+2(S)
Conv
1x1+1(S)
Conv
1x1+1(S)
Conv
1x1+1(S)
MaxPool
3x3+1(S)
DepthConcat
Conv
3x3+1(S)
Conv
5x5+1(S)
Conv
1x1+1(S)
Conv
1x1+1(S)
Conv
1x1+1(S)
Conv
1x1+1(S)
MaxPool
3x3+1(S)
AveragePool
5x5+3(V)
DepthConcat
Conv
3x3+1(S)
Conv
5x5+1(S)
Conv
1x1+1(S)
Conv
1x1+1(S)
Conv
1x1+1(S)
Conv
1x1+1(S)
MaxPool
3x3+1(S)
DepthConcat
Conv
3x3+1(S)
Conv
5x5+1(S)
Conv
1x1+1(S)
Conv
1x1+1(S)
Conv
1x1+1(S)
Conv
1x1+1(S)
MaxPool
3x3+1(S)
DepthConcat
Conv
3x3+1(S)
Conv
5x5+1(S)
Conv
1x1+1(S)
Conv
1x1+1(S)
Conv
1x1+1(S)
Conv
1x1+1(S)
MaxPool
3x3+1(S)
AveragePool
5x5+3(V)
DepthConcat
Conv
3x3+1(S)
Conv
5x5+1(S)
Conv
1x1+1(S)
MaxPool
3x3+2(S)
Conv
1x1+1(S)
Conv
1x1+1(S)
Conv
1x1+1(S)
MaxPool
3x3+1(S)
DepthConcat
Conv
3x3+1(S)
Conv
5x5+1(S)
Conv
1x1+1(S)
Conv
1x1+1(S)
Conv
1x1+1(S)
Conv
1x1+1(S)
MaxPool
3x3+1(S)
DepthConcat
Conv
3x3+1(S)
Conv
5x5+1(S)
Conv
1x1+1(S)
AveragePool
7x7+1(V)
FC
Conv
1x1+1(S)
FC
FC
SoftmaxActivation softmax0
Conv
1x1+1(S)
FC
FC
SoftmaxActivation softmax1
SoftmaxActivation softmax2
Figure 3: GoogLeNet network with all the bells and whistles.
We participated in the challenge with no external data used for training.
In addition to the training techniques aforementioned in this paper, we adopted a set of techniques during testing to obtain a higher performance, which we describe next.
1. We independently trained 7 versions of the same
GoogLeNet model (including one wider version), and performed ensemble prediction with them.
These models were trained with the same initialization (even with the same initial weights, due to an oversight) and learning rate policies. They differed only in sampling methodologies and the randomized input image order.
2. During testing, we adopted a more aggressive cropping approach than that of Krizhevsky et al.. Specifically, we resized the image to 4 scales where the shorter dimension (height or width) is 256, 288, 320 and 352 respectively, take the left, center and right square of these resized images (in the case of portrait images, we take the top, center and bottom squares).
For each square, we then take the 4 corners and the center 224×224 crop as well as the square resized to
224×224, and their mirrored versions. This leads to
4×3×6×2 = 144 crops per image.
A similar approach was used by Andrew Howard in the previous year's entry, which we empirically verified to perform slightly worse than the proposed scheme. We note that such aggressive cropping may not be necessary in real applications, as the benefit of more crops becomes marginal after a reasonable number of crops are present (as we will show later on).
3. The softmax probabilities are averaged over multiple crops and over all the individual classifiers to obtain the final prediction. In our experiments we analyzed alternative approaches on the validation data, such as max pooling over crops and averaging over classifiers, but they lead to inferior performance than the simple averaging.
In the remainder of this paper, we analyze the multiple factors that contribute to the overall performance of the final submission.
Our final submission to the challenge obtains a top-5 error of 6.67% on both the validation and testing data, ranking the first among other participants. This is a 56.5% relative reduction compared to the SuperVision approach in 2012, and about 40% relative reduction compared to the previous year's best approach (Clarifai), both of which used external data for training the classifiers. Table 2 shows the statistics of some of the top-performing approaches over the past 3 years.
We also analyze and report the performance of multiple testing choices, by varying the number of models and the Team
Year
Place
Error(top-5)
Uses external data
SuperVision
1st
16.4% no
SuperVision
1st
Imagenet 22k
Clarifai
1st
11.7% no
Clarifai
1st
Imagenet 22k
MSRA
3rd
7.35% no
VGG
2nd
7.32% no
GoogLeNet
1st
6.67% no
Table 2: Classification performance.
Number of models
Number of Crops
Cost
Top-5 error compared to base
10.07% base
-0.92%
-2.18%
-1.98%
-2.45%
-3.45%
Table 3: GoogLeNet classification performance break down. number of crops used when predicting an image in Table 3.
When we use one model, we chose the one with the lowest top-1 error rate on the validation data. All numbers are reported on the validation dataset in order to not overfit to the testing data statistics.
8. ILSVRC 2014 Detection Challenge Setup and Results
The ILSVRC detection task is to produce bounding boxes around objects in images among 200 possible classes.
Detected objects count as correct if they match the class of the groundtruth and their bounding boxes overlap by at least 50% (using the Jaccard index). Extraneous detections count as false positives and are penalized. Contrary to the classification task, each image may contain many objects or none, and their scale may vary. Results are reported using the mean average precision (mAP). The approach taken by
GoogLeNet for detection is similar to the R-CNN by, but is augmented with the Inception model as the region classifier. Additionally, the region proposal step is improved by combining the selective search approach with multibox predictions for higher object bounding box recall.
In order to reduce the number of false positives, the superTeam
Year
Place mAP external data ensemble approach
UvA-Euvision
1st
22.6% none
?
Fisher vectors
Deep Insight
3rd
ImageNet 1k
CNN
CUHK DeepID-Net
2nd
ImageNet 1k
?
CNN
GoogLeNet
1st
ImageNet 1k
CNN
Table 4: Comparison of detection performances. Unreported values are noted with question marks. pixel size was increased by 2×. This halves the proposals coming from the selective search algorithm. We added back
200 region proposals coming from multi-box resulting, in total, in about 60% of the proposals used by, while increasing the coverage from 92% to 93%. The overall effect of cutting the number of proposals with increased coverage is a 1% improvement of the mean average precision for the single model case. Finally, we use an ensemble of 6 GoogLeNets when classifying each region. This leads to an increase in accuracy from 40% to 43.9%. Note that contrary to R-CNN, we did not use bounding box regression due to lack of time.
We first report the top detection results and show the progress since the first edition of the detection task. Compared to the 2013 result, the accuracy has almost doubled.
The top performing teams all use convolutional networks.
We report the official scores in Table 4 and common strategies for each team: the use of external data, ensemble models or contextual models. The external data is typically the ILSVRC12 classification data for pre-training a model that is later refined on the detection data. Some teams also mention the use of the localization data. Since a good portion of the localization task bounding boxes are not included in the detection dataset, one can pre-train a general bounding box regressor with this data the same way classification is used for pre-training. The GoogLeNet entry did not use the localization data for pretraining.
In Table 5, we compare results using a single model only.
The top performing model is by Deep Insight and surprisingly only improves by 0.3 points with an ensemble of 3 models while the GoogLeNet obtains significantly stronger results with the ensemble.
9. Conclusions
Our results yield a solid evidence that approximating the expected optimal sparse structure by readily available dense building blocks is a viable method for improving neural networks for computer vision.
The main advantage of this method is a significant quality gain at a modest increase of computational requirements compared to shallower and narrower architectures.
Our object detection work was competitive despite not
Team mAP
Contextual model
Bounding box regression
TrimpsSoushen
31.6% no
?
Berkeley
Vision
34.5% no yes
UvAEuvision
?
?
CUHK
DeepIDNet2
37.7% no
?
GoogLeNet
38.02% no no
Deep
Insight
40.2% yes yes
Table 5: Single model performance for detection. utilizing context nor performing bounding box regression, suggesting yet further evidence of the strengths of the Inception architecture.
For both classification and detection, it is expected that similar quality of result can be achieved by much more expensive non-Inception-type networks of similar depth and width. Still, our approach yields solid evidence that moving to sparser architectures is feasible and useful idea in general. This suggest future work towards creating sparser and more refined structures in automated ways on the basis of, as well as on applying the insights of the Inception architecture to other domains.
References
 Know your meme:
We need to go deeper. http://knowyourmeme.com/memes/we-need-to-go-deeper.
Accessed: 2014-09-15.
 S. Arora, A. Bhaskara, R. Ge, and T. Ma. Provable bounds for learning some deep representations. CoRR, abs/1310.6343, 2013.
 U. V. C¸ ataly¨urek, C. Aykanat, and B. Uc¸ar.
On two-dimensional sparse matrix partitioning:
Models, methods, and a recipe.
SIAM J. Sci. Comput., 32(2):656–683, Feb. 2010.
 J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, M. Mao, M. Ranzato, A. Senior, P. Tucker, K. Yang, Q. V. Le, and A. Y. Ng. Large scale distributed deep networks. In P. Bartlett, F. Pereira, C. Burges, L. Bottou, and K. Weinberger, editors, NIPS, pages 1232–
 D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov.
Scalable object detection using deep neural networks.
In CVPR, 2014.
 R. B. Girshick, J. Donahue, T. Darrell, and J. Malik.
Rich feature hierarchies for accurate object detection and semantic segmentation. In Computer Vision and Pattern Recognition, 2014. CVPR 2014. IEEE Conference on, 2014.
 G.
E.
Hinton, N.
Srivastava, A.
Krizhevsky, I. Sutskever, and R. Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors. CoRR, abs/1207.0580, 2012.
 A. G. Howard.
Some improvements on deep convolutional neural network based image classification.
CoRR, abs/1312.5402, 2013.
 A. Krizhevsky, I. Sutskever, and G. Hinton.
Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems 25, pages 1106–1114, 2012.
 Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E.
Howard, W. Hubbard, and L. D. Jackel. Backpropagation applied to handwritten zip code recognition.
Neural Comput., 1(4):541–551, Dec. 1989.
 Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner.
Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, M. Lin, Q. Chen, and S. Yan. Network in network.
CoRR, abs/1312.4400, 2013.
 B. T. Polyak and A. B. Juditsky.
Acceleration of stochastic approximation by averaging. SIAM J. Control Optim., 30(4):838–855, July 1992.
 P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun.
Overfeat: Integrated recognition, localization and detection using convolutional networks. CoRR, abs/1312.6229, 2013.
 T. Serre, L. Wolf, S. M. Bileschi, M. Riesenhuber, and T. Poggio. Robust object recognition with cortex-like mechanisms. IEEE Trans. Pattern Anal. Mach. Intell., 29(3):411–426, 2007.
 F. Song and J. Dongarra. Scaling up matrix computations on shared-memory manycore systems with 1000 cpu cores. In Proceedings of the 28th ACM International Conference on Supercomputing, ICS '14, pages
333–342, New York, NY, USA, 2014. ACM.
 I. Sutskever, J. Martens, G. E. Dahl, and G. E. Hinton.
On the importance of initialization and momentum in deep learning. In ICML, volume 28 of JMLR Proceedings, pages 1139–1147. JMLR.org, 2013.
 C. Szegedy, A. Toshev, and D. Erhan. Deep neural networks for object detection. In C. J. C. Burges, L. Bottou, Z. Ghahramani, and K. Q. Weinberger, editors, NIPS, pages 2553–2561, 2013.
 A. Toshev and C. Szegedy.
Deeppose:
Human pose estimation via deep neural networks.
CoRR, abs/1312.4659, 2013.
 K. E. A. van de Sande, J. R. R. Uijlings, T. Gevers, and A. W. M. Smeulders. Segmentation as selective search for object recognition. In Proceedings of the 2011 International Conference on Computer Vision, ICCV '11, pages 1879–1886, Washington, DC, USA, 2011. IEEE Computer Society.
 M. D. Zeiler and R. Fergus. Visualizing and understanding convolutional networks. In D. J. Fleet, T. Pajdla, B. Schiele, and T. Tuytelaars, editors, ECCV, volume 8689 of Lecture Notes in Computer Science, pages 818–833. Springer, 2014.Transitive Transfer Learning
Ben Tan
Department of Computer
Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, China btan@cse.ust.hk
Yangqiu Song
Department of Computer
Science, University of Illinois at Urbana-Champaign, USA yqsong@illinois.edu
Erheng Zhong
Personalization Sciences, Yahoo Labs, Sunnyvale, USA erheng@yahoo-inc.com
Qiang Yang
Department of Computer
Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, China qyang@cse.ust.hk
ABSTRACT
Transfer learning, which leverages knowledge from source domains to enhance learning ability in a target domain, has been proven effective in various applications. A major limitation of transfer learning is that the source and target domains should be directly related. If there is little overlap between the two domains, performing knowledge transfer between these domains will not be effective. Inspired by human transitive inference and learning ability, whereby two seemingly unrelated concepts can be connected by series of intermediate bridges using auxiliary concepts, in this paper we study a novel learning problem: Transitive Transfer Learning (abbreviated to TTL). TTL is aimed at breaking the large domain distances and transferring knowledge even when the source and target domains share few factors directly. For example, when the source and target domains are text and images respectively, TTL can use some annotated images as the intermediate domain to bridge them. To solve the TTL problem, we propose a framework wherein we first select one or more domains to act as a bridge between the source and target domains to enable transfer learning, and then perform the transferring of knowledge via this bridge. Extensive empirical evidence shows that the framework yields state-of-the-art classification accuracies on several classification data sets.
Categories and Subject Descriptors
H.2.8 [Database Management]: Database Applications - Data Mining
General Terms
Machine Learning
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org.
KDD '15, August 11 - 14, 2015, NSW, Australia
Copyright 2015 ACM. ISBN 978-1-4503-3664-2/15/08...$15.00
DOI: http://dx.doi.org/10.1145/2783258.2783295.
Figure 1: An pictorial illustration of the transitive transfer learning problem. In TTL, the source and target domains have few common factors, but they can be connected by intermediate domains through some underlying factors.
Keywords
Transfer Learning, Transitive Transfer Learning, Nonnegative Matrix Tri-factorizations
INTRODUCTION
Transfer learning, which aims to borrow knowledge from source domains to help the learning in a target domain, has been established as one of the most important machine learning paradigms.
Various algorithms have been widely used and proven effective in many applications, for example, classification, reinforcement learning and recommendation systems, and so on. A critical requirement for successful transfer learning is that the source and target domains should be related. This relation can be in the form of related instances, features or models. If no direct relation can be found, forcibly transferring knowledge will not work. In the worst case, it could lead to having no improvement, or even worse performance, in the target domain. This is one of the major limitations of traditional transfer learning. However, as human beings, we naturally have the ability to carry out inference and learning via transitivity. This ability helps humans connect many concepts and transfer knowledge between two seemingly unrelated concepts by introducing a few intermediate concepts as a bridge.
For example, after taking a class in elementary computer science, 1155 we may find it easier to transfer the knowledge to theoretical computer science if we have taken an applied algorithm design course in between, since the algorithm course may involve both concepts in programming and theory. Likewise, having learned some basic math, we may find it impossible to directly take a course in convex optimization. However, this becomes feasible when we take an intermediate course in linear algebra and probability. The linear algebra and probability course serves as the intermediate domain for knowledge transfer.
Human ability to conduct transitive inference and learning inspires us to study a novel learning problem known as Transitive
Transfer Learning (TTL). As illustrated in Figure 1, in TTL, the source and target domains have few common factors, but they can be connected by intermediate domains through some underlining factors. We expect TTL to have wide practical applications. For example, when the source domain is composed of text documents and the target domain contains image data, they share no overlap feature spaces, knowledge learned in text documents can hardly be transferred to images. However, TTL can introduce some annotated images to learn a feature mapping between these two different feature spaces and have a smooth knowledge transfer. In other applications, such as text sentiment classification, all the data have the same feature space, but two group of data may have large distribution gap, TTL can introduce some auxiliary intermediate data to form a transitive knowledge transfer structure with which we can obtain a more versatile sentiment classification system.
In this paper, we propose a learning framework for the TTL problem. The framework is composed of two steps. The first step is to find an appropriate domain to bridge the given source and target domains. The second step is to do effective knowledge transfer among all domains. In the first step, we propose a probability model to select appropriate domains that is able to draw the source and target domains closer, based on domain characteristics such as domain difficulty and pairwise closeness. As data from different domains are collected from different data sources, each pair of domains may have distribution shift. In the second step, considering both of the domain relationship and distribution shift, we propose a transfer learning algorithm that allows to learn overlap features among domains and propagate label information through them. A high-level description of the TTL framework is summarized in Table 1. We give a formal definition of the TTL problem in Section 2, and describe the technical details of these two steps in Sections 3 and 4 respectively.
PROBLEM DEFINITION
In the problem, we have labeled source domain data S={(xs i, yi)}ns i=1, unlabeled target domain data T ={xt i}nt i=1, and k unlabeled intermediate domains Dj = {x dj i } nj i=1, j = 1,..., k, x∗ ∈
Rm∗ is a m∗ dimensional feature vector. The data from different domains could have different dimensions. S and T have a large distribution gap, thus directly transferring knowledge between them may cause a substantial performance loss in the target domain.
The TTL framework is aimed at finding intermediate domain(s) to bridge S and T, and minimizing the performance loss in T.
Formally, given a domain distribution gap measure g(·, ·), the first step is to find an intermediate domain that satisfies g(S, T |Di)
< g(S, T ). The second step performs transfer learning from the source domain S to target domain T via intermediate domain Di; this is implemented via learning two feature clustering functions psd(S, Di) and pdt(Di, T ), such that the distribution gap of data on common feature clusters selected by psd(S, Di) and pdt(Di, T ) are further reduced. The label information in the source domain is Table 1: The TTL Framework
Input: The source,target and candidate intermediate domains
Step 1: Intermediate domain selection (see Section 3)
Step 2: Transitive knowledge transfer (see Section 4)
Output: Prediction results in the target domain propagated to the intermediate and target data on the selected common feature clusters.
INTERMEDIATE DOMAIN SELECTION
Intermediate domain selection is problem specific, different problems may have different strategies. For example, when the source domain is composed of text data and the target domain is image data, one can crawl some annotated images from Flickr as the intermediate domain data. In other problems when there are multiple candidate intermediate domains, one should propose some selection algorithms according to domain properties. In this paper, we propose an algorithm for text sentiment classification problem as an example. As studied by previous research, domain difficulty and domain distance are two major factors that affect the transfer learning performance between two domains. On one hand, intuitively, if the source domain is less difficult than the intermediate and target domains, the model learned from the source data is highly predictive and is very likely to achieve high performance on the intermediate and target domains as well. On the other hand, if the intermediate domain is able to draw closer the source and target domains than their original distance, then the knowledge transfer process between the source and target domains will have less information loss. Hence, in this paper, we introduce domain complexity and A-distance to estimate domain difficulty and pairwise domain distance respectively. We summarize these measures as follows:
• Domain complexity: domain difficulty measure is problem specific, as different problem may have different feature types.
In this paper, we choose domain complexity to measure the difficulty. The domain complexity is calculated as the percentage of long tail features that have low frequency.
These long tail features bring in long tail feature distribution and significant feature diversity, thus make automatic machine learning difficult. We calculate the domain complexity as follows: cplx(D) = |{x|c(x) < t × n}| m
For non-negative features, c(x) is the number of instances whose feature x is larger than zero. |{x|c(x) < t × n}| is the number of features that appear in less than t×n instances.
In this paper, we measure the domain complexity as the percentage of long tail features that appear in less than 10% instances. For continuous features, we can measure their relative entropy as domain difficulty.
• A-distance: The A-distance estimates the distribution difference of two sets of data samples that are drawn from two probability distributions. Practically, given two sets of domain data Di and Dj, we can calculate the A-distance as follows: disA(Di, Dj) = 2(1 − 2 min h∈H error(h|Di, Dj)), Table 2: Domain characteristic features feature description cplx_src (c1) source domain complexity cplx_inter (c2) intermediate domain complexity cplx_tar (c3) target domain complexity dissi
A (c4) a_distance between source and intermediate disst
A(c5) a_distance between source and target disit
A (c6) a_distance between intermediate and target
H is a hypothesis space, h is the optimal proxy classifier that discriminates data points from different domains. In this paper, we first assign the source data positive labels, and target data negative ones, then use logistic regression as the proxy classifier to estimate the error(h|Di, Dj)) in A-distance.
In, the authors have been proven that the prediction error of the target domain is bounded by the error of the source domain, the A-distance and some other constant factor.
Given a triple tr = {S, D, T }, we can extract six features as described in Table 2. The first three features summarize individual in-domain characteristics, the last three features capture the pairwise cross domain distances. These features together affect the success probability of a transfer learning algorithm. However, it is impossible to design a universal domain selection criteria, as different problems may have different preferences (weights) on these features. To model the success probability of the introduced intermediate domain, we propose the following logistic function: f(tr) = δ(β0 +
� i=1 βici), (3) where δ(x) =
1+e−x. We estimate the parameters β = {β0, · · ·, β6} to maximize the log likelihood defined as:
L(β) = t
� i=1 l(i) log f(tri) + (1 − l(i)) log(1 − f(tri)), (4) l(i) is a binary label, indicating whether the intermediate domain in the ith triple is able to bridge the source and target domains.
We get the label by the following strategy. We perform a semisupervised label propagation algorithm with input S and T, and obtain a prediction accuracy accst on the target domain. We also perform the same algorithm with input {S, D, T }, and obtain another accuracy accsit on the target domain. If accsit > accst, we set l(i) = 1, otherwise, l(i) = 0. The label is determined by both the domain characteristics and the propagation model. A sophisticated model may accept more intermediate domains than a simple model. In this paper, we prefer to use a simple model such as KNN that are able to provide us strictly fitted candidates.
We transform the intermediate domain selection problem to a probability estimation problem. A candidate intermediate domain with high f(tr) is more likely to be selected.
TRANSITIVE KNOWLEDGE TRANSFER
In the first step, an intermediate domain that can bridge the source and target domains has been selected, however, there is still distribution shift among these domains. Thus, in the second step of the TTL framework, we propose a novel transfer learning algorithm that considers both of the transitive relationship and distribution shift among all the domains. The algorithm is based on nonnegative matrix tri-factorization that can perform feature clustering and label propagation simultaneously, so we first give some background knowledge.
Non-negative Matrix Tri-factorization
Non-negative Matrix Tri-factorization (NMTF) is a popular and effective technique for data clustering and classification. In
NMTF, the feature-instance matrix is decomposed into three submatrices. In general, given a feature-instance matrix X ∈ Rm×n, m is the number of dimensions, n is the number of instances. One can obtain the factorized sub-matrices by solving the optimization problem as follows: arg minF,A,GT
L = ||X − FAGT ||, (5) where || · || denotes the Frobenius norm of matrix.
The matrix F ∈ Rm×p indicates the information of feature clusters and p is the number of hidden feature clusters. The element
Fi,j indicates the probability that the ith feature belongs to the jth feature cluster.
The matrix G ∈ Rc×n is the instance cluster assignment matrix and c is the number of instance clusters. If the largest element of the ith row is located in the jth column, it means that the ith instance belongs to the jth instance cluster. In the classification problem, each instance cluster can be regarded as a label class.
A ∈ Rp×c is the association matrix. c is the number of instance clusters or label classes, for the binary classification problem c = 2.
The element Ai,j is the probability that the ith feature cluster is associated with the jth instance cluster.
NMTF for Transfer Learning
NMTF is also used as a basic technique for transfer learning algorithms. Given the source and target domains S and T, Xs and Xt are their feature-instance matrices respectively, one can decompose these two matrices simultaneously, and allow the decomposed matrices share some cross-domain information (sub-matrices). Formally, given two related domains S and T, their feature-instance matrices can be decomposed simultaneously as follows:
LST = ||Xs − FsAsGs|| + ||Xt − FtAtGt||
=
����Xs − [F 1, F 2 s ]
�
A1
A2 s
�
GT s
���� +
����Xt − [F 1, F 2 t ]
�
A1
A2 t
�
GT t
����, (6) where F 1 ∈ Rm×p1
+ and A1 ∈ Rp1×c
+ contain the common factors shared by the source and target domains. F 2 s, F 2 t ∈ Rm×p2
+ and A2 s, A2 t ∈ Rp2×n
+ contain domain-specific information. They are not shared by domains. p1, p2 are two parameters that indicate the number of hidden feature clusters. Gs ∈ Rn×c is the label class matrix and generated from the instance labels {yi|i = 1, · · ·, n} of the source domain S. If the ith instance belongs to jth class, then the (i, j) element in Gs equals to 1, otherwise, it equals to 0.
Gs is a constant matrix and keeps unchanged during the factorization process. Gt is the label class matrix of the target domain. Its elements are variables that we want to learn by the matrix decomposition.
From Eq. (6), we can notice that the label information of the source domain is propagated to the target domain through the shared common factors F1 and A1.
The TTL Transfer Learning Algorithm
As shown in Figure 1, the source, intermediate and target domains have a transitive relationship. In other words, the intermediate domain bridges the source and target domains, but has different common factors to them respectively. Hence, to capture these properties, we propose a coupled NMTF algorithm. The proposed
Figure 2: An illustration of the proposed transfer learning algorithm in the TTL framework.
The algorithm learns two coupled feature representations by feature clustering, and then propagates the label information from the source to the target domain through the intermediate domain on the coupled feature representation. transfer learning algorithm is illustrated in Figure 2, and written in Eq. (7)
L = ||Xs − FsAsGT s || + ||XI − FIAIGT
I ||+
||XI − F
′
IA
′
IGT
I || + ||Xt − FtAtGT t ||
=
����Xs − [ ˆF 1, ˆF 2 s ]
� ˆA1
ˆA2 s
�
GT s
���� +
����XI − [ ˆF 1, ˆF 2
I ]
� ˆA1
ˆA2
I
�
GT
I
���� +
����XI − [ ˜F 1, ˜F 2
I ]
� ˜A1
˜A2
I
�
GT
I
���� +
����Xt − [ ˜F 1, ˜F 2 t ]
� ˜A1
˜A2 t
�
GT t
����.
From the above equation, we can see that the first two terms (
||Xs − FsAsGT s || + ||XI − FIAIGT
I ||) refer to the first feature clustering and label propagation between the source and intermediate domains in Figure 2, the last two terms refer to the second feature clustering and label propagation between the intermediate and target domains. In Eq. (7), it is worth noting that we decompose
XI twice with different decomposition matrices, since XI shares different knowledge with Xs and Xt respectively. At the same time, we couple these two decomposition processes together by the label matrix GI. It is reasonable that the instances in the intermediate domain should have the same labels in different decomposition processes. Moreover, if we solve the matrix decomposition by iterative algorithms, in every iteration, each decomposition process is able to consider the feedbacks from the other decomposition.
If these two processes are separately solved, the first decomposition process will not consider the results from the second one, and may suffer from the bias problem. In the experiment, we find that the coupled strategy achieves better performance than separated decomposition.
Overall, the proposed learning algorithm fits the transitive relationship among domains. The label information in the source domain is transferred through ˆF1 and ˆA1 to the intermediate domain, and affects the learning results of GI. The knowledge on class labels incorporated with GI from the intermediate domain is further transferred to the target domain through ˜F1 and ˜A1.
As we discussed in Section 4.1, the decomposed matrix F contains the information on hidden feature clusters, indicating the distribution of features on each hidden cluster. Therefore, the summation of each column of F has to be equal to one. The label matrix
G indicates the label distribution of each instance. Thus, the summation of each row of G has to be equal to one. Considering these matrix constrains, we obtain the final optimization objective function for the proposed learning algorithm: arg minFs,As,FI,AI,GI,F ′
I,A′
I,Ft,At,Gt
L s.t.
�m i=1 ˆF 1(i, j) = 1, �m i=1 ˆF 2 s (i, j) = 1, �m i=1 ˆF 2
I (i, j) = 1, �m i=1 ˜F 1(i, j) = 1, �m i=1 ˜F 2
I (i, j) = 1, �m i=1 ˜F 2 t (i, j) = 1, �c j=1 GI(i, j) = 1
�c j=1 Gt(i, j) = 1.
Since the objective function in Eq. (8) is non-convex, it is intractable to obtain the global optimal solution. Therefore, we develop an alternating optimization algorithm to achieve the local optimal solution. We first show the updating rules of matrices ˜F 1, ˜F 2
I, ˜F 2 t, and Gt. We summarize the notations of matrix multiplications in Table 3, and show the updating rules as follows:
˜F 1(i, j) = ˜F 1(i, j) ×
�
[ ˜
M1
I+ ˜
M1 t ](i,j)
[ ˜
T 1
I + ˜
T 1 t ](i,j), ˜F 2
I (i, j) = ˜F 2
I (i, j) ×
�
˜
M2
I(i,j)
˜
T 2
I (s,t), ˜F 2 t (i, j) = ˜F 2 t (i, j) ×
�
˜
M2 t (i,j)
˜
T 2 t (s,t), Gt(i, j) = Gt(i, j) ×
�
[XT t FtAt](i,j)
[GtAT t F T t FtAt](i,j).
From Eq. (8), after the matrices are updated, the constrained matrices have to be normalized as:
˜F 1(i, j) =
˜
F 1(i,j)
�m i=1 ˜
F 1(i,j), ˜F 2
I (i, j) =
˜
F 2
I (i,j)
�m i=1 ˜
F 2
I (i,j), ˜F 2 t (i, j) =
˜
F 2 t (i,j)
�m i=1 ˜
F 2 t (i,j), Gt(i, j) =
Gt(i,j)
�c j=1 Gt(i,j).
The updating rules and normalization methods for other submatrices are similar and are shown in the Appendix.
We need not update Gs, which contains the ground-truth label information.
We give the procedure of the proposed learning algorithm in Algorithm 1. As shown in Eq. (7) and the Appendix section, the updating rule for GI is constrained by FI, F ′
I, AI and A′
I. In addition, the sub-matrices ˆF 1, ˆA1 and, ˜F 1, ˜A1 are constrained by Xs, Gs and Xt, Gt respectively. Therefore, the updating rule of Gt is transitively constrained by Xs, Gs and, the discriminative information in the source domain is transitively transferred to the target domain.
The updating processes of Fs, FI, F
′
I and Ft refer to the feature clusterings in Figure 2. The updating processes of GI and Gt refer to the label propagations in Figure 2.
We analyze the convergence property of Eq. (9) with normalization rules in Eq. (10). We first analyze the convergence of ˜F 1 with the rest of the parameters fixed. By using the properties of trace operation and frobenius norm ||X||2 = tr(XT X) = tr(XXT ), we re-formulate the objective function Eq. (8) as a Lagrangian function and keep the terms related to ˜F 1:
Table 3: Notations of matrix multiplications
ˆ
M1
I = XIGI ˆA1T
ˆ
M2
I = XIGI ˆA2T
I
ˆ
M1 t = XtGt ˆA1T
ˆ
M2 t = XtGt ˆA2T t
ˆ
NI = ˆF 1 ˆA1GT
I + ˆF 2
I ˆA2
IGT
I
ˆ
Nt = ˆF 1 ˆA1GT t + ˆF 2 t ˆA2 tGT t
ˆT 1
I = ˆ
NIGI ˆA1T
ˆT 1 t = ˆ
NtGt ˆA1T
ˆT 2
I = ˆ
NIGI ˆA2T
I
ˆT 2 t = ˆ
NtGt ˆA2T t
Ft = [ ˆF1 ˆF 2 t ]
At = [ ˆA1 ˆA2 t]
Algorithm 1 The TTL Transfer Learning Algorithm
1: Input: Source, target, intermediate domains S, T and D, the parameters p, and the number of iterations Itermax.
2: Initialize the matrices Fs, As, FI, AI, GI, Ft, At, Gt.
3: while iter < Itermax do
Update the sub-matrices of Fs, As, FI, AI, Ft, At and label matrices GI, Gt according to the updating rules given in Eq. (9) and Eq. (12) of the Appendix section.
Normalize the sub-matrices of Fs, FI, Ft, and label matrices GI, Gt according to the normalization rules given in Eq. (10) and Eq. (13) of the Appendix.
6: end while
7: Output: the predicted results of Gt.
L( ˜F 1) = tr(−2XT
I ˜F 1 ˜A1GT
I + 2GI ˜A1T ˜F 1T ˜
NI)
+tr(−2XT t ˜F 1 ˜A1GT t + 2GT ˜A1T ˜F 1T ˜
Nt)
+tr[λ( ˜F 1T 1m1T m ˜F 1 − 21p1T m ˜F 1)], (11) where λ ∈ Rp×p is a diagonal matrix. 1m and 1p are all-ones vectors with dimension m and p respectively.
LEMMA 1. Using the update rule in Eq. (9) and normalization rules in Eq. (10), the loss function in Eq. (11) will monotonously decrease.
The proof of Lemma 1 is shown in the Appendix. The convergence of other terms can be proven in the same way. According to the convergence analysis on the update rules and the multiplicative update rules, each update step in Algorithm 1 will not increase
Eq. (8). The objective has a lower bounded by zero. The convergence of the proposed transfer learning algorithm is proven.
EXPERIMENTS
In this section, we perform three tests. The first test is designed to analyze how the intermediate domain and model parameters affect the performance of the TTL framework, and to evaluate the convergence rate empirically. This is done by conducting experiments on six synthetic text classification tasks generated from the 20Newsgroups data set 1.
The second test is designed to evaluate the TTL framework when the source and target domain data have completely different structures. The experiments are conducted on the text-to-image data set.
The intermediate domains for all tasks in the data set are crawled from Flicker.
1http://qwone.com/~jason/20Newsgroups/
Finally, the third test is designed to test the efficiency of the intermediate domain selection algorithm and the transfer learning algorithm in the framework. The experiments are conducted on some text sentiment classification tasks 2. The data from different domains have the same feature space but different distribution. Moreover, there are many candidate intermediate domains for each pair of source and target domains.
Baseline methods
In the synthetic text classification and sentiment classification tasks, all the data have the same feature space. We compare the proposed framework with three baseline methods to verify the effectiveness.
The first baseline is SVM, which is a classical supervised learning algorithm. We use the linear kernel of SVM with the implementation in LibLinear3. The second one is the triplex transfer learning(TriplexTL) algorithm, which is a state-of-the-art transfer learning method implemented with NMTF. The other transfer learning algorithm is LatentMap, which is also a state-of-the-art transfer learning algorithm. It draws the joint distribution of two domains closer by mapping the data to a low dimensional latent space. The three baseline methods are tested under two different settings. The first one is direct-transfer. We train the learners based on the labeled data in the source domain and test them directly on the data in the target domain. We use subscript ST to indicate the methods under this setting in the following experiments, for example, TriplexTLST and LMST. The second setting is a 2-stage transfer learning process. We first apply TriplexTL/LM between the source and the intermediate domain to predict the intermediate domain labels, and then again apply TriplexTL/LM between the intermediate domain and the target domain. The major difference between this naive transitive transfer learning strategy and the proposed transfer learning algorithm is that no iterative feature clustering and label propagation is performed. We use subscript SIT to represent methods under this setting, for instance, TriplexTLSIT and LMSIT.
In the text-to-image data set, the data have different feature spaces.
The above mentioned baselines cannot handle these data. Hence, we compare TTL with two heterogeneous transfer learning (HTL) algorithms.
The first baseline is co-transfer. It models the problem as a coupled Markov chain with restart. The transition probabilities of the Markov chain is construdture by using the intra-relationship based on affinity metric among data in the source and target domains, and the inter-relationship between the source and target domains based on co-occurrence information of the intermediate do2http://www.cs.jhu.edu/~mdredze/datasets/ sentiment/
3http://www.csie.ntu.edu.tw/~cjlin/ liblinear/
Figure 3: The problem setting of the 20Newsgroup data set. main. The second one is HTLIC 4. It learns a new target feature representation by using data from the source, intermediate and target domain data via the collective matrix factorization technique.
A SVM classifier is then learned on the new target feature representation.
All methods in the experiments are performed ten times, and we report their average performances and variances.
Synthetic text classification tasks
20Newsgroups Data Set
The 20Newsgroups is a hierarchical text collection, containing some top categories like 'comp', 'sci', 'rec' and 'talk'. Each category has some sub-categories, such as 'sci.crypt' and 'sci.med'.
We use four main categories to generate six tasks, in each of which two top categories are chosen for generating binary categorization.
With a hierarchical structure, for each category, all of the subcategories are then organized into three parts, where each part has different subcategories and is of a different distribution. Therefore, they can be treated as the source, intermediate and target domains, respectively. To generate the transitive transfer learning setting, we divide the vocabularies into two separated subsets Set A and Set B.
Then, we set the term frequencies of words in Set A of the source domain to zero. Similarly, we set the term frequencies of words in Set B of the target domain to zero. Therefore, the source and target domains have no overlapping words. The problem setting on this data set is illustrated in Figure 3, where the blocks with texture indicate that the features have values. We can see that the source and target domains have no shared features, but they have shared features with the intermediate domain, respectively. Apparently, the intermediate domains here can bridge the generated source and target domains. We give a detailed description of the six tasks in Table 4. The feature dimensions in these tasks range from 2405 to
5984. The number of instances in these tasks are around 7000.
Performance on synthetic tasks
In experiments, we compare the proposed framework with the baseline methods on six text classification tasks.
The text classification tasks are very challenging. The source and target domains have no overlapping features. The SVM classifiers trained with labeled source data have almost no discriminative ability on the target data. From the results in Table 5, we can see that the SVMST classifiers obtain a very bad performance.
Likewise, the source classifiers can barely be adapted for the target domain data. Hence, TriplexTLST and LMST obtain bad performance also, but better than SVMST. The naive transfer learn4http://www.cse.ust.hk/~yinz/htl4ic.zip
# of labeled data
TTL
TriplexTLIT(a) vary # of labeled data
# of removed feature : d
Accuracy
TTL
TriplexTLSIT(b) remove d features
Figure 4: Performance with different intermediate domains. ing algorithms, TriplexTLSIT and LMSIT, achieve relative good performance, because they use the intermediate domain data as a bridge to perform a 2-stage knowledge transfer. The proposed TTL framework achieves the best performance. This can be ascribed to the reason that TTL not only bridges the source and target domains by using the intermediate domain data, but also has iterative feature clustering and label propagation loops where the knowledge provided by the source domain can be deeply reshaped and reorganized to be exploited for the target domain.
Performance with different intermediate domains
The intermediate domain plays an important role in bridging the source and target domains. Hence, we also conduct some experiments on the "comp-vs-talk" task to test the proposed TTL framework when 1) the amount of labeled intermediate data increases;
2) the connection between the source/target and the intermediate domains becomes weaker.
In the first setting, we compare TTL with TriplexTLIT that transfers knowledge from labeled intermediate domain data to the target data. We vary the amount of labeled intermediate data from 50 to
400. We randomly sample the labeled intermediate domain data ten times, and show the average performance and variance in Figure 4(a). From the results, we can see that the performance of TTL is better than TriplexTLIT when the amount of labeled intermediate domain data is small. However, when there is a large amount of labeled intermediate data, the performance of TriplexTLIT is better. The results are reasonable, because when we have large amount of data that are near and adaptable to the target data, we need not seek help from domains that are far away.
In the second setting, some overlap features in the intermediate domain are removed. We compare the TTL framework with
TriplexTLSIT. In each comparison experiment, we randomly remove d features ten times, and show the average performance and its variance in Figure 4(b). From the results we can see that the performance decreases as features are removed. The reason is that the connection between the intermediate and source/target domain becomes weaker when more features are removed.
Model Analysis
In the Appendix, we have theoretically proven the convergence of the transfer learning algorithm in the TTL framework. Here we test the convergence rate. We conduct an experiment on "compvs-talk" task, and set the number of iterations to 100. We show the objective value of Eq. (8) as the dashed line in Figure 5(a), and see that after around five to ten iterations, the objective value experiences almost no change. Similarly, we show the classification accuracy of the target domain of each iteration as the solid line in Table 4: Dataset Description
Task
Source
Intermediate
Target rec-vs-comp autos : misc baseball : mac hockey : windows rec-vs-talk autos : guns motorcycles : mideast hockey : misc rec-vs-sci autos : electronics motorcycles : med hockey : space sci-vs-comp electronics : graphics med : misc space : windows sci-vs-talk crypt : guns electronics : mideast med : misc comp-vs-talk graphics : guns misc : mideast windows : politic
Table 5: Accuracy (%) on the synthetic text classification tasks
SVMST
TriplexTLST
TriplexTLSIT
LMST
LMSIT
TTL rec-vs-comp
53.04 ± 1.87
56.74 ± 4.95
52.23 ± 2.97
55.34 ± 3.75
57.91 ± 3.27 rec-vs-talk
59.41 ± 9.74
61.67 ± 7.93
60.11 ± 7.22
60.97 ± 6.53
68.77 ± 1.61 rec-vs-sci
51.64 ± 1.44
51.95 ± 1.70
50.89 ± 2.13
51.23 ± 1.56
51.95 ± 0.98 sci-vs-comp
52.14 ± 2.65
55.93 ± 2.39
53.26 ± 2.95
55.29 ± 2.76
56.26 ± 2.14 sci-vs-talk
51.57 ± 2.07
52.80 ± 1.66
50.98 ± 2.14
52.69 ± 1.73
53.15 ± 1.53 comp-vs-talk
60.90 ± 9.35
64.08 ± 10.19
61.34 ± 9.73
64.58 ± 9.67
72.22 ± 3.20
Iteration
Objective Value
Accuracy(a) Convergence
The parameter p in TTL
Accuracy(b) Varying the value of p
Figure 5: Convergence analysis and model parameter analysis.
Figure 5(a). The results show that there is no change in the performance after 60-80 iterations. The convergence trends on other tasks are similar.
We also analyze the model parameter p. We vary p from 5 to
100 to test how it affects the classification performance. The experiments are also conducted on "comp-vs-talk" task. The results are shown in Figure 5(b), from which we can see that the algorithm achieves better performance when p is between 20 and 40. For different tasks, we can use ten-fold cross validation to choose the value. In this paper, we simply set p to be 30 in the experiments.
Text-to-image classification tasks
NUS-WISE data set
The NUS-WISE data set for heterogeneous transfer learning problem is generated by. It contains 45 text-to-image tasks. Each task is composed of 1200 text documents, 600 images, and 1600 co-occurred text-image pairs. The data in each task are about two different categories, such as "boat" and "flower". Therefore, we can do binary classification for each task. There are 10 categories in the data set, including "bird", "boat", "flower", "food", "rock", "sun", "tower", "toy", "tree" and "car". The text vocabulary size is 500. Each text data is represented by a 500 dimensional bagof-word vector. For image data, we extract SIFT features and represent each image in a 512 dimensional feature vector. In this
No. of labeled target data
Avg. Accuracy
TTL co−transfer
HTLIC
SVM(a) Average performance
Task
Classification Accuracy
TTL co−transfer
HTLIC
SVM(b) Detailed performance
Figure 6: The classification accuracy on the tasks of the textto-image data set. data set, our task is to transfer knowledge from source text documents to images through co-occurred text-image pairs.
Performance on text-to-image tasks
As HTLIC needs some labeled target domain data to train the SVM classifier, in the text-to-image tasks, we assume all the source domain data and a few target domain data are labeled. We vary the amount of labeled data in the target domain from 5 to 25, and show the average classification accuracies of all the tasks in Fig. 6(a), from which we can see that the performance of each algorithm increases when more labeled target data are used. We can also find that SVM achieves the worst performance, since it considers no auxiliary information. HTLIC and co-transfer achieve better performance than SVM, since they successfully leverage some knowledge from the source domain by using the intermediate domain data. The proposed TTL framework obtains the best performance.
The reason is that TTL takes the distribution shift between three domains into account and explicitly exploits the transitively shared knowledge for label propagation from the source to the target domain.
We also report the detailed results on each individual task with
25 labeled target domain data. The classification accuracies and variances on each task are shown in Fig. 6(b). The x-axis indicates
1161 the task and the y-axis represents the classification accuracy. We sort the tasks by the performance of the proposed TTL framework in ascend order. From the results, we can find that TTL is superior to other algorithms on most tasks and is always at the top. In addition, TTL is more stable than other algorithms.
Sentiment classification tasks
Sentiment Classification Data set
The sentiment classification data set used in our experiment consist of Amazon product reviews on 12 different categories, including "Apparel", "Books", "Camera_&_photo", "DVD", "Electronics", "Health_&_personal_care", "Kitchen_&_housewares", "Music", "Sports_&_outdoors", "Toys_&_games" and "Video". Each product review consists of review text and a sentiment label. The data from different domains have different distributions. For example, reviews in "Kitchen_&_housewares" may have adjectives such as "malfunctioning", "reliable" and "sturdy". However, reviews in the "DVD" domain may have "thrilling", "horrific" and "hilarious". In this data set, the data within each domain are balanced. One half of the data are positive reviews and the other half are negative. The data size in each domain ranges from 2,000 to
20,000. The vocabulary size for each domain is around 20,000. We randomly sample around 2,000 instances for each domain. From the 12 domains, we can generate P 3
12=1,320 triples, such as <"Apparel", "Books", "Camera_&_photo"> where "Apparel", "Books" and "Camera_&_photo" are the source, intermediate and target domains respectively. We conduct experiments on all the 1320 triple to evaluate the performance of the proposed intermediate domain selection algorithm. We also conduct experiments on triples that are selected by the intermediate domain selection algorithm to test the proposed transfer learning algorithm in the TTL framework.
Intermediate Domain Selection
In order to evaluate the proposed intermediate domain selection algorithm, we propagate labels from the labeled source domain data to the unlabeled target domain data, and evaluate the prediction accuracy accst on the target domain data. We also propagate labels from the labeled source domain data to the unlabeled intermediate and target domain data by the same algorithm, and evaluate the prediction accuracy accsit on the target domain data. In the experiment, we use semi-supervised learning with RBF kernel to do label propagation. If accsit > t × accst, (t > 1.0), it means that the intermediate domain data are able to bridge the source and target domain, and we assign a positive label to the triple. Otherwise, we assign a negative label. In the experiment, we set t = 1.03, and get 102 positive labels among 1,320 triples.
We then randomly split all the triples into two parts, each part contains the same number of positive and negative triples. The first part is used to train the intermediate domain selection algorithm, the second part is for testing. Since the data are unbalanced, we randomly sampled some negative triples to form a balanced data set. We do the random sampling ten times. Each time, we use 10fold cross validation to assess the performance of the intermediate domain selection algorithm on the first part. The average accuracy is 0.845 ± 0.034.
Performance on Sentiment Classification Tasks
We also test the proposed transfer learning algorithm in the TTL framework on some triples selected by the intermediate domain selection algorithm with high confidence from the second part. We learn the selection model on the training triples and select 10 triples with highest confidence from the testing triple set. The selected triples are listed in Table 6. Some results are interesting and explainable. For example, "video" domain is able to bridge the "music" and "apparel" domains. Intuitively, most music review words are about sound such as rhythm and melody. Most apparel reviews may talk about the appearance like the color. The video reviews contain both the vocal and visual aspects, and are able to draw the music and apparel domains close.
From the results in Table 6, we can see that TriplexST has almost the same results as SVMST. The direct transfer learning algorithm here achieves no performance improvement. This is because the source and target domains have large distribution gap. TTL and TriplexSIT are better than TriplexST. We can also see that TTL always achieves the best performance.
RELATED WORKS
We discuss two categories of research related to transitive transfer learning: transfer learning and multi-task learning.
Transfer Learning solves the lack of class label problem in the target domain by "borrowing" supervised knowledge from related source domains. There are mainly two typical types of algorithms. The first one is instance based knowledge transfer, which selects or adapts the weights of the relevant data from source domains for the target domain. The second one is feature based knowledge transfer, that transforms both source and target data into a common feature space where data follow similar distributions. More recently, multi-source transfer learning performs transfer learning with multiple source domains. For instance, the work in extends TrAdaboost by adding a wrapper boosting framework on weighting each source domain. Different from previous transfer learning, transitive transfer learning does not assume that the source domain and the target domain should be related.
That means, transitive learning can be more general and more useful when the existing labeled and related source domains are not adequate enough to improve the target domain.
Multi-task Learning algorithms simultaneously learn several tasks together and mutually enhance the classification results of each task. It assumes that different tasks share some natural "compact" representations, such as the information reflected by shared data clusters or subspaces. In practice, for example, classifiers for different tasks can be designed to share some global parameters or even a global classifier. More recently, approaches that learn the relationships between pairwise tasks are also being developed. However, these methods require reasonably large amounts of labeled data for each task to learn the relationship.
In contrast, transitive transfer learning works even when both intermediate and target domains are unlabeled. It only assumes that the source domain should have sufficient labeling information to transfer. The intermediate domain serves as a bridge between source and target domains. Even if the intermediate domain is not labeled, the classification information passed from the source domain still contributes to the final classification task through the latent factors learnt in the learning process.
CONCLUSIONS AND FUTURE WORK
In this paper, we study a new problem, transitive transfer learning (TTL), which transfers knowledge from a source domain to an indirectly related target domain with the help of some intermediate domains. We propose a TTL framework to solve the problem. The framework first selects one or more intermediate domains to bridge the given source and target domains, and then performs knowledge transfer along this bridge by capturing overlap hidden features among them. The experiments are conducted on three data
Table 6: Accuracy (%) on the Sentiment classification tasks
Source
Intermediate
Target
SV MST
TriplexST
TriplexSIT
TTL music video apperal
78.57 ± 1.84
78.51 ± 1.24
79.21 ± 1.47 health_&_personal_care baby books
74.15 ± 1.20
74.26 ± 1.21
75.38 ± 1.51 dvd toys_&_games apparel
80.10 ± 1.46
81.11 ± 1.46
83.57 ± 1.34 music toys_&_games baby
76.64 ± 1.52
77.64 ± 1.46
81.22 ± 1.37 books camera_&_photo apparel
80.38 ± 1.34
80.98 ± 1.21
82.74 ± 1.04 sports_&_outdoors video books
72.25 ± 1.58
73.00 ± 1.67
76.01 ± 1.05 video baby camera_&_photo
78.43 ± 1.06
79.42 ± 1.03
81.07 ± 1.06 dvd kitchen_&_housewares baby
78.01 ± 1.13
81.11 ± 1.05
81.42 ± 1.03 electronics baby toys_&_games
81.60 ± 1.63
81.95 ± 1.49
82.12 ± 0.09 electronics baby kitchen_&_housewares
83.52 ± 1.02
84.50 ± 1.06
85.63 ± 1.04 sets, showing that the proposed framework achieves state-of-theart performance. The convergence of the proposed TTL framework has also been theoretically and experimentally proven.
Future Work As a new learning problem, it raises several issues for further exploration in the future. For example, when the source and target need a string of domains to build a connection, how to find the string of intermediate domains to enable max transfer is a valuable research problem. In addition, extending the algorithm to multiple source domains may be an interesting way to generalize transitive transfer learning to be more powerful.
ACKNOWLEDGMENTS
We thank the support of China National 973 project 2014CB340304 and Hong Kong RGC Projects 621013, 620 812, and 621211. We also thank Yin Zhu, Lili Zhao, Zhongqi Lu, Kaixiang Mo and Ying
Wei for discussion.
REFERENCES
 R. K. Ando and T. Zhang. A framework for learning predictive structures from multiple tasks and unlabeled data.
JMLR, 6:1817–1853, Dec. 2005.
 J. Baxter. A bayesian/information theoretic model of learning to learn viamultiple task sampling. Machine
Learning, 28(1):7–39, 1997.
 S. Ben-David, J. Blitzer, K. Crammer, F. Pereira, et al.
Analysis of representations for domain adaptation. NIPS, S. Ben-David, J. Gehrke, and R. Schuller. A theoretical framework for learning from a pool of disparate data sources.
In KDD, pages 443–449, 2002.
 S. Ben-David and R. Schuller. Exploiting task relatedness for mulitple task learning. In COLT, pages 567–580, 2003.
 J. Blitzer, M. Dredze, and F. Pereira. Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. In ACL, Prague, Czech Republic, P. E. Bryant and T. Trabasso. Transitive inferences and memory in young children. Nature, 1971.
 O. Chapelle, P. Shivaswamy, S. Vadrevu, K. Weinberger, Y. Zhang, and B. Tseng. Multi-task learning for boosting with application to web search ranking. In KDD, pages
1189–1198, 2010.
 W. Dai, Q. Yang, G.-R. Xue, and Y. Yu. Boosting for transfer learning. In ICML, pages 193–200, 2007.
 C. Ding, T. Li, W. Peng, and H. Park. Orthogonal nonnegative matrix t-factorizations for clustering. In KDD, pages 126–135. ACM, 2006.
 T. Evgeniou and M. Pontil. Regularized multi–task learning.
In KDD, pages 109–117, 2004.
 Z. Kang, K. Grauman, and F. Sha. Learning with whom to share in multi-task feature learning. In ICML, pages
521–528, 2011.
 D. D. Lee and H. S. Seung. Algorithms for non-negative matrix factorization. In NIPS, pages 556–562. MIT Press, C.-K. Lin, Y.-Y. Lee, C.-H. Yu, and H.-H. Chen. Exploring ensemble of models in taxonomy-based cross-domain sentiment classification. CIKM '14, pages 1279–1288, New
York, NY, USA, 2014. ACM.
 M. Long, J. Wang, G. Ding, W. Cheng, X. Zhang, and W. Wang. Dual transfer learning. In SDM, pages 540–551.
SIAM, 2012.
 D. G. Lowe. Distinctive image features from scale-invariant keypoints. IJCV, 60:91–110, 2004.
 M. Ng, Q. Wu, and Y. Ye. Co-transfer learning using coupled markov chains with restart. 2013.
 S. J. Pan and Q. Yang. A survey on transfer learning. TKDE, 22(10):1345–1359, October 2010.
 W. Pan, N. N. Liu, E. W. Xiang, and Q. Yang. Transfer learning to predict missing ratings via heterogeneous user feedbacks. In IJCAI, pages 2318–2323, 2011.
 N. Ponomareva and M. Thelwall. Biographies or blenders:
Which resource is best for cross-domain sentiment analysis?
In CICLing, pages 488–499. Springer, 2012.
 M. T. Rosenstein, Z. Marx, L. P. Kaelbling, and T. G.
Dietterich. To transfer or not to transfer. In NIPS 2005
Workshop on Transfer Learning, volume 898, 2005.
 B. Tan, E. Zhong, M. Ng, and Q. Yang. Mixed-transfer: transfer learning over mixed graphs. In SDM, 2014.
 B. Tan, E. Zhong, W. Xiang, and Q. Yang. Multi-transfer:
Transfer learning with multiple views and multiple sources.
In SDM, 2013.
 M. E. Taylor and P. Stone. Transfer learning for reinforcement learning domains: A survey. JMLR, 10:1633–1685, Dec. 2009.
 S. Xie, W. Fan, J. Peng, O. Verscheure, and J. Ren. Latent space domain transfer between high dimensional overlapping distributions. In WWW, pages 91–100, 2009.
 Y. Yao and G. Doretto. Boosting for transfer learning with multiple sources. In CVPR, pages 1855–1862, 2010.
 Y. Zhang and D.-Y. Yeung. A convex formulation for learning task relationships in multi-task learning. In UAI, Y. Zhang and D.-Y. Yeung. Multi-task boosting by exploiting task relationships. In ECML/PKDD, pages 697–710, 2012.
 E. Zhong, W. Fan, Q. Yang, O. Verscheure, and J. Ren. Cross validation framework to choose amongst models and datasets for transfer learning. In ECML/PKDD, pages 547–562, 2010.
 X. Zhu. Semi-supervised learning literature survey. 2005.
 Y. Zhu, Y. Chen, Z. Lu, S. J. Pan, G.-R. Xue, Y. Yu, and Q. Yang. Heterogeneous transfer learning for image classification. In AAAI, 2011.
 F. Zhuang, P. Luo, C. Du, Q. He, and Z. Shi. Triplex transfer learning: exploiting both shared and distinct concepts for text classification. In WSDM, pages 425–434, 2013.
Appendix
Table 7: Notations of matrix multiplications
ˆ
M1 s = XsGs ˆA1T
ˆ
M2 s = XsGs ˆA2T s
ˆ
M1
I = XIGI ˆA1T
ˆ
M2
I = XIGI ˆA2T
I
ˆ
Ns = ˆF 1 ˆA1GT s + ˆF 2 s ˆA2 sGT s
ˆT 1 s = ˆ
NsGs ˆA1T
ˆ
NI = ˆF 1 ˆA1GT
I + ˆF 2
I ˆA2
IGT
I
ˆT 2 s = ˆ
NsGs ˆA2T s
ˆT 1
I = ˆ
NT GT ˆA1T
ˆT 2
I = ˆ
NIGI ˆA2T
I
FI = [ ˆF1 ˆF 2
I ]
AI = [ ˆA1 ˆA2
I]
F ′
I = [ ˜F1 ˜F 2
I ]
A′
I = [ ˜A1 ˜A2
I]
We summarize some other matrix multiplication notations in Table 7, and give the update rules for ˆFs, ˆAs, ˆFI and ˆAI as follow:
ˆF 1(i, j) = ˆF 1(i, j) ×
�
[ ˆ
M1s+ ˆ
M1
I](i,j)
[ ˆ
T 1 s + ˆ
T 1
I ](i,j), ˆF 2 s (i, j) = ˆF 2 s (i, j) ×
�
ˆ
M2s(i,j)
ˆ
T 2 s (i,j), ˆF 2
I (i, j) = ˆF 2
I (i, j) ×
�
ˆ
M2
I(i,j)
ˆ
T 2
I (i,j), ˆA1(i, j) = ˆA1(i, j) ×
�
[ ˆ
F 1T (XsGs+XIGI)](i,j)
[ ˆ
F 1T ( ˆ
NsGs+ ˆ
NIGI)](i,j), ˆA2 s(i, j) = ˆA2 s(i, j) ×
�
[ ˆ
F 2T s
XsGs](i,j)
[ ˆ
F 2T s
ˆ
NsGs](i,j), ˆA2
I(i, j) = ˆA2
I(i, j) ×
�
[ ˆ
F 2T
I
XIGI](i,j)
[ ˆ
F 2T
I
ˆ
NIGI](i,j), GI(i, j) = GI(i, j) ×
�
[XT
I F ′
IA′
I+XT
I FIAI](i,j)
[GIA′T
I
F ′T
I
F ′
IA′
I+GIAT
I F T
I FIAI](i,j)
The normalization methods for ˆFs and ˆFI are:
ˆFs(i, j) =
ˆ
Fs(i,j)
�m i=1 ˆ
Fs(i,j), ˆFI(i, j) =
ˆ
FI(i,j)
�m i=1 ˆ
FI(i,j), Convergence Analysis
We first analyze the convergence of ˆF 1 with the rest parameters are fixed. By using the properties of trace operation and frobenius norm ||X||2 = tr(XT X) = tr(XXT ), we re-formulate the objective function Eq. (8) as a Lagrangian function and keep the terms related to ˆF 1:
L( ˆF 1) = tr(−2XT s ˆF 1 ˆA1GT s + 2Gs ˆA1T ˆF 1T ˆ
Ns)
+tr(−2XT
I ˆF 1 ˆA1GT
I + 2GI ˆA1T ˆF 1T ˆ
NI)
+tr[λ( ˆF 1T 1m1T m ˆF 1 − 21p1T m ˆF 1)], (14) where λ ∈ Rp×p is a diagonal matrix. 1m and 1p are all-ones vectors with dimension ms and p respectively. The differential of Eq. (14) is:
∂L( ˆ
F 1)
∂ ˆ
F 1
= tr(−2XsGs ˆA1T + 2 ˆ
NsGs ˆA1T )
+tr(−2XIYt ˆA1T + 2 ˆ
NtGt ˆA1T )
+21m(1T m ˆF 1 − 1T p )λ, Then, we obtain the temporary updating rule:
ˆF 1(i, j) = ˆF 1(i, j) ×
�
[XsGs ˆ
A1T +XIGI ˆ
A1T +1m1T p λ](i,j)
[ ˆ
NsGs ˆ
A1T + ˆ
NIGI ˆ
A1T +1m1T m ˆ
F 1λ](i,j), As proved in, the temporary update rule in Eq. (16) is able to monotonously decrease the Eq. (14). Therefore, there is still one variable λ that needs further calculation. Considering the constrains in Eq. (8), we find that λ is used to satisfy the conditions that the summation of each column of ˆF 1 has to be equal to one. We use the the normalization method in Eq. (13) to normalize ˆF 1. The method satisfies the condition regardless of λ. After that, 1m1T p λ is equal to 1m1T m ˆF 1λ. By getting rid of the terms that contain λ, we get the final update rule in Eq. (12) that is approximately equal to Eq. (16) in terms of convergence, since both 1m1T p λ and 1m1T m ˆF 1λ are constants. Using update rule in Eq. (12) will also monotonously decrease the value of Eq. (14).
We can use similar methodology to analyze the convergence of the update rules and normalization methods for other terms in Eq. (8).
According to the Multiplicative Update Rules in, using the update rules in Eq. (9) and Eq. (12) and using the normalization methods in Eq. (10) and Eq. (13), the value of the objective function in Eq. (8) will not increase. The objective function has a zero lower bound. The convergence of Algorithm 1 is guaranteed.Manuscript accepted by Neurocomputing 2018
Deep Visual Domain Adaptation: A Survey
Mei Wang, Weihong Deng
School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China. wm0245@126.com, whdeng@bupt.edu.cn
Abstract—Deep domain adaptation has emerged as a new learning technique to address the lack of massive amounts of labeled data. Compared to conventional methods, which learn shared feature subspaces or reuse important source instances with shallow representations, deep domain adaptation methods leverage deep networks to learn more transferable representations by embedding domain adaptation in the pipeline of deep learning. There have been comprehensive surveys for shallow domain adaptation, but few timely reviews the emerging deep learning based methods. In this paper, we provide a comprehensive survey of deep domain adaptation methods for computer vision applications with four major contributions. First, we present a taxonomy of different deep domain adaptation scenarios according to the properties of data that define how two domains are diverged. Second, we summarize deep domain adaptation approaches into several categories based on training loss, and analyze and compare briefly the state-of-the-art methods under these categories. Third, we overview the computer vision applications that go beyond image classification, such as face recognition, semantic segmentation and object detection. Fourth, some potential deficiencies of current methods and several future directions are highlighted.
I. INTRODUCTION
Over the past few years, machine learning has achieved great success and has benefited real-world applications. However, collecting and annotating datasets for every new task and domain are extremely expensive and time-consuming processes, sufficient training data may not always be available.
Fortunately, the big data era makes a large amount of data available for other domains and tasks. For instance, although large-scale labeled video databases that are publicly available only contain a small number of samples, statistically, the YouTube face dataset (YTF) consists of 3.4K videos. The number of labeled still images is more than sufficient.
Hence, skillfully using the auxiliary data for the current task with scarce data will be helpful for real-world applications.
However, due to many factors (e.g., illumination, pose, and image quality), there is always a distribution change or domain shift between two domains that can degrade the performance, as shown in Fig. 1. Mimicking the human vision system, domain adaptation (DA) is a particular case of transfer learning (TL) that utilizes labeled data in one or more relevant source domains to execute new tasks in a target domain. Over the past decades, various shallow DA methods have been proposed to solve a domain shift between the source and target domains. The common algorithms for shallow DA can mainly be categorized into two classes: instance-based DA, and feature-based DA,,,. The first class reduces the discrepancy by reweighting the source samples, and it trains on the weighted source samples. For the second class, a common shared space is generally learned in which the distributions of the two datasets are matched.
Recently, neural-network-based deep learning approaches have achieved many inspiring results in visual categorization applications, such as image classification, face recognition, and object detection. Simulating the perception of the human brain, deep networks can represent high-level abstractions by multiple layers of non-linear transformations.
Existing deep network architectures include convolutional neural networks (CNNs),,,, deep belief networks (DBNs), and stacked autoencoders (SAEs), among others. Although some studies have shown that deep networks can learn more transferable representations that disentangle the exploratory factors of variations underlying the data samples and group features hierarchically in accordance with their relatedness to invariant factors, Donahue et al.
 showed that a domain shift still affects their performance. The deep features would eventually transition from general to specific, and the transferability of the representation sharply decreases in higher layers. Therefore, recent work has addressed this problem by deep DA, which combines deep learning and DA.
There have been other surveys on TL and DA over the past few years,,,,,. Pan et al. categorized TL under three subsettings, including inductive TL, transductive TL, and unsupervised TL, but they only studied homogeneous feature spaces. Shao et al. categorized TL techniques into feature-representation-level knowledge transfer and classifier-level knowledge transfer.
The survey written by Patel only focused on DA, a subtopic of TL. discussed 38 methods for heterogeneous
TL that operate under various settings, requirements, and domains. Zhang et al. were the first to summarize several transferring criteria in detail from the concept level. These five surveys mentioned above only cover the methodologies on shallow TL or DA. The work presented by Csurka et al.
 briefly analyzed the state-of-the-art shallow DA methods and categorized the deep DA methods into three subsettings based on training loss: classification loss, discrepancy loss and adversarial loss. However, Csurka's work mainly focused on shallow methods, and it only discussed deep DA in image classification applications.
In this paper, we focus on analyzing and discussing deep
DA methods. Specifically, the key contributions of this survey are as follows: 1) we present a taxonomy of different deep
DA scenarios according to the properties of data that define how two domains are diverged. 2) extending Csurka's work, we improve and detail the three subsettings (training with arXiv:1802.03601v4 [cs.CV] 25 May 2018
Manuscript accepted by Neurocomputing 2018
Amazon
Webcam
DSLR
Caltech-256
MNIST
USPS
SVHN(a)(b)
CUFS
BCS(c)
LFW
Fig. 1. (a) Some object images from the "Bike" and "Laptop" categories in Amazon, DSLR, Webcam, and Caltech-256 databases. (b) Some digit images from MNIST, USPS, and SVHN databases. (c) Some face images from LFW, BCS and CUFS databases. Realworld computer vision applications, such as face recognition, must learn to adapt to distributions specific to each domain. classification loss, discrepancy loss and adversarial loss) and summarize different approaches used in different DA scenes.
3) Considering the distance of the source and target domains, multi-step DA methods are studied and categorized into handcrafted, feature-based and representation-based mechanisms.
4) We provide a survey of many computer vision applications, such as image classification, face recognition, style translation, object detection, semantic segmentation and person reidentification.
The remainder of this survey is structured as follows.
In Section II, we first define some notations, and then we categorize deep DA into different settings (given in Fig. 2). In the next three sections, different approaches are discussed for each setting, which are given in Table I and Table II in detail.
Then, in Section VI, we introduce some successful computer vision applications of deep DA. Finally, the conclusion of this paper and discussion of future works are presented in Section
VII.
II. OVERVIEW
A. Notations and Definitions
In this section, we introduce some notations and definitions that are used in this survey. The notations and definitions match those from the survey papers by, to maintain consistency across surveys. A domain D consists of a feature space X and a marginal probability distribution P(X), where X = {x1,..., xn} ∈ X. Given a specific domain
D = {X, P(X)}, a task T consists of a feature space Y and an objective predictive function f(·), which can also be viewed as a conditional probability distribution P(Y |X) from a probabilistic perspective. In general, we can learn P(Y |X) in a supervised manner from the labeled data {xi, yi}, where xi ∈ X and yi ∈ Y.
Assume that we have two domains: the training dataset with sufficient labeled data is the source domain Ds
=
{X s, P(X)s}, and the test dataset with a small amount of labeled data or no labeled data is the target domain Dt =
{X t, P(X)t}. We see that the partially labeled part, Dtl, and the unlabeled parts, Dtu, form the entire target domain, that is, Dt = Dtl ∪ Dtu. Each domain is together with its task: the former is T s = {Ys, P(Y s|Xs)}, and the latter is T t = {Yt, P(Y t|Xt)}. Similarly, P(Y s|Xs) can be learned from the source labeled data {xs i, ys i }, while P(Y t|Xt) can be learned from labeled target data {xtl i, ytl i } and unlabeled data {xtu i }.
B. Different Settings of Domain Adaptation
The case of traditional machine learning is Ds = Dt and T s = T t. For TL, Pan et al. summarized that the differences between different datasets can be caused by domain divergence Ds ̸= Dt (i.e., distribution shift or feature space difference) or task divergence T s ̸= T t (i.e., conditional distribution shift or label space difference), or both. Based on this summary, Pan et al. categorized TL into three main groups: inductive, transductive and unsupervised TL.
According to this classification, DA methods are transductive TL solutions with the assumption that the tasks are the same, i.e., T s = T t, and the differences are only caused by domain divergence, Ds ̸= Dt. Therefore, DA can be split into two main categories based on different domain divergences (distribution shift or feature space difference): homogeneous and heterogeneous DA. Then, we can further categorize DA into supervised, semi-supervised and unsupervised DA in consideration of labeled data of the target domain.
The classification is given in Fig. 2.
• In the homogeneous DA setting, the feature spaces between the source and target domains are identical(X s = X t) with the same dimension (ds = dt). Hence, the source and target datasets are generally different in terms of data distributions (P(X)s ̸= P(X)t).
In addition, we can further categorize the homogeneous DA setting into three cases:
1) In the supervised DA, a small amount of labeled target data, Dtl, are present. However, the labeled data are commonly not sufficient for tasks.
Manuscript accepted by Neurocomputing 2018
One-step
Domain adaptation
Homogeneous
Heterogeneous
Supervised
Semi-Supervised
Unsupervised
Supervised
Semi-Supervised
Unsupervised
Labeled data are available in target domain
Labeled+unlabeled data are available in target domain
No labeled data in target domain
Labeled data are available in target domain
Labeled+unlabeled data are available in target domain
No labeled data in target domain
Feature Space is same between source and target domain
Feature Space is different between source and target domain
Multi-step
Domain adaptation
Domain adaptation
Select Intermediate Domain
Fig. 2. An overview of different settings of domain adaptation
2) In the semi-supervised DA, both limited labeled data, Dtl, and redundant unlabeled data, Dtu, in the target domain are available in the training stage, which allows the networks to learn the structure information of the target domain.
3) In the unsupervised DA, no labeled but sufficient unlabeled target domain data, Dtu, are observable when training the network.
• In the heterogeneous DA setting, the feature spaces between the source and target domains are nonequivalent(X s ̸= X t), and the dimensions may also generally differ(ds ̸= dt).
Similar to the homogeneous setting, the heterogeneous DA setting can also be divided into supervised, semi-supervised and unsupervised DA.
All of the above DA settings assumed that the source and target domains are directly related; thus, transferring knowledge can be accomplished in one step. We call them onestep DA. In reality, however, this assumption is occasionally unavailable. There is little overlap between the two domains, and performing one-step DA will not be effective. Fortunately, there are some intermediate domains that are able to draw the source and target domains closer than their original distance.
Thus, we use a series of intermediate bridges to connect two seemingly unrelated domains and then perform onestep DA via this bridge, named multi-step (or transitive) DA,. For example, face images and vehicle images are dissimilar between each other due to different shapes or other aspects, and thus, one-step DA would fail. However, some intermediate images, such as 'football helmet', can be introduced to be an intermediate domain and have a smooth knowledge transfer. Fig. 3 shows the differences between the learning processes of one-step and multi-step DA techniques.
III. APPROACHES OF DEEP DOMAIN ADAPTATION
In a broad sense, deep DA is a method that utilizes a deep network to enhance the performance of DA. Under this definition, shallow methods with deep features,,,, can be considered as a deep DA approach. DA is adopted by shallow methods, whereas deep networks only extract vectorial features and are not helpful for transferring knowledge directly. For example, extracted the convolutional activations from a CNN as the tensor representation, and then performed tensor-aligned invariant subspace learning to realize DA. This approach reliably outperforms current state-of-the-art approaches based on traditional hand-crafted features because sufficient representational and transferable features can be extracted through deep networks, which can work better on discrimination tasks.
In a narrow sense, deep DA is based on deep learning architectures designed for DA and can obtain a firsthand effect from deep networks via back-propagation. The intuitive idea is to embed DA into the process of learning representation and to learn a deep feature representation that is both semantically meaningful and domain invariant. With the "good" feature representations, the performance of the target task would improve significantly. In this paper, we focus on the narrow definition and discuss how to utilize deep networks to learn
"good" feature representations with extra training criteria.
A. Categorization of One-Step Domain Adaptation
In one-step DA, the deep approaches can be summarized into three cases, which refers to. Table 1 shows these three cases and brief descriptions. The first case is the discrepancybased deep DA approach, which assumes that fine-tuning the deep network model with labeled or unlabeled target data can diminish the shift between the two domains. Class criterion, statistic criterion, architecture criterion and geometric criterion are four major techniques for performing fine-tuning.
• Class Criterion: uses the class label information as a guide for transferring knowledge between different domains. When the labeled samples from the target domain are available in supervised DA, soft label and metric learning are always effective,,,,. When such samples are unavailable, some other techniques can be adopted to substitute for class labeled data, such as pseudo labels,,, and attribute representation,.
• Statistic Criterion: aligns the statistical distribution shift between the source and target domains using some mechanisms. The most commonly used methods for comparing and reducing distribution shift are maximum mean discrepancy (MMD),,,,,, correlation alignment (CORAL),, KullbackLeibler (KL) divergence and H divergence, among others.
• Architecture Criterion: aims at improving the ability of learning more transferable features by adjusting the architectures of deep networks. The techniques that are
Manuscript accepted by Neurocomputing 2018
Different Domain
Learning System
Learning System
Learning System intermediate
Knowledge
Knowledge
Learning System
Source Domain
Knowledge
Learning System
Target Domain
Target Domain
Source Domain
Traditional Machine Learning
One-step Domain Adaptation
Multi-step Domain Adaptation(a)(b)(c)
One-Step
Fig. 3. Different learning processes between (a) traditional machine learning, (b) one-step domain adaptation and (c) multi-step domain adaptation.
TABLE I
DIFFERENT DEEP APPROACHES TO ONE-STEP DA
One-step DA
Approaches
Brief Description
Subsettings
Discrepancy-based fine-tuning the deep network with labeled or unlabeled target data to diminish the domain shift class criterion,,,,,,,,,, statistic criterion,,,,,,, architecture criterion,,,,, geometric criterion 
Adversarial-based using domain discriminators to encourage domain confusion through an adversarial objective generative models,, non-generative models,,,, 
 
Reconstructionbased using the data reconstruction as an auxiliary task to ensure feature invariance encoder-decoder reconstruction,,, adversarial reconstruction,, 
TABLE II
DIFFERENT DEEP APPROACHES TO MULTI-STEP DA
Multi-step Approaches
Brief Description
Hand-crafted users determine the intermediate domains based on experience 
Instance-based selecting certain parts of data from the auxiliary datasets to compose the intermediate domains, 
Representation-based freeze weights of one network and use their intermediate representations as input to the new network proven to be cost effective include adaptive batch normalization (BN),,, weak-related weight, domain-guided dropout, and so forth.
• Geometric Criterion: bridges the source and target domains according to their geometrical properties. This criterion assumes that the relationship of geometric structures can reduce the domain shift.
The second case can be referred to as an adversarial-based deep DA approach. In this case, a domain discriminator that classifies whether a data point is drawn from the source or target domain is used to encourage domain confusion through an adversarial objective to minimize the distance between the empirical source and target mapping distributions.
Furthermore, the adversarial-based deep DA approach can be categorized into two cases based on whether there are generative models.
• Generative Models: combine the discriminative model with a generative component in general based on generative adversarial networks (GANs). One of the typical cases is to use source images, noise vectors or both to generate simulated samples that are similar to the target samples and preserve the annotation information of the source domain,,.
• Non-Generative Models: rather than generating models with input image distributions, the feature extractor learns a discriminative representation using the labels in the source domain and maps the target data to the same space through a domain-confusion loss, thus resulting in the domain-invariant representations,,,,.
The third case can be referred to as a reconstruction-based
DA approach, which assumes that the data reconstruction of the source or target samples can be helpful for improving the performance of DA. The reconstructor can ensure both specificity of intra-domain representations and indistinguishability of inter-domain representations.
• Encoder-Decoder Reconstruction: by using stacked autoencoders (SAEs), encoder-decoder reconstruction methods combine the encoder network for representation learning with a decoder network for data reconstruction,,,.
• Adversarial Reconstruction: the reconstruction error is measured as the difference between the reconstructed and original images within each image domain by a cyclic mapping obtained via a GAN discriminator, such as dual
GAN, cycle GAN and disco GAN.
Manuscript accepted by Neurocomputing 2018
TABLE III
DIFFERENT APPROACHES USED IN DIFFERENT DOMAIN ADAPTATION SETTINGS
Supervised DA
Unsupervised DA
Discrepancy-based
Class Criterion
√
Statistic Criterion
√
Architecture Criterion
√
√
Geometric Criterion
√
Adversarial-based
Generative Model
√
Non-Generative Model
√
Reconstruction-based
Encoder-Decoder Model
√
Adversarial Model
√
B. Categorization of Multi-Step Domain Adaptation
In multi-step DA, we first determine the intermediate domains that are more related with the source and target domains than their direct connection. Second, the knowledge transfer process will be performed between the source, intermediate and target domains by one-step DA with less information loss. Thus, the key of multi-step DA is how to select and utilize intermediate domains; additionally, it can fall into three categories referring to : hand-crafted, feature-based and representation-based selection mechanisms.
• Hand-Crafted: users determine the intermediate domains based on experience.
• Instance-Based: selecting certain parts of data from the auxiliary datasets to compose the intermediate domains to train the deep network,.
• Representation-Based: transfer is enabled via freezing the previously trained network and using their intermediate representations as input to the new one.
IV. ONE-STEP DOMAIN ADAPTATION
As mentioned in Section II-A, the data in the target domain have three types regardless of homogeneous or heterogeneous
DA: 1) supervised DA with labeled data, 2) semi-supervised
DA with labeled and unlabeled data and 3) non-supervised
DA with unlabeled data. The second setting is able to be accomplished by combining the methods of setting 1 and setting 3; thus, we only focus on the first and third settings in this paper. The cases where the different approaches are mainly used for each DA setting are shown in Table III. As shown, more work is focused on unsupervised scenes because supervised DA has its limitations. When only few labeled data in the target domain are available, using the source and target labeled data to train parameters of models typically results in overfitting to the source distribution. In addition, the discrepancy-based approaches have been studied for years and produced more methods in many research works, whereas the adversarial-based and reconstruction-based approaches are a relatively new research topic but have recently been attracting more attention.
A. Homogeneous Domain Adaptation
1) Discrepancy-Based Approaches:
Yosinski et al. proved that transferable features learned by deep networks have limitations due to fragile co-adaptation and representation specificity and that fine-tuning can enhance generalization performance. Fine-tuning (can also be viewed as a discrepancybased deep DA approach) is to train a base network with source data and then directly reuse the first n layers to conduct a target network. The remaining layers of the target network are randomly initialized and trained with loss based on discrepancy. During training, the first n layers of the target network can be fine-tuned or frozen depending on the size of the target dataset and its similarity to the source dataset. Some common rules of thumb for navigating the 4 major scenarios are given in Table IV.
Fig. 4.
The average accuracy over the validation set for a network trained with different strategies. Baseline B: the network is trained on dataset B. 2)
BnB: the first n layers are reused from baseline B and frozen. The higher layers are trained on dataset B. 3) BnB+: the same as BnB but where all layers are fine-tuned. 4) AnB: the first n layers are reused from the network trained on dataset A and frozen. The higher layers are trained on dataset B.
5) AnB+: the same as AnB but where all layers are fine-tuned.
• Class Criterion
The class criterion is the most basic training loss in deep
DA. After pre-training the network with source data, the remaining layers of the target model use the class label information as a guide to train the network. Hence, a small number of labeled samples from the target dataset is assumed to be available.
Ideally, the class label information is given directly in supervised DA. Most work commonly uses the negative loglikelihood of the ground truth class with softmax as their training loss, L = − �N i=0 yi log ˆyi (ˆyi are the softmax predictions of the model, which represent class probabilities),,,. To extend this, Hinton et al. modified the softmax function to soft label loss: qi = exp(zi/T)
� j (exp(zj/T))
Manuscript accepted by Neurocomputing 2018
TABLE IV
SOME COMMON RULES OF THUMB FOR DECIDING FINE-TUNED OR FROZEN IN THE FIRST N LAYERS. 
The Size of Target Dataset
Low
Medium
High
The Distance
Low
Freeze
Try Freeze or Tune
Tune between Medium
Try Freeze or Tune
Tune
Tune
Source and Target
High
Try Freeze or Tune
Tune
Tune where zi is the logit output computed for each class. T is a temperature that is normally set to 1 in standard softmax, but it takes a higher value to produce a softer probability distribution over classes. By using it, much of the information about the learned function that resides in the ratios of very small probabilities can be obtained. For example, when recognizing digits, one version of 2 may obtain a probability of 106 of being a 3 and 109 of being a 7; in other words, this version of 2 looks more similar to 3 than 7. Inspired by Hinton, fine-tuned the network by simultaneously minimizing the domain confusion loss (belonging to adversarial-based approaches, which will be presented in Section IV-A2) and soft label loss. Using soft labels rather than hard labels can preserve the relationships between classes across domains. Gebru et al.
 modified existing adaptation algorithms based on and utilized soft label loss at the fine-grained class level Lcsoft and attribute level Lasoft.
Fig. 5.
Deep DA by combining domain confusion loss and soft label loss.
In addition to softmax loss, there are other methods that can be used as training loss to fine-tune the target model in supervised DA. Embedding metric learning in deep networks is another method that can make the distance of samples from different domains with the same labels be closer while those with different labels are far away. Based on this idea, constructed the semantic alignment loss and the separation loss accordingly. Deep transfer metric learning is proposed by, which applies the marginal Fisher analysis criterion and MMD criterion (described in Statistic Criterion) to minimize their distribution difference: min J = S(M) c
− αS(M) b
+ βD(M) ts
�
X s, X t�
+γ
M
� m=1
���W (m)���
F +
���b(m)���(2) where α, β and γ are regularization parameters and W (m) and b(m) are the weights and biases of the mth layer of the network. D(M) ts(X s, X t) is the MMD between representations of the source and target domains. Sc and Sb define the intraclass compactness and the interclass separability.
However, what can we do if there is no class label information in the target domain directly? As we all know, humans can identify unseen classes given only a high-level description. For instance, when provided the description "tall brown animals with long necks", we are able to recognize giraffes.
To imitate the ability of humans, introduced high-level semantic attributes per class. Assume that ac = (ac
1,..., ac m) is the attribute representation for class c, which has fixedlength binary values with m attributes in all the classes. The classifiers provide estimates of p(am|x) for each attribute am.
In the test stage, each target class y obtains its attribute vector ay in a deterministic way, i.e., p(a|y) = [[a = ay]]. By applying
Bayes rule, p(y|a) = p(y) p(ay)[[a = ay]], the posterior of a test class can be calculated as follows: p(y|x) =
� a∈{0,1}M p(y|a)p(a|x) = p(y) p(ay)
M
� m=1 p(ay m|x)
Gebru et al. drew inspiration from these works and leveraged attributes to improve performance in the DA of finegrained recognition. There are multiple independent softmax losses that simultaneously perform attribute and class level to fine-tune the target model. To prevent the independent classifiers from obtaining conflicting labels with attribute and class level, an attribute consistency loss is also implemented.
Occasionally, when fine-tuning the network in unsupervised
DA, a label of target data, which is called a pseudo label, can preliminarily be obtained based on the maximum posterior probability. Yan et al. initialized the target model using the source data and then defined the class posterior probability p(yt j = c|xt j) by the output of the target model.
With p(yt j = c|xt j), they assigned pseudo-label �yt j to xt j by
�yt j = arg max c p(yt j = c|xt j). In, two different networks assign pseudo-labels to unlabeled samples, another network is trained by the samples to obtain target discriminative representations. The deep transfer network (DTN) used some base classifiers, e.g., SVMs and MLPs, to obtain the pseudo
Manuscript accepted by Neurocomputing 2018
7 labels for the target samples to estimate the conditional distribution of the target samples and match both the marginal and the conditional distributions with the MMD criterion. When casting the classifier adaptation into the residual learning framework, used the pseudo label to build the conditional entropy E(Dt, f t), which ensures that the target classifier f t fits the target-specific structures well.
• Statistic Criterion
Although some discrepancy-based approaches search for pseudo labels, attribute labels or other substitutes to labeled target data, more work focuses on learning domain-invariant representations via minimizing the domain distribution discrepancy in unsupervised DA.
MMD is an effective metric for comparing the distributions between two datasets by a kernel two-sample test. Given two distributions s and t, the MMD is defined as follows:
MMD2(s, t) = sup
∥φ∥H≤1
��Exs∼s[φ(xs)] − Ext∼s[φ(xt)]
��2
H(4) where φ represents the kernel function that maps the original data to a reproducing kernel Hilbert space (RKHS) and ∥φ∥H ≤ 1 defines a set of functions in the unit ball of RKHS
H.
Based on the above, Ghifary et al. proposed a model that introduced the MMD metric in feedforward neural networks with a single hidden layer. The MMD metric is computed between representations of each domain to reduce the distribution mismatch in the latent space. The empirical estimate of MMD is as follows:
MMD2(Ds, Dt) =
������
M
M
� i=1 φ(xs i)− 1
N
N
� j=1 φ(xt j)
������
H
Subsequently, Tzeng et al. and Long et al. extended MMD to a deep CNN model and achieved great success. The deep domain confusion network (DDC) by Tzeng et al. used two CNNs for the source and target domains with shared weights. The network is optimized for classification loss in the source domain, while domain difference is measured by an adaptation layer with the MMD metric.
L=LC(XL, y) + λMMD2(XsXt)(6) where the hyperparameter λ is a penalty parameter.
LC(XL, y) denotes classification loss on the available labeled data, XL, and the ground-truth labels, y. MMD2(XsXt) denotes the distance between the source and target data. DDC only adapts one layer of the network, resulting in a reduction in the transferability of multiple layers. Rather than using a single layer and linear MMD, Long et al. proposed the deep adaptation network (DAN) that matches the shift in marginal distributions across domains by adding multiple adaptation layers and exploring multiple kernels, assuming that the conditional distributions remain unchanged. However, this assumption is rather strong in practical applications; in other words, the source classifier cannot be directly used in the target domain. To make it more generalized, a joint adaptation network (JAN) aligns the shift in the joint distributions of input features and output labels in multiple domain-specific layers based on a joint maximum mean discrepancy (JMMD) criterion. proposed DTN, where both the marginal and the conditional distributions are matched based on MMD. The shared feature extraction layer learns a subspace to match the marginal distributions of the source and the target samples, and the discrimination layer matches the conditional distributions by classifier transduction. In addition to adapting features using MMD, residual transfer networks (RTNs) added a gated residual layer for classifier adaptation. More recently, proposed a weighted MMD model that introduces an auxiliary weight for each class in the source domain when the class weights in the target domain are not the same as those in the source domain.
If φ is a characteristic kernel (i.e., Gaussian kernel or Laplace kernel), MMD will compare all the orders of statistic moments. In contrast to MMD, CORAL learned a linear transformation that aligns the second-order statistics between domains. Sun et al. extended CORAL to deep neural networks (deep CORAL) with a nonlinear transformation.
LCORAL= 1
4d2 ∥CS − CT ∥2
F(7) where ∥ · ∥2
F denotes the squared matrix Frobenius norm. CS and CT denote the covariance matrices of the source and target data, respectively.
By the Taylor expansion of the Gaussian kernel, MMD can be viewed as minimizing the distance between the weighted sums of all raw moments. The interpretation of MMD as moment matching procedures motivated Zellinger et al.
 to match the higher-order moments of the domain distributions, which we call central moment discrepancy (CMD).
An empirical estimate of the CMD metric for the domain discrepancy in the activation space [a, b]N is given by
CMDK(Xs, Xt) =(b − a)
��E(Xs) − E(Xt)
��
K
� k=2
|b − a|k
��Ck(Xs) − Ck(Xt)
��(8) where Ck(X) = E((x − E(X))k is the vector of all kthorder sample central moments and E(X) =
|X|
� x∈X x is the empirical expectation.
The association loss Lassoc proposed by is an alternative discrepancy measure, it enforces statistical associations between source and target data by making the two-step roundtrip probabilities P aba ij be similar to the uniform distribution over the class labels.
• Architecture Criterion
Some other methods optimize the architecture of the network to minimize the distribution discrepancy. This adaptation behavior can be achieved in most deep DA models, such as supervised and unsupervised settings.
Rozantsev et al. considered that the weights in corresponding layers are not shared but related by a weight regularizer rw(·) to account for the differences between the Manuscript accepted by Neurocomputing 2018(c) The Residual Transfer Network (RTN) architecture(a) The Deep Adaptation Network (DAN) architecture(b) The Joint Adaptation Network (JAN) architecture
Fig. 6.
Different approaches with the MMD metric. (a) The deep adaptation network (DAN) architecture, (b) the joint adaptation network (JAN) architecture and (c) the residual transfer network (RTN) architecture. two domains. The weight regularizer rw(·) can be expressed as the exponential loss function: rw(θs j, θt j) = exp
���θs j − θt j
��2�
− 1(9) where θs j and θt j denote the parameters of the jth layer of the source and target models, respectively. To further relax this restriction, they allow the weights in one stream to undergo a linear transformation: rw(θs j, θt j) = exp(
��ajθs j + bj − θt j
��2) − 1(10) where aj and bj are scalar parameters that encode the linear transformation. The work of Shu et al. is similar to
 using weakly parameter-shared layers. The penalty term
Ω controls the relatedness of parameters.
Ω=
L
� i=1
���W (l)
S
− W (l)
T
���
F +
���b(l)
S − b(l)
T
���
F )(11) where {W (l)
S, b(l)
S }L l=1 and {W (l)
T, b(l)
T }L l=1 are the parameters of the lth layer in the source and target domains, respectively.
Fig. 7. The two-stream architecture with related weight.
Li et al. hypothesized that the class-related knowledge is stored in the weight matrix, whereas domain-related knowledge is represented by the statistics of the batch normalization (BN) layer. BN normalizes the mean and standard deviation for each individual feature channel such that each layer receives data from a similar distribution, irrespective of whether it comes from the source or the target domain.
Therefore, Li et al. used BN to align the distribution for recomputing the mean and standard deviation in the target domain.
BN(Xt) = λ
�x − µ(Xt) σ(Xt)
�
+ β(12) where λ and β are parameters learned from the target data and µ(x) and σ(x) are the mean and standard deviation computed independently for each feature channel. Based on, endowed BN layers with a set of alignment parameters which can be learned automatically and can decide the degree of feature alignment required at different levels of the deep network. Furthermore, Ulyanov et al. found that when replacing BN layers with instance normalization (IN) layers, where µ(x) and σ(x) are computed independently for each channel and each sample, the performance of DA can be further improved.
Occasionally, neurons are not effective for all domains because of the presence of domain biases. For example, when recognizing people, the target domain typically contains one person centered with minimal background clutter, whereas the source dataset contains many people with more clutter. Thus, the neurons that capture the features of other people and clutter are useless. Domain-guided dropout was proposed by to solve the problem of multi-DA, and it mutes non-related neurons for each domain. Rather than assigning dropout with a specific dropout rate, it depends on the gain of the loss function of each neuron on the domain sample when the neuron is removed. si = L(g(x)\i) − L(g(x))(13) where L is the softmax loss function and g(x)\i is the feature vector after setting the response of the ith neuron to zero. In, each source domain is assigned with different parameters, Θ(i) = Θ(0) + ∆(i), where Θ(0) is a domain general model, and ∆(i) is a domain specific bias term. After the low rank parameterized CNNs are trained, Θ(0) can serve as the classifier for target domain.
• Geometric Criterion
The geometric criterion mitigates the domain shift by integrating intermediate subspaces on a geodesic path from the source to the target domains. A geodesic flow curve is constructed to connect the source and target domains on the Grassmannian. The source and target subspaces are points on a Grassmann manifold. By sampling a fixed or infinite
 number of subspaces along the geodesic, we can form the Manuscript accepted by Neurocomputing 2018
9 intermediate subspaces to help to find the correlations between domains. Then, both source and target data are projected to the obtained intermediate subspaces to align the distribution.
Inspired by the intermediate representations on the geodesic path, Chopra et al. proposed a model called deep learning for DA by interpolating between domains (DLID). DLID generates intermediate datasets, starting with all the source data samples and gradually replacing source data with target data. Each dataset is a single point on an interpolating path between the source and target domains. Once intermediate datasets are generated, a deep nonlinear feature extractor using the predictive sparse decomposition is trained in an unsupervised manner.
2) Adversarial-Based Approaches: Recently, great success has been achieved by the GAN method, which estimates generative models via an adversarial process. GAN consists of two models: a generative model G that extracts the data distribution and a discriminative model D that distinguishes whether a sample is from G or training datasets by predicting a binary label. The networks are trained on the label prediction loss in a mini-max fashion: simultaneously optimizing G to minimize the loss while also training D to maximize the probability of assigning the correct label: min
G max
D V (D, G) = Ex∼pdata(x)[log D(x)]
+Ez∼pz(z)[log(1 − D(G(z)))]
In DA, this principle has been employed to ensure that the network cannot distinguish between the source and target domains. proposed a unified framework for adversarialbased approaches and summarized the existing approaches according to whether to use a generator, which loss function to employ, or whether to share weights across domains. In this paper, we only categorize the adversarial-based approaches into two subsettings: generative models and non-generative models.
Fig. 8. Generalized architecture for adversarial domain adaptation. Existing adversarial adaptation methods can be viewed as instantiations of a framework with different choices regarding their properties. 
• Generative Models
Synthetic target data with ground-truth annotations are an appealing alternative to address the problem of a lack of training data. First, with the help of source data, generators render unlimited quantities of synthetic target data, which are paired with synthetic source data to share labels or appear as if they were sampled from the target domain while maintaining labels, or something else. Then, synthetic data with labels are used to train the target model as if no DA were required.
Adversarial-based approaches with generative models are able to learn such a transformation in an unsupervised manner based on GAN.
The core idea of CoGAN is to generate synthetic target data that are paired with synthetic source ones. It consists of a pair of GANs: GAN1 for generating source data and GAN2 for generating target data. The weights of the first few layers in the generative models and the last few layers in the discriminative models are tied. This weight-sharing constraint allows CoGAN to achieve a domain-invariant feature space without correspondence supervision. A trained CoGAN can adapt the input noise vector to paired images that are from the two distributions and share the labels. Therefore, the shared labels of synthetic target samples can be used to train the target model.
Fig. 9. The CoGAN architecture. 
More work focuses on generating synthetic data that are similar to the target data while maintaining annotations. Yoo et al. transferred knowledge from the source domain to pixel-level target images with GANs. A domain discriminator ensures the invariance of content to the source domain, and a real/fake discriminator supervises the generator to produce similar images to the target domain. Shrivastava et al. developed a method for simulated+unsupervised (S+U) learning that uses a combined objective of minimizing an adversarial loss and a self-regularization loss, where the goal is to improve the realism of synthetic images using unlabeled real data. In contrast to other works in which the generator is conditioned only on a noise vector or source images, Bousmalis et al. proposed a model that exploits GANs conditioned on both.
The classifier T is trained to predict class labels of both source and synthetic images, while the discriminator is trained to predict the domain labels of target and synthetic images. In addition, to expect synthetic images with similar foregrounds and different backgrounds from the same source images, a content similarity is used that penalizes large differences between source and synthetic images for foreground pixels only by a masked pairwise mean squared error. The goal of the network is to learn G, D and T by solving the optimization problem: min
G,T max
D V (D, G) = αLd(D, G)
+βLt(T, G) + γLc(G)(15) where α, β, and γ are parameters that control the trade-off between the losses. Ld, Lt and Lc are the adversarial loss, softmax loss and content-similarity loss, respectively.
• Non-Generative Models
Manuscript accepted by Neurocomputing 2018
Fig. 10.
The model that exploits GANs conditioned on noise vector and source images. 
The key of deep DA is learning domain-invariant representations from source and target samples. With these representations, the distribution of both domains can be similar enough such that the classifier is fooled and can be directly used in the target domain even if it is trained on source samples.
Therefore, whether the representations are domain-confused or not is crucial to transferring knowledge. Inspired by GAN, domain confusion loss, which is produced by the discriminator, is introduced to improve the performance of deep DA without generators.
Fig. 11. The domain-adversarial neural network (DANN) architecture. 
The domain-adversarial neural network (DANN) integrates a gradient reversal layer (GRL) into the standard architecture to ensure that the feature distributions over the two domains are made similar. The network consists of shared feature extraction layers and two classifiers. DANN minimizes the domain confusion loss (for all samples) and label prediction loss (for source samples) while maximizing domain confusion loss via the use of the GRL. In contrast to the above methods, the adversarial discriminative domain adaptation (ADDA) considers independent source and target mappings by untying the weights, and the parameters of the target model are initialized by the pre-trained source one. This is more flexible because of allowing more domainspecific feature extractions to be learned. ADDA minimizes the source and target representation distances through iteratively minimizing these following functions, which is most similar to the original GAN: min
M s,CLcls(Xs, Y s) =
− E(xs,ys)∼(Xs,Y s)
K
� k=1
1[k=ys] log C(M s(xs)) min
D LadvD(Xs,Xt, M s, M t) =
− E(xs)∼(Xs)[log D(M s(xs))]
− E(xt)∼(Xt)[log(1 − D(M t(xt)))] min
M s,M t LadvM(M s, M t) =
− E(xt)∼(Xt)[log D(M t(xt))](16) where the mappings M s and M t are learned from the source and target data, Xs and Xt. C represents a classifier working on the source domain. The first classification loss function Lcls is optimized by training the source model using the labeled source data. The second function LadvD is minimized to train the discriminator, while the third function LadvM is learning a representation that is domain invariant.
Fig. 12.
The Adversarial discriminative domain adaptation (ADDA) architecture. 
Tzeng et al. proposed adding an additional domain classification layer that performs binary domain classification and designed a domain confusion loss to encourage its prediction to be as close as possible to a uniform distribution over binary labels. Unlike previous methods that match the entire source and target domains, Cao et al. introduced a selective adversarial network (SAN) to address partial transfer learning from large domains to small domains, which assumes that the target label space is a subspace of the source label space. It simultaneously avoids negative transfer by filtering out outlier source classes, and it promotes positive transfer by matching the data distributions in the shared label space via splitting the domain discriminator into many class-wise domain discriminators. encoded domain labels and class labels to produce four groups of pairs, and replaced the typical binary adversarial discriminator by a four-class discriminator.
Volpi et al. trained a feature generator (S) to perform data augmentation in the source feature space and obtained a domain invariant feature through playing a minimax game against features from S.
Rather than using discriminator to classify domain label, some papers make some other explorations. Inspired by
Wasserstein GAN, Shen et al. utilized discriminator to estimate empirical Wasserstein distance between the source and target samples and optimized the feature extractor network to minimize the distance in an adversarial manner. In, two classifiers are treated as discriminators and are trained to maximize the discrepancy to detect target samples outside the support of the source, while a feature extractor is trained to minimize the discrepancy by generating target features near the support.
3) Reconstruction-Based Approaches: In DA, the data reconstruction of source or target samples is an auxiliary task that simultaneously focuses on creating a shared representation
Manuscript accepted by Neurocomputing 2018
11 between the two domains and keeping the individual characteristics of each domain.
• Encoder-Decoder Reconstruction
The basic autoencoder framework is a feedforward neural network that includes the encoding and decoding processes. The autoencoder first encodes an input to some hidden representation, and then it decodes this hidden representation back to a reconstructed version. The DA approaches based on encoder-decoder reconstruction typically learn the domaininvariant representation by a shared encoder and maintain the domain-special representation by a reconstruction loss in the source and target domains.
Xavier and Bengio proposed extracting a high-level representation based on stacked denoising autoencoders (SDA). By reconstructing the union of data from various domains with the same network, the high-level representations can represent both the source and target domain data. Thus, a linear classifier that is trained on the labeled data of the source domain can make predictions on the target domain data with these representations. Despite their remarkable results, SDAs are limited by their high computational cost and lack of scalability to high-dimensional features. To address these crucial limitations, Chen et al. proposed the marginalized
SDA (mSDA), which marginalizes noise with linear denoisers; thus, parameters can be computed in closed-form and do not require stochastic gradient descent.
The deep reconstruction classification network (DRCN) proposed in learns a shared encoding representation that provides useful information for cross-domain object recognition. DRCN is a CNN architecture that combines two pipelines with a shared encoder. After a representation is provided by the encoder, the first pipeline, which is a CNN, works for supervised classification with source labels, whereas the second pipeline, which is a deconvolutional network, optimizes for unsupervised reconstruction with target data. min λLc({θenc, θlab}) + (1 − λ)Lr({θenc, θdec})(17) where λ is a hyper-parameter that controls the trade-off between classification and reconstruction. θenc, θdec and θlab denote the parameters of the encoder, decoder and source classifier, respectively. Lc is cross-entropy loss for classification, and Lr is squared loss ∥ x − fr(x) ∥2
2 for reconstruction in which fr(x) is the reconstruction of x.
Fig. 13. The deep reconstruction classification network (DRCN) architecture.
 
Domain separation networks (DSNs) explicitly and jointly model both private and shared components of the domain representations. A shared-weight encoder learns to capture shared representations, while a private encoder is used for domain-specific components in each domain. Additionally, a shared decoder learns to reconstruct the input samples by both the private and shared representations. Then, a classifier is trained on the shared representation. By partitioning the space in such a manner, the shared representations will not be influenced by domain-specific representations such that a better transfer ability can be obtained. Finding that the separation loss is simple and that the private features are only used for reconstruction in DSNs, reinforced them by incorporating a hybrid adversarial learning in a separation network and an adaptation network.
Zhuang et al. proposed transfer learning with deep autoencoders (TLDA), which consists of two encoding layers.
The distance in distributions between domains is minimized with KL divergence in the embedding encoding layer, and label information of the source domain is encoded using a softmax loss in the label encoding layer. Ghifary et al. extended the autoencoder into a model that jointly learns two types of data-reconstruction tasks taken from related domains: one is self-domain reconstruction, and the other is betweendomain reconstruction.
• Adversarial Reconstruction
Dual learning was first proposed by Xia et al. to reduce the requirement of labeled data in natural language processing.
Dual learning trains two "opposite" language translators, e.g., A-to-B and B-to-A. The two translators represent a primaldual pair that evaluates how likely the translated sentences belong to the targeted language, and the closed loop measures the disparity between the reconstructed and the original ones.
Inspired by dual learning, adversarial reconstruction is adopted in deep DA with the help of dual GANs.
Zhu et al. proposed a cycle GAN that can translate the characteristics of one image domain into the other in the absence of any paired training examples. Compared to dual learning, cycle GAN uses two generators rather than translators, which learn a mapping G : X → Y and an inverse mapping F : Y → X. Two discriminators, DX and DY, measure how realistic the generated image is (G(X) ≈ Y or G(Y ) ≈ X) by an adversarial loss and how well the original input is reconstructed after a sequence of two generations(F(G(X)) ≈ X or G(F(Y )) ≈ Y ) by a cycle consistency loss(reconstruction loss). Thus, the distribution of images from
G(X) (or F(Y )) is indistinguishable from the distribution Y(or X).
LGAN(G, DY, X, Y ) = Ey∼pdata(y)[log DY (y)]
+Ex∼pdata(x)[log(1 − DY (G(x)))]
Lcyc(G, F) = Ex∼data(x)[∥F(G(x)) − x∥1]
+Ey∼data(y)[∥G(F(y)) − y∥1](18) where LGAN is the adversarial loss produced by discriminator
DY with mapping function G : X
→ Y. Lcyc is the reconstruction loss using L1 norm.
The dual GAN and the disco GAN were proposed at the same time, where the core idea is similar to cycle
GAN. In dual GAN, the generator is configured with skip connections between mirrored downsampling and upsampling
Manuscript accepted by Neurocomputing 2018
Fig. 14. The cycle GAN architecture. layers,, making it a U-shaped net to share low-level information (e.g., object shapes, textures, clutter, and so forth).
For discriminators, the Markovian patch-GAN architecture is employed to capture local high-frequency information.
In disco GAN, various forms of distance functions, such as mean-square error (MSE), cosine distance, and hinge loss, can be used as the reconstruction loss, and the network is applied to translate images, changing specified attributes including hair color, gender and orientation while maintaining all other components.
4) Hybrid Approaches: To obtain better performance, some of the aforementioned methods have been used simultaneously.
 combined a domain confusion loss and a soft label loss, while used both statistic (MMD) and architecture criteria (adapt classifier by residual function) for unsupervised
DA. introduced class-specific auxiliary weights assigned by the pseudo-labels into the original MMD. In DSNs, encoder-decoder reconstruction approaches separate representations into private and shared representations, while the MMD criterion or domain confusion loss is helpful to make the shared representations similar and soft subspace orthogonality constraints ensure dissimilarity between the private and shared representations. used the MMD between the learned source and target representations and also allowed the weights of the corresponding layers to differ. learned domaininvariant representations by encoder-decoder reconstruction approaches and the KL divergence.
B. Heterogeneous Domain Adaptation
In heterogeneous DA, the feature spaces of the source and target domains are not the same, Xs ̸= Xt, and the dimensions of the feature spaces may also differ. According to the divergence of feature spaces, heterogeneous DA can be further divided into two scenarios. In one scenario, the source and target domain both contain images, and the divergence of feature spaces is mainly caused by different sensory devices(e.g., visual light (VIS) vs. near-infrared (NIR) or RGB vs. depth) and different styles of images (e.g., sketches vs. photos). In the other scenario, there are different types of media in source and target domain (e.g., text vs. image and language vs. image). Obviously, the cross-domain gap of the second scenario is much larger.
Most heterogeneous DA with shallow methods fall into two categories: symmetric transformation and asymmetric transformation. The symmetric transformation learns feature transformations to project the source and target features onto a common subspace. Heterogeneous feature augmentation(HFA) first transformed the source and target data into a common subspace using projection matrices P and Q respectively, then proposed two new feature mapping functions, ϕs (xs) = [Pxs, xs, 0dt]T and ϕt (xt) = [Qxt, 0ds, xt]T, to augment the transformed data with their original features and zeros. These projection matrices are found using standard
SVM with hinge loss in both the linear and nonlinear cases and an alternating optimization algorithm is proposed to simultaneously solve the dual SVM and to find the optimal transformations. treated each input domain as a manifold which is represented by a Laplacian matrix, and used labels rather than correspondences to align the manifolds.
The asymmetric transformation transforms one of source and target features to align with the other. proposed a sparse and class-invariant feature transformation matrix to map the weight vector of classifiers learned from the source domain to the target domain. The asymmetric regularized cross-domain transfer (ARC-t) used asymmetric, nonlinear transformations learned in Gaussian RBF kernel space to map the target data to the source domain. Extended from, ARC-t performed asymmetric transformation based on metric learning, and transfer knowledge between domains with different dimensions through changes of the regularizer. Since we focus on deep DA, we refer the interested readers to, which summarizes shallow approaches of heterogeneous DA.
However, as for deep methods, there is not much work focused on heterogeneous DA so far. The special and effective methods of heterogeneous deep DA have not been proposed, and heterogeneous deep DA is still performed similar to some approaches of homogeneous DA.
1) Discrepancy-Based Approach: In discrepancy-based approaches, the network generally shares or reuses the first n layers between the source and target domains, which limits the feature spaces of the input to the same dimension. However, in heterogeneous DA, the dimensions of the feature spaces of source domain may differ from those of target domain.
In first scenario of heterogeneous DA, the images in different domains can be directly resized into the same dimensions, so the Class Criterion and Statistic Criterion are still effective and are mainly used. For example, given an RGB image and its paired depth image, used the mid-level representation learned by CNNs as a supervisory signal to re-train a CNN on depth images. To transform an RGB object detector into a RGB-D detector without needing complete RGB-D data, Hoffman et al. first trained an RGB network using labeled
RGB data from all categories and finetuned the network with labeled depth data from partial categories, then combined midlevel RGB and depth representations at fc6 to incorporate both modalities into the final object class prediction. first trained the network using large face database of photos and then finetuned it using small database of composite sketches;
 transferred the VIS deep networks to the NIR domain in the same way.
In second scenario, the features of different media can not be directly resized into the same dimensions. Therefore, discrepancy-based methods fail to work without extra process.
 proposed weakly shared DTNs to transfer labeled information across heterogeneous domains, particularly from the text domain to the image domain. DTNs take paired data, such as text and image, as input to two SAEs, followed by weakly
Manuscript accepted by Neurocomputing 2018
13 parameter-shared network layers at the top. Chen et al. proposed transfer neural trees (TNTs), which consist of two stream networks to learn a domain-invariant feature representation for each modality. Then, a transfer neural decision forest(Transfer-NDF), is used with stochastic pruning for adapting representative neurons in the prediction layer.
2) Adversarial-Based Approach: Using Generative Models can generate the heterogeneous target data while transferring some information of source domain to them. employed a compound loss function that consists of a multiclass GAN loss, a regularizing component and an f-constancy component to transfer unlabeled face photos to emoji images. To generate images for birds and flowers based on text, trained a GAN conditioned on text features encoded by a hybrid characterlevel convolutional-recurrent neural network. proposed stacked generative adversarial networks (StackGAN) with conditioning augmentation for synthesizing photo-realistic images from text. It decomposes the synthesis problem into several sketch-refinement processes. Stage-I GAN sketches the primitive shape and basic colors of the object to yield low-resolution image, and Stage-II GAN completes details of the object to produce a high-resolution photo-realistic image.
Fig. 15. The StackGAN architecture. 
3) Reconstruction-Based Approach: The Adversarial Reconstruction can be used in heterogeneous DA as well. For example, the cycle GAN, dual GAN and disco
GAN used two generators, GA and GB, to generate sketches from photos and photos from sketches, respectively.
Based on cycle GAN, proposed a multi-adversarial network to avoid artifacts of facial photo-sketch synthesis by leveraging the implicit presence of feature maps of different resolutions in the generator subnetwork.
V. MULTI-STEP DOMAIN ADAPTATION
For multi-step DA, the selection of the intermediate domain is problem specific, and different problems may have different strategies.
A. Hand-Crafted Approaches
Occasionally, the intermediate domain can be selected by experience, that is, it is decided in advance. For example, when the source domain is image data and the target domain is composed of text data, some annotated images will clearly be crawled as intermediate domain data.
With the common sense that nighttime light intensities can be used as a proxy for economic activity, Xie et al.
 transferred knowledge from daytime satellite imagery to poverty prediction with the help of some nighttime light intensity information as an intermediate domain.
B. Instance-Based Approaches
In other problems where there are many candidate intermediate domains, some automatic selection criterion should be considered. Similar to the instance-transfer approaches proposed by Pan, because the samples of the source domain cannot be used directly, the mixture of certain parts of the source and target data can be useful for constructing the intermediate domain.
Tan et al. proposed distant domain transfer learning(DDTL), where long-distance domains fail to transfer knowledge by only one intermediate domain but can be related via multiple intermediate domains. DDTL gradually selects unlabeled data from the intermediate domains by minimizing reconstruction errors on the selected instances in the source and intermediate domains and all the instances in the target domain simultaneously. With removal of the unrelated source data, the selected intermediate domains gradually become closer to the target domain from the source domain:
J1(fe, fd, vS, vT ) = 1 nS nS
� i=1 vi
S
��ˆxi
S − xi
S
��2
+ 1 nI nI
� i=1 vi
I
��ˆxi
I − xi
I
��2
+ 1 nT nT
� i=1
��ˆxi
T − xi
T
��2
2 + R(vS, vT )(19) where ˆxi
S, ˆxi
T and ˆxi
I are reconstructions of source data
Si, target data T i and intermediate data Ii based on the autoencoder, respectively, and fe and fd are the parameters of the encoder and decoder, respectively. vS = (v1
S,..., vnS
S )
⊤ and vI = (v1
I,..., vnI
I )
⊤, vi
S, vi
I ∈ 0, 1 are selection indicators for the ith source and intermediate instance, respectively.
R(vS, vT ) is a regularization term that avoids all values of vS and vI being zero.
The DLID model mentioned in Section IV-A1 (Geometric Criterion) constructs the intermediate domains with a subset of the source and target domains, where source samples are gradually replaced by target samples.
C. Representation-Based Approaches
Representation-based approaches freeze the previously trained network and use their intermediate representations as input to the new network. Rusu et al. introduced progressive networks that have the ability to accumulate and transfer knowledge to new domains over a sequence of experiences. To avoid the target model losing its ability to solve the source domain, they constructed a new neural network for each domain, while transfer is enabled via lateral connections to features of previously learned networks. In the process, the parameters in the latest network are frozen to remember knowledge of intermediate domains.
Manuscript accepted by Neurocomputing 2018
Fig. 16. The progressive network architecture. 
VI. APPLICATION OF DEEP DOMAIN ADAPTATION
Deep DA techniques have recently been successfully applied in many real-world applications, including image classification, object recognition, face recognition, object detection, style translation, and so forth. In this section, we present different application examples using various visual deep DA methods. Because the information of commonly used datasets for evaluating the performance is provided in in detail, we do not introduce it in this paper.
A. Image Classification
Because image classification is a basic task of computer vision applications, most of the algorithms mentioned above were originally proposed to solve such problems. Therefore, we do not discuss this application repeatedly, but we show how much benefit deep DA methods for image classification can bring. Because different papers often use different parameters, experimental protocols and tuning strategies in the preprocessing steps, it is quite difficult to perform a fair comparison among all the methods directly. Thus, similar to the work of Pan, we show the comparison results between the proposed deep DA methods and non-adaptation methods using only deep networks. A list of simple experiments taken from some published deep DA papers are presented in Table V.
In,, and, the authors used the Office-31 dataset1 as one of the evaluation data sets, as shown in Fig.
1(a). The Office dataset is a computer vision classification data set with images from three distinct domains: Amazon (A), DSLR (D), and Webcam (W). The largest domain, Amazon, has 2817 labeled images and its corresponding 31 classes, which consists of objects commonly encountered in office settings. By using this dataset, previous works can show the performance of methods across all six possible DA tasks. showed comparison experiments among the standard AlexNet, the DANN method, and the MMD algorithm and its variations, such as DDC, DAN, JAN and RTN. Zellinger et al. evaluated their proposed CMD algorithm in comparison to other discrepancy-based methods(DDC, deep CROAL, DLID, AdaBN ) and the adversarial-based method DANN. proposed an algorithm combining soft label loss and domain confusion loss, and they
1 https://cs.stanford.edu/∼jhoffman/domainadapt/ also compared them with DANN and DLID under a supervised
DA setting.
In, MNIST2(M), USPS3(U), and SVHN4 (S) digit datasets (shown in Fig. 1(b)) are used for a cross-domain handwritten digit recognition task, and the experiment showed the comparison results on some adversarial-based methods, such as DANN, CoGAN and ADDA, where the baseline is VGG-16.
B. Face Recognition
The performance of face recognition significantly degrades when there are variations in the test images that are not present in the training images. The dataset shift can be caused by poses, resolution, illuminations, expressions, and modality.
Kan et al. proposed a bi-shifting auto-encoder network(BAE) for face recognition across view angle, ethnicity, and imaging sensor. In BAE, source domain samples are shifted to the target domain, and sparse reconstruction is used with several local neighbors from the target domain to ensure its correction, and vice versa. Single sample per person domain adaptation network (SSPP-DAN) in generates synthetic images with varying poses to increase the number of samples in the source domain and bridges the gap between the synthetic and source domains by adversarial training with a GRL in realworld face recognition. improved the performance of video face recognition by using an adversarial-based approach with large-scale unlabeled videos, labeled still images and synthesized images. Considering that age variations are difficult problems for smile detection and that networks trained on the current benchmarks do not perform well on young children, Xia et al. applied DAN and JAN (mentioned in Section IV-A1) to two baseline deep models, i.e., AlexNet and ResNet, to transfer the knowledge from adults to infants.
Fig. 17.
The single sample per person domain adaptation network (SSPPDAN) architecture. 
C. Object Detection
Recent advances in object detection are driven by regionbased convolutional neural networks (R-CNNs, fast RCNNs and faster R-CNNs ). They are composed of a window selection mechanism and classifiers that are pre-trained labeled bounding boxes by using the features extracted from CNNs. At test time, the classifier decides whether a region obtained by sliding windows contains the object. Although the R-CNN algorithm is effective, a large
2 http://yann.lecun.com/exdb/mnist/
3http://statweb.stanford.edu/∼tibs/ElemStatLearn/data.html
4http://ufldl.stanford.edu/housenumbers/
Manuscript accepted by Neurocomputing 2018
TABLE V
COMPARISON BETWEEN TRANSFER LEARNING AND NON-ADAPTATION LEARNING METHODS
Data Set(reference)
Source vs. Target
Baselines
Deep Domain Adaptation Methods
AlexNet
DDC
DAN
RTN
JAN
DANN
A vs. W
61.6±0.5
61.8±0.4
73.3±0.3
75.2±0.4
73.0±0.5
D vs. W
95.4±0.3
95.0±0.5
96.0±0.3
96.8±0.2
96.6±0.2
96.4±0.3
Office-31 Dataset
W vs. D
99.0±0.2
98.5±0.4
99.0±0.3
99.6±0.1
99.6±0.1
99.2±0.3
ACC (unit:%) 
A vs. D
63.8±0.5
64.4±0.3
67.0±0.4
71.0±0.2
72.8±0.3
72.3±0.3
D vs. A
51.1±0.6
52.1±0.6
54.0±0.5
50.5±0.3
57.5±0.2
53.4±0.4
W vs. A
49.8±0.4
52.2±0.4
53.1±0.5
51.0±0.1
56.3±0.2
51.2±0.5
Avg
AlexNet
Deep CORAL
CMD
DLID
AdaBN
DANN
A vs. W
77.0±0.6
D vs. W
96.3±0.4
Office-31 Dataset
W vs. D
99.2±0.2
ACC (unit:%) 
A vs. D
79.6±0.6D vs. A
63.8±0.7W vs. A
63.3±0.6AvgAlexNet
DLID
DANN
Soft Labels
Domain
Confusion
Confusion
+Soft
A vs. W
56.5±0.3
53.6±0.2
82.7±0.7
82.8±0.9
82.7±0.8
D vs. W
92.4±0.3
71.2±0.0
95.9±0.6
95.6±0.4
95.7±0.5
Office-31 Dataset
W vs. D
93.6±0.2
83.5±0.0
98.3±0.3
97.5±0.2
97.6±0.2
ACC (unit:%) 
A vs. D
64.6±0.484.9±1.2
85.9±1.1
86.1±1.2
D vs. A
47.6±0.166.0±0.5
66.2±0.4
66.2±0.3
W vs. A
42.7±0.165.2±0.6
64.9±0.5
65.0±0.5
AvgMNIST, USPS, VGG-16
DANN
CoGAN
ADDA and SVHN
M vs. U
75.2±1.6
77.1±1.8
91.2±0.8
89.4±0.2 digits datasets
U vs. M
57.1±1.7
73.0±2.0
89.1±0.8
90.1±0.8
ACC (unit:%) 
S vs. M
60.1±1.176.0±1.8 amount of bounding box labeled data is required to train each detection category. To solve the problem of lacking labeled data, considering the window selection mechanism as being domain independent, deep DA methods can be used in classifiers to adapt to the target domain.
Because R-CNNs train classifiers on regions just like classification, weak labeled data (such as image-level class labels) are directly useful for the detector. Most works learn the detector with limited bounding box labeled data and massive weak labeled data. The large-scale detection through adaptation(LSDA) trains a classification layer for the target domain and then uses a pre-trained source model along with output layer adaptation techniques to update the target classification parameters directly. Rochan et al. used word vectors to establish the semantic relatedness between weak labeled source objects and target objects and then transferred the bounding box labeled information from source objects to target objects based on their relatedness. Extending and, Tang et al. transferred visual (based on the LSDA model) and semantic similarity (based on work vectors) for training an object detector on weak labeled category. incorporated both an image-level and an instance-level adaptation component into faster R-CNN and minimized the domain discrepancy based on adversarial training. By using bounding box labeled data in a source domain and weak labeled data in a target domain, progressively fine-tuned the pre-trained model with domain-transfer samples and pseudo-labeling samples.
D. Semantic Segmentation
Fully convolutional network models (FCNs) for dense prediction have proven to be successful for evaluating semantic segmentation, but their performance will also degrade under domain shifts. Therefore, some work has also explored using weak labels to improve the performance of semantic segmentation. Hong et al. used a novel encoder-decoder architecture with attention model by transferring weak class labeled knowledge in the source domain, while, transferred weak object location knowledge.
Much attention has also been paid to deep unsupervised DA in semantic segmentation. Hoffman et al. first introduced it, in which global domain alignment is performed using FCNs with adversarial-based training, while transferring spatial layout is achieved by leveraging class-aware constrained multiple instance loss. Zhang et al. enhanced the segmentation performance on real images with the help of virtual ones. It uses the global label distribution loss of the images and local label distribution loss of the landmark superpixels in the target domain to effectively regularize the fine-tuning of the semantic segmentation network. Chen et al. proposed a framework for cross-city semantic segmentation. The framework assigns pseudo labels to pixels/grids in the target domain and jointly utilizes global and class-wise alignment by domain adversarial learning to minimize domain shift. In, a target guided distillation module adapts the style from the real images by imitating the pre-trained source network, and a spatial-aware adaptation module leverages the intrinsic spatial structure to reduce the domain divergence. Rather than operating a simple adversarial objective on the feature space, used a GAN
Manuscript accepted by Neurocomputing 2018
16 to address domain shift in which a generator projects the features to the image space and a discriminator operates on this projected image space.
Fig. 18.
The architecture of pixel-level adversarial and constraint-based adaptation. 
E. Image-to-Image Translation
Image-to-image translation has recently achieved great success with deep DA, and it has been applied to various tasks, such as style transferring. Specially, when the feature spaces of source and target images are not same, image-to-image translation should be performed by heterogeneous DA.
More approaches of image-to-image translation use a dataset of paired images and incorporate a DA algorithm into generative networks. Isola et al. proposed the pix2pix framework, which uses a conditional GAN to learn a mapping from source to target images. Tzeng et al. utilized domain confusion loss and pairwise loss to adapt from simulation to real-world data in a PR2 robot. However, several other methods also address the unpaired setting, such as CoGAN, cycle GAN, dual GAN and disco GAN.
Matching the statistical distribution by fine-tuning a deep network is another way to achieve image-to-image translation.
Gatys et al. fine-tuned the CNN to achieve DA by the total loss, which is a linear combination between the content and the style loss, such that the target image is rendered in the style of the source image maintaining the content. The content loss minimizes the mean squared difference of the feature representation between the original image and generated image in higher layers, while the style loss minimizes the elementwise mean squared difference between the Gram matrix of them on each layer. demonstrated that matching the Gram matrices of feature maps is equivalent to minimizing the MMD. Rather than MMD, proposed a deep generative correlation alignment network (DGCAN) that bridges the domain discrepancy between CAD synthetic and real images by applying the content and CORAL losses to different layers.
F. Person Re-identification
In the community, person re-identification (re-ID) has become increasingly popular. When given video sequences of a person, person re-ID recognizes whether this person has been in another camera to compensate for the limitations of fixed devices. Recently, deep DA methods have been used in re-ID when models trained on one dataset are directly used on another. Xiao et al. proposed the domain-guided dropout algorithm to discard useless neurons for re-identifying persons on multiple datasets simultaneously. Inspired by cycle GAN and Siamese network, the similarity preserving generative adversarial network (SPGAN) translated the labeled source image to the target domain, preserving self similarity and domain-dissimilarity in an unsupervised manner, and then it trains re-ID models with the translated images using supervised feature learning methods.
G. Image Captioning
Recently, image captioning, which automatically describes an image with a natural sentence, has been an emerging challenge in computer vision and natural language processing.
Due to lacking of paired image-sentence training data, DA leverages different types of data in other source domains to tackle this challenge. Chen et al. proposed a novel adversarial training procedure (captioner v.s. critics) for crossdomain image captioning using paired source data and unpaired target data. One captioner adapts the sentence style from source to target domain, whereas two critics, namely domain critic and multi-modal critic, aim at distinguishing them. Zhao et al. fine-tuned the pre-trained source model on limited data in the target domain via a dual learning mechanism.
VII. CONCLUSION
In a broad sense, deep DA is utilizing deep networks to enhance the performance of DA, such as shallow DA methods with features extracted by deep networks. In a narrow sense, deep DA is based on deep learning architectures designed for
DA and optimized by back propagation. In this survey paper, we focus on this narrow definition, and we have reviewed deep
DA techniques on visual categorization tasks.
Deep DA is classified as homogeneous DA and heterogeneous DA, and it can be further divided into supervised, semisupervised and unsupervised settings. The first setting is the simplest but is generally limited due to the need for labeled data; thus, most previous works focused on unsupervised cases. Semi-supervised deep DA is a hybrid method that combines the methods of the supervised and unsupervised settings.
Furthermore, the approaches of deep DA can be classified into one-step DA and multi-step DA considering the distance of the source and target domains. When the distance is small, one-step DA can be used based on training loss. It consists of the discrepancy-based approach, the adversarialbased approach, and the reconstruction-based approach. When the source and target domains are not directly related, multistep (or transitive) DA can be used. The key of multi-step
DA is to select and utilize intermediate domains, thus falling into three categories, including hand-crafted, feature-based and representation-based selection mechanisms.
Manuscript accepted by Neurocomputing 2018
Although deep DA has achieved success recently, many issues still remain to be addressed. First, most existing algorithms focus on homogeneous deep DA, which assumes that the feature spaces between the source and target domains are the same. However, this assumption may not be true in many applications. We expect to transfer knowledge without this severe limitation and take advantage of existing datasets to help with more tasks. Heterogeneous deep DA may attract increasingly more attention in the future.
In addition, deep DA techniques have been successfully applied in many real-world applications, including image classification, and style translation. We have also found that only a few papers address adaptation beyond classification and recognition, such as object detection, face recognition, semantic segmentation and person re-identification. How to achieve these tasks with no or a very limited amount of data is probably one of the main challenges that should be addressed by deep DA in the next few years.
Finally, since existing deep DA methods aim at aligning marginal distributions, they commonly assume shared label space across the source and target domains. However, in realistic scenario, the images of the source and target domain may be from the different set of categories or only a few categories of interest are shared. Recently, some papers,, have begun to focus on this issue and we believe it is worthy of more attention.
VIII. ACKNOWLEDGEMENTS
This work was partially supported by the National Natural
Science Foundation of China under Grant Nos. 61573068, 61471048, and 61375031, and Beijing Nova Program under
Grant No. Z161100004916088.
REFERENCES
 M. Arjovsky, S. Chintala, and L. Bottou.
Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.
 Y. Bengio. Learning deep architectures for ai. Foundations and Trends in Machine Learning, 2(1):1–127, 2009.
 K. M. Borgwardt, A. Gretton, M. J. Rasch, H.-P. Kriegel, B. Sch¨olkopf, and A. J. Smola.
Integrating structured biological data by kernel maximum mean discrepancy. Bioinformatics, 22(14):e49–e57, 2006.
 K. Bousmalis, N. Silberman, D. Dohan, D. Erhan, and D. Krishnan.
Unsupervised pixel-level domain adaptation with generative adversarial networks. arXiv preprint arXiv:1612.05424, 2016.
 K. Bousmalis, G. Trigeorgis, N. Silberman, D. Krishnan, and D. Erhan.
Domain separation networks.
In Advances in Neural Information
Processing Systems, pages 343–351, 2016.
 L. Bruzzone and M. Marconcini.
Domain adaptation problems: A dasvm classification technique and a circular validation strategy. IEEE transactions on pattern analysis and machine intelligence, 32(5):770–
 P. P. Busto and J. Gall.
Open set domain adaptation.
In The IEEE International Conference on Computer Vision (ICCV), volume 1, page 3, 2017.
 Z. Cao, M. Long, J. Wang, and M. I. Jordan. Partial transfer learning with selective adversarial networks. arXiv preprint arXiv:1707.07901, F. M. Carlucci, L. Porzi, B. Caputo, E. Ricci, and S. R. Bul`o. Autodial:
Automatic domain alignment layers. In International Conference on
Computer Vision, 2017.
 M. Chen, Z. Xu, K. Weinberger, and F. Sha. Marginalized denoising autoencoders for domain adaptation. arXiv preprint arXiv:1206.4683, T.-H. Chen, Y.-H. Liao, C.-Y. Chuang, W.-T. Hsu, J. Fu, and M. Sun.
Show, adapt and tell: Adversarial training of cross-domain image captioner. In The IEEE International Conference on Computer Vision(ICCV), volume 2, 2017.
 W.-Y. Chen, T.-M. H. Hsu, Y.-H. H. Tsai, Y.-C. F. Wang, and M.-S.
Chen. Transfer neural trees for heterogeneous domain adaptation. In
European Conference on Computer Vision, pages 399–414. Springer, Y. Chen, W. Li, C. Sakaridis, D. Dai, and L. Van Gool.
Domain adaptive faster r-cnn for object detection in the wild. arXiv preprint arXiv:1803.03243, 2018.
 Y. Chen, W. Li, and L. Van Gool.
Road: Reality oriented adaptation for semantic segmentation of urban scenes. arXiv preprint arXiv:1711.11556, 2017.
 Y.-H. Chen, W.-Y. Chen, Y.-T. Chen, B.-C. Tsai, Y.-C. F. Wang, and M. Sun. No more discrimination: Cross city adaptation of road scene segmenters. arXiv preprint arXiv:1704.08509, 2017.
 S. Chopra, S. Balakrishnan, and R. Gopalan.
Dlid: Deep learning for domain adaptation by interpolating between domains.
In ICML workshop on challenges in representation learning, volume 2, 2013.
 B. Chu, V. Madhavan, O. Beijbom, J. Hoffman, and T. Darrell. Best practices for fine-tuning visual classifiers to new domains. In Computer
Vision–ECCV 2016 Workshops, pages 435–442. Springer, 2016.
 W.-S. Chu, F. De la Torre, and J. F. Cohn. Selective transfer machine for personalized facial action unit detection.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages
3515–3522, 2013.
 G. Csurka. Domain adaptation for visual applications: A comprehensive survey. arXiv preprint arXiv:1702.05374, 2017.
 O. Day and T. M. Khoshgoftaar. A survey on heterogeneous transfer learning. Journal of Big Data, 4(1):29, 2017.
 W. Deng, L. Zheng, G. Kang, Y. Yang, Q. Ye, and J. Jiao.
Image-image domain adaptation with preserved self-similarity and domain-dissimilarity for person re-identification. arXiv preprint arXiv:1711.07027, 2017.
 J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and T. Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. In International conference on machine learning, pages 647–655, 2014.
 L. Duan, D. Xu, and I. Tsang.
Learning with augmented features for heterogeneous domain adaptation. arXiv preprint arXiv:1206.4660, D. Eigen, C. Puhrsch, and R. Fergus. Depth map prediction from a single image using a multi-scale deep network. In Advances in neural information processing systems, pages 2366–2374, 2014.
 Y. Ganin and V. Lempitsky.
Unsupervised domain adaptation by backpropagation. In International Conference on Machine Learning, pages 1180–1189, 2015.
 Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette, M. Marchand, and V. Lempitsky. Domain-adversarial training of neural networks. Journal of Machine Learning Research, 17(59):1–35, L. A. Gatys, A. S. Ecker, and M. Bethge. Image style transfer using convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2414–2423, 2016.
 W. Ge and Y. Yu.
Borrowing treasures from the wealthy: Deep transfer learning through selective joint fine-tuning. arXiv preprint arXiv:1702.08690, 2017.
 T. Gebru, J. Hoffman, and L. Fei-Fei.
Fine-grained recognition in the wild: A multi-task domain adaptation approach. arXiv preprint arXiv:1709.02476, 2017.
 M. Gheisari and M. S. Baghshah. Unsupervised domain adaptation via representation learning and adaptive classifier learning. Neurocomputing, 165:300–311, 2015.
 M. Ghifary, W. Bastiaan Kleijn, M. Zhang, and D. Balduzzi. Domain generalization for object recognition with multi-task autoencoders. In
Proceedings of the IEEE international conference on computer vision, pages 2551–2559, 2015.
 M. Ghifary, W. B. Kleijn, and M. Zhang. Domain adaptive neural networks for object recognition. In Pacific Rim International Conference on Artificial Intelligence, pages 898–904. Springer, 2014.
 M. Ghifary, W. B. Kleijn, M. Zhang, D. Balduzzi, and W. Li. Deep reconstruction-classification networks for unsupervised domain adaptation. In European Conference on Computer Vision, pages 597–613.
Springer, 2016.
 R. Girshick.
Fast r-cnn.
In Proceedings of the IEEE international conference on computer vision, pages 1440–1448, 2015.
 R. Girshick, J. Donahue, T. Darrell, and J. Malik.
Rich feature hierarchies for accurate object detection and semantic segmentation. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pages 580–587, 2014.
 X. Glorot, A. Bordes, and Y. Bengio. Domain adaptation for large-scale sentiment classification: A deep learning approach.
In Proceedings
Manuscript accepted by Neurocomputing 2018
18 of the 28th international conference on machine learning (ICML-11), pages 513–520, 2011.
 B. Gong, K. Grauman, and F. Sha. Connecting the dots with landmarks:
Discriminatively learning domain-invariant features for unsupervised domain adaptation. In International Conference on Machine Learning, pages 222–230, 2013.
 B. Gong, Y. Shi, F. Sha, and K. Grauman.
Geodesic flow kernel for unsupervised domain adaptation. In Computer Vision and Pattern
Recognition (CVPR), 2012 IEEE Conference on, pages 2066–2073.
IEEE, 2012.
 I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In
Advances in neural information processing systems, pages 2672–2680, R. Gopalan, R. Li, and R. Chellappa. Domain adaptation for object recognition: An unsupervised approach. In Computer Vision (ICCV), 2011 IEEE International Conference on, pages 999–1006. IEEE, 2011.
 S. Gupta, J. Hoffman, and J. Malik.
Cross modal distillation for supervision transfer.
In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pages 2827–2836, 2016.
 P. Haeusser, T. Frerix, A. Mordvintsev, and D. Cremers. Associative domain adaptation. In International Conference on Computer Vision(ICCV), volume 2, page 6, 2017.
 D. He, Y. Xia, T. Qin, L. Wang, N. Yu, T. Liu, and W.-Y. Ma. Dual learning for machine translation. In Advances in Neural Information
Processing Systems, pages 820–828, 2016.
 K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.
 G. Hinton, O. Vinyals, and J. Dean.
Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.
 G. E. Hinton, S. Osindero, and Y.-W. Teh. A fast learning algorithm for deep belief nets. Neural computation, 18(7):1527–1554, 2006.
 J. Hoffman, S. Guadarrama, E. S. Tzeng, R. Hu, J. Donahue, R. Girshick, T. Darrell, and K. Saenko. Lsda: Large scale detection through adaptation. In Advances in Neural Information Processing Systems, pages 3536–3544, 2014.
 J. Hoffman, S. Gupta, J. Leong, S. Guadarrama, and T. Darrell. Crossmodal adaptation for rgb-d detection.
In Robotics and Automation(ICRA), 2016 IEEE International Conference on, pages 5032–5039.
IEEE, 2016.
 J. Hoffman, E. Tzeng, J. Donahue, Y. Jia, K. Saenko, and T. Darrell.
One-shot adaptation of supervised deep convolutional models. arXiv preprint arXiv:1312.6204, 2013.
 J. Hoffman, D. Wang, F. Yu, and T. Darrell.
Fcns in the wild:
Pixel-level adversarial and constraint-based adaptation. arXiv preprint arXiv:1612.02649, 2016.
 S. Hong, W. Im, J. Ryu, and H. S. Yang. Sspp-dan: Deep domain adaptation network for face recognition with single sample per person. arXiv preprint arXiv:1702.04069, 2017.
 S. Hong, J. Oh, H. Lee, and B. Han. Learning transferrable knowledge for semantic segmentation with deep convolutional neural network. In
Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 3204–3212, 2016.
 J. Hu, J. Lu, and Y.-P. Tan.
Deep transfer metric learning.
In
Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 325–333, 2015.
 X. Huang and S. Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. arXiv preprint arXiv:1703.06868, N. Inoue, R. Furuta, T. Yamasaki, and K. Aizawa.
Cross-domain weakly-supervised object detection through progressive domain adaptation. arXiv preprint arXiv:1803.11365, 2018.
 S. Ioffe and C. Szegedy.
Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International
Conference on Machine Learning, pages 448–456, 2015.
 P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros.
Image-to-image translation with conditional adversarial networks. arXiv preprint arXiv:1611.07004, 2016.
 M. Kan, S. Shan, and X. Chen. Bi-shifting auto-encoder for unsupervised domain adaptation. In Proceedings of the IEEE International
Conference on Computer Vision, pages 3846–3854, 2015.
 T. Kim, M. Cha, H. Kim, J. Lee, and J. Kim. Learning to discover crossdomain relations with generative adversarial networks. arXiv preprint arXiv:1703.05192, 2017.
 A. Kolesnikov and C. H. Lampert. Seed, expand and constrain: Three principles for weakly-supervised image segmentation.
In European
Conference on Computer Vision, pages 695–711. Springer, 2016.
 P. Kontschieder, M. Fiterau, A. Criminisi, and S. Rota Bulo. Deep neural decision forests.
In Proceedings of the IEEE International
Conference on Computer Vision, pages 1467–1475, 2015.
 A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks.
In Advances in neural information processing systems, pages 1097–1105, 2012.
 B. Kulis, K. Saenko, and T. Darrell. What you saw is not what you get:
Domain adaptation using asymmetric kernel transforms. In Computer
Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 1785–1792. IEEE, 2011.
 C. H. Lampert, H. Nickisch, and S. Harmeling. Learning to detect unseen object classes by between-class attribute transfer. In Computer
Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 951–958. IEEE, 2009.
 C. Li and M. Wand.
Precomputed real-time texture synthesis with markovian generative adversarial networks. In European Conference on Computer Vision, pages 702–716. Springer, 2016.
 D. Li, Y. Yang, Y.-Z. Song, and T. M. Hospedales. Deeper, broader and artier domain generalization. In Computer Vision (ICCV), 2017
IEEE International Conference on, pages 5543–5551. IEEE, 2017.
 Y. Li, K. Swersky, and R. Zemel.
Generative moment matching networks.
In Proceedings of the 32nd International Conference on
Machine Learning (ICML-15), pages 1718–1727, 2015.
 Y. Li, N. Wang, J. Liu, and X. Hou. Demystifying neural style transfer. arXiv preprint arXiv:1701.01036, 2017.
 Y. Li, N. Wang, J. Shi, J. Liu, and X. Hou. Revisiting batch normalization for practical domain adaptation. arXiv preprint arXiv:1603.04779, M.-Y. Liu and O. Tuzel. Coupled generative adversarial networks. In
Advances in neural information processing systems, pages 469–477, W. Liu, Z. Wang, X. Liu, N. Zeng, Y. Liu, and F. E. Alsaadi.
A survey of deep neural network architectures and their applications.
Neurocomputing, 234:11–26, 2017.
 X. Liu, L. Song, X. Wu, and T. Tan. Transferring deep representation for nir-vis heterogeneous face recognition. In Biometrics (ICB), 2016
International Conference on, pages 1–8. IEEE, 2016.
 M. Long, Y. Cao, J. Wang, and M. Jordan.
Learning transferable features with deep adaptation networks. In International Conference on Machine Learning, pages 97–105, 2015.
 M. Long, J. Wang, and M. I. Jordan. Deep transfer learning with joint adaptation networks. arXiv preprint arXiv:1605.06636, 2016.
 M. Long, H. Zhu, J. Wang, and M. I. Jordan. Unsupervised domain adaptation with residual transfer networks.
In Advances in Neural
Information Processing Systems, pages 136–144, 2016.
 H. Lu, L. Zhang, Z. Cao, W. Wei, K. Xian, C. Shen, and A. van den
Hengel. When unsupervised domain adaptation meets tensor representations. In The IEEE International Conference on Computer Vision(ICCV), volume 2, 2017.
 P. Mittal, M. Vatsa, and R. Singh. Composite sketch recognition via deep network-a transfer learning approach. In Biometrics (ICB), 2015
International Conference on, pages 251–256. IEEE, 2015.
 S. Motiian, Q. Jones, S. Iranmanesh, and G. Doretto.
Few-shot adversarial domain adaptation.
In Advances in Neural Information
Processing Systems, pages 6673–6683, 2017.
 S. Motiian, M. Piccirilli, D. A. Adjeroh, and G. Doretto.
Unified deep supervised domain adaptation and generalization. In The IEEE
International Conference on Computer Vision (ICCV), volume 2, 2017.
 H. V. Nguyen, H. T. Ho, V. M. Patel, and R. Chellappa. Dash-n: Joint hierarchical domain adaptation and feature learning. IEEE Transactions on Image Processing, 24(12):5479–5491, 2015.
 S. Pachori, A. Deshpande, and S. Raman. Hashing in the zero shot framework with domain adaptation. Neurocomputing, 2017.
 S. J. Pan, I. W. Tsang, J. T. Kwok, and Q. Yang. Domain adaptation via transfer component analysis. IEEE Transactions on Neural Networks, 22(2):199–210, 2011.
 S. J. Pan and Q. Yang.
A survey on transfer learning.
IEEE
Transactions on knowledge and data engineering, 22(10):1345–1359, V. M. Patel, R. Gopalan, R. Li, and R. Chellappa.
Visual domain adaptation: A survey of recent advances.
IEEE signal processing magazine, 32(3):53–69, 2015.
 K.-C. Peng, Z. Wu, and J. Ernst. Zero-shot deep domain adaptation. arXiv preprint arXiv:1707.01922, 2017.
 X. Peng, J. Hoffman, X. Y. Stella, and K. Saenko.
Fine-to-coarse knowledge transfer for low-res image classification. In Image Processing (ICIP), 2016 IEEE International Conference on, pages 3683–3687.
IEEE, 2016.
 X. Peng and K. Saenko. Synthetic to real adaptation with deep generative correlation alignment networks. arXiv preprint arXiv:1701.05524, Manuscript accepted by Neurocomputing 2018
 A. Raj, V. P. Namboodiri, and T. Tuytelaars. Subspace alignment based domain adaptation for rcnn detector. arXiv preprint arXiv:1507.05578, S.-A. Rebuffi, H. Bilen, and A. Vedaldi.
Learning multiple visual domains with residual adapters. arXiv preprint arXiv:1705.08045, S. Reed, Z. Akata, X. Yan, L. Logeswaran, B. Schiele, and H. Lee.
Generative adversarial text to image synthesis. arXiv preprint arXiv:1605.05396, 2016.
 S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in neural information processing systems, pages 91–99, 2015.
 M. Rochan and Y. Wang. Weakly supervised localization of novel objects using appearance transfer. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4315–4324, 2015.
 O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on
Medical Image Computing and Computer-Assisted Intervention, pages
234–241. Springer, 2015.
 S. Rota Bulo and P. Kontschieder. Neural decision forests for semantic image labelling. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 81–88, 2014.
 A. Rozantsev, M. Salzmann, and P. Fua. Beyond sharing weights for deep domain adaptation. arXiv preprint arXiv:1603.06432, 2016.
 A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick, K. Kavukcuoglu, R. Pascanu, and R. Hadsell.
Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016.
 K. Saenko, B. Kulis, M. Fritz, and T. Darrell. Adapting visual category models to new domains. In European conference on computer vision, pages 213–226. Springer, 2010.
 K. Saito, Y. Ushiku, and T. Harada.
Asymmetric tri-training for unsupervised domain adaptation. arXiv preprint arXiv:1702.08400, K. Saito, K. Watanabe, Y. Ushiku, and T. Harada. Maximum classifier discrepancy for unsupervised domain adaptation. arXiv preprint arXiv:1712.02560, 2017.
 S. Sankaranarayanan, Y. Balaji, A. Jain, S. N. Lim, and R. Chellappa.
Learning from synthetic data: Addressing domain shift for semantic segmentation. 2017.
 L. Shao, F. Zhu, and X. Li. Transfer learning for visual categorization:
A survey. IEEE transactions on neural networks and learning systems, 26(5):1019–1034, 2015.
 J. Shen, Y. Qu, W. Zhang, and Y. Yu. Wasserstein distance guided representation learning for domain adaptation. 2017.
 W. Shimoda and K. Yanai. Distinct class-specific saliency maps for weakly supervised semantic segmentation. In European Conference on
Computer Vision, pages 218–234. Springer, 2016.
 A. Shrivastava, T. Pfister, O. Tuzel, J. Susskind, W. Wang, and R. Webb.
Learning from simulated and unsupervised images through adversarial training. arXiv preprint arXiv:1612.07828, 2016.
 X. Shu, G.-J. Qi, J. Tang, and J. Wang. Weakly-shared deep transfer networks for heterogeneous-domain knowledge propagation. In Proceedings of the 23rd ACM international conference on Multimedia, pages 35–44. ACM, 2015.
 K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.
 K. Sohn, S. Liu, G. Zhong, X. Yu, M.-H. Yang, and M. Chandraker.
Unsupervised domain adaptation for face recognition in unlabeled videos. arXiv preprint arXiv:1708.02191, 2017.
 B. Sun, J. Feng, and K. Saenko. Return of frustratingly easy domain adaptation. In AAAI, volume 6, page 8, 2016.
 B. Sun and K. Saenko. Deep coral: Correlation alignment for deep domain adaptation. In Computer Vision–ECCV 2016 Workshops, pages
443–450. Springer, 2016.
 C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions.
In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1–9, 2015.
 Y. Taigman, A. Polyak, and L. Wolf. Unsupervised cross-domain image generation. arXiv preprint arXiv:1611.02200, 2016.
 Y. Taigman, M. Yang, M. Ranzato, and L. Wolf. Deepface: Closing the gap to human-level performance in face verification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1701–1708, 2014.
 B. Tan, Y. Song, E. Zhong, and Q. Yang. Transitive transfer learning.
In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1155–1164. ACM, B. Tan, Y. Zhang, S. J. Pan, and Q. Yang. Distant domain transfer learning. In AAAI, pages 2604–2610, 2017.
 Y. Tang, J. Wang, B. Gao, E. Dellandr´ea, R. Gaizauskas, and L. Chen.
Large scale semi-supervised object detection using visual and semantic knowledge transfer.
In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pages 2119–2128, 2016.
 J.-C. Tsai and J.-T. Chien. Adversarial domain separation and adaptation. In Machine Learning for Signal Processing (MLSP), 2017 IEEE
27th International Workshop on, pages 1–6. IEEE, 2017.
 E. Tzeng, C. Devin, J. Hoffman, C. Finn, P. Abbeel, S. Levine, K. Saenko, and T. Darrell. Adapting deep visuomotor representations with weak pairwise constraints. CoRR, vol. abs/1511.07111, 2015.
 E. Tzeng, J. Hoffman, T. Darrell, and K. Saenko.
Simultaneous deep transfer across domains and tasks. In Proceedings of the IEEE
International Conference on Computer Vision, pages 4068–4076, 2015.
 E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell.
Adversarial discriminative domain adaptation. arXiv preprint arXiv:1702.05464, E. Tzeng, J. Hoffman, N. Zhang, K. Saenko, and T. Darrell. Deep domain confusion: Maximizing for domain invariance. arXiv preprint arXiv:1412.3474, 2014.
 D. Ulyanov, A. Vedaldi, and V. Lempitsky. Improved texture networks:
Maximizing quality and diversity in feed-forward stylization and texture synthesis. arXiv preprint arXiv:1701.02096, 2017.
 P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol.
Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion.
Journal of Machine
Learning Research, 11(Dec):3371–3408, 2010.
 R. Volpi, P. Morerio, S. Savarese, and V. Murino. Adversarial feature augmentation for unsupervised domain adaptation. arXiv preprint arXiv:1711.08561, 2017.
 C. Wang and S. Mahadevan.
Heterogeneous domain adaptation using manifold alignment.
In IJCAI proceedings-international joint conference on artificial intelligence, volume 22, page 1541, 2011.
 L. Wang, V. A. Sindagi, and V. M. Patel. High-quality facial photosketch synthesis using multi-adversarial networks. arXiv preprint arXiv:1710.10182, 2017.
 X. Wang, X. Duan, and X. Bai. Deep sketch feature for cross-domain image retrieval. Neurocomputing, 207:387–397, 2016.
 Y. Xia, D. Huang, and Y. Wang. Detecting smiles of young children via deep transfer learning. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pages 1673–1681, 2017.
 T. Xiao, H. Li, W. Ouyang, and X. Wang. Learning deep feature representations with domain guided dropout for person re-identification. In
Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 1249–1258, 2016.
 M. Xie, N. Jean, M. Burke, D. Lobell, and S. Ermon. Transfer learning from deep features for remote sensing and poverty mapping. 2015.
 H. Yan, Y. Ding, P. Li, Q. Wang, Y. Xu, and W. Zuo. Mind the class weight bias: Weighted maximum mean discrepancy for unsupervised domain adaptation. arXiv preprint arXiv:1705.00609, 2017.
 Z. Yi, H. Zhang, P. T. Gong, et al. Dualgan: Unsupervised dual learning for image-to-image translation. arXiv preprint arXiv:1704.02510, 2017.
 D. Yoo, N. Kim, S. Park, A. S. Paek, and I. S. Kweon. Pixel-level domain transfer. In European Conference on Computer Vision, pages
517–532. Springer, 2016.
 J. Yosinski, J. Clune, Y. Bengio, and H. Lipson. How transferable are features in deep neural networks? In Advances in neural information processing systems, pages 3320–3328, 2014.
 W.
Zellinger, T.
Grubinger, E.
Lughofer, T.
Natschl¨ager, and S. Saminger-Platz.
Central moment discrepancy (cmd) for domaininvariant representation learning. arXiv preprint arXiv:1702.08811, H. Zhang, T. Xu, H. Li, S. Zhang, X. Huang, X. Wang, and D. Metaxas.
Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks. In IEEE Int. Conf. Comput. Vision (ICCV), pages 5907–5915, 2017.
 J. Zhang, Z. Ding, W. Li, and P. Ogunbona.
Importance weighted adversarial nets for partial domain adaptation. arXiv preprint arXiv:1803.09210, 2018.
 J. Zhang, W. Li, and P. Ogunbona. Transfer learning for cross-dataset recognition: A survey. 2017.
 L. Zhang, Z. He, and Y. Liu.
Deep object recognition across domains based on adaptive extreme learning machine. Neurocomputing, 239:194–203, 2017.
 X. Zhang, F. X. Yu, S.-F. Chang, and S. Wang. Deep transfer network:
Unsupervised domain adaptation. arXiv preprint arXiv:1503.00591, Y. Zhang, P. David, and B. Gong. Curriculum domain adaptation for semantic segmentation of urban scenes.
In The IEEE International
Manuscript accepted by Neurocomputing 2018
Conference on Computer Vision (ICCV), volume 2, page 6, 2017.
 W. Zhao, W. Xu, M. Yang, J. Ye, Z. Zhao, Y. Feng, and Y. Qiao. Dual learning for cross-domain image captioning. In Proceedings of the 2017
ACM on Conference on Information and Knowledge Management, pages 29–38. ACM, 2017.
 J. T. Zhou, I. W. Tsang, S. J. Pan, and M. Tan. Heterogeneous domain adaptation for multiple classes. In Artificial Intelligence and Statistics, pages 1095–1103, 2014.
 J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. arXiv preprint arXiv:1703.10593, 2017.
 F. Zhuang, X. Cheng, P. Luo, S. J. Pan, and Q. He.
Supervised representation learning: Transfer learning with deep autoencoders. In
IJCAI, pages 4119–4125, 2015.Towards Robust Pattern Recognition: A Review
Xu-Yao Zhang, Cheng-Lin Liu, Ching Y. Suen
Abstract—The accuracies for many pattern recognition tasks have increased rapidly year-by-year, achieving or even outperforming human performance. From the perspective of accuracy, pattern recognition seems to be a nearly-solved problem.
However, once launched in real applications, the high-accuracy pattern recognition systems may become unstable and unreliable, due to the lack of robustness in open and changing environments.
In this paper, we present a comprehensive review of research towards robust pattern recognition from the perspective of breaking three basic and implicit assumptions: closed-world assumption, independent and identically distributed assumption, and clean and big data assumption, which form the foundation of most pattern recognition models. Actually, our brain is robust at learning concepts continually and incrementally, in complex, open and changing environments, with different contexts, modalities and tasks, by showing only a few examples, under weak or noisy supervision. These are the major differences between human intelligence and machine intelligence, which are closely related to the above three assumptions. After witnessing the significant progress in accuracy improvement nowadays, this review paper will enable us to analyze the shortcomings and limitations of current methods and identify future research directions for robust pattern recognition.
Index Terms—Robust pattern recognition, closed world, independent and identically distributed, clean and big data.
I. INTRODUCTION
In human intelligence, the ability of recognizing patterns is the most fundamental cognitive skill in the brain and serves as the building block for other high-level decision making, which is historically shown to be crucial for our survival and evolution in complex environments. On the other hand, for the purpose of machine intelligence, pattern recognition is also an essential goal for both machine learning and artificial intelligence, where the solving of many high-level intelligent problems relies heavily on the success of automatic and accurate pattern recognition.
During the past decades(see the survey papers in 1968, 1980 and 2000 ), many exciting achievements in pattern recognition have been reported, and most successful methods are statistical approaches, such as parametric and nonparametric Bayes decision rules, support vector machines, boosting algorithms, and so on. To guarantee high accuracy, these models are usually built on some well-designed hand-crafted features. In traditional approaches, the choice of the feature representation strongly influences the classification performance. Since
2006, the end-to-end approach of deep learning, which simultaneously learns the feature and classifier directly from the raw data, has become the new cutting-edge solution for many pattern recognition tasks.
Xu-Yao Zhang, Cheng-Lin Liu, Ching Y. Suen. Towards Robust Pattern
Recognition: A Review. Proceedings of the IEEE, vol. 108, no. 6, pp. 894922, June 2020.
The accuracies on many problems have been increased significantly and rapidly from time to time. For example, on the MNIST (10-class handwritten digit) dataset, with convolutional neural networks, it is easy to achieve more than 99% accuracy without traditional hand-crafted features. On the more challenging task of 1000-class
ImageNet large scale visual recognition, the accuracy was improved year-by-year, for example, AlexNet (2012, 84.7%), GoogLeNet (2015, 93.33%), ResNet (2016, 96.43%), and so on. The newest accuracies have already surpassed human-level performance with large margins. Actually, this kind of accuracy improvement and recordbreaking phenomena happen all the time for different pattern recognition tasks, such as face recognition,, speech recognition,, handwriting recognition,, and so on. It seems that: from the perspective of accuracy, pattern recognition has become a well-solved problem.
However, accuracy is only one aspect of measuring performance. When launching a high-accuracy pattern recognition system into real applications, many unsatisfying and unexpected results may still happen, making the system unstable in robustness, and the reason causing these problems is usually a mixture of different factors. For example, Nguyen et al. reveal that the state-of-the-art deep neural networks (DNNs) are easily fooled by assigning high confidence predictions for unrecognizable images, indicating that: although the accuracy of DNN is very high, it is not as robust as human vision when dealing with outliers. Moreover, as shown by, a small perturbation (particularly designed) on the input sample will cause a large perturbation (incorrect prediction) on the output of a pattern recognition system, leading to great adversarial risk when using such system in real applications with stringent safety requirement. Moreover, in traditional pattern recognition, the class set is usually assumed to be closed.
However, in real world, the open set problem with dynamically changed class set is much more common. When re-contextualized into open set problems, many once solved tasks become significant challenging tasks again.
Another phenomenon causing significant performance drop for pattern recognition is the distribution mismatch. It has been shown that even a very small distribution shift can lead to a great performance drop for high accuracy pattern recognition systems. Therefore, besides accuracy, the adaptability and transferability of a pattern recognition system become very important in real-life applications. Most pattern recognition systems only have single input and output, however, an effective strategy for improving robustness is to increase the diversity on both the input and the output of a system. Therefore, the multi-modal learning and multi-task learning are also important issues for robust pattern recognition. In real world, the patterns seldom occur arXiv:2006.06976v1 [cs.CV] 12 Jun 2020
Raw Data
Classification
Statistical Pattern Recognition:
•
Dimension Reduction: PCA, LDA, ICA, ISOMAP, LLE …
•
Feature Selection: Wrappers, Filters, Embedded …
•
Bayes Decision Theory: Gaussian, Parzen, KNN, Mixture …
•
Neural Network: MLP, RBF, Polynomial …
•
Decision Tree: ID3, C4.5, CART, Random Forests …
•
Kernel Method: Support Vector Machine …
•
Ensemble Method: Bagging, Boosting …
•
Clustering: K-means, Hierarchical, Spectral …
•
…
Feature Space
Domain-Specific Feature Representation:
•
Preprocessing
•
Feature Extraction
•
Reducing Within-class Variance
•
Enlarging Between-class Variance
End-to-end Deep Learning:
•
Auto-encoder
•
Restricted Boltzmann Machine
•
Convolutional Neural Network
•
Recurrent Neural Network
•
Figure 1.
A brief overview of pattern recognition methods. in isolation, but instead, usually appear with rich contextual information. Learning from the dependencies between patterns to improve the robustness of decision making is an important problem in pattern recognition.
Most pattern recognition systems are actually data-hungry, and their high accuracies rely highly on both the quantity and quality of the training data. Any disturbances on the number or the labeling condition of the data will usually lead to great changes on the final performance. However, in real applications, it is usually difficult to collect large databases and produce accurate manual labeling. Therefore, the few-shot or even zero-shot learning abilities of pattern recognition systems are of great value for real applications. On the other hand, in order to reduce the dependence on data quality, the pattern recognition system should be robust and learnable with noisy data. Moreover, besides supervised training, other strategies like unsupervised, self-supervised, and semi-supervised learning are also valuable for pattern recognition systems to learn from abundant unlabeled data and easily-obtained surrogate supervisory signals.
Based on the above observations, the problem of pattern recognition is far from being solved when considering different requirements in real applications. Besides accuracy, more attention should be paid on improving the robustness of pattern recognition. There are many previous works on robust pattern recognition (see Section III), however, most of them are driven from a single view point of robustness. Currently, there is no clear definition on robust pattern recognition. In order to give a comprehensive understanding of robustness, and more importantly, reduce the gap between pattern recognition research and the requirements of real applications, in this paper we study and review different perspectives that are crucial for robust pattern recognition.
Actually, most pattern recognition models are derived from three implicit assumptions: closed-world assumption, independent and identically distributed assumption, and clean and big data assumption. These assumptions are reasonable in a controlled laboratory environment and will simplify the complexity of the problem, therefore, they are fundamental for most pattern recognition models. However, in real-world, these assumptions are usually not satisfied, and in most cases, the performance of models built under these assumptions will deteriorate significantly. Therefore, to build robust systems for real environments, we should try to break these assumptions and develop new models and algorithms by reconsidering the essentials of pattern recognition.
In the rest of this paper, we first give a brief overview of pattern recognition methods in Section 2. After that, we define robustness for pattern recognition in Section 3. Then, we present detailed overviews on different attempts trying to break the three basic assumptions in Sections 4, 5 and 6, respectively. Lastly, we draw concluding remarks in Section 7.
For readers who want to acquire some background knowledge before reading this paper, please refer to the appendix.
II. A BRIEF OVERVIEW OF PATTERN RECOGNITION
METHODS
As shown in Fig. 1, pattern recognition methods can be divided into two main categories: two-stage and end-to-end.
Most traditional methods are two-stage, i.e., with cascaded handcrafted feature representation and pattern classification.
The feature representation is to transform the raw data to a feature space with the property of within-class compactness and between-class separability. Preprocessing (like removing noise and normalizing data) is firstly applied to reduce within-class variance, while feature extraction further enlarges betweenclass variance, and this procedure is usually domain-specific.
Actually, for solving new pattern recognition problems, the first thing is the design of feature representation, and a good feature will significantly reduce the burden on subsequent classifier learning. This kind of efforts can be found in different applications like iris recognition, gait recognition, action recognition, and so on.
After feature representation, the second stage is pattern classification, which is a much more general problem. Actually, classification is the main focus of many textbooks including
Fukunaga, Duda et al., Bishop, and so on. This stage is also known as statistical pattern recognition, where many different issues are considered from different perspectives. Firstly, dimensionality reduction is widely adopted to derive a lower dimensional representation to facilitate subsequent classification task. Another approach of feature selection can be viewed as a discrete dimensionality reduction. After that, many classical classification models can be applied. The most fundamental one is the Bayes decision theory which integrates class-conditional density estimation with prior probability for maximum posterior probability classification. Artificial neural network is also widely used for pattern classification, including MLP (multilayer perceptron), RBF (radial basis function), polynomial networks, and so on. Decision tree based methods use a tree structure to represent the classification rule. Kernel methods, have been widely applied to extend linear models to nonlinear ones by performing linear operations on higher or even infinite dimensional space transformed implicitly by a kernel mapping function, and the most representative method is SVM (support vector machine). Ensemble methods can further improve the performance by combining predictions from multiple complementary models. Clustering is widely used as an unsupervised strategy for pattern recognition.
In two-stage methods, we usually have multiple choices for both feature representation and classifier learning. It is hard to predict which combination will lead to the best performance, and in practice, different pattern recognition problems usually have different optimal configurations according to domainspecific experiences. Contrarily, deep learning methods are end-to-end by learning the feature representation and classification jointly from the raw data. In this way, the learned features and classifiers are more cooperative toward the given task in a data-driven manner, which is more flexible and discriminative than two-stage methods.
Formerly, deep neural networks are usually layer-wise pretrained by unsupervised models like auto-encoder and restricted Boltzmann machine. Nowadays, deeper and deeper neural networks can be trained end-to-end due to many improved strategies such as better initialization, activation, optimization, normalization, architecture, and so on. Due to shared-weights architecture and local connectivity characteristic, the convolutional neural network has been successfully used in many visual recognition tasks like image classification, detection, segmentation, and so on. Moreover, due to the ability of dealing with arbitrary-length sequences, the recurrent neural network has been widely used for sequence-based pattern recognition like speech recognition, scene text recognition, and so on. Furthermore, the attention mechanism can further improve deep learning performance by focusing on the most relevant information. Nowadays, deep learning has become the cutting-edge solution for numerous pattern recognition tasks.
Besides the broad class of statistical pattern recognition
Table I
REPRESENTATIVE STUDIES WITH "ROBUST RECOGNITION", "ROBUST
CLASSIFICATION", OR "ROBUSTNESS" IN THEIR TITLES.
Year
Ref.
Definition of Robustness
Type
 
Heterogeneous sources
II
 
Small-sample effects, distortion of samples
III
 
Environmental differences
II
 
Train/test condition mismatch
II
 
Imprecise and changing environments
II
 
New class discovery, outlier rejection
I
 
Clutter, learn from a few examples
III
 
Noise corruption and occlusion, outliers
III
 
Small number of training data
III
 
Distribution difference
II
 
Adversarial attack
I
 
Adversarial, random noise
I
 
Outlier, feature noise, label noise
III approaches, structural pattern recognition has been developed for exploiting and understanding the rich structural information in patterns. Unlike statistical feature representation, the structure of patterns is of variable dimensionality, and can be viewed as in non-Euclidean space. String matching and graph matching are basic problems in structural pattern recognition. To improve the learning ability of structural pattern recognition problems, kernel methods (with graph kernel ), probabilistic graphical models, and graph neural networks have been used. Overall, the research and application of structural pattern recognition is less popular than that of statistical methods.
III. ROBUSTNESS IN PATTERN RECOGNITION
To build a pattern recognition system, there should be some training samples Dtrain = {xi, yi}n i=1 and test samples
Dtest = {ˆxi, ˆyi}ˆn i=1 where x is the observed pattern and y is the corresponding label (the hat on the symbol is used to differentiate training and test samples). The purpose of pattern recognition is to learn the joint distribution p(x, y) or conditional distribution p(y|x) from the training set Dtrain and then evaluate the learned model on a different test set Dtest.
During this process, there are usually some basic assumptions.
Assumption I: Closed-world assumption. The output space is assumed to be composed of a fixed number (e.g., k) of classes which are pre-defined as a prior Ω = {1, 2,..., k}, and all samples are assumed to come from these classes yi ∈ Ω, ˆyi ∈ Ω, ∀i. Under this assumption, we can clearly and easily define the decision boundaries since the whole space is partitioned into k regions. However, in real world applications, this assumption does not always hold, and there is an open space O much larger than Ω. The samples in O can be outliers not belonging to any classes, unknown samples from some new classes not shown in the training set, or even adversarial samples from the confusing area. In these cases, the pattern recognition system will produce over-confident wrong predictions, because in its opinion there are only k options and the winning class is highly reliable.
Clean and big data assumption
IID assumption
Closed-world assumption
Noisy and small data robustness
Non-iid robustness
Open-world robustness
Type I
Type II
Type III
Known known: empirical risk
Known unknown: outlier risk
Unknown known: adversarial risk
Unknown unknown: open class risk
Learning with interdependent data
Domain adaptation and transfer learning
Multi-task learning
Multi-modal learning
Supervised learning with noisy data
Unsupervised (self-supervised) learning
Semi-supervised learning
Few-shot and zero-shot learning
Figure 2.
Basic assumptions and robustness issues in pattern recognition.
Assumption II: IID assumption. The samples are assumed to be independent considering the joint distribution of observations and labels p(x1, y1, x2, y2,..., xn, yn) = p(x1, y1)p(x2, y2)... p(xn, yn) or the marginal distribution of the observations p(x1, x2,..., xn) = p(x1)p(x2)... p(xn), while the training and test data are assumed to be identically distributed p(x) ≈ p(ˆx) and p(x, y) ≈ p(ˆx, ˆy).
Under the independent assumption, we can then define the empirical loss as the summation of individual losses, e.g., − log p(x1, x2,..., xn) = �n i=1 − log p(xi). Under the identical distribution assumption, we can therefore hope that minimizing the training error on Dtrain will yield good generalization performance on Dtest. However, in real world, the IID assumption is often violated: the data collected from multiple sources or conditions can not be simply viewed as independent, and moreover, a small mismatch between the training and test environments will cause significant performance degradation.
Assumption III: Clean and big data assumption. The training data are assumed to be well labeled, and the volume of data is assumed to be large enough for covering different variations. Under this assumption, the only requirement is the capacity of the model, and supervised learning can be used to achieve good generalization performance. However, in real world applications, it is hard to collect a large number of training samples and also impossible to label all of them perfectly. How to effectively build pattern recognition systems from noisy data under small sample size (labeled or unlabeled) is a fundamental difference between machine intelligence and human intelligence.
As long as these assumptions remain stable, we can count on a reliable system to do its job time after time. However, when the assumptions no longer hold and the conditions start drifting, we want the system to keep its performance and be insensitive to these variations, this is called the robustness for a pattern recognition system. Actually, in the literature, there are already a lot of research on robust pattern recognition giving different definitions for robustness from diverse perspectives. We show some representative definitions in Table I, and actually they can also be partitioned into three types corresponding to the above three assumptions.
Usually, the lack of robustness in these studies is caused by the dissatisfaction of the assumptions. On the other hand, there are many other research works in the literature focusing on breaking the above assumptions but not using the terminology of robust pattern recognition, which are also of great values to this field. Therefore, to better understand the current state and identify directions for future research, this paper surveys recent advances in robust pattern recognition and presents them in a common taxonomy according to the three assumptions.
The main contents are organized as shown in Fig. 2. Under each assumption, taxonomic sub-classification is presented to partition the content into four sub-topics, resulting in totally twelve issues. A brief and comprehensive overview is presented for each of them accompanied by a discussion on current and future research directions. For some of the topics there already exist good review papers, however, we differ from them by focusing more on recent advances and the relations to robust pattern recognition. Although these topics are interrelated, the review and discussion on each of them are made to be as self-contained as possible. Readers can choose to start at anywhere according to their own interests.
However, it should always be remembered that the purpose is to break the three basic assumptions and realize robust pattern recognition.
IV. BREAKING CLOSED-WORLD ASSUMPTION
Most pattern recognition methods are based on the closedworld assumption: although we only have finite observations of samples and categories, we still try to find a full partition of the whole space, this of course is unwise and improper. For example, the support vector machine seeks a hyperplane to partition the whole space into two half spaces under the principle of maximum margin. In deep neural networks, the softmax layer partitions the whole space into a fixed number of classes and the summation of class probabilities is assumed to be one. These closed-world models will make overconfident errors on outliers and new category samples.
Actually, there are massive unknown regions in pattern classification due to the finite set of training samples. To avoid making ridiculous mistakes, we must find methods to deal with these open and unseen spaces. In this paper, motivated by the "known and unknown" statement in, we summarize
5 known known
Empirical Risk known unknown unknown known unknown unknown
Outlier Risk
Adversarial Risk
Open Class Risk(a)(b)(c)(d)
Figure 3.
An illustration on breaking closed-world assumption. The whole open space is partitioned into four parts: known known, known unknown, unknown known, and unknown unknown. the approaches on breaking closed-world assumption into the following perspectives.
A. Known Known: Empirical Risk
As shown in Fig. 3a, in closed-world recognition, we usually assume that we can observe some samples from some predefined categories, and we denote this case as "known known"(things we know that we know). A straightforward strategy in this case is to minimize the empirical risk, which is estimated as the misclassification rate on observed samples. However, since the number of samples is finite, minimizing empirical risk can not guarantee good generalization performance due to over-fitting. For example, it is common for the nearest neighbor decision rule to achieve perfect accuracy on training data but unsatisfactory performance on test data, and k-nearest neighbor is used to improve generalization with k searched by cross-validation. In decision trees, overfitting occurs when the tree is designed to perfectly fit all training samples, and pruning methods are applied to trim off unnecessary branches. Multilayer perceptron is able to approximate any decision boundary to arbitrary accuracy, and different tricks (like early stopping, weight decay) are used to avoid over-fitting.
The theoretical research of VC-dimension suggests minimizing the structural risk instead of empirical risk, by balancing the complexity of model against its success at fitting training data. Under this principle, the support vector machine seeks a hyperplane that has the largest distance to the nearest training data point of any class, resulting in the large-margin regularization. From then on, many other regularization strategies have also been proposed, like sparsity, low-rank, manifold, and so on. These regularization operations are usually combined with the empirical loss to build a better objective function. Other strategies(not integrated into the objective function) can also be viewed as implicit regularization, such as training with noise, regularized parameter estimation, dropout, and so on. As a conclusion, a common strategy for "known known" is empirical risk minimization with a well-defined regularization to improve generalization performance.
B. Known Unknown: Outlier Risk
As shown in Fig. 3b, besides known known, there is also
"known unknown" in the open space (we know there are some things we do not know). In open-world recognition, the things we do not know are often denoted as outliers. The simplest way to deal with outlier is extending the k-class problem to k + 1 classes by adding a new class representing outliers.
However, the drawback of this approach is that we need to collect outlier samples, and the distribution of outlier class is usually too complex to model. A more general case is that we do not have outlier samples, and the problem now becomes outlier detection, anomaly detection, or novelty detection.
1) Pattern rejection: The first solution we should consider is to integrate some rejection strategies into the traditional pattern classifiers, and many such attempts can be found in the literature. For Bayes decision theory, Chow showed that the optimal rule (for rejecting ambiguous patterns) is to reject the pattern if the maximum of the a posteriori probabilities maxi p(i|x) is less than some threshold. Dubuisson and Masson proposed a modified rejection for the case
� i pip(x|i) being smaller than some threshold, which is suitable for rejecting outliers (not belonging to pre-defined classes). For many other classical models, the rejection option needs to be specifically designed according to the structure of classifier, like support vector machine, nearest neighbor, sparse representation, multilayer perceptron, and so on. Ensemble learning of multiple classifiers can also be used for rejection. It was shown that different classifier structures and learning algorithms affect the rejection performance significantly.
2) Softmax and extensions: In many pattern recognition systems like deep neural networks, the softmax function is widely used for classification: p(i|x) = ezi(x)
�k j=1 ezj(x) ∈, y = arg k max i=1 p(i|x), (1) where zi(x) is the discriminant function for class i and y is the predicted class for x. Let p1
= p(y|x) and p2 = maxi̸=y p(i|x) be the top-1 and top-2 probabilities.
To reject outliers, a straightforward strategy is to set some thresholds on p1 (confidence on predicted class) or △ = p1 − p2 (ambiguity between the top two classes), and the sample should be rejected if either p1 or △ is below some threshold. Actually, this kind of operation rejects uncertain predictions rather than unknown classes. Due to the closedworld property �k i=1 p(i|x) = 1, it can be easily fooled with outliers: a sample from a novel class (not predefined k classes) may still have large values for both p1 and △. This means although the prediction is wrong, the classifier is still very confident (known as overconfident error ), making the system hard to apply a threshold for rejection. A simple and straightforward modification is using the sigmoid function p(i|x) =
1 + e−zi(x) ∈ (2) to break the sum-to-one assumption and adopting the one-vsall training, to improve outlier rejection. In this case, for each class, the training samples from other classes are viewed as outliers, and a sample can be efficiently rejected if ∀i : p(i|x) < threshold, since it does not belong to any known classes. Transforming sigmoid (binary) probabilities to multi-class probabilities satisfying �k i=1 p(i|x) ≤ 1 by the Dempster-Shafer theory of evidence can make outlier probability measurable as 1 − �k i=1 p(i|x).
To extend softmax for open set recognition, the openmax fits a Weibull distribution on the distances between samples and class-means to give a parametric estimation of the probability for an input being an outlier with respect to each class. Another extension called generative openmax employs generative adversarial network for novel category data synthesis to explicitly model the outlier class.
3) One-class classification: In the literature, another solution for outlier detection is the one-class classification, where all training samples are assumed to come from only one class. The support vector data description uses a hyper-sphere with minimum volume to encompass as many training points as possible. The one-class SVM treats the origin in feature space as the representation for open space and maximizes the margin of training samples with respect to it using a kernel-based method. To use one-class models in multi-class recognition tasks, each class can be modeled with an individual one-class classifier, and then the outputs for different classes can be combined and normalized to grow a multi-class classifier with the reject option.
4) Open space risk: Recently, more and more attentions on this old and important issue are actually awakened due to the work of, which defined the open space risk as:
RO(f) =
�
O f(x)dx
�
So f(x)dx, (3) where f is a measurable recognition function: f(x) > 0 for recognition of the class of interest and f(x) = 0 when it is not recognized. The O is the "open space" and So is a ball that includes all of the known training samples as well as the open space O. The RO(f) is considered to be the relative measure of the open space compared to the whole space, and the challenge on using this theory lies on how to define O and get a computationally tractable open space risk term. In, the 1-vs-set machine is proposed as an extension of traditional SVM by using a slab defined by two parallel hyper-planes to define the open space. Similar idea has also been studied by under open space hyper-plane classifiers. The work of further introduces a new model called compact abating probability (CAP) by defining the open space O as the space sufficiently far from any known training sample:
O = So −
� i
Br(xi), (4) where Br(xi) is a closed ball of radius r centered around training sample xi. A technique called Weibull-calibrated SVM has been proposed by combining CAP with statistical extreme value theory for score calibration to improve multi-class open set recognition.
5) Discussion: Consider an expert pattern recognition system which can classify digits from 0 to 9 perfectly, when we feed an image of "apple" into the system, it said "this is a 6 and I am very confident", this will immediately change our feeling about this system from intelligent to foolish. The ability of learning to reject is a major difference between closed-world and open-world recognition. Besides particular designed methods, theoretical analysis on this problem is particularly important, and although some studies have made good attempts on this direction, it is still worth further exploration. In many approaches, a threshold is usually used to distinguish normal and abnormal patterns, and different thresholds will lead to different tradeoffs between the adopted measurements (like precision and recall). Therefore, the choice of the threshold is usually task-dependent (different tasks will require different tradeoffs), and to evaluate the overall performance of a particular method, the threshold-independent metric should be used, like the AUROC (area under receiver operating characteristic curve), AUPR (area under precision-recall curve), and so on.
C. Unknown Known: Adversarial Risk
An intriguing phenomenon in open space is "unknown known": things we think we know but it turns out we do not. Different from the known unknown in Fig. 3b which denotes open space far away from training data and hence we know they are unknown, the unknown known in Fig. 3c represents open space near decision boundaries where we are supposed to know but actually not, due to the limited number of training data not covering this space. Ambiguous prediction
7 will happen for points close to the decision boundaries, for example, visually we think a sample is from one class but the system classifies it to another class. Since it is hard to sample such observation (low frequency in real world), the story is started by generating such samples to fool the system, which is known as adversarial examples.
1) Generation of adversarial examples: At the beginning, Szegedy et al. show that by applying an imperceptible perturbation to an image, it is possible to arbitrarily change its prediction. Given any sample x, an adversarial example x′ = x + η can be found through constrained optimization.
Since the perturbation η is small, we can not find any obvious difference between x and x′ visually, but their predicted labels are different, indicating the system is not robust: small perturbation on input causes large perturbation on output. An efficient method to generate adversarial examples called fast gradient sign is proposed by : let θ be parameters of a model, x and y be input and ground truth, and J(θ, x, y) be the cost used to train the model, the perturbation is then defined as: η = ϵ · sign (∇xJ(θ, x, y)), (5) where ϵ > 0 is a step-parameter. The elements in η correspond to the sign of the gradient of cost function with respect to input. Since η is on the direction of gradient, moving x along
+η will increase J, and consequently, a large-enough ϵ will cause x′ = x + η to be misclassified. The iterative gradient sign is proposed as a refinement to fast gradient sign.
After that, DeepFool is proposed to search a minimal perturbation that is sufficient to change the label.
2) Threat of adversarial examples: As shown in, using adversarial examples as attacks will be great threats for many applications, such as self-driving cars, voice commands, robots, and so on. It has been shown in that a perturbation with 1/1000 magnitude as the original image is sufficient to fool state-of-the-art deep neural networks. Moreover, many new attack methods are still being gradually proposed,. The work of showed that it is even possible to fool the system by only modifying one pixel of natural images.
Furthermore, the method of is proposed to attack a system which is viewed as a black-box. More surprisingly, the existence of a single small image-agnostic perturbation (called universal perturbation) that fools state-of-the-art classifiers on most natural images is also found. All these attempts have posed significant challenges on the robustness of pattern recognition systems.
3) Defense methods: To deal with adversarial attacks, many defense methods have been proposed. A typical approach is to augment the training set with adversarial examples and then retrain the model on the augmented data set,.
The defensive distillation is proposed to smooth the model during training for making it less sensitive to adversarial samples. The defense-GAN is trained to model the distribution of unperturbed real data, and at inference time it finds a close output to a given sample which does not contain the adversarial changes. Besides making the system robust to adversarial examples, Metzen et al. show that adversarial perturbations can also be detected by augmenting
=
"panda"
"gibbon"
=
"whale"
"turtle"(a) fooling examples(b) adversarial examples
0.007 �
Figure 4.(a): Fooling examples in which are classified to digits 0-9 with 99.99% confidence. (b): Adversarial examples in and. the system with a detector network which is trained on the binary classification task of distinguishing genuine samples from perturbed ones. The deep contractive network adopts an objective function min θ
L(x) +
H
� i=1 λi∥ ∂hi
∂hi−1
∥2, (6) where L(x) is a standard loss function and hi is the hidden representation for layer i in a deep neural network. Since adversarial examples can be produced using the gradient sign method, regularizing the smoothness of gradient is therefore helpful to avoid them.
Since there are open spaces near the decision boundaries, data augmentation can be used to fill them for improving robustness. The stability training adds pixel-wise uncorrelated Gaussian noise for each input x to produce an augmented sample x′, and then forces the outputs of the system on x and x′ to be as close as possible, thus improving its robustness against small perturbations. The robust optimization uses an alternating min-max procedure to increase local stability min θ n
� i=1 max x′ i∈Ui L(x′ i, yi), (7) where (xi, yi) is a sample-label pair and L is the loss function, the Ui is a ball around xi with some radius. In the inside max procedure, the x′ i can be viewed as an augmented worst-case sample, and in the outside min problem, the loss on (x′ i, yi) is minimized, thus making the system to be stable in a small neighborhood around every training point.
An interesting work of mixup proposes a data-agnostic method to produce augmented data points x′ = λxi + (1 − λ)xj, y′ = λyi + (1 − λ)yj, (8) where xi, xj are two examples drawn at random from training data and yi, yj are their corresponding one-hot label vectors.1 The λ ∈ is a random parameter to produce the augmented sample x′ and a new soft label y′ (not one-hot anymore). This is based on the assumption that: linear interpolations of feature vectors should lead to linear interpolations
1Previously, we use y to denote the label which is an integer from
{1, 2,..., k}. Here we use y (in bold) to represent a one-hot label vector: a vector (with length k) filled with 1 at the index of the labeled class and with
0 everywhere else.
8 of the associated labels. Although this approach is simple, it is very effective to produce augmented samples spreading not only within the same class but also between different classes, and therefore, the open space near decision boundaries is well handled with these augmented samples. Similar approach is also adopted in a work of between-class learning.
Although a mixture of two examples may not make sense for humans visually, it will make sense for machines as suggested by, and it is shown by that this can not only improve the generalization performance but can also increase the robustness to adversarial examples.
4) Discussion: As shown in Fig. 4a, another related concept is fooling examples which are produced to be completely unrecognizable to human eyes but the pattern recognition system will still classify them into particular classes with high confidence. This is different from adversarial examples shown in Fig. 4b. Actually, the phenomenon of fooling examples is the result of outliers with closed-world assumption which is discussed in Section IV-B. It is shown by that retraining of the system by viewing fooling examples as a newly added class is not sufficient to solve this problem, and a new batch of fooling images can be produced to fool the new system even after many retraining iterations. This is because there are massive open spaces for outliers and it is impossible to model them completely. Contrarily, augmenting training data with adversarial examples was shown to significantly increase the robustness even with only one extra epoch, this is because the open spaces near decision boundaries are limited and constrained, and therefore giving us possibility to model them. Since many people continue to propose new attack methods for producing adversarial examples, the research of novel defense strategy becomes particularly important to guarantee the safety of pattern recognition.
D. Unknown Unknown: Open Class Risk
As shown in Fig. 3d, the last case in open space is "unknown unknown": situations where a lot of unknown samples (out of this world) are grouping into different unknown (unseen) categories. In this case, we should not simply mark them as a single and large category of unknown (like Section IV-B), but also need to identify the newly emerged categories in a fine-grained manner. This is a common situation in real applications, where the datasets are dynamic and novel categories must be continuously detected and added, which is denoted as open world recognition in and class-incremental learning in.
1) Definition of the problem: During continuous use of a pattern recognition system, abundant or even infinite test data will come in a streaming manner. As shown in Fig. 5, the open-world recognition process can be decomposed into three steps. The first step is detecting unknown samples and placing them in a buffer, which requires the system to reject samples from unseen classes and keep high accuracy for seen classes.
The second step is labeling unknown samples in the buffer into new categories, which can be either finished by humans or automatically implemented. The last step is then updating the classifier with augmented categories and samples, which
�����
����������
����
��������
�������
�������
�����
������
������
������
������
������
Figure 5.
Open-world recognition with class-incremental learning. requires the classifier to be efficiently trainable in a classincremental manner where different classes occur at different times. Step 1 has already been discussed in Section IV-B, therefore, this section focuses on steps 2 and 3.
2) Labeling unknown samples:
A simple and accurate approach for step 2 is seeking help from human beings, either in a batch manner when the buffer size reaches some threshold, or immediately when the users encounter some strange outputs from the system and then try to give some feedback. Moreover, a strategy to make this process more efficient is using active learning to reduce the labeling cost by selecting the most valuable data to query their labels.
On the contrary, a more challenging task is automatic new class discovery without human labeling. Unsupervised clustering is an efficient and effective solution for finding new classes. A cluster can be seen as a new category if the number of samples falling in this cluster is large enough, and otherwise, it should only be viewed as an outlier and ignored.
However, a difficulty for this approach is the model selection problem, i.e., how many novel classes are contained in the data? To deal with this, the clustering algorithms should have the ability of automatic model selection.
3) Class incremental learning: The solution of step 3 requires us to rethink the relationship between discriminative and generative models. A pattern recognition system usually contains class-independent feature extraction φ(x) and classspecific decision function gi(x).2 The classification is then: x ∈ arg maxi gi(x). In discriminative model, all the decision functions gi(x), ∀i are trained jointly (like hinge loss, softmax loss, and so on), which can be viewed as the competition between different classes to adjust decision boundaries. On the contrary, in generative model, gi(x) is usually used to model each class independently (like negative log-likelihood loss for some distribution). Discriminative model usually has higher accuracy, however, since g1(x),..., gk(x) are coupled in training, adding a new class gk+1(x) will affect others, requiring retraining of them with all data available. Contrarily, in generative model, class-incremental learning will become much simpler, since the training of gk+1(x) is independent of other classes. However, the drawback is that generative model usually leads to lower accuracy. Therefore, hybrid discriminative and generative models become necessary: φ(x) is discriminative while gi(x) is generative. Actually, many recent works are already using this principle.
2For example, in deep learning, the φ(x) is a multi-layer neural network, and gi(x) is usually a linear function on φ(x) like gi(x) = w⊤ i φ(x) + bi.
4) Prototype based approaches: The nearest class mean(NCM) can generalize to new classes at near-zero cost: gi(x) = −∥φ(x) − µi∥2
2, µi = 1 ni
� j:yj=i φ(xj), (9) where µi is the mean of training samples (totally ni) in φ(x) space for class i, and gi(x) is based on the Euclidean distance to class-mean. Different criteria can be defined, such as softmax or sigmoid, to learn a discriminative φ(x) for high accuracy, and the updating of µi is always a generative mean calculation. When new class arrives, it is efficient to compute µnew and augment the model with a new decision function gnew(x), without affecting other classes.
The class-independent feature extraction φ(x) can be either a linear dimensionality reduction or a nonlinear deep neural network. Similar idea is also adopted in where a lot of exemplar samples are selected dynamically to represent each class in CNN transformed space, and a nearestmean-of-exemplars strategy is used for classification. Another work of prototypical network is also a NCM classifier in deep neural network transformed space. A more general analysis on representing each class as Gaussian is given in. A work of convolutional prototype learning uses automatically learned prototypes to represent each class by regularizing the deviation of prototypes from class-means.
As shown in, when normalized to lie on a sphere, NCM is also equal to the traditional linear classifier in neural networks.
5) Discussion: Since it is hard to enumerate all categories at once, how to smoothly update the system to learn more and more concepts over time is therefore an important and challenging task. Although using class-means or prototypes to represent each class is a simple generative model, it is effective for class distribution modeling, because a powerful φ(x) (e.g., deep neural networks) can be learned to transform complex intra-class distributions into simplified Gaussian distributions, which will then be efficient and effective for class-incremental learning. Other classical classifiers can also be modified for class-incremental learning like random forest, support vector machine, and so on. For class-incremental learning of deep neural networks, the newly learned classes may erase the knowledge of old classes, due to the joint updating of φ(x) and gi(x), resulting in catastrophic forgetting. A remedy is to review the historical data of old classes occasionally to prevent forgetting, and many other recent advances are gradually proposed on this topic like the evolving neural structure (learning to grow), dynamic generative memory (learning to remember), and so on. More efficient class-incremental learning models that can handle forgetting problem effectively will be the focus of future research.
V. BREAKING IID ASSUMPTION
Independent and identically distributed (IID) random variable is a fundamental assumption for most pattern recognition methods. However, in practical applications, this assumption is often violated. On the Dagstuhl seminar organized by Darrell
Domain adaptation
Multi-modal learning
Cross-class transfer learning
Multi-task learning
Multi-modal multi-task learning(a)(b)(c)(d)
Figure 6.
Different tasks where the IID assumption no longer holds. et al. in 2015, it was the agreement of all participants that learning with interdependent and non-identically distributed data should be the focus of future research. Moreover, it was shown by that even a very small mismatch between training and test distributions will make state-of-the-art models dropping their performance significantly.
In pattern recognition, a labeled sample (x, y) is usually assumed to come from a feature space x ∈ X and a label space y ∈ Y. A specific combination of feature space and label space can be viewed as an environment E = X × Y, where a learner p(y|x) is defined and performed. The learner should be adjustable when the environment starts to change
E ̸= E′, which can be summarized in four cases:
• X = X ′ and Y = Y′. This is the most-widely considered case, where the feature spaces and label spaces are identical, and the environmental change comes from the conditional distribution p(y|x) ̸= p(y′|x′).
• X = X ′ and Y ̸= Y′. The feature spaces are identical but the label spaces are different, for example, cross-class transfer learning and multi-task learning.
• X ̸= X ′ and Y = Y′. The feature spaces are different while the label spaces are identical, which happens often in multi-modal learning.
• X ̸= X ′ and Y ̸= Y′. Both the feature spaces and label spaces are different, which is the most difficult situation, and can be viewed as multi-modal multi-task learning.
A. Learning with Interdependent Data
In traditional pattern recognition, the samples are assumed to be independent. However, in real world, we usually have some group information (also denoted as set, bag or field in the literature) for the samples, implying statistical dependencies among them. Let
Xi = {xi
1, xi
2,..., xi ni}, Yi = {yi
1, yi
2,..., yi ni}(10) denote groups of samples and their corresponding labels. The purpose now is to learn the classifier and make decision with grouped data {Xi, Yi}N i=1:
• In each group, the samples are no longer independent.
• Different groups may not be identically distributed.
• Different groups can have different cardinalities.
1) Content consistency:
A straightforward and widelyconsidered case is that the samples in each group have the same label yi
1 = yi
2 = · · · = yi ni, ∀i, which is known as image set classification or group-based classification in the literature. This kind of content consistency in a group is very common in practice, for example, the temporal coherence between consecutive images in videos, the same object captured by multi-angle camera networks, classification based on long term observations, and so on. Each group Xi can be viewed as an unordered set of samples, and therefore the task is to define the similarities between different sets, for example by: viewing each set as a linear subspace and defining the similarity between two sets as canonical correlation, describing each set with the Grassmann and Stiefel manifolds and using geodesic distances as metrics, representing each set as an affine hull and calculating between-set distance from the sparse approximated nearest points, and so on. Besides viewing each set to lie on a certain geometric surface, deep learning framework based on minimum reconstruction error can be used to automatically discover the underlying geometric structure for image set classification. The multiple samples in the same group (or set) will provide complementary information from different aspects like appearance variations, view-points, illumination changes, nonrigid deformations, and so on. Therefore, it offers new opportunities to improve the classification accuracy compared with single example based classification.
2) Style consistency: Besides content consistency, another situation is style consistency : the samples in a group are isogenous or generated by the same source. For example, in handwriting recognition, a group of characters produced by a certain writer are homogeneous with his/her individual writing style; in face recognition, face images can appear as different groups according to different poses or illumination conditions; in speech recognition, different speakers have different accents, and so on. These situations provide important group information, and in each group the style is consistent(other than content). Moreover, a new group unnecessarily enjoys the same style as the training groups, which means style transfer exists between training and test groups. In the literature, this problem is studied under the terminology of pattern field classification by,, where a field is a group of isogenous patterns. Specifically, in a class-style conditional mixture of Gaussians is used to model the isogenous patterns, in the dependencies among samples is modeled by second-order statistics with normally distributed styles, and in the intraclass and interclass styles are studied under adaptive classification. The traditional
Bayes decision theory can also be extended to pattern field classification. By utilizing style consistency, classifying groups of patterns is shown to be much more accurate than classifying single patterns on various tasks like multipose face recognition, multi-speaker vowel classification, and multi-writer handwriting recognition.
3) Group-level supervision: A useful strategy to realize weakly-supervised learning is that there is only group-level supervision and the labels for the individual samples are not provided, which is a natural fit for numerous real-world applications and is denoted as multi-instance learning (MIL) in the literature. A group of instances is denoted as a bag, and although each bag has an associated label, the labels of the individual instances that conform the bag are not known.
For example, in drug activity prediction, a molecule (bag) can adopt a wide range of shapes (instances) by rotating some of its internal bonds, and knowing a previously-synthesized molecule has desired drug effect does not directly provide information on the shapes. In image classification, a single image (bag) can be represented by a collection of regions, blocks or patches (instances), and the labels are only attached to images instead of the low-level segments. The instances in each bag can be treated as non-IID samples, and not all of them are necessarily relevant : some instances may not convey any information about the bag class, or even come from other classes, thus providing confusing information. MIL usually deals with binary classification, and it is initially defined as : a bag is considered to be positive if and only if it contains at least one positive instance. Relaxed and alternative definitions for MIL are presented by to extend applications for different domains. Due to the property of weak supervision, MIL has found wide applications like image categorization, object localization, computeraided diagnosis, and so on.
4) Decision making in context: In above discussed situations, the order of the samples in each group is actually ignored. However, the organizational structure of the samples gives us a very important information of context which is historically shown to be crucial in pattern recognition. For example, the linguistic context takes place very naturally during the process of human reading. By using a language model, the performance of many related tasks like speech recognition, character recognition and text processing can be significantly improved. Moreover, the spatial arrangement of the samples, known as geometric context, is also an important piece of information for different pattern recognition tasks like and. A widely-used strategy to learn from the context is viewing the samples as a sequence, and many methods like hidden Markov models (HMMs), conditional random fields (CRFs) and recurrent neural networks(RNNs) can be used to model the dependencies among samples from the perspectives of Markov chain, conditional joint probability and long-short-term dependency respectively.
Besides sequence, graph is another useful representation for contextual learning, and recently graph neural networks,, have gained increasing popularity in various domains by modeling the dependencies between nodes in a graph via efficient and effective message passing among them. Moreover, other than structured input representation, the dependencies can also happen in the output space, known as structured output learning, which tries to predict more complex outputs such as trees, strings or lattices other than a group of independent labels. More details on this important issue can be found in.
5) Discussion: By utilizing the dependencies among data, assigning labels for a group of patterns simultaneously will
Feature
Extractor ��
Loss
Task
Classifier ��
Domain
Classifier ��
Source
Feature
Extractor ��
Target
��� ��� � �� source/target
��
��
Figure 7.
An illustration of adversarial domain adaptation. be more accurate and robust than labeling them separately.
The key problem is how to define and learn from the group information. Content and style are two important factors in pattern recognition. The dependencies derived from content consistency and style consistency are useful information to improve the performance of group-based pattern recognition.
Multi-instance learning which only requires group-level supervision is an effective strategy for weakly-supervised learning.
The contextual information embedded in the order or arrangement of the samples is proved to be important for structured prediction. Besides using dependency to improve performance, automatic discovery of the relationship of samples (relational reasoning) is also an important direction.
B. Domain Adaptation and Transfer Learning
As shown in Fig. 6a, when both the feature space and label space are identical, the non-IIDness may happen on the conditional distribution. In this case, the domain adaptation and transfer learning are actually dealing with the same thing: there is usually a source domain with sufficient labeled data and a target domain with a small amount of labeled or unlabeled data, and the purpose is to reduce the distribution mismatch between two domains in supervised, unsupervised, or semi-supervised manners.
1) Supervised fine-tuning: When there exist some labeled data in target domain (supervised domain adaptation), a simple and straightforward solution is to fine-tune the model on these extra labeled data. Actually, every pattern classifier which can be trained via incremental or online learning can be used in this way for supervised domain adaptation. The model trained on the source domain can be viewed as not only a good initialization but also a regularization, and fine-tuning on target data will gradually reduce the distribution shift between two domains. Many classifiers can essentially be learned incrementally like neural networks trained with back-propagation.
For non-incremental classifiers, we can also develop some counterpart algorithms for them, such as incremental decision tree, incremental SVM, and so on.
2) Cross-domain mapping: Another widely used strategy is learning cross-domain mappings to reduce the distribution shift. Let {xs, ys} and {xt, yt} denote source and target data, and θ denotes the parameters in classifier. The cross-domain mapping φ(·) can be defined in various ways:
Parameter mapping: p(ys|xs; θ) = p(yt|xt; φ(θ)), Source mapping: p(ys|φ(xs); θ) = p(yt|xt; θ), Target mapping: p(ys|xs; θ) = p(yt|φ(xt); θ), Co-mapping: p(ys|φ(xs); θ) = p(yt|φ(xt); θ).
In the first approach of parameter mapping, the source and target distributions are matched using transformed parameters φ(θ). For example, Leggetter and Woodland use a linear transformation on the mean parameters of the hidden
Markov model for speaker adaptation, and in a residual transformation network is used to map source parameters into target parameters. Another strategy is the source mapping applied on the source data φ(xs), and the transformed source data can be used together with the target data to train the classifier. Meanwhile, the target mapping applies the mapping on target data φ(xt),, and the advantage is that the adaptation (learning of φ) can happen after the training of the source classifier. At last, we can also define the co-mapping by projecting both the source and target data to a shared common space. The mapping φ can be either linear,, or nonlinear,,. Different criteria can be used to learn φ, like maximum likelihood, minimum earth mover distance, minimum regularized Euclidean distance, discriminative training,, component analysis, and so on.
3) Distribution matching: The purpose of domain adaptation and transfer learning is to match the distributions of source and target domains. The importance re-weighting is a widely used strategy for distribution matching: each source sample is weighted by the importance factor w(x) = pt(x)/ps(x) where pt(x) and ps(x) are target and source densities. Using weighted source samples to train the classifier will work well on target domain. However, density estimation is known to be a hard problem especially in high-dimensional spaces, therefore, directly estimating the importance without going through density estimation would be more promising as shown by and. Another problem in distribution matching is how to measure the discrepancy of two distributions. The maximum mean discrepancy (MMD) is a widely-used strategy by measuring the distance between the means of two distributions in a reproducing kernel Hilbert space (RKHS). It is shown that MMD will asymptotically approach zero if and only if the two distributions are the same. Since MMD is easy to calculate and does not require the label information, it has been widely used as a regularization term for unsupervised domain adaptation. Other than only using the distance between first-order means as the measurement, Zhang et al. propose aligning the secondorder covariance matrices in RKHS for distribution matching.
Besides MMD, many other kinds of distances, divergences, and information theoretical measurements can also be used for distribution matching, as discussed in the survey paper of.
4) Adversarial learning: Recently, an increasingly popular idea of adversarial learning tries to make the features from both domains to be as indistinguishable as possible. As shown
12 in Fig. 7, the whole framework is composed of four components. The feature extractors θ1 (for source) and θ2 (for target) are usually defined as deep neural networks. The task classifier θ3 is used to perform the original K-way classification for both source and target data. Importantly, a domain classifier θ4 is used to judge whether a sample is from source or target domain (binary classification). Since we have two classifiers, here we can define two standard classification losses ℓ1 and ℓ2. The key of adversarial learning is that these losses are optimized like playing a min-max game: θ1 ⇒ min ℓ1, max ℓ2, (15) θ2 ⇒ min ℓ1 (optional), max ℓ2, (16) θ3 ⇒ min ℓ1, (17) θ4 ⇒ min ℓ2.
The purpose of min ℓ1 is to guarantee classification accuracy while max ℓ2 aims to confuse the domain classifier and make the feature distributions over two domains similar, thus resulting in domain-invariant features. To efficiently seek maxθ1,θ2 ℓ2, multiple strategies can be used, like using gradient reversal layer to reverse the gradient in backpropagation, adopting inverted labels to calculate another surrogate fooled loss, and minimizing the cross-entropy loss against a uniform distribution. Moreover, the feature extractor modules can be designed as shared (θ1 = θ2), partially shared, or independent (θ1 ̸= θ2). Adversarial learning is efficient and effective for domain adaptation and transfer learning, and many subsequent improvements are still being gradually proposed.
5) Multi-source problem: In the above approaches, we assume that there is only a single source domain, but in practice, multiple sources may exist during data collection, which is related to the style consistent pattern field classification problem in Section V-A. The above discussed single-source methods can be extended accordingly to multisource case. For example, the cross-domain mapping can be extended to multiple source-mappings with a shared Bayes classifier, the adversarial based method can be modified by replacing the binary domain classifier with a multi-way classifier representing multiple sources, and so on.
6) Discussion: Domain adaptation and transfer learning are useful for many applications like speaker adaptation in speech recognition, writer adaptation in handwriting recognition, view adaptation in face recognition, and so on. Fine-tuning is a straightforward and effective strategy for supervised adaptation, while cross-domain mapping is a general approach for supervised, unsupervised, and semi-supervised adaptation. Traditional approaches usually focus on distribution matching, and adversarial learning has become the new trend for deep learning based adaptation.
Multi-source phenomenon is common in practice, and how to discover latent domains in mixed-multi-source data is an important and challenging problem.
C. Multi-task Learning
As shown in Fig. 6b, another case is that the feature spaces are identical but the label spaces are changed. For example, a ������
������
������
������
������
������
������
�����������
������
������
���
���
���
���
��
��
��
��
��������������
�����
��
��
������
��
��
������
��
��
���������������������������
���������������������������
����������������������
�����������������
Figure 8.
An illustration of multi-task representation learning. face image can be classified into different races, ages, genders, and so on. These tasks are not independent, instead, they are complementary to each other, and learning one task is helpful for solving another. How to efficiently and effectively learn from multiple related tasks is known as multi-task learning.
1) Transferable representation learning: The first question for multi-task learning is that: can we find a generic feature representation that is transferable among different tasks? The traditional hand-crafted feature representations are usually task-specific, and new features need to be designed for new tasks. It has been shown by that the features extracted from the deep neural network pre-trained with a large dataset are transferable to different tasks. The usual method is to train a base network, and then copy its first few layers to a target network, for which the remaining layers are randomly initialized and trained toward target task. There are multiple layers in deep neural networks, and the first layers usually learn low-level features whereas the latter layers learn semantic or high-level features. The low-level features are more general while the high-level features are more specific. Therefore, the transferabilities of features from bottom, middle, or top of a neural network are different, depending on the distance between the base task and target task : for similar tasks the later layers are more transferable, while for dissimilar tasks the earlier layers are preferred.
2) Multi-task representation learning: The second question for multi-task learning is that: can dealing with multiple tasks simultaneously be used to integrate different supervisory signals for learning an invariant representation? Since each task will produce a task-specific loss function, generally, as soon as you find yourself optimizing more than one loss function, you are effectively doing multi-task learning.
To learn multiple tasks jointly, there should be some shared and task-specific parameters in the architecture, and sharing what is learned during training different tasks in parallel is the central idea in multi-task learning. A straightforward approach is the hard parameter sharing (Fig. 8a), which shares the bottom layers for all tasks and keeps several task-specific output layers. This is the most widely-used strategy in real applications due to its simplicity and effectiveness. However, 13 it needs intensive experiments to find the optimal split position for shared and task-specific layers. Another approach is the soft parameter sharing in Fig. 8b, where each task has its own parameters which are regularized with some constraints to encourage similarity between them, like minimizing their ℓ2 distances or with some partially shared structure.
Other than using fixed sharing mechanism, another strategy is learning to share. For example, the cross-stitch network proposes to learn an optimal combination of shared and task-specific representations as shown in Fig. 8c. The above approaches are based on sharing features among tasks, and the decision-making processes of each task are still independent.
However, the solution of one task may be useful for solving other related tasks, indicating that we need task feedback to update the representation as shown in Fig. 8d. In this way, the performance of different tasks can be improved recurrently by utilizing the solutions (not only features) from other tasks.
3) Task relationship learning: Finding the relationship between different tasks will make information sharing among tasks to be more selective and smooth. A straightforward strategy is using task clustering to partition multiple tasks into several clusters, and the tasks in the same cluster are assumed to be similar to each other. It is also possible to dynamically widen a thin neural network in a greedy manner to create a tree-like deep architecture for clustering similar tasks in the same branch as shown in. Besides task clustering, many studies have also tried to learn both the pertask model parameters and the inter-task relationships simultaneously, where the task relationship can be formulated to be a matrix, tensor, or a nonlinear structure. This topic has attracted much attention in the research community, and the work of taskonomy has won the CVPR2018 best paper award. The taskonomy (task taxonomy) is a directed hyper-graph that captures the transferability among tasks: an edge between two tasks represents a feasible transfer, while the weight is the prediction of the transfer performance. With task relationship, the transfer learning performance would be improved, due to better transfer path from most related tasks to a target task, which can not only reduce the computational complexity of using all tasks but can also avoid the phenomenon of negative transfer caused by dissimilar tasks.
4) Discussion: When multiple related tasks can be defined naturally, multi-task learning will significantly improve the performance for many problems like computer vision and natural language processing. However, this requires labeled data for each task. A more efficient strategy is to use some auxiliary tasks where data can be collected without manual labeling (to be discussed in Section VI-B). When multiple tasks are learned jointly, how to balance their loss functions becomes the key problem. A dominant approach is to assign each task a pre-defined weight. However, the optimal weight is expensive and time-consuming to find empirically. Therefore, Kendall et al. propose to learn the optimal weights automatically for multiple loss functions by considering the homoscedastic uncertainty of each task. Furthermore, gradient normalization can also be used to automatically balance training in deep multi-task models by dynamically tuning the gradient magnitudes.
D. Multi-modal Learning
The world surrounding us involves multiple modalities: we see objects, hear sounds, feel texture, smell odors, and taste flavors. Different from multi-task learning (Fig. 6b) where multiple tasks are performed with the same input, the purpose of multi-modal learning is to utilize the supplementary and complementary information in different modalities to complete a shared task (Fig. 6c) or multiple related tasks (Fig. 6d).
1) Multi-modal representation and fusion: In multi-modal learning, each instance can be viewed by multiple modalities, which can be fused at different levels. The first approach we can consider is signal-level fusion, for example, the pansharpening of multi-resolution satellite images. However, different modalities usually have different data structures with different sizes or dimensions such as images, sound waves and texts, making them hard to be fused at raw-data level.
Therefore, we must design some modality-wise representations, which could be either handcrafted features or learned with deep neural networks. For example, 2D/3D convolutional neural networks can be used for spatial structured signals like image, CT, fMRI, video, and so on, while recurrent neural networks can be used for temporal data like speech and text.
With modality-wise feature extractions, different modalities are transformed into a unified space, and a straightforward fusion strategy is therefore feature-level fusion, for example, the modality-wise representations can be concatenated into a longer representation or averaged (with learnable modalitywise weights) to a new representation. After that, any traditional model can be learned on the fused feature representation for a given task. Another common strategy is decision-level fusion which is widely-investigated in multiple classifier systems. This fusion strategy is often favored because different models are used on different modalities, making the system more flexible and robust to modalitymissing as the predictions are made independently. Recently, due to the development of deep learning which learns hierarchical representations, the intermediate-level fusion is used to dynamically integrate modalities at different levels with an automatically learned and optimized fusion structure.
Moreover, another important strategy is the learning-based fusion, for example, using cross weights to gradually learn interactions of modalities, using multiple-kernel learning to learn optimal feature fusion, learning sharable and specific features for different modalities, and so on.
A widely-occurring problem in multi-modal learning is modality-missing, i.e., some modalities are unaccessible for some instances during inference. The generative model such as deep Boltzmann machine can be used to handle modality-missing by sampling the absent modality from the conditional distribution. We can also apply the modality-wise dropout during training to improve the generalization performance for modality-missing.
2) Cross-modal matching, alignment, and generation: Besides fusing multiple modalities to make accurate and robust predictions, another vibrant research direction causing increasing attentions is the cross-modal learning. In this case, different modalities are embedded into a coordinated and well���������������
����������
����������
����������
�
����������������
������
������
������
�
������
����������
����������
����������
�
������
������
������
�
Figure 9.
Two types of multi-modal multi-task learning. aligned space, for example, the maximum correlated space by canonical correlation analysis (CCA), the semantic preserved space by joint embedding, and so on. The embedded space enables cross-modal matching tasks, such as retrieving the images that are relevant to a given textual query, deciding which face image is the speaker given an audio clip of someone speaking, and so on. A more difficult problem is cross-modal alignment for finding correspondences between sub-items of instances from multiple modalities. For example, aligning the steps in a recipe to a video showing the dish being made, aligning a movie to the script or the book chapters. According to, the cross-modal alignment can be achieved either explicitly like using dynamic time warping and CCA, or implicitly like using the attention mechanism in deep neural networks. Another task cross-modal generation, which seeks a mapping from one modality to another, has become very popular with an emphasis on language and vision, for example, generating the text description of an input image, inversely generating the image given a text description, and so on. The difficulty is increasing from matching, alignment, to generation, requiring much better understanding and high-level capture of the interaction and relationship between modalities.
3) Multi-modal multi-task learning: The last case we shall discuss is the multi-modal multi-task learning (Fig. 6d), which can be partitioned into two types as shown in Fig. 9. The first and also much simpler setting is the the synchronous case, where all the modalities are available for solving each task.
In this case, a fused representation of all modalities can be learned efficiently during joint training multiple related tasks, which is widely used in different applications such as disease diagnosis, traffic sign recognition, autonomous driving, emotion recognition, and so on. As shown in Fig. 9b, the second and also more challenging setting is the asynchronous case, where different tasks may only rely on their own modalities. For example, image classification works on image, speech recognition deals with sound waves, while machine translation handles text data. Intuitively, it is hard to consider these problems jointly, since both their inputs and outputs are different. Moreover, it is also not clear about the common knowledge shared among these seemingly unrelated problems, and how much benefit we can get from combining them. An interesting work named "one model to learn them all" shows us such possibility and potential, where a single deep neural network to learn multiple tasks from various modalities is designed using modality-specific encoders, I/O mixer, and task-specific decoders. The joint training of diverse tasks with asynchronous image, speech, and text modalities is shown to benefit from shared architecture and parameters. Amazingly, although seemed to be unrelated, incorporating image classification in training would help to improve the performance of language parsing, indicating that some computational primitives can be shared between different modalities and even unrelated tasks.
4) Discussion: There are many practical problems that can benefit from multi-modal learning. In biometric applications, a person can be identified by face, fingerprint, iris, or voice.
Although each modality is already distinguishable, their combination will improve both the accuracy and robustness. In autonomous driving, the fusion of multiple sensors (radar, camera, LIDAR, GPS, and so on) is necessary and important to make robust decisions. Multi-modal analysis will also make the decision making to be more explainable.
Moreover, the human brain is essentially both a multi-modal and a multi-task system: it continuously receives stimuli from various modes of the surrounding world and performs various perceptual and cognitive tasks. For a pattern recognition system, multi-modal perception increases the diversity on input while multi-task improves the diversity on output, and usually diversity will bring robustness. Therefore, joint multi-modal multi-task learning will be an inspiring and important future direction.
VI. BREAKING CLEAN AND BIG DATA ASSUMPTION
Pattern recognition systems usually have strong abilities to memorize training data. As shown in, even if we randomly change the labels of data completely, neural networks can still achieve near zero training error, indicating the strong capacity to fit training data. This is valuable if we have a clean(well-labeled) and large-enough (covering different variations) dataset, and fitting the training data in this case will usually also lead to good generalization performance. However, this assumption is hard to satisfy in real applications. Actually, clean data and big data are contradictory: it is easy to collect a well-labeled small dataset, but it is impossible to manually label a big dataset without any error. Therefore, in order to improve the robustness on both the quality and quantity of data, first of all, the training process should be robust to noisy data, and second, particular learning strategies should be considered to reduce the dependence on large amounts of data.
To reach this goal, we present discussions and summarizations from the following four perspectives.
A. Supervised Learning with Noisy Data
In supervised learning, the noises in data can be partitioned into three types: (1) label noise: the sample is valid but the label is wrong due to mislabeling; (2) sample noise (or attribute noise): the sample is noisy but the label is valid, for example, samples caused by corruption, occlusion, distortion, and so on; (3) outlier noise: both the sample and label are invalid, for example, samples from a new not-care class or a totally noisy signal, but still labeled as one of the classes to be classified. To deal with noisy data, different approaches have been proposed in the literature. Frenay and Verleysen 
15 have surveyed many methods for label noise before the year of 2014. Complementarily, we consider all the three noise types and focus more on methods developed in recent years.
1) Robust loss: The unbounded loss function will usually over-emphasize the noisy data, and hence, the decision boundaries will deviate severely from the optimal one. Therefore, the first solution for learning with noisy data is to redefine the loss function to be bounded. The convex functions are usually unbounded, and therefore, most redefined robust loss functions are non-convex. For example, the ramp loss and truncated hinge loss set an upper bound on hinge loss by allowing a maximum error for each training observation, resulting in a non-convex but robust SVM. The correntropyinduced loss with properties of bounded, smooth, and non-convex is shown to be robust when combined with kernel classifiers. For deep neural networks, as suggested by, the categorical cross entropy loss is sensitive to label noise, and a comparison of different loss functions tolerant to label noise is given in.
2) Noise transition: For a sample x with annotation y(either correct or wrong), we use ˆy to denote its groundtruth (clean) label. Now, the labeling noises can be modeled probabilistically by p(y|ˆy, x) which is usually a complex process. However, we can assume p(y|ˆy, x) = p(y|ˆy): noisy label depends only on true label and not on the sample. This is an approximation of real-world labeling process and can still be useful in some certain scenarios, for example, there are usually some confusable (similar) categories which are hard for human labelers to distinguish, regardless of the specific samples. In this case, we can simply use a noise transition matrix T to specify the probability of one label being wrongly annotated to another Tij = p(y = j|ˆy = i).
Since p(y|x) = �
ˆy p(ˆy|x)p(y|ˆy), we can modify the loss function to be :
L = − 1 n n
� i=1 log
�
� k
� j=1 p(j|xi)Tyi,j
�
�, (19) where k is the number of classes. The matrix T can be estimated from data and subsequently fixed during classifier training, jointly estimated with the classifier,, or estimated with human-assistance. Moreover, Vahdat proposes an undirected graphical model to directly model p(y|ˆy, x) other than p(y|ˆy).
3) Cleaning: Another approach is explicitly detecting and removing the noisy data. An effective strategy is using ensemble learning to filter noisy data. In ensemble learning, different classifiers are complementary to each other, hence, examples which are in contradiction with most learners can be identified confidently as noisy. With this kind of approach, the data pruning method is shown to significantly improve generalization performance. For large-scale dataset cleaning, the partitioning filter is proposed for noise identification from large distributed datasets. Another approach is directly incorporating the noise detection into the objective function of the learning machine, for example, the robust SVM approach uses a binary indicator variable for each sample to explicitly mark it as noisy or clean. In this way, the noisy
Table II
METHODS ON SUPERVISED LEARNING WITH NOISY DATA.
Label Noise
Sample Noise
Outlier Noise
Robust Loss
!
!
!
Noise Transition
!
#
#
Cleaning
!
!
!
Reweighting
!
!
!
Relabeling
!
#
# data can be automatically suppressed and no loss is charged for them during training. Similar idea is also used for learning distance metric from the noisy side information.
4) Reweighting: Reweighting is a soft-version of cleaning: assigning small weights for noisy data other than completely removing them. The work of proposes a reweighting module by a Siamese network to distinguish clean labels and noisy labels under iterative learning. Moreover, the cleanNet assigns weights as the sample-to-label relevance calculated from a joint neural embedding network for measuring the similarity between a sample and its noisy labeled class. The work of mentorNet treats the base model as the studentNet and a mentorNet is used to provide a curriculum (the reweighting scheme) for studentNet to focus on samples with probably correct labels, which is shown to significantly improve the performance on real-world largescale noisy dataset of WebVision.
5) Relabeling: We can also correct the labels of noisy data by relabeling in the learning process. In, a probabilistic graphical model is used to simulate the relationship between samples, labels and noises, for deducing the true label with an EM-like algorithm. The bootstrapping is also used for relabeling the noisy data by updating the labels as convex combination of original noisy label and current prediction of classifier iteratively. Similarly, Tanaka et al. propose to learn model parameters and true labels alternatively under a joint optimization framework.
6) Discussion: As shown in Table II, different methods can handle different data noises. Although the technical details are different, the purposes of robust loss, cleaning, and reweighting are actually similar, i.e., reducing the influence of noisy data in learning process. Therefore, they can be used for all the three noise types. The noise transition and relabeling methods are only suitable for the case of label noise, however, they are efficient and effective strategies to improve robustness when the noises in data are mainly caused by mislabeling.
B. Unsupervised (Self-supervised) Learning
In traditional pattern recognition, unsupervised learning is usually referred to data clustering, however, nowadays, more emphasis is actually placed on unsupervised representation learning, where good and transferrable feature representation is learned from large amounts of unlabeled data.
A widely-used strategy is self-supervised learning, a specific instance of supervised learning where the targets are directly generated from the data and therefore no need for labeling.
1) Reconstruction-based: Since the data are unlabeled, a straightforward strategy is to use them as both the inputs and targets to learn a compressed representation with an encoder f(x) and decoder g(x) to minimize the reconstruction error: min f,g
� x
∥g (f(x)) − x∥.
The first approach following this idea is principal component analysis (PCA) which learns a linear subspace via f(x) = Wx and g(x) = W ⊤x with an orthogonal matrix
W for projection. The restricted Boltzmann machine and auto-encoder are also reconstruction based methods and can be viewed as nonlinear extensions of PCA. From then on, various improvements have been proposed such as denoising auto-encoder, contractive auto-encoder, variational auto-encoder, and so on. The split-brain auto-encoder splits the model into two disjoint subnetworks for cross-channel prediction, which transforms the reconstruction objective to a prediction based one, making the learned feature representation more semantic and meaningful.
2) Pseudo label with clustering: We can also assign some pseudo labels to the data, and then transform the problem to a supervised learning task. For unlabeled data, a natural idea is to use some clustering algorithm to partition the data into different clusters, and then the cluster identities can be viewed as the pseudo label to learn representations. However, a challenge is that the clustering relies heavily on a good representation, and conversely the learning of representation also requires good clustering results as supervision, resulting in a chicken-or-egg-first problem. To solve this problem, the alternative learning strategy, can be used for joint unsupervised learning of the representations and clusters.
3) Pseudo label with exemplar learning: Other than clustering, another method of exemplar learning views each sample as a particular class, the pseudo label now is the sample identity, and the purpose is to separate all training samples from each other as much as possible. The exemplarCNN treats each patch (with random transformations) in an unlabeled image as a particular class, and the classifier is trained to separate all these classes. In this way, the learned representation not only ensures that different patches can be distinguished but also enforces invariance to specified transformations. Another approach uses noise as targets to learn the representation and the one-to-one matching of training samples to uniformly sampled vectors for separating every training instance. In exemplar learning, each instance is treated as a distinct class of its own, therefore, the number of classes is the size of the entire training set. The computational challenges imposed by large number of classes need to be carefully considered in exemplar learning. Recently, the momentum contrast (MoCo) proposes using a dictionary as a queue and a momentum update mechanism to efficiently and effectively realize the idea of exemplar learning, and shows that the gap between unsupervised and supervised representation learning can be closed in many vision tasks.
4) Surrogate tasks for computer vision: Recently, another interesting trend for unsupervised learning is seeking help from some surrogate tasks for which the labels or targets come
Colorization
?
Inpainting
Context Prediction
Rotation Prediction
Figure 10.
Different types of self-supervision for visual tasks. for "free" with the data. For example, as shown in Fig. 10, learning to colorize grayscale image,, learning by inpainting (to generate the contents of an missing area in image conditioned on its surroundings), learning by context prediction (to predict the position of one patch relative to another patch in image), learning by solving jigsaw puzzles (geometric rearrangement of random-permutated patches), learning by predicting image rotations, and so on. For all these tasks, the supervisory signal can be easily obtained automatically, and therefore, there is no need to worry about the insufficiency and labeling of the training data.
Although these tasks seem to be simply defined, doing well on these tasks requires the model to learn meaningful and semantic representations. For example, for inpainting the model needs to understand the content of the image to produce a plausible hypothesis for the missing part, in context prediction the model should learn to recognize objects and also their parts to predict relative positions. Therefore, the learned representations from these surrogate tasks can transfer well to more complicated tasks. Similar approaches can also be found on video related tasks,.
5) Surrogate tasks for natural language: The idea of using surrogate tasks to learn representations from unlabeled data can also be used for other tasks like natural language processing, such as the language model for predicting what comes next in a sequence,. The work of BERT proposes two novel surrogate tasks for unsupervised learning. The first is masked language model which randomly masks some of the tokens from the input and the objective is to predict the original vocabulary identity of the masked word based on its context. The second task is next sentence prediction which is a binary classification task to predict whether a sentence is next to another sentence or not. These strategies have achieved new benchmark performance on eleven language tasks, indicating unsupervised pre-training has become an important integral part for language understanding.
6) Discussion: Due to the abundantly available unlabeled data, unsupervised or self-supervised learning is a long pursued objective for representation learning. In addition to the methods discussed above, it is hopeful to see more and more effective and interesting self-supervised methods in the future.
Besides classification, self-supervised learning can also be
17 used for regression problems. Since different approaches have been proposed from different aspects, the combination of multiple self-supervised methods through multi-task learning (Section V-C) is an inspiring direction.
C. Semi-supervised Learning
Semi-supervised learning (SSL) deals with a small number of labeled data and a large amount of unlabeled data simultaneously, and therefore, can be viewed as a combination of supervised and unsupervised learning. In the literature, a wide variety of methods have been proposed for SSL, and comprehensive surveys can be found in and. Nowadays, new progress especially deep learning based approaches have become the new state-of-the-art, therefore, in this section we focus on recent advances in SSL.
1) Reconstruction-based: A straightforward strategy for
SSL is combining the supervised loss on labeled data and unsupervised loss on unlabeled data to build a new objective function. The ladder network combines the denoising auto-encoder (as unsupervised learning for every layer) with supervised learning at the top layer for SSL. The stacked whatwhere auto-encoder uses a convolutional net for encoding and a deconvolutional net for decoding to simultaneously minimize a combination of supervised and reconstruction losses. Similarly, Zhang et al. take a segment of the classification network as encoder and use mirrored architecture as decoding pathway to build several auto-encoders for SSL.
2) Self/co/tri-training: Another approach is using initial classifier to predict pseudo labels for unlabeled data and then retraining classifier with all data. This process is repeated iteratively for boosting both the accuracy of pseudo labels and the performance of classifier. Following this, the first idea is self-training where a single model is used to predict the pseudo label as the class with maximum predicted probability, which is equivalent to the entropy minimization in SSL.
The co-training uses two different models to label unlabeled data for each other. Moreover, the tri-training utilizes bootstrap sampling to get three different training sets for building three different models. For example, in the trinet approach, three different modules are learned and if two modules agree on the prediction of the unlabeled sample confidently, the two modules will teach the third module on this sample. The strategies of self/co/tri-training are efficient to implement and also effective for SSL.
3) Generative model: Generative model is another widely used strategy for SSL. For example, the Gaussian mixture model can be used to maximize the joint likelihood of both labeled and unlabeled data using EM algorithm. The variational auto-encoder (VAE) can be used for SSL by treating the labels as additional latent variables. A recent trend is using generative adversarial network (GAN) for SSL by setting up an adversarial game between a discriminator D and generator G. In original GAN, D is a binary classifier.
To apply GAN for SSL, D is modified to be a k-class model or extended to k + 1 classes (k real classes and one fake class). For labeled data, D should minimize their supervised loss. For unlabeled data, D is then trained to minimize their uncertainty (e.g. by entropy minimization) to k classes. Moreover, D should also try to distinguish the generated samples by either maximizing their entropy(uncertainty) to k classes or classifying them to the additional fake class. Meanwhile, G is trained from the opposite direction to generate realistic samples for the k classes. By using GAN for SSL, the advantages are two-fold.
First, it can generate synthetic samples for different classes which serve as additional training data. Second, even bad examples from the generator will benefit SSL, because they are lying in low-density areas of the manifold which will guide the classifier to better locate decision boundary.
4) Perturbation-based: Most deep learning models utilize randomness to improve generalization. Therefore, multiple passes of an individual sample through the network might lead to different predictions, and the inconsistency between them can be used as the loss for unlabeled data. Let f be a model with parameter θ, and η, η′ as different randomness(data augmentation, dropout, and so on). The mean squared error is used by to minimize the inconsistency of the predictions ∥f(x|θ, η) − f(x|θ, η′)∥2
2. This is denoted as Πmodel in, where each sample is evaluated twice and the difference of predictions is minimized.
Actually, this can also be explained from the teacher-student viewpoint. For each unlabeled sample, a teacher T (x) is used to guide the learning of the student f by minimizing
∥f(x|θ, η) − T (x)∥2
In Π-model the teacher is another evaluation with a different perturbation T (x) = f(x|θ, η′). However, a single evaluation can be very noisy, therefore, the temporal ensembling proposes the use of exponentially moving average (EMA) of the predictions to form a teacher:
T (x) ← αT (x) + (1 − α)f(x|θ, η′), (22) where 0 < α < 1 is a momentum, and each unlabeled sample has a teacher that is the temporal ensembling of previous predictions. Moreover, the mean teacher proposes to average model parameters other than predictions, i.e., the teacher uses EMA parameters θ′ of student model θ: θ′ ← αθ′ + (1 − α)θ, (23) and now the teacher is T (x) = f(x|θ′, η′), which is the same model with student but using historically-averaged parameters.
The perturbation-based approaches will smooth the predictions on unlabeled data. Moreover, other than random perturbations, the virtual adversarial training can be used to find a worst-case perturbation for better SSL.
5) Global consistency: The perturbation-based approach is actually seeking a kind of local consistency: samples that are close in input space (due to perturbation) should also be close in output space. However, a more important idea is global consistency: samples forming an underlying structure should have similar predictions. To better utilize global consistency, the traditional graph-based SSL is combined with deep learning, by dynamically creating a graph in the latent space in each iteration batch to model data manifold, 18 and then regularizing with the manifold for a more favorable state of class separation. Another interesting approach of learning by association considers global consistency from a different perspective: imagine a walker from labeled data to unlabeled data according to the similarity calculated from latent representation, and then the walker will go back from unlabeled data to labeled data. Correct walks that start and end at the same class are encouraged, and wrong walks that end at a different class are penalized. The cycle-consistent association from labeled data to unlabeled ones and back can be efficiently modeled using transition probabilities, and therefore is an effective strategy to pursue global consistency in SSL.
6) Discussion: In SSL, the massive unlabeled data and the scarce labeled data reveal the underlying manifold of the entire dataset, and by letting the predictions for all samples to be smooth on the manifold, more accurate decision boundary can be obtained compared to purely supervised learning. Reconstruction-based methods can learn a better representation from unlabeled data, while pseudo labels can be used for iterative training on unlabeled data. Recently, a new trend is using GAN to model the distribution of labeled and unlabeled data for SSL. Moreover, perturbation-based methods utilize the randomness in deep neural network to seek local consistency on unlabeled data, and how to effectively consider global consistency in deep learning based SSL still needs more exploration.
D. Few-shot and Zero-shot Learning
In human intelligence, we can instantly learn a novel concept by observing only a few examples from a particular class. However, the state-of-the-art approaches in machine intelligence are usually highly data-hungry. This difference has inspired an important research topic of few-shot learning(FSL). In FSL, we have a many-shot dataset M and few-shot dataset F (usually k-shot n-way: k labeled samples for each of the n classes, and k is small like 1 or 5). The samples in M have disjoint label space with F. The purpose of FSL is to extract transferrable knowledge from M to help us perform better on F, as illustrated in Fig. 11.
1) Metric learning: Under the principle that test and training conditions must match, the episode based training is used to mimic the k-shot n-way setting. Specifically, in each training iteration, an episode is formed by randomly selecting n classes with k samples per class from M to act as the support set S = {(xi, yi)}kn i=1, and meanwhile a fraction of the remainder samples from those n classes are selected as the query set. This support/query split is designed to simulate the real application situation on F. The purpose now is to define the probability P(y|x, S). Although the underlying classes are different between M and F, the learned P(y|x, S) is hoped to be transferable between them, which can be viewed as a point-to-set metric. For example, in, a deep neural network embedded space is learned to calculate such a metric.
To learn a better embedding (or metric), a memory module can be used to explicitly encode the whole support set S into memory for defining P. Moreover, different criteria can be class �� class �� class ��... class �� class �� class ��
?
? class �� class ��
Transfer with few samples:
� Metric
� Meta-learning
� Imagination
Transfer with side information:
� Attribute
� Class name
� Text description
Few-shot Many-shot Zero-shot
Figure 11.
From many-shot to few-shot and zero-shot learning. used to learn the metric like the mean square error and the ranking loss. Since each support set S is designed to be few-shot, the metric learned in this way can be transferred to unseen categories which also have few examples.
2) Learning to learn (meta-learning): The learning to learn or meta-learning is to train another learner at a higher level for guiding the original learner. In, the meta-learner is defined as the transformation from the model parameters learned from few samples to the model parameters learned from large enough samples, which can be viewed as a modelmodel regression. Another approach instead uses the sample-model regression as the meta-learner by transforming each single sample directly to the classifier parameters. Denote original model as ϕ(x; W) where W represents the parameters to be learned, instead of learning W directly, a meta-learner ω(x; W ′) is used in to map x to W. Now, the model becomes ϕ(x; ω(x; W ′)) and the task is changed to learn W ′.
The meta-learner ω(x; W ′) can be used to predict any parameters of another network ϕ like linear layers or convolutional layers, and once learned, the parameters for any novel category can be predicted by a simple forward pass. Besides predicting parameters, other meta-learning methods such as learning the optimization algorithm or initialization can also be used for FSL. Since the tasks considered in metalearning is category-agnostic, they can be well transferred to new few-shot categories.
3) Learning with imagination: Another explanation for the FSL ability of humans is that we can easily visualize or imagine what novel objects should look like from different views although we only see very few examples. This has inspired the learning with imagination to produce additional training examples. As pointed out by, the challenge of FSL is that the few examples can only capture very little of the category's intra-class variation. To solve this, we can use the many-shot dataset M to learn the intra-class transformations of samples and then augment the samples in F along these transformations. In another work, a hallucinator is trained by taking a single example of a category and producing other examples to expand the training set. The hallucinator is trained jointly with the classification model, and the goal is to help the algorithm to learn a better classifier, which is different from other data generation model like GAN 
Table III
EVALUATION METRIC AND REPRESENTATIVE PERFORMANCE FOR DIFFERENT ROBUSTNESS ISSUES IN PATTERN RECOGNITION.
Open-world
Evaluation Metric
Representative Performance
Section IV-A
Classification accuracy: the ratio of number of correctly classified patterns to the total number of patterns, evaluated on a test dataset different from the training dataset.
On a benchmark 1000-class ImageNet dataset, humanlevel accuracy is 94.9% (top-5), and ResNet could achieve
96.43% accuracy. On a smaller 10-class MNIST dataset, it is common to achieve more than 99% accuracy (top-1).
Section IV-B
Rejection performance: a threshold is used to distinguish normal and abnormal patterns, and to evaluate overall performance, threshold-independent metric is used like area under curve.
To detect notMNIST from MNIST, the AUROC (area under receiver operating characteristic curve) is 85% and AUPR(area under precision-recall curve) is 86%.
Section IV-C
Adversarial robustness: Let △(x, f) = minη ∥η∥2 subject to f(x + η) ̸= f(x), the robustness of classifier f is ρ(f) = Ex
△(x,f)
∥x∥2 where Ex is expectation over data.
On ILSVRC the adversarial robustness is 2.7 × 10−3 for CaffeNet and 1.9 × 10−3 for GoogLeNet, indicating that: a perturbation with 1/1000 magnitude as the original image is sufficient to fool state-of-the-art deep neural networks.
Section IV-D
Class-incremental capacity: the changing trend of classification accuracy as the number of classes increased.
On ILSVRC as the number of classes incremented from 100 to
1000, the accuracy is reduced from about 90% to 45%.
Non-iid
Evaluation Metric
Representative Performance
Section V-A
Contextual learning ability: the performance improvement caused by learning from a group of interdependent patterns.
By integrating geometric and linguistic contexts, improved the correct rate of handwritten Chinese text recognition from
69% to 91%.
Section V-B
Adaptability and transferability: the performance of adaptation and transfer between different (i.e., source and target) domains which have different data distributions.
Through writer adaptation with style transfer mapping, achieved more than 30% error reduction rate on a large scale handwriting recognition database CASIA-OLHWDB.
Section V-C
Multi-task cooperation ability: the gain of considering multiple tasks simultaneously compared to handling them independently.
The taskonomy reduced the number of labeled samples needed for solving 10 tasks by roughly 2
3 (compared to training independently) while keeping the performance nearly the same.
Section V-D
Multi-modal fusion ability: the boost of performance by utilizing the complementary information in different modalities.
By fusing multiple modalities at several spatial and temporal scales, won the first place out of 17 teams on the ChaLearn
2014 "looking at people challenge gesture recognition track".
Noisy Small Data
Evaluation Metric
Representative Performance
Section VI-A
Noisy data tolerance: the stability of classification performance when the training data contain a certain percentage of noises.
On CIFAR10, with clean data the accuracy is 93%, when 30% of the data are noisy, the accuracy is deteriorated to 72%, and a noise-robust model can recover the accuracy to 91%.
Section VI-B
Self-supervised capability: performance of learning from purely unlabeled data under some self-supervised mechanisms.
Using instance discrimination as the self-supervision, the unsupervised MoCo outperformed its supervised pre-training counterpart in 7 vision tasks on many datasets.
Section VI-C
Semi-supervised capability: performance of joint learning from massive unlabeled data and scarce labeled data.
On ImageNet, the accuracy of supervised learning (100% labeled data) is 96%, a semi-supervised model (10% labeled and 90% unlabeled data) could achieve 91% accuracy.
Section VI-D
Few-shot generalization: knowledge transfer ability from learning of old classes to new classes with few or even zero data.
On miniImageNet with 5-class, the 1-shot (one sample per class) accuracy is 49% and the 5-shot accuracy is 68%.
On CUB with 50-class, the 0-shot (no sample but side information of attribute is available) accuracy is 55%. whose goal is to generate realistic examples. Actually, the effectiveness of learning with imagination comes from the recovery of the intra-class variation missed in FSL.
4) Zero-shot learning: An extreme case of FSL is zeroshot learning (ZSL) where there is no example for the novel categories. In this case, some side information is needed to transfer the knowledge from previously learned categories to novel categories (Fig. 11), including attributes, class names, word vector, text description, and so on.
In attribute-based ZSL, attributes are typically nameable properties that are present or not for a certain category. In this way, multiple binary attribute-specific classifiers can be trained independently. After that, for a new class, training samples are no longer required, we only need the attribute associations for this class, and a test sample can be effectively classified by checking its predicted attributes. Besides userdefined attributes, learning the latent attributes and the class-attribute associations can further improve performance. A more general approach for ZSL suitable for different side information is embedding based approach, where two embedding networks are learned for both samples and side information, and the similarity between them are measured using Euclidean, cosine, or manifold distance.
In the embedded space, nearest neighbor search (cross-modal match) can then be efficiently used for ZSL. The last approach for ZSL is synthesizing class-specific samples conditioned on their side information for unseen classes, which can be implemented in various ways like: data generation at feature-level or sample-level, using variational auto-encoder or generative adversarial network, conditioned on attributes or text descriptions, and so on.
5) Discussion: Building a good model totally from scratch with a small number of observations is difficult, and actually, the FSL abilities of human beings are based on our abundant prior experiences of dealing with related tasks. Therefore, as pointed out by : the key insight for FSL is that the categories we have already learned can give us information that helps us to learn new categories with fewer examples.
Therefore, FSL can be viewed as cross-class transfer learning.
Moreover, humans are good at ZSL because we have other knowledge sources (like book and Internet) from which we can infer what a new category looks like. Therefore, ZSL is more like cross-modal learning (Section V-D). Although many approaches have been proposed in the literature, few-shot and zero-shot learning are still urgently needed skills for machine intelligence.
VII. CONCLUDING REMARKS
This paper considers the robustness of pattern recognition from the perspective of three basic assumptions, which are reasonable in controlled laboratory environments for pursuing high accuracies, however, will become unstable and unreliable in real-life applications. To improve robustness, we present a comprehensive literature review of the approaches trying to break these assumptions:
• For breaking closed-world assumption, we partition the open-space into four components: the known known corresponding to the empirical risk, the known unknown corresponding to the outlier risk, the unknown known corresponding to the adversarial risk, and the unknown unknown corresponding to the open class risk.
• For breaking independent and identically distributed assumption, we first discuss the problems in learning with interdependent data, then review recent advances in domain adaptation and transfer learning, and finally analyse the multi-task and multi-modal learning for increasing the diversity on both output and input of the system.
• For breaking clean and big data assumption, we first introduce supervised learning with noisy data, then review un/self-supervised and semi-supervised learning to learn from unlabeled data and surrogate supervision, and lastly discuss few-shot and zero-shot learning to transfer knowledge from big-data to small-data.
With the above approaches, we can improve the robustness of a pattern recognition system by: growing continuously with changing concepts in open world, adapting smoothly with changing environments under non-identical conditions, and learning stably with changing resources under different data quality and quantity. Actually, these are fundamental issues in robust pattern recognition, because these changing factors will usually greatly affect the stability of final performance in practice. Furthermore, in continuous use of a pattern recognition system, other than being a static model, how to make it evolvable during lifelong learning, or never-ending learning is an important step towards real intelligence.
Through breaking the three basic assumptions, we can actually eliminate the main obstacles in reaching this goal.
Unlike the traditional closed-world classification which is usually evaluated by accuracy, how to evaluate the recognition performance in open and changing environments is a big issue.
Besides accuracy, other evaluation measurements reflecting the ability in dealing with the changing factors are more important. As shown in Table III, when considering many other evaluation metrics (different from the classification accuracy), it is obvious that pattern recognition is far from solved.
Moreover, in the research community, different tasks are usually evaluated with different metrics and databases. How to build a general benchmark for evaluating the robustness by integrating different metrics together is an important future task for pattern recognition.
Different from the widely-used empirical risk minimization, theoretical analysis to unify different open-world risks will become the foundation for future classifier design. A future pattern recognition system should acquire complementary information from interdependent data in different modalities and boost itself through the cooperation of multiple tasks by adaptively learning from a few labeled, unlabeled or noisy data.
Although many attempts have been proposed in the literature, most of them try to solve a single problem from a single perspective. However, the three basic assumptions are actually related, and through joint consideration many new research problems can be raised, such as open-world domain adaptation, open-world semi-supervised learning, crossmodal domain adaptation, multi-task self-supervised learning, few-shot domain adaptation, and so on.
Future research of a unified framework to deal with the openworld, non-i.i.d., noisy and small data issues simultaneously is the ultimate goal of robust pattern recognition.
Besides the robustness issues, many other problems are also important for pattern recognition. For example, the interpretability of the model: besides high accuracy, the system also needs to explain why such a prediction is made, for increasing our confidence and safety in trusting the result.
Some traditional classifiers like decision tree and logistic regression are interpretable, but how to make other models especially black-box deep neural networks explainable is an important task. Another important issue is computational efficiency. Besides big data, strong computing power is also a key for the success of modern pattern recognition technologies. In order to widen the application scope and also reduce resource consumption, the compression and acceleration of pattern recognition models are of great values for practical applications. Since pattern recognition can be viewed as the simulation of human brain perception ability which enables machine to recognize objects or events in sensing data, how to effectively learn from neuroscience for developing brain-inspired, biologically-plausible or psychophysics-driven pattern recognition models is an inspiring future direction. With more attentions and efforts paid to these important issues in pattern recognition, the gap between human intelligence and machine intelligence can be narrowed in the foreseeable future.
APPENDIX A
BACKGROUND REFERENCES
• Pattern Recognition 
• Deep Learning 
• Outlier Detection 
• Adversarial Example 
• Open Set Recognition 
• Class-incremental Learning 
• Contextual Learning 
• Domain Adaptation 
• Transfer Learning 
• Multi-task Learning 
• Multi-modal Learning 
• Learning with Noise 
• Representation Learning 
• Self-supervised Learning 
• Semi-supervised Learning 
• Few-shot Learning 
• Zero-shot Learning 
REFERENCES
 Z. Akata, F. Perronnin, Z. Harchaoui, and C. Schmid. Label-embedding for image classification.
IEEE Trans. Pattern Anal. Mach. Intell., 38(7):1425–1438, 2016.
 Z. Al-Halah, M. Tapaswi, and R. Stiefelhagen. Recovering the missing link: Predicting class-attribute associations for unsupervised zero-shot learning. In IEEE Conf. Comput. Vis. Pattern Recognit., pages 5975–
 T. Almaev, B. Martinez, and M. Valstar. Learning to transfer: transferring latent task structures and its application to person-specific facial action unit detection. In Int. Conf. Comput. Vis., pages 3774–3782, J. Amores.
Multiple instance classification: Review, taxonomy and comparative study. Artificial Intelligence, 201:81–105, 2013.
 G. Andrew, R. Arora, J. Bilmes, and K. Livescu.
Deep canonical correlation analysis.
In Int. Conf. Mach. Learn., pages 1247–1255, A. Angelova, Y. Abu-Mostafam, and P. Perona. Pruning training sets for learning of object categories. In IEEE Conf. Comput. Vis. Pattern
Recognit., pages 494–501, 2005.
 R. Arandjelovic and A. Zisserman. Look, listen and learn. In Int. Conf.
Comput. Vis., pages 609–617, 2017.
 H. Azizpour, A. Razavian, J. Sullivan, A. Maki, and S. Carlsson.
Factors of transferability for a generic ConvNet representation. IEEE
Trans. Pattern Anal. Mach. Intell., 38(9):1790–1802, 2016.
 T. Baltrusaitis, C. Ahuja, and L. Morency.
Multimodal machine learning: A survey and taxonomy. IEEE Trans. Pattern Anal. Mach.
Intell., 41(2):423–443, 2019.
 M. Bautista, A. Sanakoyeu, E. Sutter, and B. Ommer. CliqueCNN:
Deep unsupervised exemplar learning. In Advances Neural Inf. Process.
Syst., pages 3846–3854, 2016.
 A. Bendale and T. Boult. Towards open world recognition. In IEEE
Conf. Comput. Vis. Pattern Recognit., pages 1893–1902, 2015.
 A. Bendale and T. Boult. Towards open set deep networks. In IEEE
Conf. Comput. Vis. Pattern Recognit., pages 1563–1572, 2016.
 Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives. IEEE Trans. Pattern Anal. Mach. Intell., 35(8):1798–1828, 2013.
 Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin.
A neural probabilistic language model.
J. Mach. Learn. Res., 3:1137–1155, Y. Bengio, D.-H. Lee, J. Bornschein, T. Mesnard, and Z. Lin. Towards biologically plausible deep learning. arXiv:1502.04156, 2015.
 L. Bertinetto, J. Henriques, J. Valmadre, P. Torr, and A. Vedaldi.
Learning feed-forward one-shot learners.
In Advances Neural Inf.
Process. Syst., pages 523–531, 2016.
 H. Bilen and A. Vedaldi. Integrated perception with recurrent multitask neural networks. In Advances Neural Inf. Process. Syst., pages
235–243, 2016.
 C. Bishop. Neural Networks for Pattern Recognition. Oxford university press, 1995.
 C. Bishop. Training with noise is equivalent to Tikhonov regularization.
Neural Computation, 7(1):108–116, 1995.
 C. Bishop. Pattern Recognition and Machine Learning. Springer-Verlag
New York, 2006.
 A. Blum and T. Mitchell. Combining labeled and unlabeled data with co-training. In Annual conf. Comput. Learn. Theory, pages 92–100, P. Bojanowski and A. Joulin.
Unsupervised learning by predicting noise. In Int. Conf. Mach. Learn., pages 517–526, 2017.
 L. Breiman. Classification and Regression Trees. Routledge, 2017.
 C. Brodley and M. Friedl. Identifying mislabeled training data. J. Artif.
Intell. Res., 11:131–167, 1999.
 J. Brooks. Support vector machines with the ramp loss and the hard margin loss. Operations Research, 59(2):467–479, 2011.
 H. Bunke and K. Riesen.
Recent advances in graph-based pattern recognition with applications in document analysis. Pattern Recognition, 44(5):1057–1067, 2011.
 H. Bunke and A. Sanfeliu. Syntactic and Structural Pattern Recognition
- Theory and Applications. World Scientific, 1990.
 P. Busto and J. Gall. Open set domain adaptation. In Int. Conf. Comput.
Vis., pages 754–763, 2017.
 Q. Cai, Y. Pan, T. Yao, C. Yan, and T. Mei. Memory matching networks for one-shot image recognition. In IEEE Conf. Comput. Vis. Pattern
Recognit., pages 4080–4088, 2018.
 J. Cao, Y. Li, and Z. Zhang. Partially shared multi-task convolutional neural network with local constraint for face attribute learning. In IEEE
Conf. Comput. Vis. Pattern Recognit., pages 4290–4299, 2018.
 N. Carlini and D. Wagner. Towards evaluating the robustness of neural networks. In IEEE Symposium Security Privacy, pages 39–57, 2017.
 R. Caruana. Multitask learning. Machine Learning, 28(1):41–75, 1997.
 H. Cevikalp. Best fitting hyperplanes for classification. IEEE Trans.
Pattern Anal. Mach. Intell., 39(6):1076–1088, 2017.
 V. Chandola, A. Banerjee, and V. Kumar. Anomaly detection: A survey.
ACM Computing Surveys, 41(3):1–58, 2009.
 I. Chang and M. Loew. Pattern recognition with new class discovery.
In IEEE Conf. Comput. Vis. Pattern Recognit., pages 438–443, 1991.
 O. Chapelle, B. Scholkopf, and A. Zien. Semi-Supervised Learning.
MIT Press, 2006.
 D.-D. Chen, W. Wang, W. Gao, and Z.-H. Zhou. Tri-net for semisupervised deep learning. In Int. Joint Conf. Artif. Intell., 2018.
 S. Chen, Q. Jin, J. Zhao, and S. Wang. Multimodal multi-task learning for dimensional and continuous emotion recognition. In ACM Annual
Workshop on Audio/Visual Emotion Challenge, pages 19–26, 2017.
 Y. Chen, J. Bi, and J. Wang. MILES: Multiple-instance learning via embedded instance selection. IEEE Trans. Pattern Anal. Mach. Intell., 28(12):1931–1947, 2006.
 Z. Chen, V. Badrinarayanan, C.-Y. Lee, and A. Rabinovich. GradNorm:
Gradient normalization for adaptive loss balancing in deep multitask networks. In Int. Conf. Mach. Learn., pages 1–10, 2018.
 C. Chibelushi, F. Deravi, and J. Mason. Adaptive classifier integration for robust pattern recognition.
IEEE Trans. Systems, Man, and Cybernetics, 29(6):902–907, 1999.
 K. Cho, A. Courville, and Y. Bengio. Describing multimedia content using attention-based encoder-decoder networks. IEEE Trans. Multimedia, 17(11):1875–1886, 2015.
 C. Chow. On optimum recognition error and reject tradeoff. IEEE
Trans. Information Theory, 16(1):41–46, 1970.
 S. Chowdhuri, T. Pankaj, and K. Zipser. MultiNet: Multi-modal multitask learning for autonomous driving. arXiv:1709.05581v4, 2019.
 C. Ciliberto, A. Rudi, L. Rosasco, and M. Pontil. Consistent multitask learning with nonlinear output relations.
In Advances Neural Inf.
Process. Syst., pages 1986–1996, 2017.
 R. Cinbis, J. Verbeek, and C. Schmid.
Weakly supervised object localization with multi-fold multiple instance learning. IEEE Trans.
Pattern Anal. Mach. Intell., 39(1):189–203, 2017.
 R. Collobert and J. Weston. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Int. Conf.
Mach. Learn., pages 160–168, 2008.
 C. Cortes and V. Vapnik. Support vector machine. Machine Learning, 20(3):273–297, 1995.
 N. Courty, R. Flamary, D. Tuia, and A. Rakotomamonjy.
Optimal transport for domain adaptation.
IEEE Trans. Pattern Anal. Mach.
Intell., 39(9):1853–1865, 2017.
 G.E. Dahl, D. Yu, L. Deng, and A. Acero. Context-dependent pretrained deep neural networks for large-vocabulary speech recognition.
IEEE Trans. Audio, Speech, Langu. Process., 20(1):30–42, 2012.
 A. Dai and Q. Le. Semi-supervised sequence learning. In Advances
Neural Inf. Process. Syst., pages 3079–3087, 2015.
 Z. Dai, Z. Yang, F. Yang, W. Cohen, and R. Salakhutdinov. Good semisupervised learning that requires a bad GAN. In Advances Neural Inf.
Process. Syst., pages 6510–6520, 2017.
 T. Darrell, M. Kloft, M. Pontil, G. Ratsch, and E. Rodner.
Machine learning with interdependent and non-identically distributed data.
Dagstuhl Reports (Dagstuhl Seminar 15152), 5(4):18–55, 2015.
 J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova.
BERT: Pretraining of deep bidirectional transformers for language understanding. arXiv:1810.04805, 2018.
 T. Dietterich. Steps toward robust artificial intelligence. AI Magazine, 38(3):3–24, 2017.
 T. Dietterich, R. Lathrop, and T. Lozano-Perez. Solving the multiple instance problem with axis-parallel rectangles. Artificial Intelligence, 89:31–71, 1997.
 C. Doersch, A. Gupta, and A. Efros. Unsupervised visual representation learning by context prediction. In Int. Conf. Comput. Vis., pages 1422–
 C. Doersch and A. Zisserman.
Multi-task self-supervised visual learning. In Int. Conf. Comput. Vis., pages 2051–2060, 2017.
 J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and T. Darrell. DeCAF: A deep convolutional activation feature for generic visual recognition. In Int. Conf. Mach. Learn., pages 647–655, 2014.
 W. Dong, G. Shi, X. Li, Y. Ma, and F. Huang. Compressive sensing via nonlocal low-rank regularization.
IEEE Trans. Image Process., 23(8):3618–3632, 2014.
 A. Dosovitskiy, P. Fischer, J. Springenberg, M. Riedmiller, and T. Brox.
Discriminative unsupervised feature learning with exemplar convolutional neural networks.
IEEE Trans. Pattern Anal. Mach. Intell., 38(9):1734–1747, 2016.
 B. Dubuisson and M. Masson.
A statistical decision rule with incomplete knowledge about classes. Pattern Recognition, 26(1):155–
 R. Duda, P. Hart, and D. Stork. Pattern Classification. John Wiley &
Sons, 2001.
 L. Duong, T. Cohn, S. Bird, and P. Cook. Low resource dependency parsing: Cross-lingual parameter sharing in a neural network parser. In
Int. Joint Conf. Natural Language Processing, pages 845–850, 2015.
 E. Elhamifar and R. Vidal. Robust classification using structured sparse representation. In IEEE Conf. Comput. Vis. Pattern Recognit., pages
1873–1879, 2011.
 S. Ertekin, L. Bottou, and C. Giles. Nonconvex online support vector machines.
IEEE Trans. Pattern Anal. Mach. Intell., 33(2):368–381, A. Fawzi, S. Moosavi-Dezfooli, and P. Frossard. The robustness of deep networks: A geometrical perspective. IEEE Signal Process. Magazine, 34(6):50–62, 2017.
 L. Fei-Fei, R. Fergus, and P. Perona.
One-shot learning of object categories. IEEE Trans. Pattern Anal. Mach. Intell., 28(4):594–611, P. Felzenszwalb, R. Girshick, D. McAllester, and D. Ramanan. Object detection with discriminatively trained part-based models. IEEE Trans.
Pattern Anal. Mach. Intell., 32(9):1627–1645, 2010.
 C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In Int. Conf. Mach. Learn., 2017.
 J. Foulds and E. Frank. A review of multi-instance learning assumptions. Knowledge Engineering Review, 25(1):1–25, 2010.
 B. Frenay and M. Verleysen. Classification in the presence of label noise: a survey. IEEE Trans. Neural Netw. Learn. Syst., 25(5):845–
 Y. Freund and R. Schapire. Experiments with a new boosting algorithm.
In Int. Conf. Mach. Learn., pages 148–156, 1996.
 J. Friedman. Regularized discriminant analysis. J. American Statistical
Association, 84(405):165–175, 1989.
 A. Frome, G. Corrado, J. Shlens, S. Bengio, J. Dean, M. Ranzato, and T. Mikolov. DeViSE: A deep visual-semantic embedding model. In
Advances Neural Inf. Process. Syst., pages 2121–2129, 2013.
 K.-S. Fu. Recent developments in pattern recognition. IEEE Trans.
Comput., 29(10):845–854, 1980.
 Z. Fu, T. Xiang, E. Kodirov, and S. Gong.
Zero-shot learning on semantic class prototype graph.
IEEE Trans. Pattern Anal. Mach.
Intell., 40(8):2009–2022, 2018.
 K. Fukunaga. Introduction to Statistical Pattern Recognition. Academic
Press, 1990.
 Y. Ganin and V. Lempitsky.
Unsupervised domain adaptation by backpropagation. In Int. Conf. Mach. Learn., pages 1180–1189, 2015.
 Z. Ge, S. Demyanov, Z. Chen, and R. Garnavi. Generative openmax for multi-class open set classification. arXiv:1707.07418, 2017.
 A. Ghosh, H. Kumar, and P. Sastry. Robust loss functions under label noise for deep neural networks.
In AAAI Conf. Artif. Intell., pages
1919–1925, 2017.
 A. Ghosh, N. Manwani, and P. Sastry.
Making risk minimization tolerant to label noise. Neurocomputing, 160:93–107, 2015.
 S. Gidaris, P. Singh, and N. Komodakis. Unsupervised representation learning by predicting image rotations. In Int. Conf. Learn. Representations, 2018.
 X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks. In Int. Conf. Artif. Intell. Stat., pages
249–256, 2010.
 M. Gonen and E. Alpaydin. Multiple kernel learning algorithms. J.
Mach. Learn. Res., 12:2211–2268, 2011.
 I. Goodfellow, Y. Bengio, and A. Courville.
Deep Learning.
MIT
Press, 2016. http://www.deeplearningbook.org.
 I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In
Advances Neural Inf. Process. Syst., pages 2672–2680, 2014.
 I. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. In Int. Conf. Learn. Representations, 2015.
 Y. Grandvalet and Y. Bengio.
Semi-supervised learning by entropy minimization. In Advances Neural Inf. Process. Syst., pages 529–536, Y. Grandvalet, A. Rakotomamonjy, J. Keshet, and S. Canu. Support vector machines with a reject option. In Advances Neural Inf. Process.
Syst., pages 537–544, 2009.
 A. Graves, S. Fernandez, F. Gomez, and J. Schmidhuber. Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks. In Int. Conf. Mach. Learn., pages 369–376, A. Graves, M. Liwicki, S. Fernandez, R. Bertolami, H. Bunke, and J. Schmidhuber.
A novel connectionist system for unconstrained handwriting recognition.
IEEE Trans. Pattern Anal. Mach. Intell., 31(5):855–868, 2009.
 A. Gretton, K. Borgwardt, M. Rasch, B. Scholkopf, and A. Smola. A kernel method for the two-sample-problem. In Advances Neural Inf.
Process. Syst., pages 513–520, 2007.
 J. Gu, J. Cai, S. Joty, L. Niu, and G. Wang. Look, imagine and match:
Improving textual-visual cross-modal retrieval with generative models.
In IEEE Conf. Comput. Vis. Pattern Recognit., pages 7181–7189, 2018.
 S. Gu and L. Rigazio. Towards deep neural network architectures robust to adversarial examples. arXiv:1412.5068, 2014.
 S. Guerriero, B. Caputo, and T. Mensink. Deep nearest class mean classifiers. In Worskhop Int. Conf. Learn. Representations, 2018.
 I. Guyon and A. Elisseeff.
An introduction to variable and feature selection. J. Mach. Learn. Res., 3:1157–1182, 2003.
 P. Haeusser, A. Mordvintsev, and D. Cremers. Learning by association: a versatile semi-supervised training method for neural networks. In
IEEE Conf. Comput. Vis. Pattern Recognit., pages 89–98, 2017.
 J. Hampshire and A. Waibel. The meta-pi network: Building distributed knowledge representations for robust multisource pattern recognition.
IEEE Trans. Pattern Anal. Mach. Intell., 14(7):751–769, 1992.
 B. Han, J. Yao, G. Niu, M. Zhou, I. Tsang, Y. Zhang, and M. Sugiyama.
Masking: A new perspective of noisy supervision. arXiv:1805.08193, S. Han, H. Mao, and W. Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding.
In Int. Conf. Learn. Representations, 2016.
 R. Haralick. Decision making in context. IEEE Trans. Pattern Anal.
Mach. Intell., 5(4):417–428, 1983.
 B. Hariharan and R. Girshick. Low-shot visual recognition by shrinking and hallucinating features. In Int. Conf. Comput. Vis., pages 3018–
 M. Hayat, M. Bennamoun, and S. An. Deep reconstruction models for image set classification. IEEE Trans. Pattern Anal. Mach. Intell., 37(4):713–727, 2015.
 C. He, R. Wang, S. Shan, and X. Chen. Exemplar-supported generative reproduction for class incremental learning. In British Machine Vision
Conf., 2018.
 K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick. Momentum contrast for unsupervised visual representation learning. arXiv:1911.05722, 2019.
 K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In IEEE Conf. Comput. Vis. Pattern Recognit., pages 770–
 R. He, W.-S. Zheng, B.-G. Hu, and X.-W. Kong. A regularized correntropy framework for robust pattern recognition. Neural Computation, 23(8):2074–2100, 2011.
 D. Hendrycks and K. Gimpel. A baseline for detecting misclassified and out-of-distribution examples in neural networks.
In Int. Conf.
Learn. Representations, 2017.
 G. Hinton and R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504–507, 2006.
 T. Ho, J. Hull, and S. Srihari.
Decision combination in multiple classifier systems. IEEE Trans. Pattern Anal. Mach. Intell., 16(1):66–
 V. Hodge and J. Austin. A survey of outlier detection methodologies.
Artif. Intell. Review, 22(2):85–126, 2004.
 J. Hoffman, S. Gupta, J. Leong, S. Guadarrama, and T. Darrell. Crossmodal adaptation for RGB-D detection. In IEEE Int. Conf. Robotics
Automation, pages 5032–5039, 2016.
 J. Hoffman, B. Kulis, T. Darrell, and K. Saenko. Discovering latent domains for multisource domain adaptation.
In European Conf.
Computer Vision, pages 702–715, 2012.
 J. Hoffman, E. Rodner, J. Donahue, T. Darrell, and K. Saenko. Efficient learning of domain-invariant image representations. In Int. Conf. Learn.
Representations, pages 1–9, 2013.
 C. Hu, Y. Chen, L. Hu, and X. Peng. A novel random forests based class incremental learning method for activity recognition.
Pattern
Recognition, 78:277–290, 2018.
 Y. Hu, A. Mian, and R. Owens.
Face recognition using sparse approximated nearest points between image sets. IEEE Trans. Pattern
Anal. Mach. Intell., 34(10):1992–2004, 2012.
 J. Huang, A. Smola, A. Gretton, K. Borgwardt, and B. Scholkopf.
Correcting sample selection bias by unlabeled data. In Advances Neural
Inf. Process. Syst., pages 601–608, 2007.
 K. Huang, R. Jin, Z. Xu, and C.-L. Liu. Robust metric learning by smooth optimization. In Conf. Uncertain. Artif. Intell., 2010.
 S.-J. Huang, R. Jin, and Z.-H. Zhou.
Active learning by querying informative and representative examples. IEEE Trans. Pattern Anal.
Mach. Intell., 36(10):1936–1949, 2014.
 H. Daume III and D. Marcu.
Domain adaptation for statistical classifiers. J. Artif. Intell. Res., 26:101–126, 2006.
 ILSVRC.
ImageNet large scale visual recognition challenge. http:
//www.image-net.org/challenges/LSVRC.
 S. Ioffe and C. Szegedy.
Batch normalization: Accelerating deep network training by reducing internal covariate shift.
In Int. Conf.
Mach. Learn., 2015.
 A. Jain. Data clustering: 50 years beyond K-means. Pattern Recognit.
Lett., 31(8):651–666, 2010.
 A.K. Jain, R.P.W. Duin, and J. Mao. Statistical pattern recognition: A review. IEEE Trans. Pattern Anal. Mach. Intell., 22(1):4–37, 2000.
 L. Jiang, Z. Zhou, T. Leung, L.-J. Li, and L. Fei-Fei.
Mentornet:
Learning data-driven curriculum for very deep neural networks on corrupted labels. In Int. Conf. Mach. Learn., pages 2309–2318, 2018.
 I. Jindal, M. Nokleby, and X. Chen. Learning deep networks from noisy labels with dropout regularization. In Int. Conf. Data Mining, pages 967–972, 2016.
 P. Junior, R. Souza, R. Werneck, B. Stein, D. Pazinato, W. Almeida, O. Penatti, R. Torres, and A. Rocha. Nearest neighbors distance ratio open-set classifier. Machine Learning, 106(3):359–386, 2017.
 L. Kaiser, A. Gomez, N. Shazeer, A. Vaswani, N. Parmar, L. Jones, and J. Uszkoreit. One model to learn them all. arXiv:1706.05137, K. Kamnitsas, D. Castro, L. Folgoc, I. Walker, R. Tanno, D. Rueckert, B. Glocker, A. Criminisi, and A. Nori. Semi-supervised learning via compact latent space clustering. In Int. Conf. Mach. Learn., 2018.
 M. Kan, J. Wu, S. Shan, and X. Chen. Domain adaptation for face recognition: Targetize source domain bridged by common subspace.
Int. Journal of Computer Vision, 109(1):94–109, 2014.
 M. Kandemir and F. Hamprecht. Computer-aided diagnosis from weak supervision: A benchmarking study. Comput. Med. Imaging Graph., 42:44–50, 2015.
 X. Kang, S. Li, and J.A. Benediktsson. Pansharpening with matting model.
IEEE Trans. Geoscience and Remote Sensing, 52(8):5088–
 B. Karmakar and N. Pal. How to make a neural network say "Don't know". Information Sciences, 430:444–466, 2018.
 A. Kendall, Y. Gal, and R. Cipolla. Multi-task learning using uncertainty to weigh losses for scene geometry and semantics.
In IEEE
Conf. Comput. Vis. Pattern Recognit., pages 7482–7491, 2018.
 Y. Kharin.
Robustness in Statistical Pattern Recognition.
Springer
Science & Business Media, 1996.
 T.-K. Kim, J. Kittler, and R. Cipolla.
Discriminative learning and recognition of image set classes using canonical correlations. IEEE
Trans. Pattern Anal. Mach. Intell., 29(6):1005–1018, 2007.
 D. Kingma and J. Ba. Adam: A method for stochastic optimization.
In Int. Conf. Learn. Representations, 2015.
 D. Kingma, S. Mohamed, D. Rezende, and M. Welling.
Semisupervised learning with deep generative models. In Advances Neural
Inf. Process. Syst., pages 3581–3589, 2014.
 D. Kingma and M. Welling.
Auto-encoding variational bayes. arXiv:1312.6114, 2013.
 T. Kipf and M. Welling.
Semi-supervised classification with graph convolutional networks. In Int. Conf. Learn. Representations, 2017.
 A. Kolesnikov, X. Zhai, and L. Beyer. Revisiting self-supervised visual representation learning. In IEEE Conf. Comput. Vis. Pattern Recognit., pages 1920–1929, 2019.
 D. Koller and N. Friedman.
Probabilistic Graphical Models.
MIT
Press, 2009.
 S. Kotz and S. Nadarajah. Extreme Value Distributions: Theory and Applications. World Scientific, 2000.
 J. Kozerawski and M. Turk. CLEAR: Cumulative learning for oneshot one-class image recognition. In IEEE Conf. Comput. Vis. Pattern
Recognit., pages 3446–3455, 2018.
 A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classification with deep convolutional neural networks. In Advances Neural Inf. Process.
Syst., pages 1097–1105, 2012.
 L. Kuncheva. Combining Pattern Classifiers: Methods and Algorithms.
John Wiley & Sons, 2004.
 A. Kurakin, I. Goodfellow, and S. Bengio. Adversarial examples in the physical world. arXiv:1607.02533, 2016.
 I. Kuzborskij, F. Orabona, and B. Caputo. From n to n+ 1: Multiclass transfer incremental learning.
In IEEE Conf. Comput. Vis. Pattern
Recognit., pages 3358–3365, 2013.
 J. Lafferty, A. McCallum, and F. Pereira. Conditional random fields:
Probabilistic models for segmenting and labeling sequence data. In Int.
Conf. Mach. Learn., pages 282–289, 2001.
 S. Laine and T. Aila.
Temporal ensembling for semi-supervised learning. In Int. Conf. Learn. Representations, 2017.
 B. Lake, R. Salakhutdinov, and J. Tenenbaum.
Human-level concept learning through probabilistic program induction.
Science, 350(6266):1332–1338, 2015.
 C. Lampert.
Kernel methods in computer vision.
Foundations and Trends in Computer Graphics and Vision, 4(3):193–285, 2009.
 C. Lampert, H. Nickisch, and S. Harmeling. Attribute-based classification for zero-shot visual object categorization. IEEE Trans. Pattern
Anal. Mach. Intell., 36(3):453–465, 2014.
 G. Larsson, M. Maire, and G. Shakhnarovich. Learning representations for automatic colorization. In Eur. Conf. Comput. Vis., pages 577–593, P. Laskov, C. Gehl, S. Kruger, and K.-R. Muller. Incremental support vector learning: Analysis, implementation and applications. J. Mach.
Learn. Res., 7:1909–1936, 2006.
 Y. LeCun, Y. Bengio, and G. Hinton.
Deep learning.
Nature, 521(7553):436–444, 2015.
 Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner.
Gradient-based learning applied to document recognition. Proc. IEEE, 86(11):2278–
 D.-H. Lee.
Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks.
In Workshop Int. Conf.
Mach. Learn., 2013.
 K.-H. Lee, X. He, L. Zhang, and L. Yang. CleanNet: Transfer learning for scalable image classifier training with label noise. In IEEE Conf.
Comput. Vis. Pattern Recognit., pages 5447–5456, 2018.
 C.J. Leggetter and P.C. Woodland.
Maximum likelihood linear regression for speaker adaptation of continuous density hidden markov models. Computer Speech and Language, 9(2):171–185, 1995.
 X. Li, Y. Zhou, T. Wu, R. Socher, and C. Xiong.
Learn to grow:
A continual structure learning framework for overcoming catastrophic forgetting. In Int. Conf. Mach. Learn., 2019.
 Z. Li, L.-F. Cheong, S. Yang, and K.-C. Toh. Simultaneous clustering and model selection: Algorithm, theory and applications. IEEE Trans.
Pattern Anal. Mach. Intell., 40(8):1964–1978, 2018.
 Z. Li and D. Hoiem. Learning without forgetting. IEEE Trans. Pattern
Anal. Mach. Intell., 40(12):2935–2947, 2018.
 Z. Li and D. Hoiem. G-distillation: Reducing overconfident errors on novel samples. arXiv:1804.03166, 2018.
 A. Liu and B. Ziebart. Robust classification under sample selection bias. In Advances Neural Inf. Process. Syst., pages 37–45, 2014.
 C.-L. Liu. Classifier combination based on confidence transformation.
Pattern Recognition, 38(1):11–28, 2005.
 C.-L. Liu.
One-vs-all training of prototype classifiers for pattern classification and retrieval. In Int. Conf. Pattern Recognition, pages
3328–3331, 2010.
 C.-L. Liu, K. Nakashima, H. Sako, and H. Fujisawa.
Handwritten digit recognition: benchmarking of state-of-the-art techniques. Pattern
Recognition, 36(10):2271–2285, 2003.
 C.-L. Liu, H. Sako, and H. Fujisawa. Performance evaluation of pattern classifiers for handwritten character recognition. Int. J. Document Anal.
Recognit., 4(3):191–204, 2002.
 K. Liu, Y. Li, N. Xu, and P. Natarajan. Learn to combine modalities in multimodal deep learning. arXiv:1805.11730, 2018.
 T. Liu and D. Tao.
Classification with noisy labels by importance reweighting. IEEE Trans. Pattern Anal. Mach. Intell., 38(3):447–461, X. Liu, J. Weijer, and A. Bagdanov. Exploiting unlabeled data in CNNs by self-supervised learning to rank. IEEE Trans. Pattern Anal. Mach.
Intell., 41(8):1862–1878, 2019.
 J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In IEEE Conf. Comput. Vis. Pattern Recognit., pages 3431–3440, 2015.
 M. Long, Y. Cao, J. Wang, and M. Jordan.
Learning transferable features with deep adaptation networks. In Int. Conf. Mach. Learn., pages 1–9, 2015.
 Y. Long, L. Liu, F. Shen, L. Shao, and X. Li. Zero-shot learning using synthesised unseen visual data with diffusion regularisation.
IEEE
Trans. Pattern Anal. Mach. Intell., 40(10):2498–2512, 2018.
 X. Lu, Y. Wang, X. Zhou, Z. Zhang, and Z. Ling. Traffic sign recognition via multi-modal tree-structure embedded multi-task learning. IEEE
Trans. Intelligent Transportation Systems, 18(4):960–972, 2017.
 Y. Lu, A. Kumar, S. Zhai, Y. Cheng, T. Javidi, and R. Feris. Fullyadaptive feature sharing in multi-task networks with applications in person attribute classification.
In IEEE Conf. Comput. Vis. Pattern
Recognit., pages 5334–5343, 2017.
 A. Maas, A. Hannun, and A. Ng.
Rectifier nonlinearities improve neural network acoustic models. In Int. Conf. Mach. Learn., 2013.
 J. Malmaud, J. Huang, V. Rathod, N. Johnston, A. Rabinovich, and K. Murphy. What's cookin'? interpreting cooking videos using text, speech and vision. arXiv:1503.01558, 2015.
 R. Mammone, X. Zhang, and R. Ramachandran. Robust speaker recognition: A feature-based approach. IEEE Signal Processing Magazine, 13(5):58–71, 1996.
 Y. Mansour, M. Mohri, and A. Rostamizadeh. Domain adaptation with multiple sources. In Advances Neural Inf. Process. Syst., pages 1041–
 H. Masnadi-Shirazi and N. Vasconcelos. On the design of loss functions for classification: theory, robustness to outliers, and savageboost. In
Advances Neural Inf. Process. Syst., pages 1049–1056, 2009.
 M. Masud, J. Gao, L. Khan, J. Han, and B. Thuraisingham. Classification and novel class detection in concept-drifting data streams under time constraints. IEEE Trans. Know. Data Eng., 23(6):859–874, 2011.
 T. Mensink, J. Verbeek, F. Perronnin, and G. Csurka. Distance-based image classification: Generalizing to new classes at near-zero cost.
IEEE Trans. Pattern Anal. Mach. Intell., 35(11):2624–2637, 2013.
 J. Metzen, T. Genewein, V. Fischer, and B. Bischoff. On detecting adversarial perturbations. In Int. Conf. Learn. Representations, 2017.
 D. Miller and J. Browning. A mixture model and EM-based algorithm for class discovery, robust classification, and outlier rejection in mixed labeled/unlabeled data sets. IEEE Trans. Pattern Anal. Mach. Intell., 25(11):1468–1483, 2003.
 I. Misra, A. Shrivastava, A. Gupta, and M. Hebert.
Cross-stitch networks for multi-task learning. In IEEE Conf. Comput. Vis. Pattern
Recognit., pages 3994–4003, 2016.
 T. Mitchell, W. Cohen, E. Hruschka, and et al. Never-ending learning.
Communications of ACM, 61(5):103–115, 2018.
 T. Miyato, S. Maeda, M. Koyama, and S. Ishii. Virtual adversarial training: a regularization method for supervised and semi-supervised learning. IEEE Trans. Pattern Anal. Mach. Intell., 41(8):1979–1993, C. Molnar.
Interpretable Machine Learning: A Guide for Making
Black Box Models Explainable.
2019. https://christophm.github.io/ interpretable-ml-book/.
 S. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and P. Frossard. Universal adversarial perturbations. In IEEE Conf. Comput. Vis. Pattern Recognit., pages 1765–1773, 2017.
 S. Moosavi-Dezfooli, A. Fawzi, and P. Frossard. DeepFool: a simple and accurate method to fool deep neural networks.
In IEEE Conf.
Comput. Vis. Pattern Recognit., pages 2574–2582, 2016.
 S. Motiian, Q. Jones, S. Iranmanesh, and G. Doretto.
Few-shot adversarial domain adaptation. In Advances Neural Inf. Process. Syst., pages 6670–6680, 2017.
 K. Muller, S. Mika, G. Riitsch, K. Tsuda, and B. Scholkopf.
An introduction to kernel-based learning algorithms. IEEE Trans. Neural
Netw., 12:181–201, 2001.
 K. Murugesan, H. Liu, J. Carbonell, and Y. Yang. Adaptive smoothed online multi-task learning.
In Advances Neural Inf. Process. Syst., pages 4296–4304, 2016.
 A. Nagrani, S. Albanie, and A. Zisserman. Seeing voices and hearing faces: Cross-modal biometric matching. In IEEE Conf. Comput. Vis.
Pattern Recognit., pages 8427–8436, 2018.
 G. Nagy. State of the art in pattern recognition. Proc. IEEE, 56(5):836–
 N. Neverova, C. Wolf, G. Taylor, and F. Nebout. ModDrop: adaptive multi-modal gesture recognition.
IEEE Trans. Pattern Anal. Mach.
Intell., 38(8):1692–1706, 2016.
 A. Nguyen, J. Yosinski, and J. Clune. Deep neural networks are easily fooled: High confidence predictions for unrecognizable images.
In
IEEE Conf. Comput. Vis. Pattern Recognit., pages 427–436, 2015.
 M. Noroozi and P. Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In Eur. Conf. Comput. Vis., pages
69–84, 2016.
 S. Nowozin and C. Lampert.
Structured learning and prediction in computer vision. Foundations and Trends in Computer Graphics and Vision, 6(3-4):185–365, 2011.
 A. Oliver, A. Odena, C. Raffel, E. Cubuk, and I. Goodfellow. Realistic evaluation of deep semi-supervised learning algorithms. In Advances
Neural Inf. Process. Syst., 2018.
 O. Ostapenko, M. Puscas, T. Klein, P. Jahnichen, and M. Nabi.
Learning to remember: A synaptic plasticity driven framework for continual learning. In IEEE Conf. Comput. Vis. Pattern Recognit., 2019.
 S. Pan, I. Tsang, J. Kwok, and Q. Yang. Domain adaptation via transfer component analysis. IEEE Trans. Neural Netw., 22(2):199–210, 2011.
 S. Pan and Q. Yang. A survey on transfer learning. IEEE Trans. Know.
Data Eng., 22(10):1345–1359, 2009.
 N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. Celik, and A. Swami. Practical black-box attacks against machine learning. In
ACM Asia Conf. Comput. Communi. Secur., pages 506–519, 2017.
 N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. Celik, and A. Swami. The limitations of deep learning in adversarial settings.
In IEEE Eur. Symposium Security Privacy, pages 372–387, 2016.
 N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami. Distillation as a defense to adversarial perturbations against deep neural networks.
In IEEE Symposium Security Privacy, pages 582–597, 2016.
 G. Parisi, R. Kemker, J. Part, C. Kanan, and S. Wermter. Continual lifelong learning with neural networks: A review.
Neural Netw., 113:54–71, 2019.
 D. Park, L. Hendricks, Z. Akata, A. Rohrbach, B. Schiele, T. Darrell, and M. Rohrbach. Multimodal explanations: Justifying decisions and pointing to the evidence. In IEEE Conf. Comput. Vis. Pattern Recognit., pages 8779–8788, 2018.
 D. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell, and A. Efros. Context encoders: Feature learning by inpainting. In IEEE Conf. Comput. Vis.
Pattern Recognit., pages 2536–2544, 2016.
 G. Patrini, A. Rozza, A. Menon, R. Nock, and L. Qu. Making deep neural networks robust to label noise: A loss correction approach. In
IEEE Conf. Comput. Vis. Pattern Recognit., pages 1944–1952, 2017.
 P. Peng, Y. Tian, T. Xiang, Y. Wang, M. Pontil, and T. Huang. Joint semantic and latent attribute modelling for cross-class transfer learning.
IEEE Trans. Pattern Anal. Mach. Intell., 40(7):1625–1638, 2018.
 A. Pentina and C. Lampert. Lifelong learning with non-i.i.d. tasks. In
Advances Neural Inf. Process. Syst., pages 1540–1548, 2015.
 M. Pimentel, D. Clifton, L. Clifton, and L. Tarassenko. A review of novelty detection. Signal Processing, 99:215–249, 2014.
 M. Poo, J. Du, N. Ip, Z. Xiong, B. Xu, and T. Tan. China brain project: basic neuroscience, brain diseases, and brain-inspired computing. Neuron, 92(3):591–596, 2016.
 F. Provost and T. Fawcett. Robust classification for imprecise environments. Machine Learning, 42(3):203–231, 2001.
 H. Qi, M. Brown, and D. Lowe. Low-shot learning with imprinted weights. In IEEE Conf. Comput. Vis. Pattern Recognit., pages 5822–
 R. Qiao, L. Liu, C. Shen, and A. Hengel.
Less is more: zero-shot learning from online textual documents with noise suppression.
In
IEEE Conf. Comput. Vis. Pattern Recognit., pages 2249–2257, 2016.
 L. Rabiner.
A tutorial on hidden Markov models and selected applications in speech recognition. Proc. IEEE, 77(2):257–286, 1989.
 A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever. Improving language understanding by generative pre-training. OpenAI Technical report, 2018.
 D. Ramachandram and G. Taylor. Deep multimodal learning: A survey on recent advances and trends.
IEEE Signal Processing Magazine, 34(6):96–108, 2017.
 A. Rasmus, H. Valpola, M. Honkala, M. Berglund, and T. Raiko. Semisupervised learning with ladder networks.
In Advances Neural Inf.
Process. Syst., pages 3546–3554, 2015.
 S. Rastegar, M. Baghshah, H. Rabiee, and S. Shojaee.
MDL-CW:
A multimodal deep learning framework with cross weights. In IEEE
Conf. Comput. Vis. Pattern Recognit., pages 2601–2609, 2016.
 S. Ravi and H. Larochelle.
Optimization as a model for few-shot learning. In Int. Conf. Learn. Representations, 2017.
 S. Rebuffi, A. Kolesnikov, G. Sperl, and C. Lampert. iCaRL: Incremental classifier and representation learning. In IEEE Conf. Comput.
Vis. Pattern Recognit., pages 2001–2010, 2017.
 B. Recht, R. Roelofs, L. Schmidt, and V. Shankar.
Do CIFAR-10 classifiers generalize to CIFAR-10? arXiv:1806.00451, 2018.
 S. Reed, Z. Akata, X. Yan, L. Logeswaran, H. Lee, and B. Schiele.
Generative adversarial text to image synthesis.
In Int. Conf. Mach.
Learn., pages 1060–1069, 2016.
 S. Reed, H. Lee, D. Anguelov, C. Szegedy, D. Erhan, and A. Rabinovich. Training deep neural networks on noisy labels with bootstrapping. In Workshop Int. Conf. Learn. Representations, 2015.
 S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: towards realtime object detection with region proposal networks.
IEEE Trans.
Pattern Anal. Mach. Intell., 39(6):1137–1149, 2017.
 B. RichardWebster, S. Anthony, and W. Scheirer.
PsyPhy: A psychophysics driven evaluation framework for visual recognition. IEEE
Trans. Pattern Anal. Mach. Intell., 41(9):2280–2286, 2019.
 S. Rifai, P. Vincent, X. Muller, X. Glorot, and Y. Bengio. Contractive auto-encoders: Explicit invariance during feature extraction.
In Int.
Conf. Mach. Learn., page 2011, 833–840.
 A. Rozantsev, M. Salzmann, and P. Fua. Residual parameter transfer for deep domain adaptation. In IEEE Conf. Comput. Vis. Pattern Recognit., pages 4339–4348, 2018.
 A. Rozantsev, M. Salzmann, and P. Fua.
Beyond sharing weights for deep domain adaptation. IEEE Trans. Pattern Anal. Mach. Intell., 41(4):801–814, 2019.
 S. Ruder. An overview of multi-task learning in deep neural networks. arXiv:1706.05098, 2017.
 M. Sajjadi, M. Javanmardi, and T. Tasdizen.
Regularization with stochastic transformations and perturbations for deep semi-supervised learning. In Advances Neural Inf. Process. Syst., pages 1163–1171, T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen. Improved techniques for training GANs. In Advances Neural
Inf. Process. Syst., pages 2234–2242, 2016.
 P. Samangouei, M. Kabkab, and R. Chellappa. Defense-GAN: Protecting classifiers against adversarial attacks using generative models. In
Int. Conf. Learn. Representations, 2018.
 N. Samsudin and A. Bradley. Nearest neighbour group-based classification. Pattern Recognition, 43(10):3458–3467, 2010.
 A. Santoro, D. Raposo, D. Barrett, M. Malinowski, R. Pascanu, P. Battaglia, and T. Lillicrap.
A simple neural network module for relational reasoning.
In Advances Neural Inf. Process. Syst., pages
4967–4976, 2017.
 G. Saon and M. Picheny. Recent advances in conversational speech recognition using convolutional and recurrent neural networks. IBM J.
Research Development, 61(4):1–10, 2017.
 P. Sarkar and G. Nagy.
Style consistent classification of isogenous patterns. IEEE Trans. Pattern Anal. Mach. Intell., 27(1):88–98, 2005.
 F. Scarselli, M. Gori, A. Tsoi, M. Hagenbuchner, and G. Monfardini.
The graph neural network model.
IEEE Trans. Neural Networks, 20(1):61–80, 2009.
 W. Scheirer, L. Jain, and T. Boult. Probability models for open set recognition.
IEEE Trans. Pattern Anal. Mach. Intell., 36(11):2317–
 W. Scheirer, A. Rocha, A. Sapkota, and T. Boult. Toward open set recognition. IEEE Trans. Pattern Anal. Mach. Intell., 35(7):1757–1772, J. Schmidhuber.
Deep learning in neural networks: An overview.
Neural Netw., 61:85–117, 2015.
 B. Scholkopf, J. Platt, J. Shawe-Taylor, and A. Smola.
Estimating the support of a high-dimensional distribution. Neural Computation, 13(7):1443–1471, 2001.
 T. Serre, L. Wolf, S. Bileschi, M. Riesenhuber, and T. Poggio. Robust object recognition with cortex-like mechanisms. IEEE Trans. Pattern
Anal. Mach. Intell., 29(3):411–426, 2007.
 U. Shaham, Y. Yamada, and S. Negahban. Understanding adversarial training: Increasing local stability of neural nets through robust optimization. arXiv:1511.05432, 2015.
 G. Shakhnarovich, J. Fisher, and T. Darrell.
Face recognition from long-term observations. In Eur. Conf. Comput. Vis., pages 851–865, B. Shi, X. Bai, and C. Yao. An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition.
IEEE Trans. Pattern Anal. Mach. Intell., 39(11):2298–
 H. Shimodaira. Improving predictive inference under covariate shift by weighting the log-likelihood function. J. Stat. Plan. Infer., 90(2):227–
 L. Shu, H. Xu, and B. Liu.
DOC: Deep open classification of text documents.
In Conf. Empirical Methods in Natural Language
Processing, pages 2911–2916, 2017.
 A. Smola, A. Gretton, L. Song, and B. Scholkopf. A Hilbert space embedding for distributions. In Int. Conf. Algor. Learn. Theory, pages
13–31, 2007.
 J. Snell, K. Swersky, and R. Zemel. Prototypical networks for fewshot learning. In Advances Neural Inf. Process. Syst., pages 4080–4090, J. Springenberg.
Unsupervised and semi-supervised learning with categorical generative adversarial networks.
In Int. Conf. Learn.
Representations, 2016.
 N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov.
Dropout: A simple way to prevent neural networks from overfitting. J. Mach. Learn. Res., 15(1):1929–1958, 2014.
 N. Srivastava and R. Salakhutdinov. Multimodal learning with deep
Boltzmann machines. In Advances Neural Inf. Process. Syst., pages
2222–2230, 2012.
 J. Su, D. Vargas, and K. Sakurai. One pixel attack for fooling deep neural networks. arXiv:1710.08864, 2017.
 C. Suen. N-gram statistics for natural language understanding and text processing.
IEEE Trans. Pattern Anal. Mach. Intell., 1(2):164–172, M. Sugiyama, S. Nakajima, H. Kashima, P. Bunau, and M. Kawanabe.
Direct importance estimation with model selection and its application to covariate shift adaptation. In Advances Neural Inf. Process. Syst., pages 1433–1440, 2008.
 S. Sukhbaatar, J. Bruna, M. Paluri, L. Bourdev, and R. Fergus. Training convolutional networks with noisy labels.
In Workshop Int. Conf.
Learn. Representations, 2015.
 Y. Sun, X. Wang, and X. Tang.
Hybrid deep learning for face verification.
IEEE Trans. Pattern Anal. Mach. Intell., 38(10):1997–
 Z. Sun and T. Tan. Ordinal measures for iris recognition. IEEE Trans.
Pattern Anal. Mach. Intell., 31(12):2211–2226, 2009.
 F. Sung, Y. Yang, L. Zhang, T. Xiang, P. Torr, and T. Hospedales.
Learning to compare: Relation network for few-shot learning. In IEEE
Conf. Comput. Vis. Pattern Recognit., pages 1199–1208, 2018.
 I. Sutskever, O. Vinyals, and Q. Le. Sequence to sequence learning with neural networks. In Advances Neural Inf. Process. Syst., pages
3104–3112, 2014.
 C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In IEEE Conf. Comput. Vis. Pattern Recognit., 2015.
 C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing properties of neural networks. In Int.
Conf. Learn. Representations, 2014.
 Y. Taigman, M. Yang, M. Ranzato, and L. Wolf. Deepface: Closing the gap to human-level performance in face verification. In IEEE Conf.
Comput. Vis. Pattern Recognit., pages 1701–1708, 2014.
 D. Tanaka, D. Ikami, T. Yamasaki, and K. Aizawa. Joint optimization framework for learning with noisy labels. In IEEE Conf. Comput. Vis.
Pattern Recognit., pages 5552–5560, 2018.
 A. Tarvainen and H. Valpola. Mean teachers are better role models:
Weight-averaged consistency targets improve semi-supervised deep
26 learning results. In Advances Neural Inf. Process. Syst., pages 1195–
 D. Tax. One-class classification: concept-learning in the absence of counter-examples. Ph.D. Thesis, Delft University of Technology, 2001.
 D. Tax and R. Duin.
Support vector data description.
Machine
Learning, 54(1):45–66, 2004.
 D. Tax and R. Duin. Growing a multi-class classifier with a reject option. Pattern Recognit. Lett., 29(10):1565–1570, 2008.
 J. Tenenbaum and W. Freeman.
Separating style and content with bilinear models. Neural Computation, 12(6):1247–1283, 2000.
 S. Thrun and J. O'Sullivan. Discovering structure in multiple learning tasks: The TC algorithm. In Int. Conf. Mach. Learn., pages 489–497, Y. Tokozume, Y. Ushiku, and T. Harada. Between-class learning for image classification. In IEEE Conf. Comput. Vis. Pattern Recognit., pages 5486–5494, 2018.
 E. Triantafillou, R. Zemel, and R. Urtasun. Few-shot learning through an information retrieval lens. In Advances Neural Inf. Process. Syst., pages 2255–2265, 2017.
 I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun. Large margin methods for structured and interdependent output variables. J. Mach.
Learn. Res., 6:1453–1484, 2005.
 P. Turaga, A. Veeraraghavan, A. Srivastava, and R. Chellappa. Statistical computations on Grassmann and Stiefel manifolds for image and video-based recognition. IEEE Trans. Pattern Anal. Mach. Intell., 33(11):2273–2286, 2011.
 E. Tzeng, J. Hoffman, T. Darrell, and K. Saenko. Simultaneous deep transfer across domains and tasks. In Int. Conf. Comput. Vis., pages
4068–4076, 2015.
 E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell.
Adversarial discriminative domain adaptation. In IEEE Conf. Comput. Vis. Pattern
Recognit., pages 7167–7176, 2017.
 P. Utgoff. Incremental induction of decision trees. Machine Learning, 4:161–186, 1989.
 A. Vahdat.
Toward robustness against label noise in training deep discriminative neural networks. In Advances Neural Inf. Process. Syst., pages 5596–5605, 2017.
 V.N. Vapnik. Statistical Learning Theory. New York: John Wiley &
Sons, 1998.
 S. Veeramachaneni and G. Nagy.
Style context with second-order statistics. IEEE Trans. Pattern Anal. Mach. Intell., 27(1):14–22, 2005.
 S. Veeramachaneni and G. Nagy. Analytical results on style-constrained
Bayesian classification of pattern fields.
IEEE Trans. Pattern Anal.
Mach. Intell., 29(7):1280–1285, 2007.
 P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y. Bengio.
Graph attention networks.
In Int. Conf. Learn. Representations, 2018.
 V. Verma, G. Arora, A. Mishra, and P. Rai.
Generalized zero-shot learning via synthesized examples. In IEEE Conf. Comput. Vis. Pattern
Recognit., pages 4281–4289, 2018.
 P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol. Extracting and composing robust features with denoising autoencoders. In Int.
Conf. Mach. Learn., pages 1096–1103, 2008.
 O. Vinyals, C. Blundell, T. Lillicrap, K. Kavukcuoglu, and D. Wierstra.
Matching networks for one shot learning.
In Advances Neural Inf.
Process. Syst., page 2016, 3630–3638.
 W. Wan, Y. Zhong, T. Li, and J. Chen. Rethinking feature distribution for loss functions in image classification. In IEEE Conf. Comput. Vis.
Pattern Recognit., pages 9117–9126, 2018.
 A. Wang, J. Cai, J. Lu, and T.-J. Cham. MMSS: Multi-modal sharable and specific feature learning for RGB-D object recognition. In Int.
Conf. Comput. Vis., pages 1125–1133, 2015.
 H. Wang, A. Kl¨aser, C. Schmid, and C.-L. Liu. Dense trajectories and motion boundary descriptors for action recognition. Int. J. Comput.
Vis., 103(1):60–79, 2013.
 L. Wang, T. Tan, H. Ning, and W. Hu. Silhouette analysis-based gait recognition for human identification. IEEE Trans. Pattern Anal. Mach.
Intell., 25(12):1505–1518, 2003.
 M. Wang and W. Deng. Deep visual domain adaptation: A survey. arXiv:1802.03601, 2018.
 Q.-F. Wang, F. Yin, and C.-L. Liu. Handwritten Chinese text recognition by integrating multiple contexts. IEEE Trans. Pattern Anal. Mach.
Intell., 34(8):1469–1481, 2012.
 X. Wang and A. Gupta. Unsupervised learning of visual representations using videos. In Int. Conf. Comput. Vis., pages 2794–2802, 2015.
 Y. Wang, W. Liu, X. Ma, J. Bailey, H. Zha, L. Song, and S.-T. Xia.
Iterative learning with open-set noisy labels. In IEEE Conf. Comput.
Vis. Pattern Recognit., pages 8688–8696, 2018.
 Y.-X. Wang, R. Girshick, M. Hebert, and B. Hariharan.
Low-shot learning from imaginary data.
In IEEE Conf. Comput. Vis. Pattern
Recognit., pages 7278–7286, 2018.
 Y.-X. Wang and M. Hebert.
Learning to learn: Model regression networks for easy small sample learning. In Eur. Conf. Comput. Vis., pages 616–634, 2016.
 Y. Wu and Y. Liu. Robust truncated hinge loss support vector machines.
J. American Statistical Association, 102(479):974–983, 2007.
 Z. Wu, Y. Xiong, S. Yu, and D. Lin. Unsupervised feature learning via non-parametric instance discrimination. In IEEE Conf. Comput. Vis.
Pattern Recognit., pages 3733–3742, 2018.
 Y. Xian, C. Lampert, B. Schiele, and Z. Akata. Zero-shot learning:
A comprehensive evaluation of the good, the bad and the ugly. IEEE
Trans. Pattern Anal. Mach. Intell., 41(9):2251–2265, 2019.
 Y. Xian, T. Lorenz, B. Schiele, and Z. Akata.
Feature generating networks for zero-shot learning. In IEEE Conf. Comput. Vis. Pattern
Recognit., pages 5542–5551, 2018.
 T. Xiao, T. Xia, Y. Yang, C. Huang, and X. Wang. Learning from massive noisy labeled data for image classification.
In IEEE Conf.
Comput. Vis. Pattern Recognit., pages 2691–2699, 2015.
 G. Xu, B.-G. Hu, and J. Principe. Robust C-loss kernel classifiers.
IEEE Trans. Neural Netw. Learn. Syst., 29(3):510–522, 2018.
 K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhutdinov, R. Zemel, and Y. Bengio. Show, attend and tell: Neural image caption generation with visual attention.
In Int. Conf. Mach. Learn., pages
2048–2057, 2015.
 L. Xu, K. Crammer, and D. Schuurmans.
Robust support vector machine training via convex outlier ablation.
In AAAI Conf. Artif.
Intell., pages 536–542, 2006.
 R. Xu, Z. Chen, W. Zuo, J. Yan, and L. Lin. Deep cocktail network:
Multi-source unsupervised domain adaptation with category shift. In
IEEE Conf. Comput. Vis. Pattern Recognit., pages 3964–3973, 2018.
 S. Yan, D. Xu, B. Zhang, H.-J. Zhang, Q. Yang, and S. Lin. Graph embedding and extensions: A general framework for dimensionality reduction. IEEE Trans. Pattern Anal. Mach. Intell., 29(1):40–51, 2007.
 H.-M. Yang, X.-Y. Zhang, F. Yin, and C.-L. Liu. Robust classification with convolutional prototype learning.
In IEEE Conf. Comput. Vis.
Pattern Recognit., pages 3474–3482, 2018.
 J. Yang, D. Parikh, and D. Batra. Joint unsupervised learning of deep representations and image clusters. In IEEE Conf. Comput. Vis. Pattern
Recognit., pages 5147–5156, 2016.
 J. Yosinski, J. Clune, Y. Bengio, and H. Lipson. How transferable are features in deep neural networks? In Advances Neural Inf. Process.
Syst., pages 3320–3328, 2014.
 A. Zamir, A. Sax, W. Shen, L. Guibas, J. Malik, and S. Savarese.
Taskonomy: Disentangling task transfer learning.
In IEEE Conf.
Comput. Vis. Pattern Recognit., pages 3712–3722, 2018.
 C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals.
Understanding deep learning requires rethinking generalization. In Int. Conf.
Learn. Representations, 2017.
 D. Zhang and D. Shen.
Multi-modal multi-task learning for joint prediction of multiple regression and classification variables in Alzheimer's disease. NeuroImage, 59(2):895–907, 2012.
 H. Zhang, M. Cisse, Y. Dauphin, and D. Lopez-Paz. Mixup: Beyond empirical risk minimization. In Int. Conf. Learn. Representations, 2018.
 H. Zhang and V. Patel. Sparse representation-based open set recognition. IEEE Trans. Pattern Anal. Mach. Intell., 39(8):1690–1696, 2017.
 J. Zhang, W. Li, and P. Ogunbona. Transfer learning for cross-dataset recognition: A survey. arXiv:1705.04396, 2017.
 R. Zhang, P. Isola, and A. Efros. Colorful image colorization. In Eur.
Conf. Comput. Vis., pages 649–666, 2016.
 R. Zhang, P. Isola, and A. Efros. Split-brain autoencoders: Unsupervised learning by cross-channel prediction. In IEEE Conf. Comput.
Vis. Pattern Recognit., pages 1058–1067, 2017.
 T. Zhang.
Analysis of multi-stage convex relaxation for sparse regularization. J. Mach. Learn. Res., 11:1081–1107, 2010.
 X.-Y. Zhang, Y. Bengio, and C.-L. Liu. Online and offline handwritten Chinese character recognition: A comprehensive study and new benchmark. Pattern Recognition, 61:348–360, 2017.
 X.-Y. Zhang, K. Huang, and C.-L. Liu.
Pattern field classification with style normalized transformation.
In Int. Joint Conf. Artificial
Intelligence, pages 1621–1626, 2011.
 X.-Y. Zhang and C.-L. Liu.
Writer adaptation with style transfer mapping. IEEE Trans. Pattern Anal. Mach. Intell., 35(7):1773–1787, Y. Zhang, K. Lee, and H. Lee. Augmenting supervised neural networks with unsupervised objectives for large-scale image classification. In Int.
Conf. Mach. Learn., pages 612–621, 2016.
 Z. Zhang, M. Wang, Y. Huang, and A. Nehorai.
Aligning infinitedimensional covariance matrices in reproducing kernel hilbert spaces for domain adaptation. In IEEE Conf. Comput. Vis. Pattern Recognit., pages 3437–3445, 2018.
 Z. Zhang and K. Zhao. Low-rank matrix approximation with manifold regularization. IEEE Trans. Pattern Anal. Mach. Intell., 35(7):1717–
 J. Zhao, M. Mathieu, R. Goroshin, and Y. LeCun. Stacked what-where auto-encoders. In Workshop Int. Conf. Learn. Representations, 2016.
 S. Zheng, Y. Song, T. Leung, and I. Goodfellow.
Improving the robustness of deep neural networks via stability training.
In IEEE
Conf. Comput. Vis. Pattern Recognit., pages 4480–4488, 2016.
 B. Zhou, D. Bau, A. Oliva, and A. Torralba. Interpreting deep visual representations via network dissection.
IEEE Trans. Pattern Anal.
Mach. Intell., 41(9):2131–2145, 2019.
 D. Zhou, O. Bousquet, T. Lal, J. Weston, and B. Scholkopf. Learning with local and global consistency. In Advances Neural Inf. Process.
Syst., pages 321–328, 2004.
 J. Zhou, G. Cui, Z. Zhang, C. Yang, Z. Liu, and M. Sun. Graph neural networks: A review of methods and applications. arXiv:1812.08434, Z.-H. Zhou and M. Li. Tri-training: exploiting unlabeled data using three classifiers. IEEE Trans. Know. Data Eng., 17(11):1529–1541, Z.-H. Zhou, Y.-Y. Sun, and Y.-F. Li. Multi-instance learning by treating instances as non-iid samples. In Int. Conf. Mach. Learn., pages 1249–
 X. Zhu. Semi-supervised learning literature survey. Computer Science, University of Wisconsin-Madison, 2006.
 X. Zhu, X. Wu, and Q. Chen. Eliminating class noise in large datasets.
In Int. Conf. Mach. Learn., pages 920–927, 2003.
 Y. Zhu, M. Elhoseiny, B. Liu, X. Peng, and A. Elgammal. A generative adversarial approach for zero-shot learning from noisy texts. In IEEE
Conf. Comput. Vis. Pattern Recognit., pages 1004–1013, 2018.
 Y. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, and S. Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books.
In Int. Conf.
Comput. Vis., pages 19–27, 2015.Deep Learning of Representations:
Looking Forward
Yoshua Bengio
Department of Computer Science and Operations Research
Universit´e de Montr´eal, Canada
Abstract. Deep learning research aims at discovering learning algorithms that discover multiple levels of distributed representations, with higher levels representing more abstract concepts. Although the study of deep learning has already led to impressive theoretical results, learning algorithms and breakthrough experiments, several challenges lie ahead. This paper proposes to examine some of these challenges, centering on the questions of scaling deep learning algorithms to much larger models and datasets, reducing optimization difficulties due to ill-conditioning or local minima, designing more efficient and powerful inference and sampling procedures, and learning to disentangle the factors of variation underlying the observed data. It also proposes a few forward-looking research directions aimed at overcoming these challenges.
Background on Deep Learning
Deep learning is an emerging approach within the machine learning research community. Deep learning algorithms have been proposed in recent years to move machine learning systems towards the discovery of multiple levels of representation. They have had important empirical successes in a number of traditional AI applications such as computer vision and natural language processing.
See (Bengio, 2009; Bengio et al., 2013d) for reviews and Bengio (2013c) and the other chapters of the book by Montavon and Muller (2012) for practical guidelines. Deep learning is attracting much attention both from the academic and industrial communities. Companies like Google, Microsoft, Apple, IBM and Baidu are investing in deep learning, with the first widely distributed products being used by consumers aimed at speech recognition. Deep learning is also used for object recognition(Google Goggles), image and music information retrieval (Google Image Search, Google Music), as well as computational advertising (Corrado, 2012). A deep learning building block (the restricted
Boltzmann machine, or RBM) was used as a crucial part of the winning entry of a million-dollar machine learning competition (the Netflix competition) (Salakhutdinov et al., 2007; T¨oscher et al., 2009). The New York Times covered the subject twice in 2012, with front-page articles.1 Another series of articles (including a third New York Times article) covered a more recent event showing off the application of deep learning in a major Kaggle competition for drug discovery (for example see "Deep Learning - The Biggest Data Science Breakthrough of the Decade"2. Much more recently, Google bought out ("acqui-hired") a company (DNNresearch) created by University of Toronto professor Geoffrey Hinton (the founder and leading researcher of deep learning) and two of his
PhD students, Ilya Sutskever and Alex Krizhevsky, with the press writing titles such as "Google
Hires Brains that Helped Supercharge Machine Learning" (Robert McMillan for Wired, March 13th, The performance of many machine learning methods is heavily dependent on the choice of data representation (or features) on which they are applied. For that reason, much of the actual effort in deploying machine learning algorithms goes into the design of preprocessing pipelines that result in a hand-crafted representation of the data that can support effective machine learning. Such feature
1 http://www.nytimes.com/2012/11/24/science/scientists-see-advancesin-deep-learning-a-part-of-artificial-intelligence.html
2 http://oreillynet.com/pub/e/2538
Y. Bengio engineering is important but labor-intensive and highlights the weakness of many traditional learning algorithms: their inability to extract and organize the discriminative information from the data.
Feature engineering is a way to take advantage of human ingenuity and prior knowledge to compensate for that weakness. In order to expand the scope and ease of applicability of machine learning, it would be highly desirable to make learning algorithms less dependent on feature engineering, so that novel applications could be constructed faster, and more importantly for the author, to make progress towards artificial intelligence (AI).
A representation learning algorithm discovers explanatory factors or features. A deep learning algorithm is a particular kind of representation learning procedure that discovers multiple levels of representation, with higher-level features representing more abstract aspects of the data. This area of research was kick-started in 2006 by a few research groups, starting with Geoff Hinton's group, who initially focused on stacking unsupervised representation learning algorithms to obtain deeper representations (Hinton et al., 2006; Bengio et al., 2007; Ranzato et al., 2007; Lee et al., 2008). Since then, this area has seen rapid growth, with an increasing number of workshops (now one every year at the NIPS and ICML conferences, the two major conferences in machine learning) and even a new specialized conference just created in 2013 (ICLR – the International Conference on Learning
Representations).
Transfer learning is the ability of a learning algorithm to exploit commonalities between different learning tasks in order to share statistical strength, and transfer knowledge across tasks. Among the achievements of unsupervised representation learning algorithms are the impressive successes they obtained at the two transfer learning challenges held in 2011. First, the Transfer Learning Challenge, presented at an ICML 2011 workshop of the same name, was won using unsupervised layer-wise pretraining (Bengio, 2011; Mesnil et al., 2011). A second Transfer Learning Challenge was held the same year and won by Goodfellow et al. (2011) using unsupervised representation learning. Results were presented at NIPS 2011's Challenges in Learning Hierarchical Models Workshop.
Quick Overview of Deep Learning Algorithms
The central concept behind all deep learning methodology is the automated discovery of abstraction, with the belief that more abstract representations of data such as images, video and audio signals tend to be more useful: they represent the semantic content of the data, divorced from the low-level features of the raw data (e.g., pixels, voxels, or waveforms). Deep architectures lead to abstract representations because more abstract concepts can often be constructed in terms of less abstract ones.
Deep learning algorithms are special cases of representation learning with the property that they learn multiple levels of representation. Deep learning algorithms often employ shallow (single-layer) representation learning algorithms as subroutines. Before covering the unsupervised representation learning algorithms, we quickly review the basic principles behind supervised representation learning algorithms such as the good old multi-layer neural networks. Supervised and unsupervised objectives can of course be combined (simply added, with a hyper-parameter as coefficient), like in Larochelle and Bengio (2008)'s discriminative RBM.
Deep Supervised Nets, Convolutional Nets, Dropout
Before 2006, it was believed that training deep supervised neural networks (Rumelhart et al., 1986) was too difficult (and indeed did not work). The first breakthrough in training them happened in Geoff Hinton's lab with unsupervised pre-training by RBMs (Hinton et al., 2006), as discussed in the next subsection. However, more recently, it was discovered that one could train deep supervised nets by proper initialization, just large enough for gradients to flow well and activations to convey useful information (Glorot and Bengio, 2010; Sutskever, 2012).3 Another interesting ingredient in the 3 and potentially with the use of momentum (Sutskever, 2012)
Deep Learning of Representations: Looking Forward
3 success of training the deep supervised networks of Glorot and Bengio (2010) (and later of Krizhevsky et al. (2012)) is the presence of rectifying non-linearities (such as max(0, x)) instead of sigmoidal nonlinearities (such as 1/(1+exp(−x)) or tanh(x)). See Jarrett et al. (2009); Nair and Hinton (2010) for earlier work on rectifier-like non-linearities. We return to this topic in Section 4. These good results with purely supervised training of deep nets seem to be especially clear when large quantities of labeled data are available, and it was demonstrated with great success for speech recognition (Seide et al., 2011a; Hinton et al., 2012a; Deng et al., 2013) and object recognition (Krizhevsky et al., 2012) with breakthroughs reducing the previous state-of-the-art error rates by 30% to 50% on difficult to beat benchmarks.
One of the key ingredients for success in the applications of deep learning to speech, images, and natural language processing (Bengio, 2008; Collobert et al., 2011) is the use of convolutional architectures (LeCun et al., 1998b), which alternate convolutional layers and pooling layers. Units on hidden layers of a convolutional network are associated with a spatial or temporal position and only depend on (or generate) the values in a particular window of the raw input. Furthermore, units on convolutional layers share parameters with other units of the same "type" located at different positions, while at each location one finds all the different types of units. Units on pooling layers aggregate the outputs of units at a lower layer, either aggregating over different nearby spatial positions (to achieve a form of local spatial invariance) or over different unit types. For example, a max-pooling unit outputs the maximum over some lower level units, which can therefore be seen to compete towards sending their signal forward.
Another key ingredient in the success of many recent breakthrough results in the area of object recognition is the idea of dropouts (Hinton et al., 2012b; Krizhevsky et al., 2012; Goodfellow et al., 2013b). Interestingly, it consists in injecting noise (randomly dropping out units with probability
1/2 from the neural network during training, and correspondingly multiplying by 1/2 the weights magnitude at test time) that prevents a too strong co-adaptation of hidden units: hidden units must compute a feature that will be useful even when half of the other hidden units are stochastically turned off (masked). This acts like a powerful regularizer that is similar to bagging aggregation but over an exponentially large number of models (corresponding to different masking patterns, i.e., subsets of the overall network) that share parameters.
Unsupervised or Supervised Layer-wise Pre-Training
One of the key results of recent years of research in deep learning is that deep compositions of non-linearities – such as found in deep feedforward networks or in recurrent networks applied over long sequences – can be very sensitive to initialization (some initializations can lead much better or much worse results after training). The first type of approaches that were found useful to reduce that sensitivity is based on greedy layer-wise pre-training (Hinton et al., 2006; Bengio et al., 2007).
The idea is to train one layer at a time, starting from lower layers (on top of the input), so that there is a clear training objective for the currently added layer (which typically avoids the need for back-propagating error gradients through many layers of non-linearities). With unsupervised pre-training, each layer is trained to model the distribution of values produced as output of the previous layer. As a side-effect of this training, a new representation is produced, which can be used as input for deeper layers. With the less common supervised pre-training (Bengio et al., 2007; Yu et al., 2010; Seide et al., 2011b), each additional layer is trained with a supervised objective (as part of a one hidden layer network). Again, we obtain a new representation (e.g., the hidden or output layer of the newly trained supervised model) that can be re-used as input for deeper layers.
The effect of unsupervised pre-training is apparently most drastic in the context of training deep auto-encoders (Hinton and Salakhutdinov, 2006), unsupervised learners that learn to reconstruct their input: unsupervised pre-training allows to find much lower training and test reconstruction error.
Y. Bengio
Directed and Undirected Graphical Models with Anonymous Latent Variables
Anonymous latent variables are latent variables that do not have a predefined semantics in terms of predefined human-interpretable concepts. Instead they are meant as a means for the computer to discover underlying explanatory factors present in the data. We believe that although non-anonymous latent variables can be very useful when there is sufficient prior knowledge to define them, anonymous latent variables are very useful to let the machine discover complex probabilistic structure: they lend flexibility to the model, allowing an otherwise parametric model to non-parametrically adapt to the amount of data when more anonymous variables are introduced in the model.
Principal components analysis (PCA), independent components analysis (ICA), and sparse coding all correspond to a directed graphical model in which the observed vector x is generated by first independently sampling some underlying factors (put in vector h) and then obtaining x by Wh plus some noise. They only differ in the type of prior put on h, and the corresponding inference procedures to recover h (its posterior P(h | x) or expected value E[h | x]) when x is observed. Sparse coding tends to yield many zeros in the estimated vector h that could have generated the observed x. See section 3 of Bengio et al. (2013d) for a review of representation learning procedures based on directed or undirected graphical models.4 Section 2.5 describes sparse coding in more detail.
An important thing to keep in mind is that directed graphical models tend to enjoy the property that in computing the posterior, the different factors compete with each other, through the celebrated explaining away effect. Unfortunately, except in very special cases (e.g., when the columns of W are orthogonal, which eliminates explaining away and its need), this results in computationally expensive inference. Although maximum a posteriori (MAP) inference5 remains polynomial-time in the case of sparse coding, this is still very expensive, and unnecessary in other types of models (such as the stacked auto-encoders discussed below). In fact, exact inference becomes intractable for deeper models, as discussed in section 5.
Although RBMs enjoy tractable inference, this is obtained at the cost of a lack of explaining away between the hidden units, which could potentially limit the representational power of E[h | x] as a good representation for the factors that could have generated x. However, RBMs are often used as building blocks for training deeper graphical models such as the deep belief network (DBN) (Hinton et al., 2006) and sthe deep Boltzmann machine (DBM) (Salakhutdinov and Hinton, 2009), which can compensate for the lack of explaining away in the RBM hidden units via a rich prior (provided by the upper layers) which can introduce potentially complex interactions and competition between the hidden units. Note that there is explaining away (and intractable exact inference) iin DBNs and something analogous in DBMs.
Regularized Auto-Encoders
Auto-encoders include in their training criterion a form of reconstruction oerror, such as ||r(x)−x||2, where r(·) is the learned reconstruction function, often decomposed as r(x) = g(f(x)) where f(·) is an encoding function and g(·) a decoding function. The idea is that auto-encoders should have low reconstruction error at the training examples, but high reconstruction error in most other configurations of the input. In the case of auto-encoders, good generalization means that test examples(sampled from the same distribution as training examples) also get low reconstruction error. Autoencoders have to be regularized to prevent them from simply learning the identity function r(x) = x, which would be useless. Regularized auto-encoders include the old bottleneck auto-encoders (like in PCA) with less hidden units than input, as well as the denoising auto-encoders (Vincent et al., 2008) and contractive auto-encoders (Rifai et al., 2011a). The denoising auto-encoder takes a noisy version N(x) of original input x and tries to reconstruct x, e.g., it minimizes ||r(N(x)) − x||2. The contractive auto-encoder has a regularization penalty in addition to the reconstruction error, trying to make hidden units f(x) as constant as possible with respect to x (minimizing the contractive
4 Directed and undirected: just two different views on the semantics of probabilistic models, not mutually exclusive, but views that are more convenient for some models than others.
5 finding h that approximately maximizes P(h | x)
Deep Learning of Representations: Looking Forward
5 penalty || ∂f(x)
∂x ||2
F ). A Taylor expansion of the denoising error shows that it is also approximately equivalent to minimizing reconstruction error plus a contractive penalty on r(·) (Alain and Bengio, 2013). As explained in Bengio et al. (2013d), the tug-of-war between minimization of reconstruction error and the regularizer means that the intermediate representation must mostly capture the variations necessary to distinguish training examples, i.e., the directions of variations on the manifold(a lower dimensional region) near which the data generating distribution concentrates. Score matching (Hyv¨arinen, 2005) is an inductive principle that can be an interesting alternative to maximum likelihood, and several nconnections have been drawn between reconstruction error in auto-encoders and score matching (Swersky et al., 2011). It has also been shown that denoising auto-encoders and some forms of contractive auto-encoders estimate the score6 of the underlying data generating distribution (Vincent, 2011; Alain and Bengio, 2013). This can be used to endow regularized auto-encoders with a probabilistic interpretation and to sample from the implicitly learned density models (Rifai et al., 2012b; Bengio et al., 2012; Alain and Bengio, 2013) through some variant of Langevin or Metropolis-Hastings Monte-Carlo Markov chains (MCMC). More recently, the results from Alain and Bengio (2013) have been generalized: whereas the score estimation result was only valid for asymptotically small Gaussian corruption noise, squared reconstruction error, and continuous inputs, the result from Bengio et al. (2013c) is applicable for any type of input, any form of reconstruction loss (so long as it is a negative log-likelihood), any form of corruption (so long as it prevents learning the identity mapping) and does not depend on the level of corruption noise going to zero.
Even though there is a probabilistic interpretation to regularized auto-encoders, this interpretation does not involve the definition of intermediate anonymous latent variables. Instead, they are based on the construction of a direct parametrization of an encoding function which immediately maps an input x to its representation f(x), and they are motivated by geometrical considerations in the spirit of manifold learning algorithms (Bengio et al., 2013d). Consequently, there is no issue of tractability of inference, even with deep auto-encoders obtained by stacking single-layer ones. This is true even in the recently proposed multi-layer versions of the denoising auto-encoders (Bengio and Thibodeau-Laufer, 2013) in which noise is injected not just in input, but in hidden units (like in the Gibbs chain of a deep Boltzmann machine).
It was previously believed (Ranzato et al., 2008), including by the author himself, that reconstruction error should only be small where the estimated density has a peak, e.g., near the data.
However, recent theoretical and empirical results (Alain and Bengio, 2013) show that the reconstruction error will be small where the estimated density has a peak (a mode) but also where it has a trough (a minimum). This is because the reconstruction error vector (reconstruction minus input) estimates the score ∂ log p(x)
∂x, i.e., the reconstruction error is small where || ∂ log p(x)
∂x
|| is small. This can happen at a local maximum but also at a local minimum (or saddle point) of the estimated density. This argues against using reconstruction error itself as an energy function,7 which should only be low near high probability points.
Sparse Coding and PSD
Sparse coding (Olshausen and Field, 1996) is a particular kind of directed graphical model with a linear relationship between visible and latent variables (like in PCA), but in which the latent variables have a prior (e.g., Laplace density) that encourages sparsity (many zeros) in the MAP posterior. Sparse coding is not actually very good as a generative model, but has been very successful for unsupervised feature learning (Raina et al., 2007; Coates and Ng, 2011; Yu et al., 2011; Grosse et al., 2007; Jenatton et al., 2009; Bach et al., 2011). See Bengio et al. (2013d) for a brief overview in the context of deep learning, along with connections to other unsupervised representation learning algorithms. Like other directed graphical models, it requires somewhat expensive inference, but the 6 derivative of the log-density with respect to the data; this is different from the usual definition of score in statistics, where the derivative is with respect to the parameters
7 To define energy, we write probability as the normalized exponential of minus the energy.
Y. Bengio good news is that for sparse coding, MAP inference is a convex optimization problem for which several fast approximations have been proposed (Mairal et al., 2009; Gregor and LeCun, 2010a). It is interesting to note the results obtained by Coates and Ng (2011) which suggest that sparse coding is a better encoder but not a better learning algorithm than RBMs and sparse auto-encoders (none of which has explaining away). Note also that sparse coding can be generalized into the spike-and-slab sparse coding algorithm (Goodfellow et al., 2012), in which MAP inference is replaced by variational inference, and that was used to win the NIPS 2011 transfer learning challenge (Goodfellow et al., Another interesting variant on sparse coding is the predictive sparse coding (PSD) algorithm (Kavukcuoglu et al., 2008) and its variants, which combine properties of sparse coding and of auto-encoders. Sparse coding can be seen as having only a parametric "generative" decoder (which maps latent variable values to visible variable values) and a non-parametric encoder (find the latent variables value that minimizes reconstruction error and minus the log-prior on the latent variable). PSD adds a parametric encoder (just an affine transformation followed by a non-linearity) and learns it jointly with the generative model, such that the output of the parametric encoder is close to the latent variable values that reconstructs well the input.
Scaling Computations
From a computation point of view, how do we scale the recent successes of deep learning to much larger models and huge datasets, such that the models are actually richer and capture a very large amount of information?
Scaling Computations: The Challenge
The beginnings of deep learning in 2006 have focused on the MNIST digit image classification problem (Hinton et al., 2006; Bengio et al., 2007), breaking the supremacy of SVMs (1.4% error) on this dataset.8 The latest records are still held by deep networks: Ciresan et al. (2012) currently claim the title of state-of-the-art for the unconstrained version of the task (e.g., using a convolutional architecture and stochastically deformed data), with 0.27% error.
In the last few years, deep learning has moved from digits to object recognition in natural images, and the latest breakthrough has been achieved on the ImageNet dataset.9 bringing down the state-of-the-art error rate (out of 5 guesses) from 26.1% to 15.3% (Krizhevsky et al., 2012)
To achieve the above scaling from 28×28 grey-level MNIST images to 256×256 RGB images, researchers have taken advantage of convolutional architectures (meaning that hidden units do not need to be connected to all units at the previous layer but only to those in the same spatial area, and that pooling units reduce the spatial resolution as we move from lower to higher layers). They have also taken advantage of GPU technology to speed-up computation by one or two orders of magnitude (Raina et al., 2009; Bergstra et al., 2010, 2011; Krizhevsky et al., 2012).
We can expect computational power to continue to increase, mostly through increased parallelism such as seen in GPUs, multicore machines, and clusters. In addition, computer memory has become much more affordable, allowing (at least on CPUs) to handle potentially huge models (in terms of capacity).
However, whereas the task of recognizing handwritten digits is solved to the point of achieving roughly human-level performance, this is far from true for tasks such as general object recognition, scene understanding, speech recognition, or natural language understanding. What is needed to nail those tasks and scale to even more ambitious ones?
8 for the knowledge-free version of the task, where no image-specific prior is used, such as image deformations or convolutions, where the current state-of-the-art is around 0.8% and involves deep learning (Rifai et al., 2011b; Hinton et al., 2012b).
9 The 1000-class ImageNet benchmark, whose results are detailed here: http://www.image-net.org/challenges/LSVRC/2012/ results.html
Deep Learning of Representations: Looking Forward
As we approach AI-scale tasks, it should become clear that our trained models will need to be much larger in terms of number of parameters. This is suggested by two observations. First, AI means understanding the world around us at roughly the same level of competence as humans.
Extrapolating from the current state of machine learning, the amount of knowledge this represents is bound to be large, many times more than what current models can capture. Second, more and more empirical results with deep learning suggest that larger models systematically work better (Coates et al., 2011; Hinton et al., 2012b; Krizhevsky et al., 2012; Goodfellow et al., 2013b), provided appropriate regularization is used, such as the dropouts technique described above.
Part of the challenge is that the current capabilities of a single computer are not sufficient to achieve these goals, even if we assume that training complexity would scale linearly with the complexity of the task. This has for example motivated the work of the Google Brain team (Le et al., 2012; Dean et al., 2012) to parallelize training of deep nets over a very large number of nodes.
As we will see in Section 4, we hypothesize that as the size of the models increases, our current ways of training deep networks become less and less efficient, so that the computation required to train larger models (to capture correspondingly more information) is likely to scale much worse than linearly (Dauphin and Bengio, 2013).
Another part of the challenge is that the increase in computational power has been mostly coming(and will continue to come) from parallel computing. Unfortunately, when considering very large datasets, our most efficient training algorithms for deep learning (such as variations on stochastic gradient descent or SGD) are inherently sequential (each update of the parameters requires having completed the previous update, so they cannot be trivially parallelized). Furthermore, for some tasks, the amount of available data available is becoming so large that it does not fit on a disk or even on a file server, so that it is not clear how a single CPU core could even scan all that data (which seems necessary in order to learn from it and exploit all of it, if training is inherently sequential).
Scaling Computations: Solution Paths
Parallel Updates: Asynchronous SGD. One idea that we explored in Bengio et al. (2003) is that of asynchronous SGD: train multiple versions of the model in parallel, each running on a different node and seeing different subsets of the data (on different disks), but with an asynchronous lock-free sharing mechanism which keeps the different versions of the model not too far from each other. If the sharing were synchronous, it would be too inefficient because most nodes would spend their time waiting for the sharing to be completed and would be waiting for the slowest of the nodes. This idea has been analyzed theoretically (Recht et al., 2011) and successfully engineered on a grand scale recently at Google (Le et al., 2012; Dean et al., 2012). However, current large-scale implementations(with thousands of nodes) are still very inefficient (in terms of use of the parallel resources), mostly because of the communication bottleneck requiring to regularly exchange parameter values between nodes. The above papers also take advantage of a way to train deep networks which has been very successful for GPU implementations, namely the use of rather large minibatches (blocks of examples after which an update is performed), making some parallelization (across the examples in the minibatch) easier. One option, explored by Coates et al. (2012) is to use as building blocks for learning features algorithms such as k-means that can be run efficiently over large minibatches (or the whole data) and thus parallelized easily on a cluster (they learned 150,000 features on a cluster with only 30 machines).
Another interesting consideration is the optimization of trade-off between communication cost and computation cost in distributed optimization algorithms, e.g., as discussed in Tsianos et al.
Sparse Updates. One idea that we propose here is to change the learning algorithms so as to obtain sparse updates, i.e., for any particular minibatch there is only a small fraction of parameters that are updated. If the amount of sparsity in the update is large, this would mean that a much smaller fraction of the parameters need to be exchanged between nodes when performing an asynchronous
Y. Bengio
SGD10. Sparse updates could be obtained simply if the gradient is very sparse. This gradient sparsity can arise with approaches that select paths in the neural network. We already know methods which produce slightly sparse updates, such as dropouts (Hinton et al., 2012b),11 maxout (Goodfellow et al., 2013b)12 and other hard-pooling mechanisms, such as the recently proposed and very successful stochastic pooling (Zeiler and Fergus, 2013). These methods do not provide enough sparsity, but this could be achieved in two ways. First of all, we could choose to only pay attention to the largest elements of the gradient vector. Second, we could change the architecture along the lines proposed next.
Conditional Computation. A central idea (that applies whether one parallelizes or not) that we put forward is that of conditional computation: instead of dropping out paths independently and at random, drop them in a learned and optimized way. Decision trees remain some of the most appealing machine learning algorithms because prediction time can be on the order of the logarithm of the number of parameters. Instead, in most other machine learning predictors, scaling is linear (i.e., much worse). This is because decision trees exploit conditional computation: for a given example, as additional computations are performed, one can discard a gradually larger set of parameters (and avoid performing the associated computation). In deep learning, this could be achieved by combining truly sparse activations (values not near zero like in sparse auto-encoders, but actual zeros) and multiplicative connections whereby some hidden units gate other hidden units (when the gater output is zero it turns off the output of the gated unit). When a group A of hidden units has a sparse activation pattern (with many actual zeros) and it multiplicatively gates other hidden units
B, then only a small fraction of the hidden units in B may need to be actually computed, because we know that these values will not be used. Such gating is similar to what happens when a decision node of a decision tree selects a subtree and turns off another subtree. More savings can thus be achieved if units in B themselves gate other units, etc. The crucial difference with decision trees (and e.g., the hard mixture of experts we introduced a decade ago (Collobert et al., 2003)) is that the gating units should not be mutually exclusive and should instead form a distributed pattern. Indeed, we want to keep the advantages of distributed representations and avoid the limited local generalization suffered by decision trees (Bengio et al., 2010). With a high level of conditional computation, some parameters are used often (and are well tuned) whereas other parameters are used very rarely, requiring more data to estimate. A trade-off and appropriate regularization therefore needs to be established which will depend on the amount of training signals going into each parameter. Interestingly, conditional computation also helps to achieve sparse gradients, and the fast convergence of hard mixtures of experts (Collobert et al., 2003) provides positive evidence that a side benefit of conditional computation will be easier and faster optimization.
Another existing example of conditional computation and sparse gradients is with the first layer of neural language models, deep learning models for text data (Bengio et al., 2003; Bengio, 2008). In that case, there is one parameter vector per word in the vocabulary, but each sentence only "touches" the parameters associated with the words in the sentence. It works because the input can be seen as extremely sparse. The question is how to perform conditional computation in the rest of the model.
One issue with the other example we mentioned, hard mixtures of experts (Collobert et al., 2003), is that its training mechanism only make sense when the gater operates at the output layer.
In that case, it is easy to get a strong and clean training signal for the gater output: one can just evaluate what the error would have been if a different expert had been chosen, and train the gater to produce a higher output for the expert that would have produced the smallest error (or to reduce computation and only interrogate two experts, require that the gater correctly ranks their
10 although the gain would be reduced considerably in a minibatch mode, roughly by the size of the minibatch
11 where half of the hidden units are turned off, although clearly, this is not enough sparsity for reaching our objective; unfortunately, we observed that randomly and independently dropping a lot more than half of the units yielded substantially worse results
12 where in addition to dropouts, only one out of k filters wins the competition in max-pooling units, and only one half of those survives the dropouts masking, making the sparsity factor 2k
Deep Learning of Representations: Looking Forward
9 probability of being the best one). The challenge is how to produce training signals for gating units that operate in the middle of the model. One cannot just enumerate all the gating configurations, because in a distributed setting with many gating units, there will be an exponential number of configurations. Interestingly, this suggests introducing randomness in the gating process itself, e.g., stochastically choosing one or two choices out of the many that a group of gating units could take. This is interesting because this is the second motivation (after the success of dropouts as a regularizer) for re-introducing randomness in the middle of deep networks. This randomness would allow configurations that would otherwise not be selected (if only a kind of "max" dictated the gating decision) to be sometimes selected, thus allowing to accumulate a training signal about the value of this configuration, i.e., a training signal for the gater. The general question of estimating or propagating gradients through stochastic neurons is treated in another exploratory article (Bengio, 2013a), where it is shown that one can obtain an unbiased (but noisy) estimator of the gradient of a loss through a discrete stochastic decision. Another interesting idea explored in that paper is that of adding noise just before the non-linearity (max-pooling (maxi xi) or rectifier (max(0, x))). Hence the winner is not always the same, and when a choice wins it has a smooth influence on the result, and that allows a gradient signal to be provided, pushing that winner closer or farther from winning the competition on another example.
Optimization
Optimization: The Challenge
As we consider larger and larger datasets (growing faster than the size of the models), training error and generalization error converge. Furthermore many pieces of evidence in the results of experiments on deep learning suggest that training deep networks (including recurrent networks) involves a difficult optimization (Bengio, 2013b; Gulcehre and Bengio, 2013; Bengio et al., 2013a). It is not yet clear how much of the difficulty is due to local minima and how much is due to ill-conditioning(the two main types of optimization difficulties in continuous optimization problems). It is therefore interesting to study the optimization methods and difficulties involved in deep learning, for the sake of obtaining better generalization. Furthermore, better optimization could also have an impact on scaling computations, discussed above.
One important thing to keep in mind, though, is that in a deep supervised network, the top two layers (the output layer and the top hidden layer) can rather easily be made to overfit, simply by making the top hidden layer large enough. However, to get good generalization, what we have found is that one needs to optimize the lower layers, those that are far removed from the immediate supervised training signal (Bengio et al., 2007). These observations mean that only looking at the training criterion is not sufficient to assess that a training procedure is doing a good job at optimizing the lower layers well. However, under constraints on the top hidden layer size, training error can be a good guide to the quality of the optimization of lower layers. Note that supervised deep nets are very similar (in terms of the optimization problem involved) to deep auto-encoders and to recurrent or recursive networks, and that properly optimizing RBMs (and more so deep Boltzmann machines) seems more difficult: progress on training deep nets is therefore likely to be a key to training the other types of deep learning models.
One of the early hypotheses drawn from experiments with layer-wise pre-training as well as of other experiments (semi-supervised embeddings (Weston et al., 2008) and slow feature analysis (Wiskott and Sejnowski, 2002a; Bergstra and Bengio, 2009)) is that the training signal provided by backpropagated gradients is sometimes too weak to properly train intermediate layers of a deep network. This is supported by the observation that all of these successful techniques somehow inject a training signal into the intermediate layers, helping them to figure out what they should do. However, the more recent successful results with supervised learning on very large labeled datasets suggest that with some tweaks in the optimization procedure (including initialization), it is sometimes possible to achieve as good results with or without unsupervised pre-training or semi-supervised embedding intermediate training signals.
Y. Bengio
Optimization: Solution Paths
In spite of these recent encouraging results, several more recent experimental results again point to a fundamental difficulty in training intermediate and lower layers.
Diminishing Returns with Larger Networks. First, Dauphin and Bengio (2013) show that with well-optimized SGD training, as the size of a neural net increases, the "return on investment"(number of training errors removed per added hidden unit) decreases, given a fixed number of training iterations, until the point where it goes below 1 (which is the return on investment that would be obtained by a brain-dead memory-based learning mechanism – such as Parzen Windows – which just copies an incorrectly labeled example into the weights of the added hidden unit so as to produce just the right answer for that example only). This suggests that larger models may be fundamentally more difficult to train, probably because there are now more second-order interactions between the parameters, increasing the condition number of the Hessian matrix (of second derivatives of model parameters with respect to the training criterion). This notion of return on investment may provide a useful metric by which to measure the effect of different methods to improve the scaling behavior of training and optimization procedures for deep learning.
Intermediate Concepts Guidance and Curriculum. Second, Gulcehre and Bengio (2013) show that there are apparently simple tasks on which standard black-box machine learning algorithms completely fail. Even supervised and pre-trained deep networks were tested and failed at these tasks.
These tasks have in common the characteristic that the correct labels are obtained by the composition of at least two levels of non-linearity and abstraction: e.g., the first level involves the detection of objects in a scene and the second level involves a non-linear logical operation on top of these (such as the detecting presence of multiple objects of the same category). On the other hand, the task becomes easily solvable by a deep network whose intermediate layer is first pre-trained to solve the first-level sub-task. This raises the question of how humans might learn even more abstract tasks, and Bengio (2013b) studies the hypothesis that the use of language and the evolution of culture could have helped humans reduce that difficulty (and gain a serious advantage over other less cultured animals). It would be interesting to explore multi-agent learning mechanisms inspired by the the mathematical principles behind the evolution of culture in order to bypass this optimization difficulty. The basic idea is that humans (and current learning algorithms) are limited to "local descent" optimization methods, that make small changes in the parameter values with the effect of reducing the expected loss in average. This is clearly prone to the presence of local minima, while a more global search (in the spirit of both genetic and cultural evolution) could potentially reduce this difficulty. One hypothesis is that more abstract learning tasks involve more challenging optimization difficulties, which would make such global optimization algorithms necessary if we want computers to learn such abstractions from scratch. Another option, following the idea of curriculum learning (Bengio et al., 2009), is to provide guidance ourselves to learning machines (as exemplified in the toy example of Gulcehre and Bengio (2013)), by "teaching them" gradually more complex concepts to help them understand the world around us (keeping in mind that we also have to do that for humans and that it takes 20 years to complete).
Changing the learning procedure and the architecture. Regarding the basic optimization difficulty of a single deep network, three types of solutions should be considered. First, there are solutions based on improved general-purpose optimization algorithms, such as for example the recent work on adaptive learning rates (Schaul et al., 2012), online natural gradient (Le Roux et al., 2008;
Pascanu and Bengio, 2013) or large-minibatch second order methods (Martens, 2010).
Another class of attacks on the optimization problem is based on changing the architecture (family of functions and its parametrization) or the way that the outputs are produced (for example by adding noise). As already introduced in LeCun et al. (1998a), changes in the preprocessing, training objective and architecture can change the difficulty of optimization, and in particularly improve the Deep Learning of Representations: Looking Forward
11 conditioning of the Hessian matrix (of second derivatives of the loss with respect to parameters).
With gradient descent, training time into a quadratic bowl is roughly proportional to the condition number of the Hessian matrix (ratio of largest to smallest eigenvalue). For example LeCun et al.(1998a) recommends centering and normalizing the inputs, an idea recently extended to hidden layers of Boltzmann machines with success (Montavon and Muller, 2012). A related idea that may have an impact on ill-conditioning is the idea of skip-connections, which forces both the mean output and the mean slope of each hidden unit of a deep multilayer network to be zero (Raiko et al., 2012), a centering idea which originates from Schraudolph (1998).
There has also been very successful recent work exploiting rectifier non-linearities for deep supervised networks (Glorot et al., 2011a; Krizhevsky et al., 2012). Interestingly, such non-linearities can produce rather sparse unit outputs, which could be exploited, if the amount of sparsity is sufficiently large, to considerably reduce the necessary computation (because when a unit output is 0, there is no need to actually multiply it with its outgoing weights). Very recently, we have discovered a variant on the rectifier non-linearity called maxout (Goodfellow et al., 2013b) which appears to open a very promising door towards more efficient training of deep networks. As confirmed experimentally (Goodfellow et al., 2013b), maxout networks can train deeper networks and allow lower layers to undergo more training. The more general principle at stake here may be that when the gradient is sparse, i.e., only a small subset of the hidden units and parameters is touched by the gradient, the optimization problem may become easier. We hypothesize that sparse gradient vectors have a positive effect on reducing the ill-conditioning difficulty involved in training deep nets.
The intuition is that by making many terms of the gradient vector 0, one also knocks off many off-diagonal terms of the Hessian matrix, making this matrix more diagonal-looking, which would reduce many of the ill-conditioning effects involved, as explained below. Indeed, gradient descent relies on an invalid assumption: that one can modify a parameter θi (in the direction of the gradient
∂C
∂θi ) without taking into account the changes in ∂C
∂θi that will take place when also modifying other parameters θj. Indeed, this is precisely the information that is captured (e.g. with second-order methods) by the off-diagonal entries
∂2C
∂θi∂θj =
∂
∂θj
∂C
∂θi, i.e., how changing θj changes the gradient on θi. Whereas second-order methods may have their own limitations13 it would be interesting if substantially reduced ill-conditioning could be achieved by modifying the architecture and training procedure. Sparse gradients would be just one weapon in this line of attack.
As we have argued above, adding noise in an appropriate way can be useful as a powerful regularizer (as in dropouts), and it can also be used to make the gradient vector sparser, which would reinforce the above positive effect on the optimization difficulty. If some of the activations are also sparse (as our suggestions for conditional computation would require), then more entries of the gradient vector will be zeroed out, also reinforcing that beneficial optimization effect. In addition, it is plausible that the masking noise found in dropouts (as well as in denoising auto-encoders) encourages a faster symmetry-breaking: quickly moving away from the condition where all hidden units of a neural network or a Boltzmann machine do the same thing (due to a form of symmetry in the signals they receive), which is a non-attractive fixed point with a flat (up to several orders) likelihood function. This means that gradient descent can take a lot of time to pull apart hidden units which are behaving in a very similar way. Furthermore, when starting from small weights, these symmetry conditions (where many hidden units do something similar) are actually attractive from far away, because initially all the hidden units are trying to grab the easiest and most salient job (explain the gradients on the units at the layer above). By randomly turning off hidden units we obtain a faster specialization which helps training convergence.
A related concept that has been found useful in understanding and reducing the training difficulty of deep or recurrent nets is the importance of letting the training signals (back-propagated gradients) flow, in a focused way. It is important that error signals flow so that credit and blame is clearly assigned to different components of the model, those that could change slightly to improve the training loss. The problem of vanishing and exploding gradients in recurrent nets (Hochreiter, 1991;
13 first, practical implementations never come close to actually inverting the Hessian, and second, they often require line searches that may be computationally inefficient if the optimal trajectory is highly curved
Y. Bengio
Bengio et al., 1994) arises because the effect of a long series of non-linear composition tends to produce gradients that can either be very small (and the error signal is lost) or very large (and the gradient steps diverge temporarily). This idea has been exploited to propose successful initialization procedures for deep nets (Glorot and Bengio, 2010). A composition of non-linearities is associated with a product of Jacobian matrices, and a way to reduce the vanishing problem would be to make sure that they have a spectral radius (largest eigenvalue) close to 1, like what is done in the weight initialization for Echo State Networks (Jaeger, 2007) or in the carousel self-loop of LSTM (Hochreiter and Schmidhuber, 1997) to help propagation of influences over longer paths. A more generic way to avoid gradient vanishing is to incorporate a training penalty that encourages the propagated gradient vectors to maintain their magnitude (Pascanu and Bengio, 2012). When combined with a gradient clipping14 heuristic (Mikolov, 2012) to avoid the detrimental effect of overly large gradients, it allows to train recurrent nets on tasks on which it was not possible to train them before (Pascanu and Bengio, 2012).
Inference and Sampling
All of the graphical models studied for deep learning except the humble RBM require a non-trivial form of inference, i.e., guessing values of the latent variables h that are appropriate for the given visible input x. Several forms of inference have been investigated in the past: MAP inference is formulated like an optimization problem (looking for h that approximately maximizes P(h | x));
MCMC inference attempts to sample a sequence of h's from P(h | x); variational inference looks for a simple (typically factorial) approximate posterior qx(h) that is close to P(h | x), and usually involves an iterative optimization procedure. See a recent machine learning textbook for more details (Bishop, 2006; Barber, 2011; Murphy, 2012).
In addition, a challenge related to inference is sampling (not just from P(h | x) but also from
P(h, x) or P(x)), which like inference is often needed in the inner loop of learning algorithms for probabilistic models with latent variables, energy-based models (LeCun et al., 2006) or Markov
Random Fields (Kindermann, 1980) (also known as undirected graphical models), where P(x) or P(h, x) is defined in terms of a parametrized energy function whose normalized exponential gives probabilities.
Deep Boltzmann machines (Salakhutdinov and Hinton, 2009) combine the challenge of inference(for the "positive phase" where one tries to push the energies associated with the observed x down) and the challenge of sampling (for the "negative phase" where one tries to push up the energies associated with x's sampled from P(x)). Sampling for the negative phase is usually done by MCMC, although some learning algorithms (Collobert and Weston, 2008; Gutmann and Hyvarinen, 2010;
Bordes et al., 2013) involve "negative examples" that are sampled through simpler procedures (like perturbations of the observed input). In Salakhutdinov and Hinton (2009), inference for the positive phase is achieved with a mean-field variational approximation.15
Inference and Sampling: The Challenge
There are several challenges involved with all of the these inference and sampling techniques.
The first challenge is practical and computational: these are all iterative procedures that can considerably slow down training (because inference and/or sampling is often in the inner loop of learning).
14 When the norm of the gradient is above a threshold τ, reduce it to τ
15 In the mean-field approximation, computation proceeds like in Gibbs sampling, but with stochastic binary values replaced by their conditional expected value (probability of being 1), given the outputs of the other units. This deterministic computation is iterated like in a recurrent network until convergence is approached, to obtain a marginal (factorized probability) approximation over all the units.
Deep Learning of Representations: Looking Forward
Potentially Huge Number of Modes. The second challenge is more fundamental and has to do with the potential existence of highly multi-modal posteriors: all of the currently known approaches to inference and sampling are making very strong explicit or implicit assumptions on the form the distribution of interest (P(h | x) or P(h, x)). As we argue below, these approaches make sense if this target distribution is either approximately unimodal (MAP), (conditionally) factorizes (variational approximations, i.e., the different factors hi are approximately independent16 of each other given x), or has only a few modes between which it is easy to mix (MCMC). However, approximate inference can be potentially hurtful, not just at test time but for training, because it is often in the inner loop of the learning procedure (Kulesza and Pereira, 2008).
Imagine for example that h represents many explanatory variables of a rich audio-visual scene with a highly ambiguous raw input x, including the presence of several objects with ambiguous attributes or categories, such that one cannot really disambiguate one of the objects independently of the others (the so-called "structured output" scenario, but at the level of latent explanatory variables). Clearly, a factorized or unimodal representation would be inadequate (because these variables are not at all independent, given x) while the number of modes could grow exponentially with the number of ambiguous factors present in the scene. For example, consider a visual scene x through a haze hiding most details, yielding a lot of uncertainty. Say it involves 10 objects (e.g., people), each having 5 ambiguous binary attributes (out of 20) (e.g., how they are dressed) and uncertainty between 100 categorical choices for each element (e.g., out of 10000 persons in the database, the marginal evidence allows to reduce the uncertainty for each person to about 100 choices). Furthermore, suppose that these uncertainties cannot be factorized (e.g., people tend to be in the same room with other people involved in the same activity, and friends tend to stand physically close to each other, and people choose to dress in a way that socially coherent). To make life hard on mean-field and other factorized approximations, this means that only a small fraction (say 1%) of these configurations are really compatible. So one really has to consider 1% × (25 × 100)10 ≈ 1033 plausible configurations of the latent variables. If one has to take a decision y based on x, e.g., P(y | x) = � h P(y | h)P(h | x) involves summing over a huge number of non-negligible terms of the posterior P(h | x), which we can consider as modes (the actual dimension of h is much larger, so we have reduced the problem from (220 × 10000)10 ≈ 10100 to about 1033, but that is still huge. One way or another, summing explicitly over that many modes seems implausible, and assuming single mode (MAP) or a factorized distribution (mean-field) would yield very poor results. Under some assumptions on the underlying data-generating process, it might well be possible to do inference that is exact or a provably good approximations, and searching for graphical models with these properties is an interesting avenue to deal with this problem. Basically, these assumptions work because we assume a specific structure in the form of the underlying distribution. Also, if we are lucky, a few
Monte-Carlo samples from P(h | x) might suffice to obtain an acceptable approximation for our y, because somehow, as far as y is concerned, many probable values of h yield the same answer y and a Monte-Carlo sample will well represent these different "types" of values of h. That is one form of regularity that could be exploited (if it exists) to approximately solve that problem. What if these assumptions are not appropriate to solve challenging AI problems? Another, more general assumption (and thus one more likely to be appropriate for these problems) is similar to what we usually do with machine learning: although the space of functions is combinatorially large, we are able to generalize by postulating a rather large and flexible family of functions (such as a deep neural net). Thus an interesting avenue is to assume that there exists a computationally tractable function that can compute P(y | x) in spite of the apparent complexity of going through the intermediate steps involving h, and that we may learn P(y | x) through (x, y) examples. This idea will be developed further in Section 5.2.
Mixing Between Modes. What about MCMC methods? They are hurt by the problem of mode mixing, discussed at greater length in Bengio et al. (2013b), and summarized here. To make the 16 this can be relaxed by considering tree-structured conditional dependencies (Saul and Jordan, 1996) and mixtures thereof
Y. Bengio mental picture simpler, imagine that there are only two kinds of probabilities: tiny and high. MCMC transitions try to stay in configurations that have a high probability (because they should occur in the chain much more often than the tiny probability configurations). Modes can be thought of as islands of high probability, but they may be separated by vast seas of tiny probability configurations. Hence, it is difficult for the Markov chain of MCMC methods to jump from one mode of the distribution to another, when these are separated by large low-density regions embedded in a high-dimensional space, a common situation in real-world data, and under the manifold hypothesis (Cayton, 2005;
Narayanan and Mitter, 2010). This hypothesis states that natural classes present in the data (e.g., visual object categories) are associated with low-dimensional regions17 (i.e., manifolds) near which the distribution concentrates, and that different class manifolds are well-separated by regions of very low density. Here, what we consider a mode may be more than a single point, it could be a whole (low-dimensional) manifold. Slow mixing between modes means that consecutive samples tend to be correlated (belong to the same mode) and that it takes a very large number of consecutive sampling steps to go from one mode to another and even more to cover all of them, i.e., to obtain a large enough representative set of samples (e.g. to compute an expected value under the sampled variables distribution). This happens because these jumps through the low-density void between modes are unlikely and rare events. When a learner has a poor model of the data, e.g., in the initial stages of learning, the model tends to correspond to a smoother and higher-entropy (closer to uniform) distribution, putting mass in larger volumes of input space, and in particular, between the modes (or manifolds). This can be visualized in generated samples of images, that look more blurred and noisy18. Since MCMCs tend to make moves to nearby probable configurations, mixing between modes is therefore initially easy for such poor models. However, as the model improves and its corresponding distribution sharpens near where the data concentrate, mixing between modes becomes considerably slower. Making one unlikely move (i.e., to a low-probability configuration) may be possible, but making N such moves becomes exponentially unlikely in N. Making moves that are far and probable is fundamentally difficult in a high-dimensional space associated with a peaky distribution (because the exponentially large fraction of the far moves would be to an unlikely configuration), unless using additional (possibly learned) knowledge about the structure of the distribution.
Inference and Sampling: Solution Paths
Going into a space where mixing is easier. The idea of tempering (Iba, 2001) for MCMCs is analogous to the idea of simulated annealing (Kirkpatrick et al., 1983) for optimization, and it is designed for and looks very appealing to solve the mode mixing problem: consider a smooth version (higher temperature, obtained by just dividing the energy by a temperature greater than
1) of the distribution of interest; it therefore spreads probability mass more uniformly so one can mix between modes at that high temperature version of the model, and then gradually cool to the target distribution while continuing to make MCMC moves, to make sure we end up in one of the "islands" of high probability. Desjardins et al. (2010); Cho et al. (2010); Salakhutdinov (2010b,a) have all considered various forms of tempering to address the failure of Gibbs chain mixing in RBMs.
Unfortunately, convincing solutions (in the sense of making a practical impact on training efficiency) have not yet been clearly demonstrated. It is not clear why this is so, but it may be due to the need to spend much time at some specific (critical) temperatures in order to succeed. More work is certainly warranted in that direction.
An interesting observation (Bengio et al., 2013b) which could turn out to be helpful is that after we train a deep model such as a DBN or a stack of regularized auto-encoders, we can observe that mixing between modes is much easier at higher levels of the hierarchy (e.g. in the top-level
RBM or top-level auto-encoder): mixing between modes is easier at deeper levels of representation.
17 e.g. they can be charted with a few coordinates
18 See examples of generated images with some of the current state-of-the-art in learned generative models of images (Courville et al., 2011; Luo et al., 2013)
Deep Learning of Representations: Looking Forward
This is achieved by running the MCMC in a high-level representation space and then projecting back in raw input space to obtain samples at that level. The hypothesis proposed (Bengio et al., 2013b) to explain this observation is that unsupervised representation learning procedures (such as for the RBM and contractive or denoising auto-encoders) tend to discover a representation whose distribution has more entropy (the distribution of vectors in higher layers is more uniform) and that better "disentangles" or separates out the underlying factors of variation (see next section for a longer discussion of the concept of disentangling). For example, suppose that a perfect disentangling had been achieved that extracted the factors out of images of objects, such as object category, position, foreground color, etc. A single Gibbs step could thus switch a single top-level variable (like object category) when that variable is resampled given the others, a very local move in that top-level disentangled representation but a very far move (going to a very different place) in pixel space. Note that maximizing mutual information between inputs and their learned deterministic representation, which is what auto-encoders basically do (Vincent et al., 2008), is equivalent to maximizing the entropy of the learned representation,19 which supports this hypothesis. An interesting idea20 would therefore be to use higher levels of a deep model to help the lower layers mix better, by using them in a way analogous to parallel tempering, i.e., to suggest configurations sampled from a different mode.
Another interesting potential avenue for solving the problem of sampling from a complex and rough (non-smooth) distribution would be to take advantage of quantum annealing effects (Rose and Macready, 2007) and analog computing hardware (such as produced by D-Wave). NP-hard problems (such as sampling or optimizing exactly in an Ising model) still require exponential time but experimental evidence has shown that for some problems, quantum annealing is far superior to standard digital computation (Brooke et al., 2001). Since quantum annealing is performed by essentially implementing a Boltzmann machine in analog hardware, it might be the case that drawing samples from a Boltzmann machine is one problem where quantum annealing would be dramatically superior to classical digital computing.
Learning a Computational Graph that Does What we Want If we stick to the idea of obtaining actual values of the latent variables (either through MAP, factorized variational inference or MCMC), then a promising path is based on learning approximate inference, i.e., optimizing a learned approximate inference mechanism so that it performs a better inference faster. This idea is not new and has been shown to work well in many settings. This idea was actually already present in the wake-sleep algorithm (Hinton et al., 1995; Frey et al., 1996; Hinton et al., 2006) in the context of variational inference for Sigmoidal Belief Networks and DBNs. Learned approximate inference is also crucial in the predictive sparse coding (PSD) algorithm (Kavukcuoglu et al., 2008). This approach is pushed further with Gregor and LeCun (2010b) in which the parametric encoder has the same structural form as a fast iterative sparse coding approximate inference algorithm. The important consideration in both cases is not just that we have fast approximate inference, but that (a) it is learned, and (b) the model is learned jointly with the learned approximate inference procedure.
See also Salakhutdinov and Larochelle (2010) for learned fast approximate variational inference in DBMs, or Bagnell and Bradley (2009); Stoyanov et al. (2011) for learning fast approximate inference(with fewer steps than would otherwise be required by standard general purpose inference) based on loopy belief propagation.
The traditional view of probabilistic graphical models is based on the clean separation between modeling (defining the model), optimization (tuning the parameters), inference (over the latent variables) and sampling (over all the variables, and possibly over the parameters as well in the Bayesian scenario). This modularization has clear advantages but may be suboptimal. By bringing learning into inference and jointly learning the approximate inference and the "generative model" itself, one can hope to obtain "specialized" inference mechanisms that could be much more efficient and accurate than generic purpose ones; this was the subject of a recent ICML workshop (Eisner, 19 Salah Rifai, personal communication
20 Guillaume Desjardins, personal communication
Y. Bengio
2012). The idea of learned approximate inference may help deal with the first (purely computational) challenge raised above regarding inference, i.e., it may help to speed up inference to some extent, but it generally keeps the approximate inference parameters separate from the model parameters.
But what about the challenge from a huge number of modes? What if the number of modes is too large and/or these are too well-separated for MCMC to visit efficiently or for variational/MAP inference to approximate satisfactorily? If we stick to the objective of computing actual values of the latent variables, the logical conclusion is that we should learn to approximate a posterior that is represented by a rich multi-modal distribution. To make things concrete, imagine that we learn(or identify) a function f(x) of the visible variable x that computes the parameters θ = f(x) of an approximate posterior distribution Qθ=f(x)(h) but where Qθ=f(x)(h) ≈ P(h | x) can be highly multimodal, e.g., an RBM with visible variables h (coupled with additional latent variables used only to represent the richness of the posterior over h itself). Since the parameters of the RBM are obtained through a parametric computation taking x as input,21 this is really a conditional RBM (Taylor et al., 2007; Taylor and Hinton, 2009). Whereas variational inference is usually limited to a nonparametric approximation of the posterior, Q(h) (one that is analytically and iteratively optimized for each given x) one could consider a parametric approximate posterior that is learned (or derived analytically) while allowing for a rich multi-modal representation (such as what an RBM can capture, i.e., up to an exponential number of modes).
Avoiding inference and explicit marginalization over latent variables altogether. We now propose to consider an even more radical departure from traditional thinking regarding probabilistic models with latent variables. It is motivated by the observation that even with the last proposal, something like a conditional RBM to capture the posterior P(h | x), when one has to actually make a decision or a prediction, it is necessary for optimal decision-making to marginalize over the latent variables. For example, if we want to predict y given x, we want to compute something like
� h P(y | h)P(h | x). If P(h | x) is complex and highly multi-modal (with a huge number of modes), then even if we can represent the posterior, performing this sum exactly is out of the question, and even an MCMC approximation may be either very poor (we can only visit at most N modes with N
MCMC steps, and that is very optimistic because of the mode mixing issue) or very slow (requiring an exponential number of terms being computed or a very very long MCMC chain). It seems that we have not really addressed the original "fundamental challenge with highly multi-modal posteriors" raised above.
To address this challenge, we propose to avoid explicit inference altogether by avoiding to sample, enumerate, or represent actual values of the latent variables h. In fact, our proposal is to completely skip the latent variables themselves. Instead, if we first consider the example of the previous paragraph, one can just directly learn to predict P(y | x). In general, what we seek is that the only approximation error we are left with is due to to function approximation. This might be important because the compounding of approximate inference with function approximation could be very hurtful (Kulesza and Pereira, 2008).
To get there, one may wish to mentally go through an intermediate step. Imagine we had a good approximate posterior Qθ=f(x)(h) as proposed above, with parameters θ = f(x). Then we could imagine learning an approximate decision model that approximates and skips the intractable sum over h, instead directly going from θ = f(x) to a prediction of y, i.e., we would estimate P(y | x) by g(f(x)). Now since we are already learning f(x), why learn g(θ) separately? We could simply directly learn to estimate π(x) = g(f(x)) ≈ P(y | x).
Now that may look trivial, because this is already what we do in discriminant training of deep networks or recurrent networks, for example. And don't we lose all the advantages of probabilistic models, such as, handling different forms of uncertainty, missing inputs, and being able to answer any "question" of the form "predict any variables given any subset of the others"? Yes, if we stick
21 for many models, such as deep Boltzmann machines, or bipartite discrete Markov random fields (Martens and Sutskever, 2010), f does not even need to be learned, it can be derived analytically from the form of P(h | x)
Deep Learning of Representations: Looking Forward
17 to the traditional deep (or shallow) neural networks like those discussed in Section 2.1.22 But there are other options.
We propose to get the advantages of probabilistic models without the need for explicitly going through many configurations of the latent variables. The general principle of what we propose to achieve this is to construct a family of computational graphs which perform the family of tasks we are interested in. A recent proposal (Goodfellow et al., 2013a) goes in this direction. Like previous work on learned approximate inference (Stoyanov et al., 2011), one can view the approach as constructing a computational graph associated to approximate inference (e.g. a fixed number of iterations of meanfield updates) in a particular setting (here, filling missing input with a variational approximation over hidden and unclamped inputs). An interesting property is that depending on which input variables are clamped and which are considered missing (either during training or at test time), we get a different computational graph, while all these computational graphs share the same parameters.
In Goodfellow et al. (2013a), training the shared parameters of these computational graph is achieved through a variational criterion that is similar to a generalized pseudo-likelihood, i.e., approximately maximizing log P(xv | xc) for randomly chosen partitions (v, c) of s.
This would be similar to dependency networks (Heckerman et al., 2000), but re-using the same parameters for every possible question-answer partition and training the system to answer for any subset of variables rather than singletons like in pseudo-likelihood. For the same reason, it raises the question of whether the different estimated conditionals are coherent with a global joint distribution.
In the case where the computational graph is obtained from the template of an inference mechanism for a joint distribution (such as variational inference), then clearly, we keep the property that these conditionals are coherent with a global joint distribution. With the mean-field variational inference, the computational graph looks like a recurrent neural network converging to a fixed point, and where we stop the iterations after a fixed number of steps or according to a convergence criterion. Such a trained parametrized computational graph is used in the iterative variational approach introduced in Goodfellow et al. (2013a) for training and missing value inference in deep Boltzmann machines, with an inpainting-like criterion in which arbitrary subsets of pixels are predicted given the others (a generalized pseudo-likelihood criterion). It has also been used in a recursion that follows the template of loopy belief propagation to fill-in the missing inputs and produce outputs (Stoyanov et al., 2011).
Although in these cases there is still a notion of latent variables (e.g. the latent variables of the deep Boltzmann machine) that motivate the "template" used for the learned approximate inference, what we propose here is to stop thinking about them as actual latent factors, but rather just as a way to parametrize this template for a question answering mechanism regarding missing inputs, i.e., the "generic conditional prediction mechanism" implemented by the recurrent computational graph that is trained to predict any subset of variables given any other subset. Although Goodfellow et al. (2013a) assume a factorial distribution across the predicted variables, we propose to investigate non-factorial posterior distributions over the observed variables, i.e., in the spirit of the recent flurry of work on structured output machine learning (Tsochantaridis et al., 2005). We can think of this parametrized computational graph as a family of functions, each corresponding to answering a different question (predict a specific set of variables given some others), but all sharing the same parameters. We already have examples of such families in machine learning, e.g., with recurrent neural networks or dynamic Bayes nets (where the functions in the family are indexed by the length of the sequence). This is also analogous to what happens with dropouts, where we have an exponential number of neural networks corresponding to different sub-graphs from input to output(indexed by which hidden units are turned on or off). For the same reason as in these examples, we obtain a form of generalization across subsets. Following the idea of learned approximate inference, the parameters of the question-answering inference mechanism would be taking advantage of the specific underlying structure in the data generating distribution. Instead of trying to do inference on the anonymous latent variables, it would be trained to do good inference only over observed
22 although, using something like these deep nets would be appealing because they are currently beating benchmarks in speech recognition, language modeling and object recognition
Y. Bengio variables or over high-level features learned by a deep architecture, obtained deterministically from the observed input.
An even more radically different solution to the problem of avoiding explicit latent variables was recently introduced in Bengio et al. (2013c) and Bengio and Thibodeau-Laufer (2013). These introduce training criteria respectively for generalized forms of denoising auto-encoders and for generative stochastic networks, with the property that maximum likelihood training of the reconstruction probabilities yields consistent but implicit estimation of the data generating distribution. These
Generative Stochastic Networks (GSNs) can be viewed as inspired by the Gibbs sampling procedure in deep Boltzmann machines (or deep belief networks) in the sense that one can construct computational graphs that perform similar computation, i.e., these are stochastic computational graphs (or equivalently, deterministic computational graphs with noise sources injected in the graph). These models are not explicitly trained to fill-in missing inputs but simply to produce a Markov chain whose asymptotic distribution estimates the data generating distribution. However, one can show that this chain can be manipulated in order to obtain samples of the estimated conditional distribution P(xv | xc), i.e., if one clamps some of the inputs, one can sample from a chain that stochastically fills-in from the missing inputs.
The approximate inference is not anymore an approximation of something else, it is the definition of the model itself. This is actually good news because we thus eliminate the issue that the approximate inference may be poor. The only thing we need to worry about is whether the parameterized computational graph is rich enough (or may overfit) to capture the unknown data generating distribution, and whether it makes it easy or difficult to optimize the parameters.
The idea that we should train with the approximate inference as part of the computational graph for producing a decision (and a loss) was first introduced by Stoyanov et al. (2011), and we simply push it further here, by proposing to allow the computational graph to depart in any way we care to explore from the template provided by existing inference or sampling mechanisms, i.e., potentially losing the connection and the reference to probabilistic latent variables. Once we free ourselves from the constraint of interpreting this parametrized question answering computational graph as corresponding to approximate inference or approximate sampling involving latent variables, all kinds of architectures and parametrizations are possible, where current approximate inference mechanisms can serve as inspiration and starting points. Interestingly, Bengio and Thibodeau-Laufer (2013) provides a proper training criterion for training any such stochastic computational graph simply using backprop over the computational graph, so as to maximize the probability of reconstructing the observed data under a reconstruction probability distribution that depends on the inner nodes of the computational graph. The noise injected in the computational graph must be such that the learner cannot get rid of the noise and obtain perfect reconstruction (a dirac at the correct observed input), just like in denoising auto-encoders. It is quite possible that this new freedom could give rise to much better models.
To go farther than Bengio and Thibodeau-Laufer (2013); Goodfellow et al. (2013a); Stoyanov et al. (2011) it would be good to go beyond the kind of factorized prediction common in variational and loopy belief propagation inference. We would like the reconstruction distribution to be able to capture multi-modal non-factorial distributions. Although the result from Alain and Bengio (2013) suggests that when the amount of injected noise is small, a unimodal distribution is sufficient, it is convenient to accomodate large amounts of injected noise to make training more efficient, as discussed by Bengio et al. (2013c). One idea is to obtain such multi-modal reconstruction distributions is to represent the estimated joint distribution of the predicted variables (possibly given the clamped variables) by a powerful model such as an RBM or a regularized auto-encoder, e.g., as has been done for structured output predictions when there is complex probabilistic structure between the output variables (Mnih et al., 2011; Li et al., 2013).
Although conditional RBMs have been already explored, conditional distributions provided by regularized auto-encoders remain to be studied. Since a denoising auto-encoder can be shown to estimate the underlying data generating distribution, making its parameters dependent on some other variables yields an estimator of a conditional distribution, which can also be trained by simple gradient-based methods (and backprop to obtain the gradients).
Deep Learning of Representations: Looking Forward
All these ideas lead to the question: what is the interpretation of hidden layers, if not directly of the underlying generative latent factors? The answer may simply be that they provide a better representation of these factors, a subject discussed in the next section. But what about the representation of uncertainty about these factors? The author believes that humans and other animals carry in their head an internal representation that implicitly captures both the most likely interpretation of any of these factors (in case a hard decision about some of them has to be taken) and uncertainty about their joint assignment. This is of course a speculation. Somehow, our brain would be operating on implicit representations of the joint distribution between these explanatory factors, generally without having to commit until a decision is required or somehow provoked by our attention mechanisms (which seem related to our tendancy to verbalize a discrete interpretation). A good example is foreign language understanding for a person who does not master that foreign language. Until we consciously think about it, we generally don't commit to a particular meaning for ambiguous word(which would be required by MAP inference), or even to the segmentation of the speech in words, but we can take a hard or a stochastic decision that depends on the interpretation of these words if we have to, without having to go through this intermediate step of discrete interpretation, instead treating the ambiguous information as soft cues that may inform our decision. In that example, a factorized posterior is also inadequate because some word interpretations are more compatible with each other.
To summarize, what we propose here, unlike in previous work on approximate inference, is to drop the pretense that the learned approximate inference mechanism actually approximates the latent variables distribution, mode, or expected value. Instead, we only consider the construction of a computational graph (deterministic or stochastic) which produces answers to the questions we care about, and we make sure that we can train a family of computational graphs (sharing parameters) whose elements can answer any of these questions. By removing the interpretation of approximately marginalizing over latent variables, we free ourselves from a strong constraint and the possible hurtful approxiations involved in approximate inference, especially when the true posterior would have a huge number of significant modes.
This discussion is of course orthogonal to the use of Bayesian averaging methods in order to produce better-generalizing predictions, i.e., handling uncertainty due to a small number of training examples. The proposed methods can be made Bayesian just like neural networks have their Bayesian variants (Neal, 1994), by somehow maintaining an implicit or explicit distribution over parameters.
A promising step in this direction was proposed by Welling and Teh (2011), making such Bayesian computation tractable by exploiting the randomness introduced with stochastic gradient descent to also produce the Bayesian samples over the uncertain parameter values.
Disentangling
Disentangling: The Challenge
What are "underlying factors" explaining the data? The answer is not obvious. One answer could be that these are factors that can be separately controlled (one could set up way to change one but not the others). This can actually be observed by looking at sequential real-world data, where only a small proportion of the factors typically change from t to t + 1. Complex data arise from the rich interaction of many sources. These factors interact in a complex web that can complicate AI-related tasks such as object classification. If we could identity and separate out these factors (i.e., disentangle them), we would have almost solved the learning problem. For example, an image is composed of the interaction between one or more light sources, the object shapes and the material properties of the various surfaces present in the image. It is important to distinguish between the related but distinct goals of learning invariant features and learning to disentangle explanatory factors. The central difference is the preservation of information. Invariant features, by definition, have reduced sensitivity in the directions of invariance. This is the goal of building features that are insensitive to variation in the data that are uninformative to the task at hand. Unfortunately, it is often difficult to determine a priori which set of features and variations will ultimately be relevant to the task at
Y. Bengio hand. Further, as is often the case in the context of deep learning methods, the feature set being trained may be destined to be used in multiple tasks that may have distinct subsets of relevant features. Considerations such as these lead us to the conclusion that the most robust approach to feature learning is to disentangle as many factors as possible, discarding as little information about the data as is practical.
Deep learning algorithms that can do a much better job of disentangling the underlying factors of variation would have tremendous impact. For example, suppose that the underlying factors can be
"guessed" (predicted) from a simple (e.g. linear) transformation of the learned representation, ideally a transformation that only depends on a few elements of the representation. That is what we mean by a representation that disentangles the underlying factors. It would clearly make learning a new supervised task (which may be related to one or a few of them) much easier, because the supervised learning could quickly learn those linear factors, zooming in on the parts of the representation that are relevant.
Of all the challenges discussed in this paper, this is probably the most ambitious, and success in solving it the most likely to have far-reaching impact. In addition to the obvious observation that disentangling the underlying factors is almost like pre-solving any possible task relevant to the observed data, having disentangled representations would also solve other issues, such as the issue of mixing between modes. We believe that it would also considerably reduce the optimization problems involved when new information arrives and has to be reconciled with the world model implicit in the current parameter setting. Indeed, it would allow only changing the parts of the model that involve the factors that are relevant to the new observation, in the spirit of sparse updates and reduced ill-conditioning discussed above.
Disentangling: Solution Paths
Deeper Representations Disentangle Better. There are some encouraging signs that our current unsupervised representation-learning algorithms are reducing the "entanglement" of the underlying factors23 when we apply them to raw data (or to the output of a previous representation learning procedure, like when we stack RBMs or regularized auto-encoders).
First, there are experimental observations suggesting that sparse convolutional RBMs and sparse denoising auto-encoders achieve in their hidden units a greater degree of disentangling than in their inputs (Goodfellow et al., 2009; Glorot et al., 2011b). What these authors found is that some hidden units were particularly sensitive to a known factor of variation while being rather insensitive (i.e., invariant) to others. For example, in a sentiment analysis model that sees unlabeled paragraphs of customer comments from the Amazon web site, some hidden units specialized on the topic of the paragraph (the type of product being evaluated, e.g., book, video, music) while other units specialized on the sentiment (positive vs negative). The disentanglement was never perfect, so the authors made quantitative measurements of sensitivity and invariance and compared these quantities on the input and the output (learned representation) of the unsupervised learners.
Another encouraging observation (already mentioned in the section on mixing) is that deeper representations were empirically found to be more amenable to quickly mixing between modes (Bengio et al., 2013b). Two (compatible) hypotheses were proposed to explain this observation: (1) RBMs and regularized auto-encoders deterministically transform24 their input distribution into one that is more uniform-looking, that better fills the space (thus creating easier paths between modes), and(2) these algorithms tend to discover representations that are more disentangled. The advantage of a higher-level disentangled representation is that a small MCMC step (e.g. Gibbs) in that space(e.g. flipping one high-level variable) can move in one step from one input-level mode to a distant one, e.g., going from one shape / object to another one, adding or removing glasses on the face of a person (which requires a very sharp coordination of pixels far from each other because glasses occupy a very thin image area), or replacing foreground and background colors (such as going into a "reverse video" mode).
23 as measured by how predictive some individual features are of known factors
24 when considering the features learned, e.g., the P(hi = 1 | x), for RBMs
Deep Learning of Representations: Looking Forward
Although these observations are encouraging, we do not yet have a clear understanding as to why some representation algorithms tend to move towards more disentangled representations, and there are other experimental observations suggesting that this is far from sufficient. In particular, Gulcehre and Bengio (2013) show an example of a task on which deep supervised nets (and every other black-box machine learning algorithm tried) fail, on which a completely disentangled input representation makes the task feasible (with a maxout network (Goodfellow et al., 2013b)). Unfortunately, unsupervised pre-training applied on the raw input images failed to produce enough disentangling to solve the task, even with the appropriate convolutional structure. What is interesting is that we now have a simple artificial task on which we can evaluate new unsupervised representation learning methods for their disentangling ability. It may be that a variant of the current algorithms will eventually succeed at this task, or it may be that altogether different unsupervised representation learning algorithms are needed.
Generic Priors for Disentangling Factors of Variation. A general strategy was outlined in Bengio et al. (2013d) to enhance the discovery of representations which disentangle the underlying and unknown factors of variation: it relies on exploiting priors about these factors. We are most interested in broad generic priors that can be useful for a large class of learning problems of interest in AI. We list these priors here:
• Smoothness: assumes the function f to be learned is s.t. x ≈ y generally implies f(x) ≈ f(y).
This most basic prior is present in most machine learning, but is insufficient to get around the curse of dimensionality.
• Multiple explanatory factors: the data generating distribution is generated by different underlying factors, and for the most part what one learns about one factor generalizes in many configurations of the other factors. The objective is to recover or at least disentangle these underlying factors of variation. This assumption is behind the idea of distributed representations. More specific priors on the form of the model can be used to enhance disentangling, such as multiplicative interactions between the factors (Tenenbaum and Freeman, 2000; Desjardins et al., 2012) or orthogonality of the features derivative with respect to the input (Rifai et al., 2011b, 2012a; Sohn et al., 2013). The parametrization and training procedure may also be used to disentangle discrete factors (e.g., detecting a shape) from associated continuous-valued factors (e.g., pose parameters), as in transforming auto-encoders (Hinton et al., 2011), spike-and-slab RBMs with pooled slab variables (Courville et al., 2011) and other pooling-based models that learn a feature subspace (Kohonen, 1996; Hyv¨arinen and Hoyer, 2000).
• A hierarchical organization of explanatory factors: the concepts that are useful for describing the world around us can be defined in terms of other concepts, in a hierarchy, with more abstract concepts higher in the hierarchy, defined in terms of less abstract ones. This assumption is exploited with deep representations. Although stacking single-layer models has been rather successful, much remains to be done regarding the joint training of all the layers of a deep unsupervised model.
• Semi-supervised learning: with inputs X and target Y to predict, given X, a subset of the factors explaining X's distribution explain much of Y, given X. Hence representations that are useful for spelling out P(X) tend to be useful when learning P(Y | X), allowing sharing of statistical strength between the unsupervised and supervised learning tasks. However, many of the factors that explain X may dominate those that also explain Y, which can make it useful to incorporate observations of Y in training the learned representations, i.e., by semi-supervised representation learning.
• Shared factors across tasks: with many Y 's of interest or many learning tasks in general, tasks(e.g., the corresponding P(Y | X, task)) are explained by factors that are shared with other tasks, allowing sharing of statistical strength across tasks, e.g. for multi-task and transfer learning or domain adaptation. This can be achieved by sharing embeddings or representation functions across tasks (Collobert and Weston, 2008; Bordes et al., 2013).
• Manifolds: probability mass concentrates near regions that have a much smaller dimensionality than the original space where the data lives. This is exploited with regularized auto-encoder algorithms, but training criteria that would explicitly take into account that we are looking for a Y. Bengio concentration of mass in an integral number directions remain to be developed.
• Natural clustering: different values of categorical variables such as object classes are associated with separate manifolds. More precisely, the local variations on the manifold tend to preserve the value of a category, and a linear interpolation between examples of different classes in general involves going through a low density region, i.e., P(X | Y = i) for different i tend to be well separated and not overlap much. For example, this is exploited in the Manifold Tangent Classifier (Rifai et al., 2011b). This hypothesis is consistent with the idea that humans have named categories and classes because of such statistical structure (discovered by their brain and propagated by their culture), and machine learning tasks often involves predicting such categorical variables.
• Temporal and spatial coherence: this prior introduced in Becker and Hinton (1992) is similar to the natural clustering assumption but concerns sequences of observations: consecutive (from a sequence) or spatially nearby observations tend to be easily predictable from each other. In the special case typically studied, e.g., slow feature analysis (Wiskott and Sejnowski, 2002b), one assumes that consecutive values are close to each other, or that categorical concepts remain either present or absent for most of the transitions. More generally, different underlying factors change at different temporal and spatial scales, and this could be exploited to sift different factors into different categories based on their temporal scale.
• Sparsity: for any given observation x, only a small fraction of the possible factors are relevant.
In terms of representation, this could be represented by features that are often zero (as initially proposed by Olshausen and Field (1996)), or more generally by the fact that most of the extracted features are insensitive to small variations of x. This can be achieved with certain forms of priors on latent variables (peaked at 0), or by using a non-linearity whose value is often flat at 0 (i.e., 0 and with a 0 derivative), or simply by penalizing the magnitude of the derivatives of the function mapping input to representation. A variant on that hypothesis is that for any given input, only a small part of the model is relevant and only a small subset of the parameters need to be updated.
• Simplicity of Factor Dependencies: in good high-level representations, the factors are related to each other through simple, typically linear, dependencies. This can be seen in many laws of physics, and is assumed when plugging a linear predictor on top of a learned representation.
Conclusion
Deep learning and more generally representation learning are recent areas of investigation in machine learning and recent years of research have allowed to clearly identify several major challenges for approaching the performance of these algorithms from that of humans. We have broken down these challenges into four major areas: scaling computations, reducing the difficulties in optimizing parameters, designing (or avoiding) expensive inference and sampling, and helping to learn representations that better disentangle the unknown underlying factors of variation. There is room for exploring many paths towards addressing all of these issues, and we have presented here a few appealing directions of research towards these challenges.
Acknowledgments
The author is extremely grateful for the feedback and discussions he enjoyed with collaborators
Ian Goodfellow, Guillaume Desjardins, Aaron Courville, Pascal Vincent, Roland Memisevic and Nicolas Chapados, which greatly contributed to help form the ideas presented here and fine-tune this manuscript. He is also grateful for the funding support from NSERC, CIFAR, the Canada
Research Chairs, and Compute Canada.
Bibliography
Alain, G. and Bengio, Y. (2013). What regularized auto-encoders learn from the data generating distribution.
In International Conference on Learning Representations (ICLR'2013).
Bach, F., Jenatton, R., Mairal, J., and Obozinski, G. (2011). Structured sparsity through convex optimization. Technical report, arXiv.1109.2397.
Bagnell, J. A. and Bradley, D. M. (2009). Differentiable sparse coding. In NIPS'2009, pages 113–120.
Barber, D. (2011). Bayesian Reasoning and Machine Learning. Cambridge University Press.
Becker, S. and Hinton, G. (1992). A self-organizing neural network that discovers surfaces in random-dot stereograms. Nature, 355, 161–163.
Bengio, Y. (2008). Neural net language models. Scholarpedia, 3(1).
Bengio, Y. (2009). Learning deep architectures for AI. Now Publishers.
Bengio, Y. (2011). Deep learning of representations for unsupervised and transfer learning. In JMLR W&CP:
Proc. Unsupervised and Transfer Learning.
Bengio, Y. (2013a). Estimating or propagating gradients through stochastic neurons. Technical Report arXiv:1305.2982, Universite de Montreal.
Bengio, Y. (2013b). Evolving culture vs local minima. In Growing Adaptive Machines: Integrating Development and Learning in Artificial Neural Networks, number also as ArXiv 1203.2990v1, pages T. Kowaliw, N. Bredeche & R. Doursat, eds. Springer-Verlag.
Bengio, Y. (2013c). Practical recommendations for gradient-based training of deep architectures. In K.-R.
M¨uller, G. Montavon, and G. B. Orr, editors, Neural Networks: Tricks of the Trade. Springer.
Bengio, Y. and Thibodeau-Laufer, E. (2013). Deep generative stochastic networks trainable by backprop.
Technical Report arXiv:1306.1091, Universite de Montreal.
Bengio, Y., Simard, P., and Frasconi, P. (1994). Learning long-term dependencies with gradient descent is difficult. IEEE Transactions on Neural Networks, 5(2), 157–166.
Bengio, Y., Ducharme, R., Vincent, P., and Jauvin, C. (2003). A neural probabilistic language model. JMLR, 3, 1137–1155.
Bengio, Y., Lamblin, P., Popovici, D., and Larochelle, H. (2007). Greedy layer-wise training of deep networks.
In NIPS'2006.
Bengio, Y., Louradour, J., Collobert, R., and Weston, J. (2009). Curriculum learning. In ICML'09.
Bengio, Y., Delalleau, O., and Simard, C. (2010). Decision trees do not generalize to new variations. Computational Intelligence, 26(4), 449–467.
Bengio, Y., Alain, G., and Rifai, S. (2012). Implicit density estimation by local moment matching to sample from auto-encoders. Technical report, arXiv:1207.0057.
Bengio, Y., Boulanger-Lewandowski, N., and Pascanu, R. (2013a). Advances in optimizing recurrent networks. In ICASSP'2013.
Bengio, Y., Mesnil, G., Dauphin, Y., and Rifai, S. (2013b). Better mixing via deep representations. In
ICML'2013.
Bengio, Y., Li, Y., Alain, G., and Vincent, P. (2013c). Generalized denoising auto-encoders as generative models. Technical Report arXiv:1305.6663, Universite de Montreal.
Bengio, Y., Courville, A., and Vincent, P. (2013d). Unsupervised feature learning and deep learning: A review and new perspectives. IEEE Trans. Pattern Analysis and Machine Intelligence (PAMI).
Bergstra, J. and Bengio, Y. (2009). Slow, decorrelated features for pretraining complex cell-like networks.
In NIPS'2009.
Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., Turian, J., Warde-Farley, D., and Bengio, Y. (2010). Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientific Computing Conference (SciPy).
Bergstra, J., Bastien, F., Breuleux, O., Lamblin, P., Pascanu, R., Delalleau, O., Desjardins, G., Warde-Farley, D., Goodfellow, I., Bergeron, A., and Bengio, Y. (2011). Theano: Deep learning on gpus with python. In
Big Learn workshop, NIPS'11.
Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.
Bordes, A., Glorot, X., Weston, J., and Bengio, Y. (2013). A semantic matching energy function for learning with multi-relational data. Machine Learning: Special Issue on Learning Semantics.
Brooke, J. J., Bitko, D., Rosenbaum, T. F., and Aeppli, G. (2001). Quantum annealing of a disordered magnet. Technical Report cond-mat/0105238.
Y. Bengio
Cayton, L. (2005). Algorithms for manifold learning. Technical Report CS2008-0923, UCSD.
Cho, K., Raiko, T., and Ilin, A. (2010). Parallel tempering is efficient for learning restricted Boltzmann machines. In IJCNN'2010.
Ciresan, D., Meier, U., and Schmidhuber, J. (2012). Multi-column deep neural networks for image classification. Technical report, arXiv:1202.2745.
Coates, A. and Ng, A. Y. (2011). The importance of encoding versus training with sparse coding and vector quantization. In ICML'2011.
Coates, A., Lee, H., and Ng, A. Y. (2011). An analysis of single-layer networks in unsupervised feature learning. In AISTATS'2011.
Coates, A., Karpathy, A., and Ng, A. (2012). Emergence of object-selective features in unsupervised feature learning. In NIPS'2012.
Collobert, R. and Weston, J. (2008). A unified architecture for natural language processing: Deep neural networks with multitask learning. In ICML'2008.
Collobert, R., Bengio, Y., and Bengio., S. (2003). Scaling large learning problems with hard parallel mixtures.
International Journal of Pattern Recognition and Artificial Intelligence, 17(3), 349–365.
Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., and Kuksa, P. (2011). Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12, 2493–2537.
Corrado, G. (2012). Deep networks for predicting ad click through rates. In ICML'2012 Online Advertising
Workshop.
Courville, A., Bergstra, J., and Bengio, Y. (2011). Unsupervised models of images by spike-and-slab RBMs.
In ICML'2011.
Dauphin, Y. and Bengio, Y. (2013). Big neural networks waste capacity. Technical Report arXiv:1301.3583, Universite de Montreal.
Dean, J., Corrado, G., Monga, R., Chen, K., Devin, M., Le, Q., Mao, M., Ranzato, M., Senior, A., Tucker, P., Yang, K., and Ng, A. Y. (2012). Large scale distributed deep networks. In NIPS'2012.
Deng, L., Li, J., Huang, J.-T., Yao, K., Yu, D., Seide, F., Seltzer, M., Zweig, G., He, X., Williams, J., Gong, Y., and Acero, A. (2013). Recent advances in deep learning for speech research at Microsoft. In ICASSP
Desjardins, G., Courville, A., Bengio, Y., Vincent, P., and Delalleau, O. (2010). Tempered Markov chain
Monte Carlo for training of restricted Boltzmann machine. In AISTATS, volume 9, pages 145–152.
Desjardins, G., Courville, A., and Bengio, Y. (2012).
Disentangling factors of variation via generative entangling.
Eisner, J. (2012).
Learning approximate inference policies for fast prediction.
Keynote talk at ICML
Workshop on Inferning: Interactions Between Search and Learning.
Frey, B. J., Hinton, G. E., and Dayan, P. (1996). Does the wake-sleep algorithm learn good density estimators? In NIPS'95, pages 661–670. MIT Press, Cambridge, MA.
Glorot, X. and Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural networks.
In AISTATS'2010.
Glorot, X., Bordes, A., and Bengio, Y. (2011a). Deep sparse rectifier neural networks. In AISTATS.
Glorot, X., Bordes, A., and Bengio, Y. (2011b). Domain adaptation for large-scale sentiment classification:
A deep learning approach. In ICML'2011.
Goodfellow, I., Le, Q., Saxe, A., and Ng, A. (2009). Measuring invariances in deep networks. In NIPS'09, pages 646–654.
Goodfellow, I., Courville, A., and Bengio, Y. (2011). Spike-and-slab sparse coding for unsupervised feature discovery. In NIPS Workshop on Challenges in Learning Hierarchical Models.
Goodfellow, I., Courville, A., and Bengio, Y. (2012). Large-scale feature learning with spike-and-slab sparse coding. In ICML'2012.
Goodfellow, I. J., Courville, A., and Bengio, Y. (2013a). Joint training of deep Boltzmann machines for classification. In International Conference on Learning Representations: Workshops Track.
Goodfellow, I. J., Warde-Farley, D., Mirza, M., Courville, A., and Bengio, Y. (2013b). Maxout networks. In
ICML'2013.
Gregor, K. and LeCun, Y. (2010a).
Learning fast approximations of sparse coding.
In L. Bottou and M. Littman, editors, Proceedings of the Twenty-seventh International Conference on Machine Learning(ICML-10). ACM.
Gregor, K. and LeCun, Y. (2010b). Learning fast approximations of sparse coding. In ICML'2010.
Grosse, R., Raina, R., Kwong, H., and Ng, A. Y. (2007). Shift-invariant sparse coding for audio classification.
In UAI'2007.
Deep Learning of Representations: Looking Forward
Gulcehre, C. and Bengio, Y. (2013). Knowledge matters: Importance of prior information for optimization.
Technical Report arXiv:1301.4083, Universite de Montreal.
Gutmann, M. and Hyvarinen, A. (2010).
Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In AISTATS'2010.
Heckerman, D., Chickering, D. M., Meek, C., Rounthwaite, R., and Kadie, C. (2000). Dependency networks for inference, collaborative filtering, and data visualization. Journal of Machine Learning Research, 1, 49–75.
Hinton, G., Krizhevsky, A., and Wang, S. (2011). Transforming auto-encoders. In ICANN'2011.
Hinton, G., Deng, L., Dahl, G. E., Mohamed, A., Jaitly, N., Senior, A., Vanhoucke, V., Nguyen, P., Sainath, T., and Kingsbury, B. (2012a). Deep neural networks for acoustic modeling in speech recognition. IEEE
Signal Processing Magazine, 29(6), 82–97.
Hinton, G. E. and Salakhutdinov, R. (2006). Reducing the dimensionality of data with neural networks.
Science, 313(5786), 504–507.
Hinton, G. E., Dayan, P., Frey, B. J., and Neal, R. M. (1995). The wake-sleep algorithm for unsupervised neural networks. Science, 268, 1558–1161.
Hinton, G. E., Osindero, S., and Teh, Y. (2006). A fast learning algorithm for deep belief nets. Neural
Computation, 18, 1527–1554.
Hinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2012b).
Improving neural networks by preventing co-adaptation of feature detectors. Technical report, arXiv:1207.0580.
Hochreiter, S. (1991).
Untersuchungen zu dynamischen neuronalen Netzen. Diploma thesis, Institut f¨ur
Informatik, Lehrstuhl Prof. Brauer, Technische Universit¨at M¨unchen.
Hochreiter, S. and Schmidhuber, J. (1997). Long short-term memory. Neural Computation, 9(8), 1735–1780.
Hyv¨arinen, A. (2005). Estimation of non-normalized statistical models using score matching. J. Machine
Learning Res., 6.
Hyv¨arinen, A. and Hoyer, P. (2000). Emergence of phase and shift invariant features by decomposition of natural images into independent feature subspaces. Neural Computation, 12(7), 1705–1720.
Iba, Y. (2001). Extended ensemble monte carlo. International Journal of Modern Physics, C12, 623–656.
Jaeger, H. (2007). Echo state network. Scholarpedia, 2(9), 2330.
Jarrett, K., Kavukcuoglu, K., Ranzato, M., and LeCun, Y. (2009). What is the best multi-stage architecture for object recognition? In ICCV'09.
Jenatton, R., Audibert, J.-Y., and Bach, F. (2009). Structured variable selection with sparsity-inducing norms. Technical report, arXiv:0904.3523.
Kavukcuoglu, K., Ranzato, M., and LeCun, Y. (2008).
Fast inference in sparse coding algorithms with applications to object recognition. CBLL-TR-2008-12-01, NYU.
Kindermann, R. (1980). Markov Random Fields and Their Applications (Contemporary Mathematics ; V.
1). American Mathematical Society.
Kirkpatrick, S., Jr., C. D. G.,, and Vecchi, M. P. (1983). Optimization by simulated annealing. Science, 220, 671–680.
Kohonen, T. (1996). Emergence of invariant-feature detectors in the adaptive-subspace self-organizing map.
Biological Cybernetics, 75, 281–291. 10.1007/s004220050295.
Krizhevsky, A., Sutskever, I., and Hinton, G. (2012). ImageNet classification with deep convolutional neural networks. In NIPS'2012.
Kulesza, A. and Pereira, F. (2008). Structured learning with approximate inference. In NIPS'2007.
Larochelle, H. and Bengio, Y. (2008). Classification using discriminative restricted Boltzmann machines. In
ICML'2008.
Le, Q., Ranzato, M., Monga, R., Devin, M., Corrado, G., Chen, K., Dean, J., and Ng, A. (2012). Building high-level features using large scale unsupervised learning. In ICML'2012.
Le Roux, N., Manzagol, P.-A., and Bengio, Y. (2008). Topmoumoute online natural gradient algorithm. In
NIPS'07.
LeCun, Y., Bottou, L., Orr, G. B., and M¨uller, K. (1998a). Efficient backprop. In Neural Networks, Tricks of the Trade.
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998b). Gradient based learning applied to document recognition. Proc. IEEE.
LeCun, Y., Chopra, S., Hadsell, R., Ranzato, M.-A., and Huang, F.-J. (2006). A tutorial on energy-based learning. In G. Bakir, T. Hofman, B. Scholkopf, A. Smola, and B. Taskar, editors, Predicting Structured
Data, pages 191–246. MIT Press.
Lee, H., Ekanadham, C., and Ng, A. (2008). Sparse deep belief net model for visual area V2. In NIPS'07.
Y. Bengio
Li, Y., Tarlow, D., and Zemel, R. (2013). Exploring compositional high order pattern potentials for structured output learning. In CVPR'2013.
Luo, H., Carrier, P. L., Courville, A., and Bengio, Y. (2013). Texture modeling with convolutional spikeand-slab RBMs and deep extensions. In AISTATS'2013.
Mairal, J., Bach, F., Ponce, J., and Sapiro, G. (2009). Online dictionary learning for sparse coding. In
ICML'2009.
Martens, J. (2010). Deep learning via Hessian-free optimization. In L. Bottou and M. Littman, editors, Proceedings of the Twenty-seventh International Conference on Machine Learning (ICML-10), pages 735–
742. ACM.
Martens, J. and Sutskever, I. (2010). Parallelizable sampling of Markov random fields. In AISTATS'2010.
Mesnil, G., Dauphin, Y., Glorot, X., Rifai, S., Bengio, Y., Goodfellow, I., Lavoie, E., Muller, X., Desjardins, G., Warde-Farley, D., Vincent, P., Courville, A., and Bergstra, J. (2011).
Unsupervised and transfer learning challenge: a deep learning approach. In JMLR W&CP: Proc. Unsupervised and Transfer Learning, volume 7.
Mikolov, T. (2012). Statistical Language Models based on Neural Networks. Ph.D. thesis, Brno University of Technology.
Mnih, V., Larochelle, H., and Hinton, G. (2011). Conditional restricted Boltzmann machines for structure output prediction. In Proc. Conf. on Uncertainty in Artificial Intelligence (UAI).
Montavon, G. and Muller, K.-R. (2012). Deep Boltzmann machines and the centering trick. In G. Montavon, G. Orr, and K.-R. M¨uller, editors, Neural Networks: Tricks of the Trade, volume 7700 of Lecture Notes in Computer Science, pages 621–637.
Murphy, K. P. (2012). Machine Learning: a Probabilistic Perspective. MIT Press, Cambridge, MA, USA.
Nair, V. and Hinton, G. E. (2010).
Rectified linear units improve restricted Boltzmann machines.
In
ICML'10.
Narayanan, H. and Mitter, S. (2010). Sample complexity of testing the manifold hypothesis. In NIPS'2010.
Neal, R. M. (1994). Bayesian Learning for Neural Networks. Ph.D. thesis, Dept. of Computer Science, University of Toronto.
Olshausen, B. A. and Field, D. J. (1996). Emergence of simple-cell receptive field properties by learning a sparse code for natural images. Nature, 381, 607–609.
Pascanu, R. and Bengio, Y. (2012). On the difficulty of training recurrent neural networks. Technical Report arXiv:1211.5063, Universite de Montreal.
Pascanu, R. and Bengio, Y. (2013).
Revisiting natural gradient for deep networks.
Technical report, arXiv:1301.3584.
Raiko, T., Valpola, H., and LeCun, Y. (2012).
Deep learning made easier by linear transformations in perceptrons. In AISTATS'2012.
Raina, R., Battle, A., Lee, H., Packer, B., and Ng, A. Y. (2007). Self-taught learning: transfer learning from unlabeled data. In ICML'2007.
Raina, R., Madhavan, A., and Ng, A. Y. (2009). Large-scale deep unsupervised learning using graphics processors. In L. Bottou and M. Littman, editors, ICML 2009, pages 873–880, New York, NY, USA.
ACM.
Ranzato, M., Poultney, C., Chopra, S., and LeCun, Y. (2007). Efficient learning of sparse representations with an energy-based model. In NIPS'2006.
Ranzato, M., Boureau, Y.-L., and LeCun, Y. (2008). Sparse feature learning for deep belief networks. In
NIPS'07, pages 1185–1192, Cambridge, MA. MIT Press.
Recht, B., Re, C., Wright, S., and Niu, F. (2011). Hogwild: A lock-free approach to parallelizing stochastic gradient descent. In NIPS'2011.
Rifai, S., Vincent, P., Muller, X., Glorot, X., and Bengio, Y. (2011a). Contractive auto-encoders: Explicit invariance during feature extraction. In ICML'2011.
Rifai, S., Dauphin, Y., Vincent, P., Bengio, Y., and Muller, X. (2011b). The manifold tangent classifier. In
NIPS'2011.
Rifai, S., Bengio, Y., Courville, A., Vincent, P., and Mirza, M. (2012a). Disentangling factors of variation for facial expression recognition. In Proceedings of the European Conference on Computer Vision (ECCV
6), pages 808–822.
Rifai, S., Bengio, Y., Dauphin, Y., and Vincent, P. (2012b). A generative process for sampling contractive auto-encoders. In ICML'2012.
Rose, G. and Macready, W. (2007).
An introduction to quantum annelaing.
Technical report, D-Wave
Systems.
Deep Learning of Representations: Looking Forward
Rumelhart, D., Hinton, G., and Williams, R. (1986). Learning representations by back-propagating errors.
Nature, 323, 533–536.
Salakhutdinov, R. (2010a). Learning deep Boltzmann machines using adaptive MCMC. In ICML'2010.
Salakhutdinov, R. (2010b). Learning in Markov random fields using tempered transitions. In NIPS'2010.
Salakhutdinov, R. and Hinton, G. (2009). Deep Boltzmann machines. In AISTATS'2009, pages 448–455.
Salakhutdinov, R. and Larochelle, H. (2010).
Efficient learning of deep Boltzmann machines.
In AISTATS'2010.
Salakhutdinov, R., Mnih, A., and Hinton, G. (2007).
Restricted Boltzmann machines for collaborative filtering. In ICML'2007, pages 791–798.
Saul, L. K. and Jordan, M. I. (1996). Exploiting tractable substructures in intractable networks. In NIPS'95.
MIT Press, Cambridge, MA.
Schaul, T., Zhang, S., and LeCun, Y. (2012). No More Pesky Learning Rates. Technical report, New York
University, arxiv 1206.1106.
Schraudolph, N. N. (1998). Centering neural network gradient factors. In G. B. Orr and K.-R. Muller, editors, Neural Networks: Tricks of he Trade, pages 548–548. Springer.
Seide, F., Li, G., and Yu, D. (2011a). Conversational speech transcription using context-dependent deep neural networks. In Interspeech 2011, pages 437–440.
Seide, F., Li, G., and Yu, D. (2011b). Feature engineering in context-dependent deep neural networks for conversational speech transcription. In ASRU'2011.
Sohn, K., Zhou, G., and Lee, H. (2013).
Learning and selecting features jointly with point-wise gated
Boltzmann machines. In ICML'2013.
Stoyanov, V., Ropson, A., and Eisner, J. (2011). Empirical risk minimization of graphical model parameters given approximate inference, decoding, and model structure. In AISTATS'2011.
Sutskever, I. (2012). Training Recurrent Neural Networks. Ph.D. thesis, CS Dept., U. Toronto.
Swersky, K., Ranzato, M., Buchman, D., Marlin, B., and de Freitas, N. (2011). On autoencoders and score matching for energy based models. In ICML'2011. ACM.
Taylor, G. and Hinton, G. (2009). Factored conditional restricted Boltzmann machines for modeling motion style. In L. Bottou and M. Littman, editors, ICML 2009, pages 1025–1032. ACM.
Taylor, G., Hinton, G. E., and Roweis, S. (2007). Modeling human motion using binary latent variables. In
NIPS'06, pages 1345–1352. MIT Press, Cambridge, MA.
Tenenbaum, J. B. and Freeman, W. T. (2000). Separating style and content with bilinear models. Neural
Computation, 12(6), 1247–1283.
Tsianos, K., Lawlor, S., and Rabbat, M. (2012). Communication/computation tradeoffs in consensus-based distributed optimization. In NIPS'2012.
Tsochantaridis, I., Joachims, T., Hofmann, T., and Altun, Y. (2005). Large margin methods for structured and interdependent output variables. J. Mach. Learn. Res., 6, 1453–1484.
T¨oscher, A., Jahrer, M., and Bell, R. M. (2009). The bigchaos solution to the netflix grand prize.
Vincent, P. (2011). A connection between score matching and denoising autoencoders. Neural Computation, 23(7), 1661–1674.
Vincent, P., Larochelle, H., Bengio, Y., and Manzagol, P.-A. (2008).
Extracting and composing robust features with denoising autoencoders. In ICML 2008.
Welling, M. and Teh, Y.-W. (2011).
Bayesian learning via stochastic gradient Langevin dynamics.
In
ICML'2011, pages 681–688.
Weston, J., Ratle, F., and Collobert, R. (2008). Deep learning via semi-supervised embedding. In ICML
Wiskott, L. and Sejnowski, T. (2002a). Slow feature analysis: Unsupervised learning of invariances. Neural
Computation, 14(4), 715–770.
Wiskott, L. and Sejnowski, T. J. (2002b). Slow feature analysis: Unsupervised learning of invariances. Neural
Computation, 14(4), 715–770.
Yu, D., Wang, S., and Deng, L. (2010). Sequential labeling using deep-structured conditional random fields.
IEEE Journal of Selected Topics in Signal Processing.
Yu, K., Lin, Y., and Lafferty, J. (2011). Learning image representations from the pixel level via hierarchical sparse coding. In CVPR'2011.
Zeiler, M. D. and Fergus, R. (2013).
Stochastic pooling for regularization of deep convolutional neural networks. In International Conference on Learning Representations.arXiv:1305.0445v2 [cs.LG] 7 Jun 2013Semantic Matching in Search
Hang Li and Jun Xu
Noah's Ark Lab, Huawei Technologies, Hong Kong
ABSTRACT
In this talk, we will give a high level introduction to our book entitled "Semantic Matching in Search", which was published recently in Foundations and Trends in Information Retrieval. We will start our talk by pointing out the importance of semantic matching for natural language processing and information retrieval. Most of the tasks in natural language processing and information retrieval, including search, question answering, and machine translation, are based on matching between language expressions.
This approach works quite well in practice; its limitation is also obvious, however. Sometimes mismatch can occur. We argue that 'semantic matching' is an effective approach to overcome the challenge, that is to conduct more semantic analysis on the language expressions and perform matching between language expressions at semantic level. We will then introduce the major approaches to semantic matching in search, developed in the research community in recent
SMIR'14, July 11, 2014, Gold Coast, Queensland, Australia.. years, including matching by query reformulation, matching with term dependency model, matching with translation model, matching with topic model, and matching with latent space model. We will conclude the talk by explaining our view on the open problems and future directions of semantic matching.
Categories and Subject Descriptors
H.4.0 [Information Systems Applications]: General
Keywords semantic matching, information retrieval, web search
REFERENCES
 H. Li and J. Xu. Semantic Matching in Search. Foundations and Trends in Information Retrieval, 7(5):343–469, 2013.Unsupervised Learning of Visual Representations using Videos
Xiaolong Wang, Abhinav Gupta
Robotics Institute, Carnegie Mellon University
Abstract
Is strong supervision necessary for learning a good visual representation?
Do we really need millions of semantically-labeled images to train a Convolutional Neural Network (CNN)? In this paper, we present a simple yet surprisingly powerful approach for unsupervised learning of CNN. Specifically, we use hundreds of thousands of unlabeled videos from the web to learn visual representations.
Our key idea is that visual tracking provides the supervision. That is, two patches connected by a track should have similar visual representation in deep feature space since they probably belong to the same object or object part. We design a Siamese-triplet network with a ranking loss function to train this CNN representation. Without using a single image from ImageNet, just using 100K unlabeled videos and the VOC 2012 dataset, we train an ensemble of unsupervised networks that achieves 52% mAP (no bounding box regression). This performance comes tantalizingly close to its ImageNet-supervised counterpart, an ensemble which achieves a mAP of 54.4%. We also show that our unsupervised network can perform competitively in other tasks such as surface-normal estimation.
1. Introduction
What is a good visual representation and how can we learn it? At the start of this decade, most computer vision research focused on "what" and used hand-defined features such as SIFT and HOG as the underlying visual representation. Learning was often the last step where these low-level feature representations were mapped to semantic/3D/functional categories. However, the last three years have seen the resurgence of learning visual representations directly from pixels themselves using the deep learning and Convolutional Neural Networks (CNNs).
At the heart of CNNs is a completely supervised learning paradigm. Often millions of examples are first labeled using Mechanical Turk followed by data augmentation to create tens of millions of training instances. CNNs are then trained using gradient descent and back propagation. But one question still remains: is strong-supervision necessary for training these CNNs? Do we really need millions of semantically-labeled images to learn a good representation?
…
…
…
…
Learning to Rank
Conv
Net
Conv
Net
Conv
Net
Query(First Frame)
Tracked(Last Frame)
Negative(Random)(a) Unsupervised Tracking in Videos
𝐷, 𝐷, 𝐷, 𝐷, 𝐷: Distance in deep feature space(b) Siamese-triplet Network(c) Ranking Objective
Figure 1. Overview of our approach. (a) Given unlabeled videos, we perform unsupervised tracking on the patches in them. (b)
Triplets of patches including query patch in the initial frame of tracking, tracked patch in the last frame, and random patch from other videos are fed into our siamese-triplet network for training. (c) The learning objective: Distance between the query and tracked patch in feature space should be smaller than the distance between query and random patches.
It seems humans can learn visual representations using little or no semantic supervision but our approaches still remain completely supervised.
In this paper, we explore the alternative: how we can exploit the unlabeled visual data on the web to train CNNs(e.g. AlexNet )? In the past, there have been several attempts at unsupervised learning using millions of static images or frames extracted from videos.
The most common architecture used is an auto-encoder which learns representations based on its ability to reconstruct the input images. While these approaches have been able to automatically learn V1-like filters given unlabeled data, they are still far away from supervised approaches on tasks such as object detection. So, what is the missing link? We argue that static images themselves might not have enough information to learn a good
1 arXiv:1505.00687v2 [cs.CV] 6 Oct 2015 visual representation. But what about videos? Do they have enough information to learn visual representations? In fact, humans also learn their visual representations not from millions of static images but years of dynamic sensory inputs.
Can we have similar learning capabilities for CNNs?
We present a simple yet surprisingly powerful approach for unsupervised learning of CNNs using hundreds of thousands of unlabeled videos from the web. Visual tracking is one of the first capabilities that develops in infants and often before semantic representations are learned1. Taking a leaf from this observation, we propose to exploit visual tracking for learning CNNs in an unsupervised manner. Specifically, we track millions of "moving" patches in hundreds of thousands of videos. Our key idea is that two patches connected by a track should have similar visual representation in deep feature space since they probably belong to the same object. We design a Siamese-triplet network with ranking loss function to train the CNN representation. This ranking loss function enforces that in the final deep feature space the first frame patch should be much closer to the tracked patch than any other randomly sampled patch. We demonstrate the strength of our learning algorithm using extensive experimental evaluation. Without using a single image from ImageNet, just 100K unlabeled videos and VOC 2012 dataset, we train an ensemble of AlexNet networks that achieves 52% mAP (no bounding box regression). This performance is similar to its ImageNet-supervised counterpart, an ensemble which achieves 54.4% mAP. We also show that our network trained using unlabeled videos achieves similar performance to its completely supervised counterpart on other tasks such as surface normal estimation. We believe this is the first time an unsupervised-pretrained CNN has been shown so competitive; that too on varied datasets and tasks. Specifically for VOC, we would like to put our results in context: this is the best results till-date by using only PASCAL-provided annotations (next best is scratch at
2. Related Work
Unsupervised learning of visual representations has a rich history starting from original auto-encoders work of Olhausen and Field. Most of the work in this area can be broadly divided into three categories.
The first class of algorithms focus on learning generative models with strong priors. These algorithms essentially capture co-occurrence statistics of features.
The second class of algorithms use manually defined features such as
SIFT or HOG and perform clustering over training data to discover semantic classes. Some of these recent algorithms also focus on learning mid-level representations rather than discovering semantic classes them1http://www.aoa.org/patients-and-public/good-vision-throughoutlife/childrens-vision/infant-vision-birth-to-24-months-of-age selves. The third class of algorithms and more related to our paper is unsupervised learning of visual representations from the pixels themselves using deep learning approaches. Starting from the seminal work of Olhausen and Field, the goal is to learn visual representations which are (a) sparse and(b) reconstructive. Olhausen and Field showed that using this criteria they can learn V1-like filters directly from the data. However, this work only focused on learning a single layer. This idea was extended by Hinton and Salakhutdinov to train a deep belief network in an unsupervised manner via stacking layer-by-layer RBMs. Similar to this, Bengio et al. investigated stacking of both RBMs and autoencoders. As a next step, Le et al. scaled up the learning of multi-layer autoencoder on large-scale unlabeled data. They demonstrated that although the network is trained in an unsupervised manner, the neurons in high layers can still have high responses on semantic objects such as human heads and cat faces. Sermanet et al. applied convolutional sparse coding to pre-train the model layer-bylayer in unsupervised manner. The model is then fine-tuned for pedestrian detection. In a contemporary work, Doersch et al. explored to use spatial context as a cue to perform unsupervised learning for CNNs.
However, it is not clear if static images is the right way to learn visual representations. Therefore, researchers have started focusing on learning feature representations using videos.
Early work such as focused on inclusion of constraints via video to autoencoder framework. The most common constraint is enforcing learned representations to be temporally smooth.
Similar to this, Goroshin et al. proposed to learn autoencoders based on the slowness prior. Other approaches such as Taylor et al. trained convolutional gated RBMs to learn latent representations from pairs of successive images. This was extended in a recent work by Srivastava et al. where they proposed to learn a LSTM model in an unsupervised manner to predict future frames.
Finally, our work is also related to metric learning via deep networks. For example, Chopra et al. proposed to learn convolutional networks in a siamese architecture for face verification.
Wang et al. introduced a deep triplet ranking network to learn fine-grained image similarity. Zhang et al. optimized the max-margin loss on triplet units to learn deep hashing function for image retrieval. However, all these methods required labeled data.
Our work is also related to, which used CNN pre-trained on ImageNet classification and detection dataset as initialization, and performed semisupervised learning in videos to tackle object detection in target domain. However, in our work, we propose an unsupervised approach instead of semi-supervised algorithm.
3. Overview
Our goal is to train convolutional neural networks using hundreds of thousands of unlabeled videos from the Internet. We follow the AlexNet architecture to design our base network. However, since we do not have labels, it is not clear what should be the loss function and how we should optimize it. But in case of videos, we have another supervisory information: time. For example, we all know that the scene does not change drastically within a short time in a video and same object instances appear in multiple frames of the video. So, how do we exploit this information to train a CNN-based representation?
We sample millions of patches in these videos and track them over time. Since we are tracking these patches, we know that the first and last tracked frames correspond to the same instance of the moving object or object part. Therefore, any visual representation that we learn should keep these two data points close in the feature space. But just using this constraint is not sufficient: all points can be mapped to a single point in feature space. Therefore, for training our
CNN, we sample a third patch which creates a triplet. For training, we use a loss function that enforces that the first two patches connected by tracking are closer in feature space than the first one and a random one.
Training a network with such triplets converges fast since the task is easy to overfit to. One way is to increase the number of training triplets. However, after initial convergence most triplets satisfy the loss function and therefore back-propagating gradients using such triplets is inefficient.
Instead, analogous to hard-negative mining, we select the third patch from multiple patches that violates the constraint(loss is maximum).
Selecting this patch leads to more meaningful gradients for faster learning.
4. Patch Mining in Videos
Given a video, we want to extract patches of interest(patches with motion in our case) and track these patches to create training instances. One obvious way to find patches of interest is to compute optical flow and use the high magnitude flow regions. However, since YouTube videos are noisy with a lot of camera motion, it is hard to localize moving objects using simple optical flow magnitude vectors. Thus we follow a two-step approach: in the first step, we obtain SURF interest points and use Improved Dense
Trajectories (IDT) to obtain motion of each SURF point. Note that since IDT applies a homography estimation(video stabilization) method, it reduces the problem caused by camera motion. Given the trajectories of SURF interest points, we classify these points as moving if the flow magnitude is more than 0.5 pixels. We also reject frames if (a) very few (< 25%) SURF interest points are classified as moving because it might be just noise; (b) majority of SURF interest points (> 75%) are classified as moving as
…
…
Query(First Frame)
Tracked(Last Frame)
Sliding Window Searching
Tracking
Small Motion
Camera Motion
Figure 2.
Given the video about buses (the "bus" label are not utilized), we perform IDT on it. red points represents the SURF feature points, green represents the trajectories for the points. We reject the frames with small and large camera motions (top pairs).
Given the selected frame, we find the bounding box containing most of the moving SURF points. We then perform tracking. The first and last frame of the track provide pair of patches for training
CNN. it corresponds to moving camera. Once we have extracted moving SURF interest points, in the second step, we find the best bounding box such that it contains most of the moving
SURF points. The size of the bounding box is set as h × w, and we perform sliding window with it in the frame. We take the bounding box which contains the most number of moving SURF interest points as the interest bounding box.
In the experiment, we set h = 227, w = 227 in the frame with size 448 × 600. Note that these patches might contain objects or part of an object as shown in Figure 2.
Tracking.
Given the initial bounding box, we perform tracking using the KCF tracker. After tracking along 30 frames in the video, we obtain the second patch. This patch acts as the similar patch to the query patch in the triplet.
Note that the KCF tracker does not use any supervised information except for the initial bounding box.
5. Learning Via Videos
In the previous section, we discussed how we can use tracking to generate pairs of patches. We use this procedure to generate millions of such pairs (See Figure 3 for examples of pairs of patches mined). We now describe how we use these as training instances for our visual representation learning.
5.1. Siamese Triplet Network
Our goal is to learn a feature space such that the query patch is closer to the tracked patch as compared to any other randomly sampled patch. To learn this feature space we design a Siamese-triplet network. A Siamese-triplet network consist of three base networks which share the same paramQuery(First Frame)
Tracked(Last Frame)
Query(First Frame)
Tracked(Last Frame)
Patch
Pairs
Patch
Pairs
Figure 3. Examples of patch pairs we obtain via patch mining in the videos. eters (see Figure 4). For our experiments, we take the image with size 227 × 227 as input. The base network is based on the AlexNet architecture for the convolutional layers. Then we stack two fully connected layers on the pool5 outputs, whose neuron numbers are 4096 and 1024 respectively. Thus the final output of each single network is 1024 dimensional feature space f(·). We define the loss function on this feature space.
5.2. Ranking Loss Function
Given the set of patch pairs S sampled from the video, we propose to learn an image similarity model in the form of CNN. Specifically, given an image X as an input for the network, we can obtain its feature in the final layer as f(X).
Then, we define the distance of two image patches X1, X2 based on the cosine distance in the feature space as, D(X1, X2) = 1 − f(X1) · f(X2)
∥f(X1)∥∥f(X2)∥.
We want to train a CNN to obtain feature representation f(·), so that the distance between query image patch and the tracked patch is small and the distance between query patch and other random patches is encouraged to be larger. Formally, given the patch set S, where Xi is the original query patch (first patch in tracked frames), X+ i is the tracked patch and X− i is a random patch from a different video, we want to enforce D(Xi, X− i ) > D(Xi, X+ i ). Therefore, the loss of our ranking model is defined by hinge loss as, L(Xi, X+ i, X− i ) = max{0, D(Xi, X+ i ) − D(Xi, X− i ) + M}, (2) where M represents the gap parameters between two distances. We set M = 0.5 in the experiment. Then our objective function for training can be represented as, min
W λ
2 ∥ W ∥2
N
� i=1 max{0, D(Xi, X+ i ) − D(Xi, X− i ) + M}, (3) where W is the parameter weights of the network, i.e., parameters for function f(·). N is the number of the triplets of samples. λ is a constant representing weight decay, which is set to λ = 0.0005.
5.3. Hard Negative Mining for Triplet Sampling
One non-trivial part for learning to rank is the process of selecting negative samples. Given a pair of similar images
Xi, X+ i, how can we select the patch X− i, which is a negative match to Xi, from the large pool of patches? Here we first select the negative patches randomly, and then find hard examples (in a process analogous to hard negative mining).
Random Selection:
During learning, we perform mini-batch Stochastic Gradient Descent (SGD). For each
Xi, X+ i, we randomly sample K negative matches in the same batch B, thus we have K sets of triplet of samples.
For every triplet of samples, we calculate the gradients over three of them respectively and perform back propagation.
Note that we shuffle all the images randomly after each epoch of training, thus the pair of patches Xi, X+ i can look at different negative matches each time.
Hard Negative Mining: While one can continue to sample random patches for creating the triplets, it is more efficient to search the negative patches smartly. After 10 epochs of training using negative data selected randomly, we want to make the problem harder to get more robust feature representations. Analogous to hard-negative mining procedure in SVM, where gradient descent learning is only performed on hard-negatives (not all possible negative), we search for
𝑋𝑖
𝑋𝑖
−
𝑋𝑖 𝑓(𝑋𝑖
+) 𝑓(𝑋𝑖
−) 𝑓(𝑋𝑖)
Ranking
Loss
Layer
Shared Weights
Shared Weights
Figure 4.
Siamese-triplet network. Each base network in the Siamese-triplet network share the same architecture and parameter weights. The architecture is rectified from AlexNet by using only two fully connected layers. Given a triplet of training samples, we obtain their features from the last layer by forward propagation and compute the ranking loss.
Figure 5.
Top response regions for the pool5 neurons of our unsupervised-CNN. Each row shows top response of one neuron. the negative patch such that the loss is maximum and use that patch to compute and back propagate gradients.
Specifically, the sampling of negative matches is similar as random selection before, except that this time we select according to the loss(Eq. 2). For each pair Xi, X+ i, we calculate the loss of all other negative matches in batch B, and select the top K ones with highest losses. We apply the loss on these K negative matches as our final loss and calculate the gradients over them. Since the feature of each sample is already computed after the forward propagation, we only need to calculate the loss over these features, thus the extra computation for hard negative mining is very small. For the experiments, we use K = 4. Note that while some of the negatives might be semantically similar patches, our embedding constraint only requires same instance examples to be closer than category examples (which can be closer than other negatives in the space).
5.4. Adapting for Supervised Tasks
Given the CNN learned by using unsupervised data, we want to transfer the learned representations to the tasks with supervised data. In our experiments, we apply our model to two different tasks including object detection and surface normal estimation. In both tasks we take the base network from our Siamese-triplet network and adjust the fully connected layers and outputs accordingly.
We introduce two ways to fine-tune and transfer the information obtained from unsupervised data to supervised learning.
One straight forward approach is directly applying our ranking model as a pre-trained network for the target task.
More specifically, we use the parameters of the convolutional layers in the base network of our triplet architecture as initialization for the target task. For the fully connected layers, we initialize them randomly. This method of transferring feature representation is very similar to the approach applied in RCNN. However, RCNN uses the network pre-trained with ImageNet Classification data. In our case, the unsupervised ranking task is quite different from object detection and surface normal estimation. Thus, we need to adapt the learning rate to the fine-tuning procedure introduced in RCNN. We start with the learning rate with ϵ = 0.01 instead of 0.001 and set the same learning rate for all layers. This setting is crucial since we want the pretrained features to be used as initialization of supervised learning, and adapting the features to the new task.
In this paper, we explore one more approach to transfer/fine-tune the network. Specifically, we note that there might be more juice left in the millions of unsupervised training data (which could not be captured in the initial learning stage).
Therefore, we use an iterative finetuning scheme. Given the initial unsupervised network, we first fine-tune using the PASCAL VOC data. Given the new fine-tuned network, we use this network to re-adapt to ranking triplet task. Here we again transfer convolutional parameters for re-adapting. Finally, this re-adapted network is fine-tuned on the VOC data yielding a better trained model.
We show in the experiment that this circular approach gives improvement in performance. We also notice that after two iterations of this approach the network converges.
5.5. Model Ensemble
We proposed an approach to learn CNNs using unlabeled videos. However, there is absolutely no limit to generating training instances and pairs of tracked patches (YouTube has more than billions of videos). This opens up the possibility of training multiple CNNs using different sets of data.
Once we have trained these CNNs, we append the fc7 features from each of these CNNs to train the final SVM. Note that the ImageNet trained models also provide initial boost for adding more networks (See Table 1).
5.6. Implementation Details
We apply mini-batch SGD in training. As the 3 networks share the same parameters, instead of inputting 3 samples to the triplet network, we perform the forward propagation for the whole batch by a single network and calculate the loss based on the output feature. Given a pair of patches
Xi, X+ i, we randomly select another patch X− i ∈ B which is extracted in a different video from Xi, X+ i. Given their features from forward propagation f(Xi), f(X+ i ), f(X− i ), we can compute the loss as Eq. 2.
For unsupervised learning, we download 100K videos from YouTube using the URLs provided by. used thousands of keywords to retrieve videos from YouTube.
Note we drop the labels associated with each video. By performing our patch mining method on the videos, we obtain
8 million image patches. We train three different networks separately using 1.5M, 5M and 8M training samples. We report numbers based on these three networks. To train our siamese-triplet networks, we set the batch size as |B| = 100, (a) Unsupervised Pre-trained(b) Fine-tuned
Figure 6.
Conv1 filters visualization. (a) The filters of the first convolutional layer of the siamese-triplet network trained in unsupervised manner. (b) By fine-tuning the unsupervised pre-trained network on PASCAL VOC 2012, we obtain sharper filters. the learning rate starting with ϵ0 = 0.001. We first train our network with random negative samples at this learning rate for 150K iterations, and then we apply hard negative mining based on it. For training on 1.5M patches, we reduce the learning rate by a factor of 10 at every 80K iterations and train for 240K iterations. For training on 5M and 8M patches, we reduce the learning rate by a factor of 10 at every 120K iterations and train for 350K iterations.
6. Experiments
We demonstrate the quality of our learned visual representations with qualitative and quantitative experiments.
Qualitatively, we show the convolutional filters learned in layer 1 (See Figure 6). Our learned filters are similar to V1 though not as strong. However, after fine-tuning on PASCAL VOC 2012, these filters become quite strong. We also show that the underlying representation learns a reasonable nearness metric by showing what the units in Pool5 layers represent (See Figure 5). Ignoring boundary effects, each pool5 unit has a receptive field of 195 × 195 pixels in the original 227 × 227 pixel input. A central pool5 unit has a nearly global view, while one near the edge has a smaller, clipped support. Each row displays top 6 activations for a pool5 unit. We have chosen 5 pool5 units for visualization.
For example, the first neuron represents animal heads, second represents potted plant, etc. This visualization indicates the nearness metric learned by the network since each row corresponds to similar firing patterns inside the CNN. Our unsupervised networks are available for download.
6.1. Unsupervised CNNs without Fine-tuning
First, we demonstrate that the unsupervised-CNN representation learned using videos (without fine-tuning) is reasonable.
We perform Nearest Neighbors (NN) using ground-truth (GT) windows in VOC 2012 val set as query.
The retrieval-database consists of all selective search windows (more than 0.5 overlap with GT windows) in VOC
2012 train set. See Figure 7 for qualitative results. Our unsupervised-CNN is far superior to a random AlexNet architecture and the results are quite comparable to AlexNet trained on ImageNet.
Quantitatively, we measure the retrieval rate by counting number of correct retrievals in top-K (K=20) retrievals. A retrieval is correct if the semantic class for retrieved patch and query patch are the same. Using our unsupervised-CNN(Pool5 features) without fine-tuning and cosine distance, we obtain 40% retrieval rate. Our performance is significantly better as compared to 24% by ELDA on HOG and 19% by AlexNet with random parameters (our initialization). This clearly demonstrates our unsupervised network learns a good visual representation compared to a random parameter CNN. As a baseline, ImageNet CNN performs
62% (but note it already learns on semantics).
We also evaluate our unsupervised-CNN without finetuning for scene classification task on MIT Indoor 67.
We train a linear classifier using softmax loss.
Using pool5 features from unsupervised-CNN without fine-tuning gives 41% classification accuracy compared to 21% for
GIST+SVM and 16% for random AlexNet.
ImageNettrained AlexNet has 54% accuracy. We also provide object detection results without fine-tuning in the next section.
6.2. Unsupervised CNNs with Fine-tuning
Next, we evaluate our approach by transferring the feature representation learned in unsupervised manner to the tasks with labeled data. We focus on two challenging problems: object detection and surface normal estimation.
Object Detection
For object detection, we perform our experiments on PASCAL VOC 2012 dataset.
We follow the detection pipeline introduced in RCNN, which borrowed the CNNs pre-trained on other datasets and fine-tuned on it with the VOC data. The fine-tuned CNN was then used to extract features followed by training SVMs for each object class.
However, instead of using ImageNet pre-trained network as initialization in RCNN, we use our unsupervised-CNN. We fine-tune our network with the trainval set (11540 images) and train SVMs with them. Evaluation is performed in the standard test set (10991 images).
At the fine-tuning stage, we change the output to 21 classes and initialize the convolutional layers with our unsupervised pre-trained network. To fine-tune the network, we start with learning rate as ϵ = 0.01 and reduce the learning rate by a factor of 10 at every 80K iterations. The network is fine-tuned for 200K iterations. Note that for all the experiments, no bounding box regression is performed.
Query(a) Random AlexNet(b) Imagenet AlexNet(c) Unsupervised AlexNet
Figure 7. Nearest neighbors results. Given the query object from VOC 2012 val, we retrieve the NN from VOC 2012 train via calculating the cosine distance on pool5 feature space. We compare the results of 3 different models: (a) AlexNet with random parameters; (b) AlexNet trained with Imagenet data; (c) AlexNet trained using our unsupervised method on 8M data.
Table 1. mean Average Precision (mAP) on VOC 2012. "external" column shows the number of patches used to pre-train unsupervised-CNN.
VOC 2012 test external aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAP scratch
44.0 scratch (3 ensemble)
47.3 unsup + ft
1.5M
46.2 unsup + ft
5M
47.0 unsup + ft
8M
47.5 unsup + ft (2 ensemble)
6.5M
50.5 unsup + ft (3 ensemble)
8M
52.0 unsup + iterative ft
5M
RCNN 70K
RCNN 70K (2 ensemble)
RCNN 70K (3 ensemble)
RCNN 200K (big stepsize)
We compare our method with the model trained from scratch as well as using ImagNet pre-trained network. Notice that the results for VOC 2012 reported in RCNN are obtained by only fine-tuning on the train set without using the val set. For fair comparison, we fine-tuned the ImageNet pre-trained network with VOC 2012 trainval set.
Moreover, as the step size of reducing learning rate in RCNN is set to 20K and iterations for fine-tuning is 70K, we also try to enlarge the step size to 50K and finetune the network for 200K iterations. We report the results for both of these settings.
Single Model. We show the results in Table 1. As a baseline, we train the network from scratch on VOC 2012 dataset and obtain 44% mAP. Using our unsupervised network pre-trained with 1.5M pair of patches and then finetuned on VOC 2012, we obtain mAP of 46.2% (unsup+ft, external data = 1.5M). However, using more data, 5M and 8M patches in pre-training and then fine-tune, we can achieve 47% and 47.5% mAP. These results indicate that our unsupervised network provides a significant boost as compared to the scratch network. More importantly, when more unlabeled data is applied, we can get better performance ( 3.5% boost compared to training from scratch).
Model Ensemble. We also try combining different models using different sets of unlabeled data in pre-training. By ensembling two fine-tuned networks which are pre-trained using 1.5M and 5M patches, we obtained a boost of 3.5% comparing to the single model, which is 50.5%(unsup+ft(2 ensemble)). Finally, we ensemble all three different networks pre-trained with different sets of data, whose size are
1.5M, 5M and 8M respectively. We get another boost and reach 52% mAP (unsup+ft (3 ensemble)).
Baselines. We compare our approach with RCNN which uses ImageNet pre-trained models. Following the procedure in, we obtain 50.1% mAP (RCNN 70K) by setting the step size to 20K and fine-tuning for 70K iterations. To generate a model ensemble, the CNNs are first trained on the ImageNet dataset separately, and then they are fine-tuned with the VOC 2012 dataset. The result of ensembling two of these networks is 53.6% mAP (RCNN
70K (2 ensemble)). If we ensemble three networks, we get a mAP of 54.4%. For fair of comparison, we also finetuned the ImageNet pre-trained model with larger step size(50K) and more iterations (200K). The result is 52.3% mAP(RCNN 200K (big stepsize)). Note that while ImageNet network shows diminishing returns with ensembling since the training data remains similar, in our case since every network in the ensemble looks at different sets of data, we get huge performance boosts.
Exploring a better way to transfer learned representation. Given our fine-tuned model using 5M patches in pre-training (unsup+ft, external = 5M), we use it to re-learn and re-adapt to the unsupervised triplet task. After that, the network is re-applied to fine-tune on VOC 2012. The final
Table 2. Results on NYU v2 for per-pixel surface normal estimation, evaluated over valid pixels.(Lower Better)(Higher Better)
Mean
Median 11.25◦ 22.5◦ 30◦ scratch
46.8 52.5 unsup + ft
ImageNet + ft
UNFOLD 
Discr. 
3DP (MW) 
52.0 57.8 result for this single model is 48% mAP (unsup + iterative ft), which is 1% better than the initial fine-tuned network.
Unsupervised network without fine-tuning: We also perform object detection without fine-tuning on VOC 2012.
We extract pool5 features using our unsupervised-CNN and train SVM on top of it. We obtain mAP of 26.1% using our unsupervised network (training with 8M data). The ensemble of two unsupervised-network (training with 5M and 8M data) gets mAP of 28.2%. As a comparison, Imagenet pretrained network without fine-tuning gets mAP of 40.4%.
Surface Normal Estimation
To illustrate that our unsupervised representation can be generalized to different tasks, we adapt the unsupervised
CNN to the task of surface normal estimation from a RGB image.
In this task, we want to estimate the orientation of the pixels.
We perform our experiments on the NYUv2 dataset, which includes 795 images for training and 654 images for testing. Each image is has corresponding depth information which can be used to generate groundtruth surface normals. For evaluation and generating the groundtruth, we adopt the protocols introduced in which is used by different methods on this task.
To apply deep learning to this task, we followed the same form of outputs and loss function as the coarse network mentioned in. Specifically, we first learn a codebook by performing k-means on surface normals and generate 20 codewords. Each codeword represents one class and thus we transform the problem to 20-class classification for each pixel. Given a 227 × 227 image as input, our network generates surface normals for the whole scene. The output of our network is 20 × 20 pixels, each of which is represented by a distribution over 20 codewords. Thus the dimension of output is 20 × 20 × 20 = 8000.
The network architecture for this task is also based on the AlexNet. To relieve over-fitting, we only stack two fully connected layers with 4096 and 8000 neurons on the pool5 layer. During training, we initialize the network with the unsupervised pre-trained network (single network using 8M external data). We use the same learning rate 1.0 × 10−6 as and fine-tune with 10K iterations given the small number of training data. Note that unlike, we do not utilize any data from the videos in NYU dataset for training.
Figure 8. Surface normal estimation results on NYU dataset. For visualization, we use green for horizontal surface, blue for facing right and red for facing left, i.e., blue → X; green → Y; red → Z.
For comparison, we also trained networks from scratch as well as using ImageNet pre-trained. For evaluation, we report mean and median error (in degrees). We also report percentage of pixels with less than 11.25, 22.5 and 30 degree errors. We show our qualitative results in in Figure 8. and quantitative results in Table 2. Our approach (unsup + ft) is significantly better than network trained from scratch and comes very close to Imagenet-pretrained CNN (∼ 1%).
7. Discussion and Conclusion
We have presented an approach to train CNNs in an unsupervised manner using videos. Specifically, we track millions of patches and learn an embedding using CNN that keeps patches from same track closer in the embedding space as compared to any random third patch. Our unsupervised pre-trained CNN fine-tuned using VOC training data outperforms CNN trained from scratch by 3.5%. An ensemble version of our approach outperforms scratch by 4.7% and comes tantalizingly close to an Imagenet-pretrained
CNN (within 2.5%). We believe this is an extremely surprising result since until recently semantic supervision was considered a strong requirement for training CNNs. We believe our successful implementation opens up a new space for designing unsupervised learning algorithms for CNN training.
Acknowledgement: This work was partially supported by ONR MURI
N000141010934 and NSF IIS 1320083. This material was also based on research partially sponsored by DARPA under agreement number FA875014-2-0244. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA or the U.S. Government. The authors would like to thank Yahoo! and Nvidia for the compute cluster and GPU donations respectively.
References
 H. Bay, T. Tuytelaars, and L. V. Gool.
Surf: Speeded up robust features. In ECCV, 2006. 3
 Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives. TPAMI, 35(8):1798–1828, 2013. 2
 Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. Greedy layerwise training of deep networks. In NIPS, 2007. 1, 2
 S. Chopra, R. Hadsell, and Y. LeCun. Learning a similarity metric discriminatively, with application to face verification. In CVPR, N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In CVPR, 2005. 1
 C. Doersch, A. Gupta, and A. A. Efros. Mid-level visual element discovery as discriminative mode seeking. In NIPS, 2013. 2
 C. Doersch, A. Gupta, and A. A. Efros. Context as supervisory signal: Discovering objects with predictable context. In ECCV, 2014.
 C. Doersch, A. Gupta, and A. A. Efros. Unsupervised visual representation learning by context prediction. In ICCV, 2015. 2
 S. M. A. Eslami, N. Heess, and J. Winn. The shape boltzmann machine: a strong model of object shape. In CVPR, 2012. 2
 M. Everingham, L. V. Gool, C. K. Williams, J. Winn,, and A. Zisserman.
The pascal visual object classes (voc) challenge.
IJCV, 88(2):303–338, 2010. 6
 P. Foldiak. Learning invariance from transformation sequences. Neural Computation, 3(2):194–200, 1991. 2
 D. F. Fouhey, A. Gupta, and M. Hebert. Data-driven 3D primitives for single image understanding. In ICCV, 2013. 8
 D. F. Fouhey, A. Gupta, and M. Hebert. Unfolding an indoor origami world. In ECCV, 2014. 8
 R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In
CVPR, 2014. 5, 6, 7
 Y. Gong, Y. Jia, T. K. Leung, A. Toshev, and S. Ioffe. Deep convolutional ranking for multilabel image annotation. In ICLR, 2007.
 R. Goroshin, J. Bruna, J. Tompson, D. Eigen, and Y. LeCun. Unsupervised learning of spatiotemporally coherent metrics.
CoRR, abs/1412.6056, 2015. 2
 R. Hadsell, S. Chopra, and Y. LeCun. Dimensionality reduction by learning an invariant mapping. In CVPR, 2006. 2
 B. Hariharan, J. Malik, and D. Ramanan. Discriminative decorrelation for clustering and classification. In ECCV, 2012. 6
 J. F. Henriques, R. Caseiro, P. Martins, and J. Batista. High-speed tracking with kernelized correlation filters. TPAMI, 2015. 3
 G. E. Hinton, P. Dayan, B. J. Frey, and R. M. Neal.
The" wake-sleep" algorithm for unsupervised neural networks. Science, 268(5214):1158–1161, 1995. 2
 G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313:504–507, 2006. 2
 E. Hoffer and N. Ailon. Deep metric learning using triplet network.
CoRR, /abs/1412.6622, 2015. 2
 Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for fast feature embedding. CoRR, /abs/1408.5093, 2014. 1
 A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012. 1, L. Ladick´y, B. Zeisl, and M. Pollefeys.
Discriminatively trained dense surface normal estimation. In ECCV, 2014. 8
 Q. V. Le, M. A. Ranzato, R. Monga, M. Devin, K. Chen, G. S. Corrado, J. Dean, and A. Y. Ng. Building high-level features using large scale unsupervised learning. In ICML, 2012. 1, 2
 Q. V. Le, W. Y. Zou, S. Y. Yeung, and A. Y. Ng. Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis. In CVPR, 2011. 2
 Y. LeCun, B. Boser, J. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Handwritten digit recognition with a backpropagation network. In NIPS, 1990. 1
 H. Lee, R. Grosse, R. Ranganath, and A. Y. Ng. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. In ICML, 2009. 2
 X. Liang, S. Liu, Y. Wei, L. Liu, L. Lin, and S. Yan. Computational baby learning. CoRR, abs/1411.2861, 2014. 2, 5
 S. Liu, X. Liang, L. Liu, X. Shen, J. Yang, C. Xu, X. Cao, and S. Yan. Matching-cnn meets knn: Quasi-parametric human parsing.
In CVPR, 2015. 2
 D. Lowe.
Distinctive Image Features from Scale-Invariant Keypoints. IJCV, 60(2):91–110, 2004. 1
 P. Luo, X. Wang, and X. Tang. Hierarchical face parsing via deep learning. In CVPR, 2012. 2
 H. Mobahi, R. Collobert, and J. Weston. Deep learning from temporal coherence in video. In ICML, 2009. 1, 2
 B. A. Olshausen and D. J. Field. Sparse coding with an overcomplete basis set: A strategy employed by v1? Vision research, 1997. 1, 2
 A. Quattoni and A.Torralba. Recognizing indoor scenes. In CVPR, M. A. Ranzato, F. J. Huang, Y.-L. Boureau, and Y. LeCun. Unsupervised learning of invariant feature hierarchies with applications to object recognition. In CVPR, 2007. 1
 B. C. Russell, A. A. Efros, J. Sivic, W. T. Freeman, and A. Zisserman.
Using multiple segmentations to discover objects and their extent in image collections. In CVPR, 2006. 2
 P. Sermanet, K. Kavukcuoglu, S. Chintala, and Y. LeCun. Pedestrian detection with unsupervised multi-stage feature learning. In CVPR, N. Silberman, D. Hoiem, P. Kohli, and R. Fergus. Indoor segmentation and support inference from RGBD images. In ECCV, 2012.
 S. Singh, A. Gupta, and A. A. Efros. Unsupervised discovery of mid-level discriminative patches. In ECCV, 2012. 2
 J. Sivic, B. C. Russell, A. A. Efros, A. Zisserman, and W. T. Freeman.
Discovering objects and their location in images. In ICCV, 2005. 2
 N. Srivastava, E. Mansimov, and R. Salakhutdinov.
Unsupervised learning of video representations using lstms.
CoRR, abs/1502.04681, 2015. 2
 N. Srivastava and R. R. Salakhutdinov. Multimodal learning with deep boltzmann machines. In NIPS, 2012. 1, 2
 D. Stavens and S. Thrun. Unsupervised learning of invariant features using video. In CVPR, 2010. 2
 E. B. Sudderth, A. Torralba, W. T. Freeman, and A. S. Willsky.
Describing visual scenes using transformed dirichlet processes. In
NIPS, 2005. 2
 Y. Tang, R. Salakhutdinov, and G. Hinton. Robust boltzmann machines for recognition and denoising. In CVPR, 2012. 2
 G. W. Taylor, R. Fergus, Y. LeCun, and C. Bregler. Convolutional learning of spatio-temporal features. In ECCV, 2010. 1, 2
 P. Vincent, H. Larochelle, Y. Bengio, and P. Manzagol.
Extracting and composing robust features with denoising autoencoders. In
ICML, 2008. 1, 2
 H. Wang and C. Schmid. Action recognition with improved trajectories. In ICCV, 2013. 3
 J. Wang, Y. Song, T. Leung, C. Rosenberg, J. Wang, J. Philbin, B. Chen, and Y. Wu. Learning fine-grained image similarity with deep ranking. In CVPR, 2014. 2, 3
 X. Wang, D. F. Fouhey, and A. Gupta. Designing deep networks for surface normal estimation. In CVPR, 2015. 8
 L. Wiskott and T. J. Sejnowski. Slow feature analysis:unsupervised learning of invariances. Neural Computation, 14:715–770, 2002. 2
 P. Wohlhart and V. Lepetit. Learning descriptors for object recognition and 3d pose estimation. In CVPR, 2015. 2
 R. Zhang, L. Lin, R. Zhang, W. Zuo, and L. Zhang. Bit-scalable deep hashing with regularized similarity learning for image retrieval and person re-identification. TIP, 24(12):4766–4779, 2015. 2
 W. Y. Zou, S. Zhu, A. Y. Ng, and K. Yu. Deep learning of invariant features via simulated fixations in video. In NIPS, 2012. 1, 2Densely Connected Convolutional Networks
Gao Huang∗
Cornell University gh349@cornell.edu
Zhuang Liu∗
Tsinghua University liuzhuang13@mails.tsinghua.edu.cn
Laurens van der Maaten
Facebook AI Research lvdmaaten@fb.com
Kilian Q. Weinberger
Cornell University kqw4@cornell.edu
Abstract
Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections—one between each layer and its subsequent layer—our network has
L(L+1)
2 direct connections.
For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet.
1. Introduction
Convolutional neural networks (CNNs) have become the dominant machine learning approach for visual object recognition. Although they were originally introduced over 20 years ago, improvements in computer hardware and network structure have enabled the training of truly deep
CNNs only recently. The original LeNet5 consisted of 5 layers, VGG featured 19, and only last year Highway
∗Authors contributed equally x0 x1
H1 x2
H2
H3
H4 x3 x4
Figure 1: A 5-layer dense block with a growth rate of k = 4.
Each layer takes all preceding feature-maps as input.
Networks and Residual Networks (ResNets) have surpassed the 100-layer barrier.
As CNNs become increasingly deep, a new research problem emerges: as information about the input or gradient passes through many layers, it can vanish and "wash out" by the time it reaches the end (or beginning) of the network. Many recent publications address this or related problems. ResNets and Highway Networks bypass signal from one layer to the next via identity connections. Stochastic depth shortens ResNets by randomly dropping layers during training to allow better information and gradient flow. FractalNets repeatedly combine several parallel layer sequences with different number of convolutional blocks to obtain a large nominal depth, while maintaining many short paths in the network.
Although these different approaches vary in network topology and training procedure, they all share a key characteristic: they create short paths from early layers to later layers.
In this paper, we propose an architecture that distills this insight into a simple connectivity pattern: to ensure maximum information flow between layers in the network, we connect all layers (with matching feature-map sizes) directly with each other. To preserve the feed-forward nature, each layer obtains additional inputs from all preceding layers and passes on its own feature-maps to all subsequent layers. Figure 1 illustrates this layout schematically. Crucially, in contrast to ResNets, we never combine features through summation before they are passed into a layer; instead, we combine features by concatenating them. Hence, the ℓth layer has ℓ inputs, consisting of the feature-maps of all preceding convolutional blocks. Its own feature-maps are passed on to all L−ℓ subsequent layers. This introduces
L(L+1)
2 connections in an L-layer network, instead of just
L, as in traditional architectures. Because of its dense connectivity pattern, we refer to our approach as Dense Convolutional Network (DenseNet).
A possibly counter-intuitive effect of this dense connectivity pattern is that it requires fewer parameters than traditional convolutional networks, as there is no need to relearn redundant feature-maps. Traditional feed-forward architectures can be viewed as algorithms with a state, which is passed on from layer to layer. Each layer reads the state from its preceding layer and writes to the subsequent layer.
It changes the state but also passes on information that needs to be preserved. ResNets make this information preservation explicit through additive identity transformations.
Recent variations of ResNets show that many layers contribute very little and can in fact be randomly dropped during training. This makes the state of ResNets similar to (unrolled) recurrent neural networks, but the number of parameters of ResNets is substantially larger because each layer has its own weights. Our proposed DenseNet architecture explicitly differentiates between information that is added to the network and information that is preserved.
DenseNet layers are very narrow (e.g., 12 filters per layer), adding only a small set of feature-maps to the "collective knowledge" of the network and keep the remaining featuremaps unchanged—and the final classifier makes a decision based on all feature-maps in the network.
Besides better parameter efficiency, one big advantage of DenseNets is their improved flow of information and gradients throughout the network, which makes them easy to train. Each layer has direct access to the gradients from the loss function and the original input signal, leading to an implicit deep supervision. This helps training of deeper network architectures. Further, we also observe that dense connections have a regularizing effect, which reduces overfitting on tasks with smaller training set sizes.
We evaluate DenseNets on four highly competitive benchmark datasets (CIFAR-10, CIFAR-100, SVHN, and ImageNet). Our models tend to require much fewer parameters than existing algorithms with comparable accuracy.
Further, we significantly outperform the current state-ofthe-art results on most of the benchmark tasks.
2. Related Work
The exploration of network architectures has been a part of neural network research since their initial discovery. The recent resurgence in popularity of neural networks has also revived this research domain. The increasing number of layers in modern networks amplifies the differences between architectures and motivates the exploration of different connectivity patterns and the revisiting of old research ideas.
A cascade structure similar to our proposed dense network layout has already been studied in the neural networks literature in the 1980s. Their pioneering work focuses on fully connected multi-layer perceptrons trained in a layerby-layer fashion. More recently, fully connected cascade networks to be trained with batch gradient descent were proposed. Although effective on small datasets, this approach only scales to networks with a few hundred parameters. In, utilizing multi-level features in CNNs through skip-connnections has been found to be effective for various vision tasks. Parallel to our work, derived a purely theoretical framework for networks with cross-layer connections similar to ours.
Highway Networks were amongst the first architectures that provided a means to effectively train end-to-end networks with more than 100 layers. Using bypassing paths along with gating units, Highway Networks with hundreds of layers can be optimized without difficulty. The bypassing paths are presumed to be the key factor that eases the training of these very deep networks. This point is further supported by ResNets, in which pure identity mappings are used as bypassing paths. ResNets have achieved impressive, record-breaking performance on many challenging image recognition, localization, and detection tasks, such as ImageNet and COCO object detection. Recently, stochastic depth was proposed as a way to successfully train a 1202-layer ResNet. Stochastic depth improves the training of deep residual networks by dropping layers randomly during training. This shows that not all layers may be needed and highlights that there is a great amount of redundancy in deep (residual) networks. Our paper was partly inspired by that observation. ResNets with pre-activation also facilitate the training of state-of-the-art networks with > 1000 layers.
An orthogonal approach to making networks deeper(e.g., with the help of skip connections) is to increase the network width. The GoogLeNet uses an "Inception module" which concatenates feature-maps produced by filters of different sizes. In, a variant of ResNets with wide generalized residual blocks was proposed.
In fact, simply increasing the number of filters in each layer of C o n v o l u t i o n
P o o l i n g
Dense Block 1
C o n v o l u t i o n
P o o l i n g
P o o l i n g
L i n e a r
C o n v o l u t i o n
Input
Prediction
�horse�
Dense Block 2
Dense Block 3
Figure 2: A deep DenseNet with three dense blocks. The layers between two adjacent blocks are referred to as transition layers and change feature-map sizes via convolution and pooling.
ResNets can improve its performance provided the depth is sufficient. FractalNets also achieve competitive results on several datasets using a wide network structure.
Instead of drawing representational power from extremely deep or wide architectures, DenseNets exploit the potential of the network through feature reuse, yielding condensed models that are easy to train and highly parameterefficient. Concatenating feature-maps learned by different layers increases variation in the input of subsequent layers and improves efficiency. This constitutes a major difference between DenseNets and ResNets. Compared to Inception networks, which also concatenate features from different layers, DenseNets are simpler and more efficient.
There are other notable network architecture innovations which have yielded competitive results. The Network in Network (NIN) structure includes micro multi-layer perceptrons into the filters of convolutional layers to extract more complicated features. In Deeply Supervised Network (DSN), internal layers are directly supervised by auxiliary classifiers, which can strengthen the gradients received by earlier layers. Ladder Networks introduce lateral connections into autoencoders, producing impressive accuracies on semi-supervised learning tasks.
In, Deeply-Fused Nets (DFNs) were proposed to improve information flow by combining intermediate layers of different base networks. The augmentation of networks with pathways that minimize reconstruction losses was also shown to improve image classification models.
3. DenseNets
Consider a single image x0 that is passed through a convolutional network. The network comprises L layers, each of which implements a non-linear transformation Hℓ(·), where ℓ indexes the layer. Hℓ(·) can be a composite function of operations such as Batch Normalization (BN), rectified linear units (ReLU), Pooling, or Convolution (Conv). We denote the output of the ℓth layer as xℓ.
ResNets.
Traditional convolutional feed-forward networks connect the output of the ℓth layer as input to the(ℓ + 1)th layer, which gives rise to the following layer transition: xℓ = Hℓ(xℓ−1).
ResNets add a skip-connection that bypasses the non-linear transformations with an identity function: xℓ = Hℓ(xℓ−1) + xℓ−1.
An advantage of ResNets is that the gradient can flow directly through the identity function from later layers to the earlier layers. However, the identity function and the output of Hℓ are combined by summation, which may impede the information flow in the network.
Dense connectivity.
To further improve the information flow between layers we propose a different connectivity pattern: we introduce direct connections from any layer to all subsequent layers. Figure 1 illustrates the layout of the resulting DenseNet schematically. Consequently, the ℓth layer receives the feature-maps of all preceding layers, x0,..., xℓ−1, as input: xℓ = Hℓ([x0, x1,..., xℓ−1]), (2) where [x0, x1,..., xℓ−1] refers to the concatenation of the feature-maps produced in layers 0,..., ℓ−1. Because of its dense connectivity we refer to this network architecture as
Dense Convolutional Network (DenseNet). For ease of implementation, we concatenate the multiple inputs of Hℓ(·) in eq. (2) into a single tensor.
Composite function.
Motivated by, we define Hℓ(·) as a composite function of three consecutive operations: batch normalization (BN), followed by a rectified linear unit (ReLU) and a 3 × 3 convolution (Conv).
Pooling layers.
The concatenation operation used in Eq. (2) is not viable when the size of feature-maps changes.
However, an essential part of convolutional networks is down-sampling layers that change the size of feature-maps.
To facilitate down-sampling in our architecture we divide the network into multiple densely connected dense blocks; see Figure 2. We refer to layers between blocks as transition layers, which do convolution and pooling. The transition layers used in our experiments consist of a batch normalization layer and an 1×1 convolutional layer followed by a 2×2 average pooling layer.
Growth rate.
If each function Hℓ produces k featuremaps, it follows that the ℓth layer has k0 +k ×(ℓ−1) input feature-maps, where k0 is the number of channels in the input layer. An important difference between DenseNet and existing network architectures is that DenseNet can have very narrow layers, e.g., k = 12. We refer to the hyperparameter k as the growth rate of the network. We show in Section 4 that a relatively small growth rate is sufficient to
Layers
Output Size
DenseNet-121(k = 32)
DenseNet-169(k = 32)
DenseNet-201(k = 32)
DenseNet-161(k = 48)
Convolution
112 × 112
7 × 7 conv, stride 2
Pooling
56 × 56
3 × 3 max pool, stride 2
Dense Block
56 × 56
� 1 × 1 conv
3 × 3 conv
�
× 6
� 1 × 1 conv
3 × 3 conv
�
× 6
� 1 × 1 conv
3 × 3 conv
�
× 6
� 1 × 1 conv
3 × 3 conv
�
× 6
Transition Layer
56 × 56
1 × 1 conv
28 × 28
2 × 2 average pool, stride 2
Dense Block
28 × 28
� 1 × 1 conv
3 × 3 conv
�
× 12
� 1 × 1 conv
3 × 3 conv
�
× 12
� 1 × 1 conv
3 × 3 conv
�
× 12
� 1 × 1 conv
3 × 3 conv
�
× 12
Transition Layer
28 × 28
1 × 1 conv
14 × 14
2 × 2 average pool, stride 2
Dense Block
14 × 14
� 1 × 1 conv
3 × 3 conv
�
× 24
� 1 × 1 conv
3 × 3 conv
�
× 32
� 1 × 1 conv
3 × 3 conv
�
× 48
� 1 × 1 conv
3 × 3 conv
�
× 36
Transition Layer
14 × 14
1 × 1 conv
7 × 7
2 × 2 average pool, stride 2
Dense Block
7 × 7
� 1 × 1 conv
3 × 3 conv
�
× 16
� 1 × 1 conv
3 × 3 conv
�
× 32
� 1 × 1 conv
3 × 3 conv
�
× 32
� 1 × 1 conv
3 × 3 conv
�
× 24
Classification
Layer
1 × 1
7 × 7 global average pool
1000D fully-connected, softmax
Table 1: DenseNet architectures for ImageNet. The growth rate for the first 3 networks is k = 32, and k = 48 for DenseNet-161. Note that each "conv" layer shown in the table corresponds the sequence BN-ReLU-Conv. obtain state-of-the-art results on the datasets that we tested on. One explanation for this is that each layer has access to all the preceding feature-maps in its block and, therefore, to the network's "collective knowledge". One can view the feature-maps as the global state of the network. Each layer adds k feature-maps of its own to this state. The growth rate regulates how much new information each layer contributes to the global state. The global state, once written, can be accessed from everywhere within the network and, unlike in traditional network architectures, there is no need to replicate it from layer to layer.
Bottleneck layers.
Although each layer only produces k output feature-maps, it typically has many more inputs. It has been noted in that a 1×1 convolution can be introduced as bottleneck layer before each 3×3 convolution to reduce the number of input feature-maps, and thus to improve computational efficiency. We find this design especially effective for DenseNet and we refer to our network with such a bottleneck layer, i.e., to the BN-ReLU-Conv(1×
1)-BN-ReLU-Conv(3×3) version of Hℓ, as DenseNet-B. In our experiments, we let each 1×1 convolution produce 4k feature-maps.
Compression.
To further improve model compactness, we can reduce the number of feature-maps at transition layers. If a dense block contains m feature-maps, we let the following transition layer generate ⌊θm⌋ output featuremaps, where 0 <θ ≤1 is referred to as the compression factor. When θ = 1, the number of feature-maps across transition layers remains unchanged. We refer the DenseNet with θ<1 as DenseNet-C, and we set θ = 0.5 in our experiment.
When both the bottleneck and transition layers with θ < 1 are used, we refer to our model as DenseNet-BC.
Implementation Details.
On all datasets except ImageNet, the DenseNet used in our experiments has three dense blocks that each has an equal number of layers. Before entering the first dense block, a convolution with 16 (or twice the growth rate for DenseNet-BC) output channels is performed on the input images. For convolutional layers with kernel size 3×3, each side of the inputs is zero-padded by one pixel to keep the feature-map size fixed. We use 1×1 convolution followed by 2×2 average pooling as transition layers between two contiguous dense blocks. At the end of the last dense block, a global average pooling is performed and then a softmax classifier is attached. The feature-map sizes in the three dense blocks are 32× 32, 16×16, and 8×8, respectively. We experiment with the basic DenseNet structure with configurations {L = 40, k = 12}, {L =
100, k = 12} and {L = 100, k = 24}.
For DenseNetBC, the networks with configurations {L = 100, k = 12}, {L=250, k=24} and {L=190, k=40} are evaluated.
In our experiments on ImageNet, we use a DenseNet-BC structure with 4 dense blocks on 224×224 input images.
The initial convolution layer comprises 2k convolutions of size 7×7 with stride 2; the number of feature-maps in all other layers also follow from setting k. The exact network configurations we used on ImageNet are shown in Table 1.
4. Experiments
We empirically demonstrate DenseNet's effectiveness on several benchmark datasets and compare with state-of-theart architectures, especially with ResNet and its variants.
4.1. Datasets
CIFAR.
The two CIFAR datasets consist of colored natural images with 32×32 pixels. CIFAR-10 (C10) conMethod
Depth
Params
C10
C10+
C100
C100+
SVHN
Network in Network All-CNN Deeply Supervised Net Highway Network FractalNet 
38.6M
2.01 with Dropout/Drop-path
38.6M
ResNet 
1.7MResNet (reported by )
1.7M
ResNet with Stochastic Depth 
1.7M
10.2MWide ResNet 
11.0M36.5M20.50 with Dropout
2.7MResNet (pre-activation) 
1.7M
11.26∗
35.58∗10.2M
10.56∗
33.47∗DenseNet (k = 12)
1.0M
DenseNet (k = 12)
7.0M
DenseNet (k = 24)
27.2M
DenseNet-BC (k = 12)
0.8M
DenseNet-BC (k = 24)
15.3M
DenseNet-BC (k = 40)
25.6MTable 2: Error rates (%) on CIFAR and SVHN datasets. k denotes network's growth rate. Results that surpass all competing methods are bold and the overall best results are blue. "+" indicates standard data augmentation (translation and/or mirroring). ∗ indicates results run by ourselves. All the results of DenseNets without data augmentation (C10, C100, SVHN) are obtained using Dropout. DenseNets achieve lower error rates while using fewer parameters than ResNet. Without data augmentation, DenseNet performs better by a large margin. sists of images drawn from 10 and CIFAR-100 (C100) from
100 classes. The training and test sets contain 50,000 and 10,000 images respectively, and we hold out 5,000 training images as a validation set. We adopt a standard data augmentation scheme (mirroring/shifting) that is widely used for these two datasets. We denote this data augmentation scheme by a "+" mark at the end of the dataset name (e.g., C10+). For preprocessing, we normalize the data using the channel means and standard deviations. For the final run we use all 50,000 training images and report the final test error at the end of training.
SVHN.
The Street View House Numbers (SVHN) dataset
 contains 32×32 colored digit images. There are 73,257 images in the training set, 26,032 images in the test set, and 531,131 images for additional training. Following common practice we use all the training data without any data augmentation, and a validation set with 6,000 images is split from the training set. We select the model with the lowest validation error during training and report the test error. We follow and divide the pixel values by
255 so they are in the range.
ImageNet.
The ILSVRC 2012 classification dataset consists 1.2 million images for training, and 50,000 for validation, from 1, 000 classes. We adopt the same data augmentation scheme for training images as in, and apply a single-crop or 10-crop with size 224×224 at test time. Following, we report classification errors on the validation set.
4.2. Training
All the networks are trained using stochastic gradient descent (SGD). On CIFAR and SVHN we train using batch size 64 for 300 and 40 epochs, respectively.
The initial learning rate is set to 0.1, and is divided by 10 at 50% and 75% of the total number of training epochs. On ImageNet, we train models for 90 epochs with a batch size of 256. The learning rate is set to 0.1 initially, and is lowered by 10 times at epoch 30 and 60. Due to GPU memory constraints, our largest model (DenseNet-161) is trained with a mini-batch size 128. To compensate for the smaller batch size, we train this model for 100 epochs, and divide the learning rate by
10 at epoch 90.
Following, we use a weight decay of 10−4 and a Nesterov momentum of 0.9 without dampening. We adopt the weight initialization introduced by. For the three datasets without data augmentation, i.e., C10, C100 and SVHN, we add a dropout layer after each convolutional layer (except the first one) and set the dropout rate to
0.2. The test errors were only evaluated once for each task and model setting.
Model top-1 top-5
DenseNet-121 (k=32) 25.02 (23.61) 7.71 (6.66)
DenseNet-169 (k=32) 23.80 (22.08) 6.85 (5.92)
DenseNet-201 (k=32) 22.58 (21.46) 6.34 (5.54)
DenseNet-161 (k=48) 22.33 (20.85) 6.15 (5.30)
Table 3: The top-1 and top-5 error rates on the ImageNet validation set, with single-crop (10crop) testing.
8 x 10
#parameters validation error
ResNet−34
ResNet−101
ResNet−152
DenseNet−121
ResNet−50
DenseNet−169
DenseNet−201
DenseNet−161(k=48)
ResNets
DenseNets−BC
2.5 x 10
#FLOPs validation error
ResNet−34
DenseNet−121
ResNet−50
DenseNet−169
DenseNet−201
ResNets
DenseNets−BC
ResNet−152
DenseNet−161(k=48)
ResNet−101
Figure 3: Comparison of the DenseNets and ResNets top-1 error rates (single-crop testing) on the ImageNet validation dataset as a function of learned parameters (left) and FLOPs during test-time (right).
4.3. Classification Results on CIFAR and SVHN
We train DenseNets with different depths, L, and growth rates, k. The main results on CIFAR and SVHN are shown in Table 2. To highlight general trends, we mark all results that outperform the existing state-of-the-art in boldface and the overall best result in blue.
Accuracy.
Possibly the most noticeable trend may originate from the bottom row of Table 2, which shows that
DenseNet-BC with L = 190 and k = 40 outperforms the existing state-of-the-art consistently on all the CIFAR datasets. Its error rates of 3.46% on C10+ and 17.18% on
C100+ are significantly lower than the error rates achieved by wide ResNet architecture.
Our best results on
C10 and C100 (without data augmentation) are even more encouraging: both are close to 30% lower than FractalNet with drop-path regularization. On SVHN, with dropout, the DenseNet with L = 100 and k = 24 also surpasses the current best result achieved by wide ResNet.
However, the 250-layer DenseNet-BC doesn't further improve the performance over its shorter counterpart. This may be explained by that SVHN is a relatively easy task, and extremely deep models may overfit to the training set.
Capacity.
Without compression or bottleneck layers, there is a general trend that DenseNets perform better as
L and k increase. We attribute this primarily to the corresponding growth in model capacity. This is best demonstrated by the column of C10+ and C100+. On C10+, the error drops from 5.24% to 4.10% and finally to 3.74% as the number of parameters increases from 1.0M, over 7.0M to 27.2M. On C100+, we observe a similar trend. This suggests that DenseNets can utilize the increased representational power of bigger and deeper models. It also indicates that they do not suffer from overfitting or the optimization difficulties of residual networks.
Parameter Efficiency.
The results in Table 2 indicate that
DenseNets utilize parameters more efficiently than alternative architectures (in particular, ResNets). The DenseNetBC with bottleneck structure and dimension reduction at transition layers is particularly parameter-efficient. For example, our 250-layer model only has 15.3M parameters, but it consistently outperforms other models such as FractalNet and Wide ResNets that have more than 30M parameters. We also highlight that DenseNet-BC with L = 100 and k = 12 achieves comparable performance (e.g., 4.51% vs 4.62% error on C10+, 22.27% vs 22.71% error on C100+) as the 1001-layer pre-activation ResNet using 90% fewer parameters. Figure 4 (right panel) shows the training loss and test errors of these two networks on C10+. The 1001-layer deep
ResNet converges to a lower training loss value but a similar test error. We analyze this effect in more detail below.
Overfitting.
One positive side-effect of the more efficient use of parameters is a tendency of DenseNets to be less prone to overfitting. We observe that on the datasets without data augmentation, the improvements of DenseNet architectures over prior work are particularly pronounced. On C10, the improvement denotes a 29% relative reduction in error from 7.33% to 5.19%. On C100, the reduction is about 30% from 28.20% to 19.64%. In our experiments, we observed potential overfitting in a single setting: on C10, a 4× growth of parameters produced by increasing k =12 to k =24 lead to a modest increase in error from 5.77% to 5.83%. The DenseNet-BC bottleneck and compression layers appear to be an effective way to counter this trend.
4.4. Classification Results on ImageNet
We evaluate DenseNet-BC with different depths and growth rates on the ImageNet classification task, and compare it with state-of-the-art ResNet architectures. To ensure a fair comparison between the two architectures, we eliminate all other factors such as differences in data preprocessing and optimization settings by adopting the publicly available Torch implementation for ResNet by 1. We simply replace the ResNet model with the DenseNet-BC network, and keep all the experiment settings exactly the same as those used for ResNet. The only exception is our largest
DenseNet model is trained with a mini-batch size of 128
1https://github.com/facebook/fb.resnet.torch
#parameters
×105
16 test error (%)
DenseNet
DenseNet-C
DenseNet-B
DenseNet-BC
#parameters
×105
16 test error (%)
ResNet
DenseNet-BC
3x fewer parameters
300 epoch
16 test error (%)
Test error: ResNet-1001 (10.2M)
Test error: DenseNet-BC-100 (0.8M)
Training loss: ResNet-1001 (10.2M)
Training loss: DenseNet-BC-100 (0.8M)
10−3
10−2
10−1
100 training loss
Figure 4: Left: Comparison of the parameter efficiency on C10+ between DenseNet variations. Middle: Comparison of the parameter efficiency between DenseNet-BC and (pre-activation) ResNets. DenseNet-BC requires about 1/3 of the parameters as ResNet to achieve comparable accuracy. Right: Training and testing curves of the 1001-layer pre-activation ResNet with more than 10M parameters and a 100-layer DenseNet with only 0.8M parameters. because of GPU memory limitations; we train this model for 100 epochs with a third learning rate drop after epoch
90 to compensate for the smaller batch size.
We report the single-crop and 10-crop validation errors of DenseNets on ImageNet in Table 3.
Figure 3 shows the single-crop top-1 validation errors of DenseNets and ResNets as a function of the number of parameters (left) and FLOPs (right). The results presented in the figure reveal that
DenseNets perform on par with the state-of-the-art ResNets, whilst requiring significantly fewer parameters and computation to achieve comparable performance. For example, a DenseNet-201 with 20M parameters model yields similar validation error as a 101-layer ResNet with more than 40M parameters. Similar trends can be observed from the right panel, which plots the validation error as a function of the number of FLOPs: a DenseNet that requires as much computation as a ResNet-50 performs on par with a ResNet-101, which requires twice as much computation.
It is worth noting that our experimental setup implies that we use hyperparameter settings that are optimized for
ResNets but not for DenseNets. It is conceivable that more extensive hyper-parameter searches may further improve the performance of DenseNet on ImageNet.2
5. Discussion
Superficially, DenseNets are quite similar to ResNets:
Eq. (2) differs from Eq. (1) only in that the inputs to Hℓ(·) are concatenated instead of summed. However, the implications of this seemingly small modification lead to substantially different behaviors of the two network architectures.
Model compactness.
As a direct consequence of the input concatenation, the feature-maps learned by any of the DenseNet layers can be accessed by all subsequent layers.
This encourages feature reuse throughout the network, and leads to more compact models.
2Our DenseNet implementation contains some memory inefficiencies which temporarily precludes experiments with over 30M parameters.
The left two plots in Figure 4 show the result of an experiment that aims to compare the parameter efficiency of all variants of DenseNets (left) and also a comparable
ResNet architecture (middle). We train multiple small networks with varying depths on C10+ and plot their test accuracies as a function of network parameters.
In comparison with other popular network architectures, such as
AlexNet or VGG-net, ResNets with pre-activation use fewer parameters while typically achieving better results. Hence, we compare DenseNet (k = 12) against this architecture. The training setting for DenseNet is kept the same as in the previous section.
The graph shows that DenseNet-BC is consistently the most parameter efficient variant of DenseNet. Further, to achieve the same level of accuracy, DenseNet-BC only requires around 1/3 of the parameters of ResNets (middle plot). This result is in line with the results on ImageNet we presented in Figure 3. The right plot in Figure 4 shows that a DenseNet-BC with only 0.8M trainable parameters is able to achieve comparable accuracy as the 1001-layer(pre-activation) ResNet with 10.2M parameters.
Implicit Deep Supervision.
One explanation for the improved accuracy of dense convolutional networks may be that individual layers receive additional supervision from the loss function through the shorter connections. One can interpret DenseNets to perform a kind of "deep supervision".
The benefits of deep supervision have previously been shown in deeply-supervised nets (DSN; ), which have classifiers attached to every hidden layer, enforcing the intermediate layers to learn discriminative features.
DenseNets perform a similar deep supervision in an implicit fashion: a single classifier on top of the network provides direct supervision to all layers through at most two or three transition layers. However, the loss function and gradient of DenseNets are substantially less complicated, as the same loss function is shared between all layers.
Stochastic vs. deterministic connection.
There is an interesting connection between dense convolutional net4706 works and stochastic depth regularization of residual networks. In stochastic depth, layers in residual networks are randomly dropped, which creates direct connections between the surrounding layers. As the pooling layers are never dropped, the network results in a similar connectivity pattern as DenseNet: there is a small probability for any two layers, between the same pooling layers, to be directly connected—if all intermediate layers are randomly dropped. Although the methods are ultimately quite different, the DenseNet interpretation of stochastic depth may provide insights into the success of this regularizer.
Feature Reuse.
By design, DenseNets allow layers access to feature-maps from all of its preceding layers (although sometimes through transition layers). We conduct an experiment to investigate if a trained network takes advantage of this opportunity. We first train a DenseNet on
C10+ with L = 40 and k = 12. For each convolutional layer ℓ within a block, we compute the average (absolute) weight assigned to connections with layer s. Figure 5 shows a heat-map for all three dense blocks. The average absolute weight serves as a surrogate for the dependency of a convolutional layer on its preceding layers. A red dot in position(ℓ, s) indicates that the layer ℓ makes, on average, strong use of feature-maps produced s-layers before. Several observations can be made from the plot:
1. All layers spread their weights over many inputs within the same block. This indicates that features extracted by very early layers are, indeed, directly used by deep layers throughout the same dense block.
2. The weights of the transition layers also spread their weight across all layers within the preceding dense block, indicating information flow from the first to the last layers of the DenseNet through few indirections.
3. The layers within the second and third dense block consistently assign the least weight to the outputs of the transition layer (the top row of the triangles), indicating that the transition layer outputs many redundant features (with low weight on average). This is in keeping with the strong results of DenseNet-BC where exactly these outputs are compressed.
4. Although the final classification layer, shown on the very right, also uses weights across the entire dense block, there seems to be a concentration towards final feature-maps, suggesting that there may be some more high-level features produced late in the network.
6. Conclusion
We proposed a new convolutional network architecture, which we refer to as Dense Convolutional Network(DenseNet). It introduces direct connections between any
Dense Block 1
Source layer (s)
Dense Block 2
Dense Block 3
Target layer ()
Transition layer 1
Transition layer 2
Classification layer
Target layer ()
Target layer ()
Figure 5: The average absolute filter weights of convolutional layers in a trained DenseNet. The color of pixel (s, ℓ) encodes the average L1 norm (normalized by number of input feature-maps) of the weights connecting convolutional layer s to ℓ within a dense block. Three columns highlighted by black rectangles correspond to two transition layers and the classification layer. The first row encodes weights connected to the input layer of the dense block. two layers with the same feature-map size. We showed that
DenseNets scale naturally to hundreds of layers, while exhibiting no optimization difficulties. In our experiments, DenseNets tend to yield consistent improvement in accuracy with growing number of parameters, without any signs of performance degradation or overfitting.
Under multiple settings, it achieved state-of-the-art results across several highly competitive datasets.
Moreover, DenseNets require substantially fewer parameters and less computation to achieve state-of-the-art performances. Because we adopted hyperparameter settings optimized for residual networks in our study, we believe that further gains in accuracy of DenseNets may be obtained by more detailed tuning of hyperparameters and learning rate schedules.
Whilst following a simple connectivity rule, DenseNets naturally integrate the properties of identity mappings, deep supervision, and diversified depth. They allow feature reuse throughout the networks and can consequently learn more compact and, according to our experiments, more accurate models. Because of their compact internal representations and reduced feature redundancy, DenseNets may be good feature extractors for various computer vision tasks that build on convolutional features, e.g.,. We plan to study such feature transfer with DenseNets in future work.
Acknowledgements.
The authors are supported in part by the III-1618134, III-1526012, IIS-1149882 grants from the National Science Foundation, and the Bill and Melinda
Gates foundation. Gao Huang is supported by the International Postdoctoral Exchange Fellowship Program of China
Postdoctoral Council (No.20150015). Zhuang Liu is supported by the National Basic Research Program of China
Grants 2011CBA00300, 2011CBA00301, the National Natural Science Foundation of China Grant 61361136003. We also thank Daniel Sedra, Geoff Pleiss and Yu Sun for many insightful discussions.
References
 C. Cortes, X. Gonzalvo, V. Kuznetsov, M. Mohri, and S. Yang. Adanet: Adaptive structural learning of artificial neural networks. arXiv preprint arXiv:1607.01097, 2016. 2
 J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. FeiFei. Imagenet: A large-scale hierarchical image database. In
CVPR, 2009. 5
 S. E. Fahlman and C. Lebiere. The cascade-correlation learning architecture. In NIPS, 1989. 2
 J. R. Gardner, M. J. Kusner, Y. Li, P. Upchurch, K. Q.
Weinberger, and J. E. Hopcroft. Deep manifold traversal:
Changing labels with convolutional features. arXiv preprint arXiv:1511.06421, 2015. 8
 L. Gatys, A. Ecker, and M. Bethge. A neural algorithm of artistic style. Nature Communications, 2015. 8
 X. Glorot, A. Bordes, and Y. Bengio. Deep sparse rectifier neural networks. In AISTATS, 2011. 3
 I. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, and Y. Bengio. Maxout networks. In ICML, 2013. 5
 S. Gross and M. Wilber. Training and investigating residual nets, 2016. 5, 6
 B. Hariharan, P. Arbeláez, R. Girshick, and J. Malik. Hypercolumns for object segmentation and fine-grained localization. In CVPR, 2015. 2
 K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In ICCV, 2015. 5
 K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, 2016. 1, 2, 3, 4, 5, 6
 K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. In ECCV, 2016. 2, 3, 5, 7
 G. Huang, Y. Sun, Z. Liu, D. Sedra, and K. Q. Weinberger.
Deep networks with stochastic depth. In ECCV, 2016. 1, 2, S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In
ICML, 2015. 3
 A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Tech Report, 2009. 4
 A. Krizhevsky, I. Sutskever, and G. E. Hinton.
Imagenet classification with deep convolutional neural networks. In
NIPS, 2012. 3, 7
 G. Larsson, M. Maire, and G. Shakhnarovich. Fractalnet:
Ultra-deep neural networks without residuals. arXiv preprint arXiv:1605.07648, 2016. 1, 3, 5, 6
 Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E.
Howard, W. Hubbard, and L. D. Jackel. Backpropagation applied to handwritten zip code recognition. Neural computation, 1(4):541–551, 1989. 1
 Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradientbased learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. 1, 3
 C.-Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu. Deeplysupervised nets. In AISTATS, 2015. 2, 3, 5, 7
 Q. Liao and T. Poggio. Bridging the gaps between residual learning, recurrent neural networks and visual cortex. arXiv preprint arXiv:1604.03640, 2016. 2
 M. Lin, Q. Chen, and S. Yan. Network in network. In ICLR, J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In CVPR, 2015. 2
 Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y.
Ng. Reading digits in natural images with unsupervised feature learning, 2011. In NIPS Workshop, 2011. 5
 M. Pezeshki, L. Fan, P. Brakel, A. Courville, and Y. Bengio.
Deconstructing the ladder network architecture. In ICML, A. Rasmus, M. Berglund, M. Honkala, H. Valpola, and T. Raiko. Semi-supervised learning with ladder networks.
In NIPS, 2015. 3
 A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and Y. Bengio. Fitnets: Hints for thin deep nets. In ICLR, O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al.
Imagenet large scale visual recognition challenge.
IJCV. 1, 7
 P. Sermanet, S. Chintala, and Y. LeCun. Convolutional neural networks applied to house numbers digit classification. In
ICPR, pages 3288–3291. IEEE, 2012. 5
 P. Sermanet, K. Kavukcuoglu, S. Chintala, and Y. LeCun.
Pedestrian detection with unsupervised multi-stage feature learning. In CVPR, 2013. 2
 J. T. Springenberg, A. Dosovitskiy, T. Brox, and M. Riedmiller.
Striving for simplicity: The all convolutional net. arXiv preprint arXiv:1412.6806, 2014. 5
 N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. JMLR, 2014. 5
 R. K. Srivastava, K. Greff, and J. Schmidhuber. Training very deep networks. In NIPS, 2015. 1, 2, 5
 I. Sutskever, J. Martens, G. Dahl, and G. Hinton. On the importance of initialization and momentum in deep learning.
In ICML, 2013. 5
 C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In CVPR, 2015. 2, 3
 C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna.
Rethinking the inception architecture for computer vision. In
CVPR, 2016. 2, 3, 4
 S. Targ, D. Almeida, and K. Lyman.
Resnet in resnet: Generalizing residual architectures. arXiv preprint arXiv:1603.08029, 2016. 2
 J. Wang, Z. Wei, T. Zhang, and W. Zeng. Deeply-fused nets. arXiv preprint arXiv:1605.07716, 2016. 3
 B. M. Wilamowski and H. Yu.
Neural network learning without backpropagation. IEEE Transactions on Neural Networks, 21(11):1793–1803, 2010. 2
 S. Yang and D. Ramanan. Multi-scale recognition with dagcnns. In ICCV, 2015. 2
 S. Zagoruyko and N. Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016. 3, 5, 6
 Y. Zhang, K. Lee, and H. Lee. Augmenting supervised neural networks with unsupervised objectives for large-scale image classification. In ICML, 2016. 3Segmentation as Selective Search for Object Recognition
Koen E. A. van de Sande∗
Jasper R. R. Uijlings†
Theo Gevers∗
Arnold W. M. Smeulders∗
∗University of Amsterdam
†University of Trento
Amsterdam, The Netherlands
Trento, Italy ksande@uva.nl, jrr@disi.unitn.it, th.gevers@uva.nl, a.w.m.smeulders@uva.nl
Abstract
For object recognition, the current state-of-the-art is based on exhaustive search. However, to enable the use of more expensive features and classifiers and thereby progress beyond the state-of-the-art, a selective search strategy is needed. Therefore, we adapt segmentation as a selective search by reconsidering segmentation: We propose to generate many approximate locations over few and precise object delineations because (1) an object whose location is never generated can not be recognised and (2) appearance and immediate nearby context are most effective for object recognition. Our method is class-independent and is shown to cover 96.7% of all objects in the Pascal
VOC 2007 test set using only 1,536 locations per image.
Our selective search enables the use of the more expensive bag-of-words method which we use to substantially improve the state-of-the-art by up to 8.5% for 8 out of 20 classes on the Pascal VOC 2010 detection challenge.
1. Introduction
Object recognition, i.e. determining the position and the class of an object within an image, has made impressive progress over the past few years, see the Pascal VOC challenge.
The state-of-the-art is based on exhaustive search over the image to find the best object positions. However, as the total number of images and windows to evaluate in an exhaustive search is huge and growing, it is necessary to constrain the computation per location and the number of locations considered. The computation is currently reduced by using a weak classifier with simple-to-compute features, and by reducing the number of locations on a coarse grid and with fixed window sizes. This comes at the expense of overlooking some object locations and misclassifying others. Therefore, we propose selective search, greatly reducing the number of locations to consider. Specifically, we propose to use segmentation to generate a limited set of locations, permitting the more powerful yet expensive bag-of(a)(c)(d)(b)
Figure 1. Given an image (a) our aim is to find its objects for which the ground truth is shown in (b). To achieve this, we adapt segmentation as a selective search strategy: We aim for high recall by generating locations at all scales and account for many different scene conditions by employing multiple invariant colour spaces.
Example object hypotheses are visualised in (d). words features.
Selective search has been exploited successfully by for object delineation, i.e. creating a pixel-wise classification of the image. Both concentrate on 10-100 possibly overlapping segments per image, which best correspond to an object. They focus on finding accurate object contours, which is why both references use a powerful, specialized contour detector. In this paper, we reconsider segmentation to use as an instrument to select the best locations for object recognition. Rather than aiming for 10-100 accurate locations, we aim to generate 1,000-10,000 approximate locations. For boosting object recognition, (1) generating several thousand locations per image guarantees the inclusion of virtually all objects, and (2) rough segmentation includes the local context known to be beneficial for object classification. Hence we place our computational attention precisely on these parts of the image which bear the most information for object classification.
Emphasizing recall (encouraging to include all image fragments of potential relevance) was earlier proposed by
Hoiem et al. for surface layout classification and adopted by Russell et al. for latent object discovery.
In the references its use is limited to changing the scale of the segmentation, while its potential for finding objects has yet to be investigated. Malisiewicz and Efros investigated how well segments capture objects as opposed to the bounding boxes of an exhaustive search. They also mainly change the scale of the segmentation. In contrast, this paper uses a full segmentation hierarchy and accounts for as many different scene conditions as possible, such as shadows, shading, and highlights, by using a variety of invariant colour spaces. Furthermore, we demonstrate the power of segmentation as selective search on the challenging Pascal
VOC dataset in terms of both recall and recognition accuracy.
To summarize, we make the following contributions: (1)
We reconsider segmentation by adapting it as an instrument to select the best locations for object recognition. We put most emphasis on recall and prefer good object approximations over exact object boundaries. (2) We demonstrate that accounting for scene conditions through invariant colour spaces results in a powerful selective search strategy with high recall. (3) We show that our selective search enables the use of more expensive features such as bag-of-words and substantially improves the state-of-the-art on the Pascal
VOC 2010 detection challenge for 8 out of 20 classes.
2. Related Work
In Figure 2, the relation of this paper with other work is visualized. Research within localisation can generally be divided into two categories. 1) Work with emphasis on recognition (Section 2.1). Here determining the object class is more important than finding the exact contours and an exhaustive search is the norm. 2) Work with emphasis on object delineation (Section 2.2). Here object contours are most important and the use of segmentation is the norm.
There are two exceptions to these categories. Vedaldi et al. use jumping windows, in which the relation between individual visual words and the object location is learned to predict the object location in new images.
Maji and Malik combine multiple of these relations to predict the object location using a Hough-transform, after which they randomly sample windows close to the Hough maximum. Both methods can be seen as a selective search.
In contrast to learning, we adopt segmentation as selective search to generate class independent object hypotheses.
2.1. Exhaustive Search for Recognition
As an object can be located at any position and scale in the image, it is natural to search everywhere.
However, the visual search space is huge, making an exhaustive search computationally expensive. This imposes constraints on the evaluation cost per location and/or the Localisation
Exhaustive search
Selective search
Object
Recognition
Object
Recognition
Object
Delineation
100,000-10,000,000
Coarse
Weak/Cascade
Weak (appearance)
Recall
 
1,000-10,000
Approximate
Strong
Strong (appearance)
Recall, This paper
10-100
Precise
Strong
Strong (shape, contour)
Precision
 
#Locations
Location
Classifiers
Features
Focus
References
Figure 2. Positioning of this paper with respect to related work. number of locations considered. Hence most of these sliding window techniques use a coarse search grid and fixed aspect ratios, using weak classifiers and economic image features such as HOG. This method is often used as a preselection step in a cascade of classifiers.
Related to the sliding window technique is the highly successful part-based object localisation method of Felzenszwalb et al.. Their method also performs an exhaustive search using a linear SVM and HOG features. However, they search for objects and object parts, whose combination results in an impressive object detection performance.
Lampert et al. developed a branch and bound technique to directly search for the optimal window within an image. While they obtain impressive results for linear classifiers, found that for non-linear classifiers the method in practice still visits over a 100,000 windows per image.
While the previous methods are all class-specific, Alexe et al. propose to search for any object, independent of its class. They train a classifier on the object windows of those objects which have a well-defined shape (as opposed to e.g. grass). Then instead of a full exhaustive search they randomly sample boxes to which they apply their classifier.
The boxes with the highest "objectness" measure serve as a set of object hypotheses. This set is then used to greatly reduce the number of windows evaluated by class-specific object detectors.
Instead of an exhaustive search, in this paper, we propose to do segmentation as a selective search enabling the immediate use of expensive and potentially more powerful recognition techniques. In contrast to all exhaustive methods except, our method yields an object hypotheses set which is completely class independent.
2.2. Selective Search for Object Delineation
In the domain of object delineation, both Carreira et al. and Endres and Hoiem propose to generate a set of class independent object hypotheses using segmentation.
Both methods generate multiple foreground/background segmentations, learn to predict the likelihood that a fore(a)(b)
Figure 3. Two examples of our hierarchical grouping algorithm showing the necessity of different scales. On the left we find many objects at different scales. On the right we necessarily find the objects at different scales as the girl is contained by the tv. ground segment is a complete object, and use this to rank the segments. Both algorithms show a promising ability to accurately delineate objects within images, confirmed by
 who achieve state-of-the-art results on pixel-wise image classification using. This paper uses selective search for object recognition, hence we put more emphasis on recall and welcome rough object locations instead of precise object delineations. We can omit the excellent yet expensive contour detector of included in, making our algorithm computationally feasible on large datasets. In contrast to, we use a hierarchical grouping algorithm instead of multiple foreground/background segmentations.
Gu et al. address the problem of carefully segmenting and recognizing objects based on their parts. They first generate a set of part hypotheses using a grouping method based on. Each part hypothesis is described by both appearance and shape features. Then an object is recognized and carefully delineated by using its parts, achieving good results for shape recognition. In their work, the segmentation is limited to a single hierarchy while its power of discovering parts or objects is not evaluated. In this paper, we use multiple hierarchical segmentations diversified through employing a variety of colour spaces, and evaluate their potential to find complete objects.
3. Segmentation as Selective Search
In this section, we adapt segmentation as selective search for object recognition. This adaptation leads to the following considerations:
High recall. Objects whose locations are not generated can never be recognized. Recall is therefore the most important criterion. To obtain a high recall we observe the following: (1) Objects can occur at any scale within an image.
Moreover, some objects are contained within other objects.
Hence it is necessary to generate locations at all scales, as illustrated in Figure 3. (2) There is no single best strategy to group regions together: An edge may represent an object boundary in one image, while the same edge in another image may be the result of shading. Hence rather than aiming for the single best segmentation, it is important to combine multiple complementary segmentations, i.e. we want to diversify the set of segmentations used.
Coarse locations are sufficient. As the state-of-the-art in object recognition uses appearance features, the exact object contours of the object hypotheses are less important.
Hence instead of a strong focus on object boundaries (e.g.
 ), the evaluation should focus on finding reasonable approximations of the object locations, such as is measured by the Pascal overlap criterion.
Fast to compute. The generation of the object hypotheses should not become a bottleneck when performing object localisation on a large dataset.
3.1. Our Segmentation Algorithm
The most natural way to generate locations at all scales is to use all locations from a hierarchical segmentation algorithm (illustrated in Figure 1). Our algorithm uses size and appearance features which are efficiently propagated throughout the hierarchy, making it reasonably fast. Note that we keep the algorithm basic to ensure repeatability and make clear that our results do not stem from parameter tuning but from rethinking the goal of segmentation.
As regions can yield richer information than pixels, we start with an oversegmentation, i.e. a set of small regions which do not spread over multiple objects. We use the fast method of as our starting point, which found wellsuited for generating an oversegmentation.
Starting from the initial regions, we use a greedy algorithm which iteratively groups the two most similar regions together and calculates the similarities between this new region and its neighbours. We continue until the whole image becomes a single region. As potential object locations, we consider either all segments throughout the hierarchy (including initial segments), or we consider the tight bounding boxes around these segments.
We define the similarity S between region a and b as
S(a, b) = Ssize(a, b) + Stexture(a, b). Both components result in a number in range and are weighed equally.
Ssize(a,b) is defined as the fraction of the image that the segment a and b jointly occupy. This measure encourages small regions to merge early and prevents a single region from gobbling up all others one by one.
Stexture(a, b) is defined as the histogram intersection between SIFT-like texture measurements. For these measurements, we aggregate the gradient magnitude in 8 directions over a region, just like in a single subregion of SIFT with no Gaussian weighting. As we use colour, we follow and do texture measurements in each colour channel separately and concatenate the results.
3.2. Shadow, Shading and Highlight Edges
To obtain multiple segmentations which are complementary, we perform our segmentation in a variety of colour channels with different invariance properties. Specifically, we consider multiple colour spaces with different degrees of sensitivity to shadow, shading and highlight edges.
Standard RGB is the most sensitive. The opponent colour space is insensitive to highlight edges, but sensitive to shadows and shading edges. The normalized RGB space is insensitive to shadow and shading edges but still sensitive to highlights. The hue H is the most invariant and is insensitive to shadows, shading and highlights. Note that we always perform each segmentation in a single colour space, including the initial segmentation of.
An alternative approach to multiple colour spaces would be the use of different thresholds for the starting segmentation. We evaluate this approach as well.
3.3. Discussion
Our adaptation of segmentation as selective search for object recognition is designed to obtain high recall by considering all levels of a hierarchical grouping of image segments. Furthermore, by considering multiple colour spaces with increasing levels of invariance to imaging conditions, we are robust to the additional edges introduced into an image by shadows, shading and highlights. Finally, our approach is fast which makes it applicable to large datasets.
4. Object Recognition System
In this section, we detail how to use the selective search strategy from Section 3 for a complete object recognition system. As feature representation, two types of features are dominant: histograms of oriented gradients (HOG) and bag-of-words. HOG has been shown to be successful in combination with the part-based model by Felzenszwalb et al.. However, as they use an exhaustive search, HOG features in combination with a linear classifier is the only feasible choice.
To show that our selective search strategy enables the use of more expensive and potentially more powerful features, we use Bag-of-Words for object recognition. We use a more powerful (and expensive) implementation than by employing multiple colour spaces and a finer spatial pyramid division.
Specifically we sample descriptors at each pixel on a single scale.
We extract SIFT and two recommended colour SIFTs from, OpponentSIFT and RGB-SIFT.
Software from is used. We use a visual codebook of size 4,096 and a spatial pyramid with 4 levels. Because a spatial pyramid results in a coarser spatial subdivision than the cells which make up a HOG descriptor, our features contain less information about the specific spatial layout of the object. Therefore, HOG is better suited for rigid objects and our features are better suited for deformable object types.
As classifier we employ a Support Vector Machine with a histogram intersection kernel using. We use the fast, approximate classification strategy of.
Our training procedure is illustrated in Figure 4. The initial positive examples consist of all ground truth object windows. As initial negative examples we use all object locations generated by our selective search that have an overlap of 20% to 50% with a positive example, unless they have more than 70% overlap with another negative, i.e. we avoid near duplicates. This selection of training examples gives reasonably good initial classification models.
Then we enter a retraining phase to iteratively add hard negative examples (e.g. ): We apply the learned models to the training set using the locations generated by our selective search. For each negative image we add the highest scoring location. As our initial training set already yields good models, our models converge in only two iterations.
For the test set, the final model is applied to all locations generated by our selective search. The windows are sorted by classifier score while windows which have more than
30% overlap with a higher scoring window are considered near-duplicates and are removed.
5. Evaluation
To evaluate the quality of our selective search strategy, we perform the following four experiments:
• Experiment 1 evaluates how to adapt segmentation for selective search. Specifically we compare multiple flat segmentations against a hierarchy and evaluate the use of increasingly invariant colour spaces.
• Experiment 2 compares segmentation as selective search on the task of generating good object locations for recognition with.
• Experiment 3 compares segmentation as selective search on the task of generating good object delineations for segmentation with.
• Experiment 4 evaluates the use of our object hypotheses in the object recognition system of Section 4, on the widely accepted object localisation method of and compares it to the state-of-the-art.
Positive examples
Object hypotheses
Ground truth
Difficult negatives if overlap with positive 20-50%
Training Examples
Train
SVM(Histogram Intersection
Kernel)
Model
Search for false positives
False Positives
Add to training examples
Training Examples
Retrain
Figure 4. The training procedure of our object recognition pipeline. As positive learning examples we use the ground truth. As negatives we use examples that have a 20-50% overlap with the positive examples. We iteratively add hard negatives using a retraining phase.
In all experiments, we report results on the challenging
Pascal VOC 2007 or 2010 datasets. These datasets contain images of twenty object categories and the ground truth in terms of object labels, the location in terms of bounding boxes, and for a subset of the data the object location in terms of a pixel-wise segmentation.
As in, the quality of the hypotheses is defined in terms of the average recall over all classes versus the number of locations retrieved. We use the standard Pascal overlap criterion where an object is considered found if the area of the intersection of a candidate location and the ground truth location, divided by the area of their union is larger than 0.5. Note that in the first two experiments the location is a bounding box, and in the third it is a segment.
Any parameter selection was done on the training set only, while results in this paper are reported on the test set.
5.1. Exp. 1: Segmentation for Selective Search
In this experiment, we evaluate how to adapt segmentation for selective search. First, we compare multiple flat segmentations against a hierarchical segmentation. Second, we evaluate the use of a variety of colour spaces.
Flat versus Hierarchy. As our segmentation algorithm starts with the initial oversegmentation of, we compare our hierarchical version with multiple flat segmentations by. We do this in RGB colour space. We vary the scale of by setting the threshold k from 100 to 1000 both in steps of 10 and in steps of 50. For our hierarchical algorithm we use the smallest threshold 100. Varying the threshold k results in many more segments than a single hierarchical grouping, because in the segment boundaries resulting from a high threshold are not a subset of those from a small threshold. Therefore we additionally consider two hierarchical segmentations using a threshold of 100 and 200.
Experiment 1: Multiple Flat segmentations versus Hierarchy
Max. recall (%)
# windows
 k = 100, 150... 1000
 k = 100, 110... 1000
Hierarchical k = 100
Hierarchical k = 100, 200
Table 1. Comparison of multiple flat segmentations versus a hierarchy in terms of recall and the number of windows per image.
As can be seen from Table 1, multiple flat segmentations yield a higher recall than a single hierarchical grouping but using many more locations. However, if we choose two initial thresholds and combine results, our algorithm yields recall of 89.4 instead of 87.7, while using only 511 locations instead of 1159. Hence a hierarchical approach is preferable over multiple flat segmentations as it yields better results, fewer parameters, and selects all scales naturally. Additionally, we found it to be much faster.
Multiple Colour Spaces. We now test two diversification strategies to obtain higher recall. As seen in the previous experiment it is beneficial to use multiple starting segmentations. Furthermore we test how combining different colour spaces with different invariance properties can increase the number of objects found. Specifically, we take a segmentation in RGB colour space, and subsequently add the segmentation in Opponent colour space, normalized rgb colour space, and the Hue channel. We do this for a single initial segmentation with k = 100, two initial segmentations with k = 100, 200, and four initial segmentations with k = 100, 150, 200, 250. Results are shown in Figure 5.
Experiment 1: Influence of Multiple Colour Spaces
RGB
RGB+Opp
RGB+Opp+rgb
RGB+Opp+rgb+H
Colour Spaces
Recall k=100,150,200,250 k=100,200 k=100
Figure 5. Using multiple colour spaces clearly improves recall; along the horizontal axis increasingly invariant colour spaces are added.
As can be seen, both changing the initial segmentation and using a variety of different colour channels yield complementary object locations. Note that using four different colour spaces works better than using four different initial segmentations. Furthermore, when using all four colour spaces the difference between two and four initial segmentations is negligible. We conclude that varying the colour spaces with increasing invariances is better than varying the threshold of the initial segmentation. In subsequent experiments we always use these two initial segmentations.
On the sensitivity of parameters. In preliminary experiments on the training set we used other colour spaces such as HSV, HS, normalized rg plus intensity, intensity only, etc. However, we found that as long as one selects colour spaces with a range of invariance properties, the outcome is very similar. For illustration purposes we used in this paper the colour spaces with the most clear invariance properties.
Furthermore, we found that as long as a good oversegmentation is generated, the exact choice for k is unimportant.
Finally, different implementations of the texture histogram yielded little changes overall. We conclude that the recall obtained in this paper is not caused by parameter tuning but rather by having a good diversification of segmentation strategies through different colour invariance properties.
5.2. Exp. 2: Selective Search for Recognition
We now compare our selective search method to the sliding windows of, the jumping windows of, and the 'objectness' measure of. Table 2 shows the maximum recall obtained for each method together with the average number of locations generated per image.
Our method achieves the best results with a recall of 96.7% with on average 1,536 windows per image. The jumping windows of come second with 94% recall but uses 10,000 windows instead. Moreover, their method is specifically trained for each class whereas our method is completely class-independent. Hence, with only a limited number of object locations our method yields the highest recall.
Experiment 2: Maximum Recall of Selective Search for Recognition
Max. recall (%)
# windows
Sliding Windows 
200 per class
Jumping Windows 
10,000 per class
'Objectness' 
Our hypotheses
Table 2. Comparison of maximum recall between our method and. We achieve the highest recall of 96.7%. Second comes
 with 94.0% but using an order of magnitude more locations.
We also compare the trade-off between recall and the number of windows in Figure 6. As can be seen, our method gives a higher recall using fewer windows than. The method of seems to need only few windows to obtain their maximum recall of 83%. However, they use 200 windows per image per class, which means they generate 4,000 windows per image. Moreover, the ordering of their hypotheses is based on a class specific recognition score while the ordering of our hypotheses is imposed by the inclusion of segmentations in increasingly invariant colour spaces.
In conclusion, our selective search outperforms other methods in terms of maximum recall while using fewer locations. Additionally, our method is completely classindependent. This shows that segmentation, when adapted
Experiment 2: Recall of Selective Search for Recognition
Number of candidate windows
Recall
Sliding Windows (# per class)
Jumping Windows (# per class)
Objectness
Our locations
Figure 6. The trade-off between the number of retrieved windows and recall on the Pascal VOC 2007 object detection dataset. Note that for the reported number of locations is per class; the total number of windows per image is a factor 20 higher. for high recall by using all scales and a variety of colour spaces with different invariance properties, is a highly effective selective search strategy for object recognition.
5.3.Exp. 3: Selective Search for Object Delineation
The methods of are designed for object delineation and computationally too expensive to apply them to the VOC 2007 detection dataset. Instead we compare to them on the much smaller segmentation dataset using not boxes but the segments instead. We generated candidate segments for by using their publicly available code. Note that we excluded the background category in the evaluation.
Results are shown in Table 3. The method of achieves the best recall of 82.2% using 1,989 windows. Our method comes second with a recall of 79.8% using 1973 segments.
The method of results in a recall of 78.2% using only
697 windows. However, our method is 28 times faster than
 and 54 times faster than.
We conclude that our method is competitive in terms of recall while still computationally feasible on large datasets.
Experiment 3: Recall of Selective Search for Segmentation
Max. recall (%)
# windows
Time (s)
Carreira 
Endres 
Our hypotheses
Combination
Table 3. Comparison of our paper with in terms of recall on the Pascal VOC 2007 segmentation task. Our method has competitive recall while being more than an order of magnitude faster.
Interestingly, we tried to diversify the selective search by combining all three methods.
The resulting recall is 90.1%(!), much higher than any single method. We conclude that for the purpose of recognition, instead of aiming for the best segmentation, it is prudent to investigate how segmentations can complement each other.
Experiment 4: Object Recognition Accuracy on VOC2007 Test Set
Average Precision bicycle car horse bus motorbike train person tv/monitor sofa aeroplane bottle cow dining table chair cat sheep boat potted plant dog bird
Object Category
Search strategies using part-based models
Part-based + Exhaustive search (baseline)
Part-based + Our selective search
Average Precision bicycle car horse bus motorbike train person tv/monitor sofa aeroplane bottle cow dining table chair cat sheep boat potted plant dog bird
Object Category
Part-based models versus bag-of-words models
Part-based + Exhaustive search (baseline)
Our Bag-of-Words + Our selective search
Figure 7. Object recognition results on the PASCAL VOC 2007 test set. For the left plot, object models are trained using the part-based
Felzenszwalb system, which uses exhaustive search by default. For the right plot, object models are trained using more expensive bag-of-words features and classifiers; exhaustive search is not feasible with these models.
5.4. Exp. 4: Object Recognition Accuracy
In this experiment, we evaluate our object hypotheses on a widely accepted part-based object recognition method and inside the object recognition system described in Section 4. The latter is compared to the state-of-the-art on the challenging Pascal VOC 2010 detection task.
Search strategies using part-based models. We compare various search strategies on the method of Felzenszwalb. We consider the exhaustive search of to be our baseline. We use our selective search boxes as a filter on the output of, as facilitated by their code, where we discard all locations whose Pascal Overlap is smaller than
0.8. In practice this reduces the number of considered windows from around 100,000 per image per class to around
5,000. Results are shown on the left in Figure 7. Overall using our boxes as a filter reduces Mean Average Precision from 0.323 MAP to 0.296 MAP, 0.03 MAP less while evaluating 20 times fewer boxes. Note that for some concepts like aeroplane, dog, dining table, and sheep there is even a slight improvement, suggesting a trade-off between high recall and precision for object detection accuracy.
If we use all 10,000 boxes of in the same manner on, the MAP reduces to 0.215. But in they have an additional hill-climbing step which enables them to consider only 2,000 windows at the expense of 0.04 MAP. This suggest that a hill-climbing step as suggested by could improve results further when using our boxes.
Part-based HOG versus bag-of-words. A major advantage of selective search is that it enables the use of more expensive features and classifiers. To evaluate the potential of better features and classifiers, we compare the bag-ofwords recognition pipeline described in Section 4 with the baseline of which uses HOG and linear classifiers. Results on the right in Figure 7 show improvements for 10 out of 20 object categories. Especially significant are the improvements the object categories cat, cow, dog, sheep, diningtable, and aeroplane, which we improve with 11% to 20%. Except aeroplane, these object categories all have flexible shape on which bag-of-words is expected to work well (Section 4). The baseline achieves a higher accuracy for object categories with rigid shape characteristics such as bicycle, car, bottle, person and chair. If we select the best method for each class, instead of a MAP of 0.323 of the baseline, we get a MAP of 0.378, a significant, absolute improvement of 5% MAP.
To check whether the differences on the right in Figure 7 originate mainly from the different features, we combined bag-of-words features with the exhaustive search of for the concepts cat and car.
With cat, bag-of-words gives
0.392 AP for selective and 0.375 AP for exhaustive search, compared to 0.193 AP for part-based HOG features. With car, bag-of-words gives 0.547 for selective and 0.535 for exhaustive search, and 0.579 for part-based HOG features.
Comparison to the state-of-the-art. To compare our results to the current state-of-the-art in object recognition, we have submitted our bag-of-words models for the Pascal
VOC 2010 detection task to the official evaluation server.
Results are shown in Table 4, together with the top-4 from the competition. In this independent evaluation, our system improves the state-of-the-art by up to 8.5% for 8 out of 20 object categories compared to all other competition entries.
In conclusion, our selective search yields good object locations for part-based models, as even without the hillclimbing step of we need to evaluate 20 times fewer windows at the expense of 0.03 MAP in average precision.
More importantly, our selective search enables the use of Experiment 4: Object Recogntion Accuracy on VOC2010 Test Set
System plane bike bird boat bottle bus car cat chair cow table dog horse motor person plant sheep sofa train tv
NLPR
MIT UCLA 
NUS
UoCTTI 
This paper
Table 4. Results from the Pascal VOC 2010 detection task test set, comparing the approach from this paper to the current state-of-the-art.
We improve the state-of-the-art up to 0.085 AP for 8 categories and equal the state-of-the-art for one more category. expensive features and classifiers which allow us to substantially improve the state-of-the-art for 8 out of 20 classes on the VOC2010 detection challenge.
6. Conclusions
In this paper, we have adopted segmentation as a selective search strategy for object recognition. For this purpose we prefer to generate many approximate locations over few and precise object delineations, as objects whose locations are not generated can never be recognised and appearance and immediate nearby context are effective for object recognition. Therefore our selective search uses locations at all scales. Furthermore, rather than using a single best segmentation algorithm, we have shown that for recognition it is prudent to use a set of complementary segmentations. In particular this paper accounts for different scene conditions such as shadows, shading, and highlights by employing a variety of invariant colour spaces. This results in a powerful selective search strategy that generates only 1,536 classindependent locations per image to capture 96.7% of all the objects in the Pascal VOC 2007 test set. This is the highest recall reported to date.
We show that segmentation as a selective search strategy is highly effective for object recognition: For the part-based system of the number of considered windows can be reduced by 20 times at a loss of 3% MAP overall. More importantly, by capitalizing on the reduced number of locations we can do object recognition using a powerful yet expensive bag-of-words implementation and improve the state-of-the-art for 8 out of 20 classes for up to 8.5% in terms of Average Precision.
References
 B. Alexe, T. Deselaers, and V. Ferrari. What is an object?
In
CVPR, 2010. 2, 4, 6, 7
 P. Arbel´aez, M. Maire, C. Fowlkes, and J. Malik. Contour detection and hierarchical image segmentation. TPAMI, 2011. 1, 3
 J. Carreira and C. Sminchisescu. Constrained parametric min-cuts for automatic object segmentation. In CVPR, 2010. 1, 2, 3, 4, 6
 O. Chum and A. Zisserman. An exemplar model for learning object classes. In CVPR, 2007. 2
 G. Csurka, C. R. Dance, L. Fan, J. Willamowski, and C. Bray.
Visual categorization with bags of keypoints. In ECCV Statistical
Learning in Computer Vision, 2004. 1, 4
 N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In CVPR, 2005. 1, 2, 4
 I. Endres and D. Hoiem. Category independent object proposals.
In ECCV, 2010. 1, 2, 3, 4, 6
 M. Everingham, L. van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The pascal visual object classes (voc) challenge.
IJCV, 88:303–338, 2010. 1, 3, 4, 5
 P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan. Object detection with discriminatively trained part based models. TPAMI, 32:1627–1645, 2010. 1, 2, 4, 7, 8
 P. F. Felzenszwalb and D. P. Huttenlocher. Efficient Graph-Based
Image Segmentation. IJCV, 59:167–181, 2004. 3, 4, 5
 T. Gevers and H. M. G. Stokman. Classification of color edges in video into shadow-geometry, highlight, or material transitions.
TMM, 5(2):237–243, 2003. 4
 C. Gu, J. J. Lim, P. Arbel´aez, and J. Malik. Recognition using regions. In CVPR, 2009. 3
 H. Harzallah, F. Jurie, and C. Schmid. Combining efficient object localization and image classification. In ICCV, 2009. 1, 2, 4, 5, 6
 D. Hoiem, A. A. Efros, and M. Hebert. Recovering surface layout from an image. IJCV, 2007. 2
 C. H. Lampert, M. B. Blaschko, and T. Hofmann. Efficient subwindow search: A branch and bound framework for object localization. TPAMI, 31:2129–2142, 2009. 2, 4
 S. Lazebnik, C. Schmid, and J. Ponce. Beyond bags of features:
Spatial pyramid matching for recognizing natural scene categories.
In CVPR, 2006. 4
 F. Li, J. Carreira, and C. Sminchisescu. Object recognition as ranking holistic figure-ground hypotheses. In CVPR, 2010. 3
 D. G. Lowe. Distinctive image features from scale-invariant keypoints. IJCV, 60:91–110, 2004. 4
 S. Maji, A. C. Berg, and J. Malik. Classification using intersection kernel support vector machines is efficient. In CVPR, 2008. 4
 S. Maji and J. Malik. Object detection using a max-margin hough transform. In CVPR, 2009. 2
 T. Malisiewicz and A. A. Efros.
Improving spatial support for objects via multiple segmentations. In BMVC, 2007. 2
 B. Russell, A. Efros, J. Sivic, W. Freeman, and A. Zisserman. Using multiple segmentations to discover objects and their extent in image collections. In CVPR, 2006. 2
 J. Sivic and A. Zisserman. Video google: A text retrieval approach to object matching in videos. In ICCV, 2003. 1, 4
 S. Sonnenburg, G. Raetsch, S. Henschel, C. Widmer, J. Behr, A. Zien, F. de Bona, A. Binder, C. Gehl, and V. Franc. The shogun machine learning toolbox. JMLR, 11:1799–1802, 2010. 4
 J. R. R. Uijlings, A. W. M. Smeulders, and R. J. H. Scha. What is the spatial extent of an object? In CVPR, 2009. 1
 K. E. A. van de Sande, T. Gevers, and C. G. M. Snoek. Evaluating color descriptors for object and scene recognition. TPAMI, 32:1582–1596, 2010. 1, 4
 A. Vedaldi, V. Gulshan, M. Varma, and A. Zisserman. Multiple kernels for object detection. In ICCV, 2009. 1, 2, 4, 5, 6
 P. Viola and M. J. Jones. Robust real-time face detection. IJCV, 57:137–154, 2004. 1, 2
 L. Zhu, Y. Chen, A. Yuille, and W. Freeman. Latent hierarchical structural learning for object detection. In CVPR, 2010. 1, 4, 8Simultaneous Detection and Segmentation
Bharath Hariharan1, Pablo Arbel´aez1,2, Ross Girshick1, and Jitendra Malik1
1 University of California, Berkeley
2 Universidad de los Andes, Colombia
{bharath2,arbelaez,rbg,malik}@eecs.berkeley.edu
Abstract. We aim to detect all instances of a category in an image and, for each instance, mark the pixels that belong to it. We call this task Simultaneous Detection and Segmentation (SDS). Unlike classical bounding box detection, SDS requires a segmentation and not just a box. Unlike classical semantic segmentation, we require individual object instances.
We build on recent work that uses convolutional neural networks to classify category-independent region proposals (R-CNN ), introducing a novel architecture tailored for SDS. We then use category-specific, topdown figure-ground predictions to refine our bottom-up proposals. We show a 7 point boost (16% relative) over our baselines on SDS, a 5 point boost (10% relative) over state-of-the-art on semantic segmentation, and state-of-the-art performance in object detection. Finally, we provide diagnostic tools that unpack performance and provide directions for future work.
Keywords: detection, segmentation, convolutional networks.
Introduction
Object recognition comes in many flavors, two of the most popular being object detection and semantic segmentation. Starting with face detection, the task in object detection is to mark out bounding boxes around each object of a particular category in an image. In this task, a predicted bounding box is considered a true positive if it overlaps by more than 50% with a ground truth box, and different algorithms are compared based on their precision and recall. Object detection systems strive to find every instance of the category and estimate the spatial extent of each. However, the detected objects are very coarsely localized using just bounding boxes.
In contrast, semantic segmentation requires one to assign a category label to all pixels in an image. The MSRC dataset was one of the first publicly available benchmarks geared towards this task. Later, the standard metric used to evaluate algorithms in this task converged on pixel IU (intersection over union): for each category, this metric computes the intersection over union of the predicted pixels and ground truth pixels over the entire dataset. This task deals with "stuff" categories (such as grass, sky, road) and "thing" categories (such as cow, person, car) interchangeably. For things, this means that there is no notion
D. Fleet et al. (Eds.): ECCV 2014, Part VII, LNCS 8695, pp. 297–312, 2014. c
⃝ Springer International Publishing Switzerland 2014
B. Hariharan et al. of object instances. A typical semantic segmentation algorithm might accurately mark out the dog pixels in the image, but would provide no indication of how many dogs there are, or of the precise spatial extent of any one particular dog.
These two tasks have continued to this day and were part of the PASCAL
VOC challenge. Although often treated as separate problems, we believe the distinction between them is artificial. For the "thing" categories, we can think of a unified task: detect all instances of a category in an image and, for each instance, correctly mark the pixels that belong to it. Compared to the bounding boxes output by an object detection system or the pixel-level category labels output by a semantic segmentation system, this task demands a richer, and potentially more useful, output. Our aim in this paper is to improve performance on this task, which we call Simultaneous Detection and Segmentation(SDS).
The SDS algorithm we propose has the following steps (Figure 1):
1. Proposal Generation: We start with category-independent bottom-up object proposals. Because we are interested in producing segmentations and not just bounding boxes, we need region proposals. We use MCG to generate
2000 region candidates per image. We consider each region candidate as a putative object hypothesis.
2. Feature Extraction: We use a convolutional neural network to extract features on each region. We extract features from both the bounding box of the region as well as from the region foreground. This follows work by Girshick et al. (R-CNN) who achieved competitive semantic segmentation results and dramatically improved the state-of-the-art in object detection by using
CNNs to classify region proposals. We consider several ways of training the CNNs. We find that, compared to using the same CNN for both inputs (image windows and region masks), using separate networks where each network is finetuned for its respective role dramatically improves performance. We improve performance further by training both networks jointly, resulting in a feature extractor that is trained end-to-end for the SDS task.
3. Region Classification: We train an SVM on top of the CNN features to assign a score for each category to each candidate.
4. Region Refinement: We do non-maximum suppression (NMS) on the scored candidates. Then we use the features from the CNN to produce category-specific coarse mask predictions to refine the surviving candidates.
Combining this mask with the original region candidates provides a further boost.
Since this task is not a standard one, we need to decide on evaluation metrics.
The metric we suggest in this paper is an extension to the bounding box detection metric. It has been proposed earlier. Given an image, we expect the algorithm to produce a set of object hypotheses, where each hypothesis comes with a predicted segmentation and a score. A hypothesis is correct if its segmentation overlaps with the segmentation of a ground truth instance by more than
50%. As in the classical bounding box task, we penalize duplicates. With this labeling, we compute a precision recall (PR) curve, and the average precision
Simultaneous Detection and Segmentation(AP), which is the area under the curve. We call the AP computed in this way
APr, to distinguish it from the traditional bounding box AP, which we call APb(the superscripts r and b correspond to region and bounding box respectively).
APr measures the accuracy of segmentation, and also requires the algorithm to get each instance separately and completely. Our pipeline achieves an APr of 49.5% while at the same time improving APb from 51.0% (R-CNN) to 53.0%.
One can argue that the 50% threshold is itself artificial. For instance if we want to count the number of people in a crowd, we do not need to know their accurate segmentations. On the contrary, in a graphics application that seeks to matte an object into a scene, we might want extremely accurate segmentations.
Thus the threshold at which we regard a detection as a true positive depends on the application. In general, we want algorithms that do well under a variety of thresholds. As the threshold varies, the PR curve traces out a PR surface. We can use the volume under this PR surface as a metric. We call this metric APr vol and APb vol respectively. APr vol has the attractive property that an APr vol of 1 implies we can perfectly detect and precisely segment all objects. Our pipeline gets an APr vol of 41.4%. We improve APb vol from 41.9% (R-CNN) to 44.2%.
We also find that our pipeline furthers the state-of-the-art in the classic
PASCAL VOC semantic segmentation task, from 47.9% to 52.6%. Last but not the least, following work in object detection, we also provide a set of diagnostic tools for analyzing common error modes in the SDS task. Our algorithm, the benchmark and all diagnostic tools are publicly available at http://www.eecs.berkeley.edu/Research/Projects/CS/vision/shape/sds.
����
���
�������
���
��������
����
���������
����������
��������
����������
�������
��������������
�������
����������
Fig. 1. Overview of our pipeline. Our algorithm is based on classifying region proposals using features extracted from both the bounding box of the region and the region foreground with a jointly trained CNN. A final refinement step improves segmentation.
Related Work
For semantic segmentation, several researchers have tried to use activations from off-the-shelf object detectors to guide the segmentation process. Yang et al. use object detections from the deformable parts model to segment the image, pasting figure-ground masks and reasoning about their relative depth ordering.
B. Hariharan et al.
Arbel´aez et al. use poselet detections as features to score region candidates, in addition to appearance-based cues. Ladicky et al. use object detections as higher order potentials in a CRF-based segmentation system: all pixels in the foreground of a detected object are encouraged to share the category label of the detection. In addition, their system is allowed to switch off these potentials by assigning a true/false label to each detection. This system was extended by Boix et al. who added a global, image-level node in the CRF to reason about the categories present in the image, and by Kim et al. who added relationships between objects. In more recent work, Tighe et al. use exemplar object detectors to segment out the scene as well as individual instances.
There has also been work on localizing detections better using segmentation.
Parkhi et al. use color models from predefined rectangles on cat and dog faces to do GrabCut and improve the predicted bounding box. Dai and Hoiem generalize this to all categories and use instance and category appearance models to improve detection. These approaches do well when the objects are coherent in color or texture. This is not true of many categories such as people, where each object can be made of multiple regions of different appearance. An alternative to doing segmentation post facto is to use segmentation to generate object proposals which are then classified. The proposals may be used as just bounding boxes or as region proposals. These proposals incorporate both the consistency of appearance in an object as well as the possibility of having multiple disparate regions for each object. State-of-the-art detection systems and segmentation systems are now based on these methods.
In many of these approaches, segmentation is used only to localize the detections better. Other authors have explored using segmentation as a stronger cue. Fidler et al. use the output of a state-of-the-art semantic segmentation approach to score detections better. Mottaghi uses detectors based on non-rectangular patches to both detect and segment objects.
The approaches above were typically built on features such as SIFT or HOG. Recently the computer vision community has shifted towards using convolutional neural networks (CNNs). CNNs have their roots in the Neocognitron proposed by Fukushima. Trained with the back-propagation algorithm, LeCun showed that they could be used for handwritten zip code recognition. They have since been used in a variety of tasks, including detection and semantic segmentation. Krizhevsky et al. showed a large increase in performance by using CNNs for classification in the ILSVRC challenge.
Donahue et al. showed that Krizhevsky's architecture could be used as a generic feature extractor that did well across a wide variety of tasks. Girshick et al. build on this and finetune Krizhevsky's architecture for detection to nearly double the state-of-the-art performance. They use a simple pipeline, using
CNNs to classify bounding box proposals from. Our algorithm builds on this system, and on high quality region proposals from.
Simultaneous Detection and Segmentation
Our Approach
Proposal Generation
A large number of methods to generate proposals have been proposed in the literature. The methods differ on the type of outputs they produce (boxes vs segments) and the metrics they do well on. Since we are interested in the APr metric, we care about segments, and not just boxes. Keeping our task in mind, we use candidates from MCG for this paper. This approach significantly outperforms all competing approaches on the object level Jaccard index metric, which measures the average best overlap achieved by a candidate for a ground truth object. In our experiments we find that simply switching to MCG from
Selective Search improves APb slightly (by 0.7 points), justifying this choice.
We use the proposals from MCG as is. MCG starts by computing a segmentation hierarchy at multiple image resolutions, which are then fused into a single multiscale hierarchy at the finest scale. Then candidates are produced by combinatorially grouping regions from all the single scale hierarchies and from the multiscale hierarchy. The candidates are ranked based on simple features such as size and location, shape and contour strength.
Feature Extraction
We start from the R-CNN object detector proposed by Girshick et al. and adapt it to the SDS task. Girshick et al. train a CNN on ImageNet Classification and then finetune the network on the PASCAL detection set. For finetuning they took bounding boxes from Selective Search, padded them, cropped them and warped them to a square and fed them to the network. Bounding boxes that overlap with the ground truth by more than 50% were taken as positives and other boxes as negatives. The class label for each positive box was taken to be the class of the ground truth box that overlaps the most with the box.
The network thus learned to predict if the bounding box overlaps highly with a ground truth bounding box. We are working with MCG instead of Selective
Search, so we train a similar object detection network, finetuned using bounding boxes of MCG regions instead of Selective Search boxes.
At test time, to extract features from a bounding box, Girshick et al. pad and crop the box, warp it to a square and pass it through the network, and extract features from one of the later layers, which is then fed into an SVM. In this paper we will use the penultimate fully connected layer.
For the SDS task, we can now use this network finetuned for detection to extract feature vectors from MCG bounding boxes. However these feature vectors do not contain any information about the actual region foreground, and so will be ill-equipped to decide if the region overlaps highly with a ground truth segmentation or not. To get around this, we start with the idea used by Girshick et al. for their experiment on semantic segmentation: we extract a second set of features from the region by feeding it the cropped, warped box, but with
B. Hariharan et al. the background of the region masked out (with the mean image.) Concatenating these two feature vectors together gives us the feature vector we use. (In their experiments Girshick et al. found both sets of features to be useful.) This method of extracting features out of the region is the simplest way of extending the object detection system to the SDS task and forms our baseline. We call this feature extractor A.
The network we are using above has been finetuned to classify bounding boxes, so its use in extracting features from the region foreground is suboptimal. Several neurons in the network may be focussing on context in the background, which will be unavailable when the network is fed the region foreground. This suggests that we should use a different network to extract the second set of features: one that is finetuned on the kinds of inputs that it is going to see. We therefore finetune another network (starting again from the net trained on ImageNet) which is fed as input cropped, padded bounding boxes of MCG regions with the background masked out. Because this region sees the actual foreground, we can actually train it to predict region overlap instead, which is what we care about. Therefore we change the labeling of the MCG regions to be based on segmentation overlap of the region with a ground truth region (instead of overlap with bounding box). We call this feature extractor B.
The previous strategy is still suboptimal, because the two networks have been trained in isolation, while at test time the two feature sets are going to be combined and fed to the classifier. This suggests that one should train the networks jointly. We formalize this intuition as follows. We create a neural network with the architecture shown in Figure 2. This architecture is a single network with two pathways. The first pathway operates on the cropped bounding box of the region (the "box" pathway) while the second pathway operates on the cropped bounding box with the background masked (the "region" pathway). The two pathways are disjoint except at the very final classifier layer, which concatenates the features from both pathways. Both these pathways individually have the same architecture as that of Krizhevsky et al. Note that both A and B can be seen as instantiations of this architecture, but with different sets of weights.
A uses the same network parameters for both pathways. For B, the box pathway gets its weights from a network finetuned separately using bounding box overlap, while the region pathway gets its parameters from a network finetuned separately using region overlap.
Instead of using the same network in both pathways or training the two pathways in isolation, we now propose to train it as a whole directly. We use segmentation overlap as above. We initialize the box pathway with the network finetuned on boxes and the region pathway with the network finetuned on regions, and then finetune the entire network. At test time, we discard the final classification layer and use the output of the penultimate layer, which concatenates the features from the two pathways. We call this feature extractor C.
Simultaneous Detection and Segmentation
�������
�������
�������
�������
�������
�����
�����
�������
�������
�������
�������
�������
�����
�����
�������
����������
Fig. 2. Left: The region with its bounding box. Right: The architecture that we train for C. The top pathway operates on cropped boxes and the bottom pathway operates on region foregrounds.
Region Classification
We use the features from the previous step to train a linear SVM. We first train an initial SVM using ground truth as positives and regions overlapping ground truth by less than 20% as negative. Then we re-estimate the positive set: for each ground truth we pick the highest scoring MCG candidate that overlaps by more than 50%. Ground truth regions for which no such candidate exists (very few in number) are discarded. We then retrain the classifier using this new positive set. This training procedure corresponds to a multiple instance learning problem where each ground truth defines a positive bag of regions that overlap with it by more than 50%, and each negative region is its own bag. We found this training to work better than using just the ground truth as positives.
At test time we use the region classifiers to score each region. Because there may be multiple overlapping regions, we do a strict non-max suppression using a region overlap threshold of 0. This is because while the bounding box of two objects can in fact overlap, their pixel support in the image typically shouldn't.
Post NMS, we work with only the top 20,000 detections for each category (over the whole dataset) and discard the rest for computational reasons. We confirmed that this reduction in detections has no effect on the APr metric.
Region Refinement
We take each of the remaining regions and refine its support. This is necessary because our region candidates have been created by a purely bottom-up, class agnostic process. Since the candidate generation has not made use of categoryspecific shape information, it is prone to both undershooting (i.e. missing some part of the object) and overshooting (i.e. including extraneous stuff).
We first learn to predict a coarse, top-down figure-ground mask for each region. To do this, we take the bounding box of each predicted region, pad it as for feature extraction, and then discretize the resulting box into a 10 × 10 grid. For each grid cell we train a logistic regression classifier to predict the probability that the grid cell belongs to the foreground. The features we use are the features extracted from the CNN, together with the figure-ground mask of the region
B. Hariharan et al.
Fig. 3. Some examples of region refinement. We show in order the image, the original region, the coarse 10 × 10 mask, the coarse mask projected to superpixels, the output of the final classifier on superpixels and the final region after thresholding. Refinement uses top-down category specific information to fill in the body of the train and the cat and remove the road from the car. discretized to the same 10 × 10 grid. The classifiers are trained on regions from the training set that overlap by more than 70% with a ground truth region.
This coarse figure-ground mask makes a top-down prediction about the shape of the object but does not necessarily respect the bottom-up contours. In addition, because of its coarse nature it cannot do a good job of modeling thin structures like aircraft wings or structures that move around. This information needs to come from the bottom-up region candidate. Hence we train a second stage to combine this coarse mask with the region candidate. We project the coarse mask to superpixels by assigning to each superpixel the average value of the coarse mask in the superpixel. Then we classify each superpixel, using as features this projected value in the superpixel and a 0 or 1 encoding if the superpixel belongs to the original region candidate. Figure 3 illustrates this refinement.
Experiments and Results
We use the segmentation annotations from SBD to train and evaluate. We train all systems on PASCAL VOC 2012 train. For all training and finetuning of the network we use the recently released Caffe framework.
Results on APr and APr vol
Table 1 and Table 2 show results on the APr and the APr vol metrics respectively on PASCAL VOC 2012 val (ground truth segmentations are not available for test). We compute APr vol by averaging the APr obtained for 9 thresholds.
1. O2P uses features and regions from Carreira et al., which is the state-ofthe-art in semantic segmentation. We train region classifiers on these features and do NMS to get detections. This baseline gets a mean APr of 25.2% and a mean APr vol of 23.4%.
Simultaneous Detection and Segmentation
2. A is our most naive feature extractor. It uses MCG candidates and features from the bounding box and region foreground, using a single CNN finetuned using box overlaps. It achieves a mean APr of 42.9% and a mean APr vol of 37.0%, a large jump over O2P. This mirrors gains in object detection observed by Girshick et al., although since O2P is not designed for this task the comparison is somewhat unfair.
3. B is the result of finetuning a separate network exclusively on region foregrounds with labels defined by region overlap. This gives a large jump of the APr metric (of about 4 percentage points) and a smaller but significant jump on the APr vol metric of about 2.5 percentage points.
4. C is the result of training a single large network with two pathways. There is a clear gain over using two isolated networks: on both metrics we gain about 0.7 percentage points.
5. C+ref is the result of refining the masks of the regions obtained from C.
We again gain 2 points in the APr metric and 1.2 percentage points in the APr vol metric. This large jump indicates that while MCG candidates we start from are very high quality, there is still a lot to be gained from refining the regions in a category specific manner.
A paired sample t-test indicates that each of the above improvements are statistically significant at the 0.05 significance level.
The left part of Figure 5 plots the improvement in mean APr over A as we vary the threshold at which a detection is considered correct. Each of our improvements increases APr across all thresholds, indicating that we haven't overfit to a particular regime.
Clearly we get significant gains over both our naive baseline as well as O2P.
However, prior approaches that reason about segmentation together with detection might do better on the APr metric. To see if this is the case, we compare to the SegDPM work of Fidler et al.. SegDPM combined DPMs with
O2P and achieved a 9 point boost over DPMs in classical object detection.
For this method, only the bounding boxes are available publicly, and for some boxes the algorithm may choose not to have associated segments. We therefore compute an upper bound of its performance by taking each detection, considering all MCG regions whose bounding box overlaps with the detection by more than 70%, and selecting the region which best overlaps a ground truth.
Since SegDPM detections are only available on PASCAL VOC2010 val, we restrict our evaluations only to this set. Our upper bound on SegDPM has a mean APr of 31.3, whereas C+ref achieves a mean APr of 50.3.
Producing Diagnostic Information
Inspired by, we created tools for figuring out error modes and avenues for improvement for the SDS task. As in, we evaluate the impact of error modes by measuring the improvement in APr if the error mode was corrected. For localization, we assign labels to detections under two thresholds: the usual strict
B. Hariharan et al.
Table 1. Results on APr on VOC2012 val. All numbers are %.
O2P
A
B
C
C+ref aeroplane
68.4 bicycle
49.4 bird
52.1 boat
32.8 bottle
33.0 bus
67.8 car
53.6 cat
73.9 chair
19.9 cow
43.7 diningtable
25.7 dog
60.6 horse
55.9 motorbike
58.9 person
56.7 pottedplant
28.5 sheep
55.6 sofa
32.1 train
64.7 tvmonitor
Mean
Table 2. Results on APr vol on VOC2012 val. All numbers are %.
O2P
A
B
C
C+ref aeroplane
52.3 bicycle
42.6 bird
42.2 boat
28.6 bottle
28.6 bus
58.0 car
45.4 cat
58.9 chair
19.7 cow
37.1 diningtable
22.8 dog
49.5 horse
42.9 motorbike
45.9 person
48.5 pottedplant
25.5 sheep
44.5 sofa
30.2 train
52.6 tvmonitor
Mean
Simultaneous Detection and Segmentation
307 threshold of 0.5 and a more lenient threshold of 0.1 (note that this is a threshold on region overlap). Detections that count as true positives under the lenient threshold but as false positives under the strict threshold are considered mislocalizations. Duplicate detections are also considered mislocalizations. We then consider the performance if either a) all mislocalized instances were removed, or b) all mislocalized instances were correctly localized and duplicates removed.
Figure 4 shows how the PR curve for the APr benchmark changes if mislocalizations are corrected or removed for two categories. For the person category, removing mislocalizations brings precision up to essentially 100%, indicating that mislocalization is the predominant source of false positives. Correcting the mislocalizations provides a huge jump in recall. For the cat category the improvement provided by better localization is much less, indicating that there are still some false positives arising from misclassifications.
We can do this analysis for all categories. The average improvement in APr by fixing mislocalization is a measure of the impact of mislocalization on performance. We can also measure impact in this way for other error modes: for instance, false positives on objects of other similar categories, or on background.(For defining similar and non-similar categories, we divide object categories into
"animals", "transport" and "indoor" groups.) The left subfigure in Figure 6 shows the result of such an analysis on our best system (C+ref). The dark blue bar shows the APr improvement if we remove mislocalized detections and the light blue bar shows the improvement if we correct them. The other two bars show the improvement from removing confusion with similar categories and background. Mislocalization has a huge impact: it sets us back by about
16 percentage points. Compared to that confusion with similar categories or background is virtually non-existent.
We can measure the impact of mislocalization on the other algorithms in Table 1 as well, as shown in Table 3. It also shows the upper bound APr achievable when all mislocalization is fixed. Improvements in the feature extractor improve the upper bound (indicating fewer misclassifications) but also reduce the gap due to mislocalization (indicating better localization). Refinement doesn't change the upper bound and only improves localization, as expected.
To get a better handle on what one needs to do to improve localization, we considered two statistics. For each detection and a ground truth, instead of just taking the overlap (i.e. intersection over union), we can compute the pixel precision (fraction of the region that lies inside the ground truth) and pixel recall(fraction of the ground truth that lies inside the region). It can be shown that having both a pixel precision > 67% and a pixel recall > 67% is guaranteed to give an overlap of greater than 50%. We assign detection labels using pixel precision or pixel recall using a threshold of 67% and compute the respective
AP. Comparing these two numbers then gives us a window into the kind of localization errors: a low pixel precision AP indicates that the error mode is overshooting the region and predicting extraneous background pixels, while a low pixel recall AP indicates that the error mode is undershooting the region and missing out some ground truth pixels.
B. Hariharan et al.
The second half of Figure 6 shows the difference between pixel precision AP(APpp) and pixel recall AP (APpr). Bars to the left indicate higher pixel recall
AP, while bars to the right indicate higher pixel precision AP. For some categories such as person and bird we tend to miss ground truth pixels, whereas for others such as bicycle we tend to leak into the background.
Recall
Precision
C+ref
No misloc
Corr misloc
Recall
Precision
C+ref
No misloc
Corr misloc
Fig. 4. PR on person(left) and cat(right). Blue is C+ref. Green is if an oracle removes mislocalized predictions, and red is if the oracle corrects our mislocalizations.
−1
Overlap Threshold
Change in APr (percentage points)
B
C
C+ref
−4
−2
Overlap Threshold
Change in APb (percentage points)
R−CNN−MCG
A
B
C
Fig. 5. Left: Improvement in mean APr over A due to our 3 variants for a variety of overlap thresholds. We get improvements for all overlap thresholds. Right: A similar plot for APb. Improvements are relative to R-CNN with Selective Search proposals.
As the threshold becomes stricter, the better localization of our approach is apparent.
Results on APb and APb vol
Comparison with prior work is easier on the classical bounding box and segmentation metrics. It also helps us evaluate if handling the SDS task also improves performance on the individual tasks. To compare on APb, we retrain our final region classifiers for the bounding box detection task. This is because the ranking of regions based on bounding box overlap is different from that based on
Simultaneous Detection and Segmentation
B
S
L
Improvement in APr (percentage points)
−0.4
−0.2
0.4 aeroplane bicycle bird boat bottle bus car cat chair cow diningtable dog horse motorbike person pottedplant sheep sofa train tvmonitor
APpp−APpr
Fig. 6. Left: Impact of the three kinds of false positives on mean APr. L : mislocalization, B : detection on background, and S : misfirings on similar categories. Right:
Disambiguating between two kinds of mislocalizations. Bars to the left mean that we frequently overshoot the ground truth, while bars to the right mean that we undershoot.
Table 3. Maximum achievable APr (assuming perfect localization) and loss in APr due to mislocalization for all systems
A
B
C
C+ref
AP Upper bound
Loss due to mislocalization
15.8 segmentation overlap. As in, we use ground truth boxes as positive, and MCG boxes overlapping by less than 50% as negative. At test time we do not do any region refinement.
We add two baselines: R-CNN is the system of Girshick et al. taken as is, and R-CNN-MCG is R-CNN on boxes from MCG instead of Selective Search. Note that neither of these baselines uses features from the region foreground.
Table 4 shows the mean APb and APb vol. We get improvements over R-CNN on both APb and APb vol, with improvements on the latter metric being somewhat larger. The right half of Figure 5 shows the variation in APb as we vary the overlap threshold for counting something as correct. We plot the improvement in APb over vanilla R-CNN. We do worse than R-CNN for low thresholds, but are much better for higher thresholds. This is also true to some extent for RCNN-MCG, so this is partly a property of MCG, and partly a consequence of our algorithm's improved localization. Interestingly, C does worse than B. We posit that this is because now the entire network has been finetuned for SDS.
Finally we evaluated C on PASCAL VOC 2012 test. Our mean APb of 50.7 is an improvement over the R-CNN mean APb of 49.6 (both without bounding box regression), and much better than other systems, such as SegDPM (40.7).
B. Hariharan et al.
Table 4. Results on APb and APb vol on VOC12 val. All numbers are %.
R-CNN R-CNN-MCG
A
B
C
Mean APb
Mean APb vol
Results on Pixel IU
For the semantic segmentation task, we convert the output of our final system(C+ref) into a pixel-level category labeling using the simple pasting scheme proposed by Carreira et al.. We cross validate the hyperparameters of this pasting step on the VOC11 segmentation Val set. The results are in Table 5. We compare to O2P and R-CNN which are the current state-of-the-art on this task. We advance the state-of-the-art by about 5 points, or 10% relative.
To conclude, our pipeline achieves good results on the SDS task while improving state-of-the-art in object detection and semantic segmentation. Figure 7 shows examples of the output of our system.
Table 5. Results on Pixel IU. All numbers are %.
O2P R-CNN C+ref
Mean Pixel IU (VOC2011 Test)
Mean Pixel IU (VOC2012 Test)Fig. 7. Top detections: 3 persons, 2 bikes, diningtable, sheep, chair, cat. We can handle uncommon pose and clutter and are able to resolve individual instances.
Acknowledgments. This work was supported by ONR MURI N000141010933, a Google Research Grant and a Microsoft Research fellowship. We thank the NVIDIA Corporation for providing GPUs through their academic program.
Simultaneous Detection and Segmentation
References
1. Arbel´aez, P., Pont-Tuset, J., Barron, J., Marques, F., Malik, J.: Multiscale combinatorial grouping. In: CVPR (2014)
2. Arbel´aez, P., Hariharan, B., Gu, C., Gupta, S., Malik, J.: Semantic segmentation using regions and parts. In: CVPR (2012)
3. Boix, X., Gonfaus, J.M., van de Weijer, J., Bagdanov, A.D., Serrat, J., Gonz`alez, J.: Harmony potentials. IJCV 96(1) (2012)
4. Bourdev, L., Maji, S., Brox, T., Malik, J.: Detecting people using mutually consistent poselet activations. In: Daniilidis, K., Maragos, P., Paragios, N. (eds.) ECCV 2010, Part VI. LNCS, vol. 6316, pp. 168–181. Springer, Heidelberg (2010)
5. Carreira, J., Caseiro, R., Batista, J., Sminchisescu, C.: Semantic segmentation with second-order pooling. In: Fitzgibbon, A., Lazebnik, S., Perona, P., Sato, Y., Schmid, C. (eds.) ECCV 2012, Part VII. LNCS, vol. 7578, pp. 430–443. Springer, Heidelberg (2012)
6. Carreira, J., Sminchisescu, C.: Constrained parametric min-cuts for automatic object segmentation. In: CVPR (2010)
7. Dai, Q., Hoiem, D.: Learning to localize detected objects. In: CVPR (2012)
8. Dalal, N., Triggs, B.: Histograms of oriented gradients for human detection. In:
CVPR (2005)
9. Deng, J., Berg, A., Satheesh, S., Su, H., Khosla, A., Fei-Fei, L.: ImageNet Large
Scale Visual Recognition Competition 2012 (ILSVRC 2012) (2012), http://www.image-net.org/challenges/LSVRC/2012/
10. Donahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E., Darrell, T.:
Decaf: A deep convolutional activation feature for generic visual recognition. arXiv preprint arXiv:1310.1531 (2013)
11. Everingham, M., Van Gool, L., Williams, C.K.I., Winn, J., Zisserman, A.: The Pascal Visual Object Classes (VOC) Challenge. IJCV 88(2) (2010)
12. Farabet, C., Couprie, C., Najman, L., LeCun, Y.: Learning hierarchical features for scene labeling. TPAMI 35(8) (2013)
13. Felzenszwalb, P.F., Girshick, R.B., McAllester, D., Ramanan, D.: Object detection with discriminatively trained part-based models. TPAMI 32(9) (2010)
14. Fidler, S., Mottaghi, R., Yuille, A., Urtasun, R.: Bottom-up segmentation for topdown detection. In: CVPR (2013)
15. Fukushima, K.: Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. Biological Cybernetics
16. Girshick, R., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for accurate object detection and semantic segmentation. In: CVPR (2014)
17. Hariharan, B., Arbelaez, P., Bourdev, L., Maji, S., Malik, J.: Semantic contours from inverse detectors. In: ICCV (2011)
18. Hoiem, D., Chodpathumwan, Y., Dai, Q.: Diagnosing error in object detectors. In:
Fitzgibbon, A., Lazebnik, S., Perona, P., Sato, Y., Schmid, C. (eds.) ECCV 2012, Part III. LNCS, vol. 7574, pp. 340–353. Springer, Heidelberg (2012)
19. Jia, Y.: Caffe: An open source convolutional architecture for fast feature embedding(2013), http://caffe.berkeleyvision.org/
20. Kim, B.-S., Sun, M., Kohli, P., Savarese, S.: Relating things and stuff by high-order potential modeling. In: Fusiello, A., Murino, V., Cucchiara, R. (eds.) ECCV 2012
Ws/Demos, Part III. LNCS, vol. 7585, pp. 293–304. Springer, Heidelberg (2012)
B. Hariharan et al.
21. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep convolutional neural networks. In: NIPS (2012)
22. Ladick´y, L., Sturgess, P., Alahari, K., Russell, C., Torr, P.H.S.: What, where and how many? Combining object detectors and CRFs. In: Daniilidis, K., Maragos, P., Paragios, N. (eds.) ECCV 2010, Part IV. LNCS, vol. 6314, pp. 424–437. Springer, Heidelberg (2010)
23. LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W., Jackel, L.D.: Backpropagation applied to handwritten zip code recognition. Neural
Computation 1(4) (1989)
24. Lowe, D.G.: Distinctive image features from scale-invariant keypoints. IJCV 60(2)
25. Mottaghi, R.: Augmenting deformable part models with irregular-shaped object patches. In: CVPR (2012)
26. Parkhi, O.M., Vedaldi, A., Jawahar, C., Zisserman, A.: The truth about cats and dogs. In: ICCV (2011)
27. van de Sande, K.E., Uijlings, J.R., Gevers, T., Smeulders, A.W.: Segmentation as selective search for object recognition. In: ICCV (2011)
28. Sermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., LeCun, Y.: Overfeat:
Integrated recognition, localization and detection using convolutional networks. In:
ICLR (2014)
29. Sermanet, P., Kavukcuoglu, K., Chintala, S., LeCun, Y.: Pedestrian detection with unsupervised multi-stage feature learning. In: CVPR (2013)
30. Shotton, J., Winn, J.M., Rother, C., Criminisi, A.: TextonBoost: Joint appearance, shape and context modeling for multi-class object recognition and segmentation.
In: Leonardis, A., Bischof, H., Pinz, A. (eds.) ECCV 2006, Part I. LNCS, vol. 3951, pp. 1–15. Springer, Heidelberg (2006)
31. Tighe, J., Niethammer, M., Lazebnik, S.: Scene parsing with object instances and occlusion handling. In: ECCV (2010)
32. Yang, Y., Hallman, S., Ramanan, D., Fowlkes, C.C.: Layered object models for image segmentation. TPAMI 34(9) (2012)Fully Convolutional Networks for Semantic Segmentation
Jonathan Long∗, Evan Shelhamer∗, Trevor Darrell
UC Berkeley
21 backward/learning forward/inference pixelwise prediction segmentation g.t.
Figure 1: Fully convolutional networks can efficiently learn to make dense predictions for per-pixel tasks like semantic segmentation.
Fully convolutional networks (FCNs) trained end-to-end, pixels-to-pixels without further machinery exceed the state-of-the-art for semantic segmentation. Convolutional networks are powerful visual models that yield hierarchies of features. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet
 ) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a skip architecture that combines semantic information from deep, coarse layers with appearance information from shallow, fine layers to produce accurate and detailed segmentations. Our fully convolutional network achieves stateof-the-art segmentation of PASCAL VOC (20% relative improvement over SDS to 62.2% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes ∼ 175 ms for a typical image.
Semantic segmentation faces an inherent tension between semantics and location: global information resolves what while local information resolves where. Deep feature hierarchies encode location and semantics in a nonlinear local-to-global pyramid. To take advantage of this feature spectrum, we add skips between layers to combine multi-resolution information. We call this representation at a pixel the deep jet. This skip architecture is learned end-to-end to refine the semantics and spatial precision of the output.
FCN learning and inference are performed on a whole image at a time by dense feedforward computation and backpropagation. Fully convolutional versions of existing networks predict dense outputs from arbitrarysized inputs. This lets us transfer the representations learned by classification models to semantic segmentation. In-network upsampling layers enable pixelwise prediction and learning in nets with subsampled pooling. To our knowledge, this is the first work to train FCNs end-to-end (1) for pixelwise prediction and (2) from supervised pre-training.
This method is efficient, both asymptotically and absolutely, and precludes the need for the machinery in other works. We compare to the common approach of patchwise training and find that our accelerated fully convolutional training does no harm to model quality. Our approach does not make use of pre- and post-processing complications including superpixels, proposals, image pyramids, post-hoc refinement by random fields or local classifiers, or ensembles.
We test our FCN on semantic segmentation and scene parsing, exploring PASCAL VOC, NYUDv2, and SIFT Flow. Although these tasks have historically distinguished between objects and regions, we treat both uniThis is an extended abstract. The full paper is available at the Computer Vision Foundation webpage. ∗Authors contributed equally.
FCN-32s
FCN-16s
FCN-8s
Ground truth
Figure 2:
Refining fully convolutional nets by fusing information from layers with different strides improves segmentation detail. The first three images show the output from our 32, 16, and 8 pixel stride nets. formly as pixel prediction. We evaluate our FCN skip architecture on each of these datasets, and then extend it to multi-modal input for NYUDv2 and multi-task prediction for the semantic and geometric labels of SIFT Flow.
Our code and models are publicly available at http://fcn.berkeleyvision.org.
 Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell. DeCAF: A deep convolutional activation feature for generic visual recognition. In ICML, 2014.
 Bharath Hariharan, Pablo Arbeláez, Ross Girshick, and Jitendra Malik.
Simultaneous detection and segmentation. In European Conference on
Computer Vision (ECCV), 2014.
 Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012.
 K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556, 2014.
 Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew
Rabinovich. Going deeper with convolutions. CoRR, abs/1409.4842, 2014. URL http://arxiv.org/abs/1409.4842.Racial Faces in the Wild: Reducing Racial Bias by Information Maximization
Adaptation Network
Mei Wang1, Weihong Deng1*, Jiani Hu1, Xunqiang Tao2, Yaohai Huang2
1Beijing University of Posts and Telecommunications, 2Canon Information Technology (Beijing) Co., Ltd
{wangmei1, whdeng, jnhu}@bupt.edu.cn, {taoxunqiang, huangyaohai}@canon-ib.com.cn
Abstract
Racial bias is an important issue in biometric, but has not been thoroughly studied in deep face recognition. In this paper, we first contribute a dedicated dataset called
Racial Faces in-the-Wild (RFW) database, on which we firmly validated the racial bias of four commercial APIs and four state-of-the-art (SOTA) algorithms. Then, we further present the solution using deep unsupervised domain adaptation and propose a deep information maximization adaptation network (IMAN) to alleviate this bias by using Caucasian as source domain and other races as target domains.
This unsupervised method simultaneously aligns global distribution to decrease race gap at domain-level, and learns the discriminative target representations at cluster level.
A novel mutual information loss is proposed to further enhance the discriminative ability of network output without label information. Extensive experiments on RFW, GBU, and IJB-A databases show that IMAN successfully learns features that generalize well across different races and across different databases.
1. Introduction
The emergence of deep convolutional neural networks(CNN) greatly advances the frontier of face recognition (FR). However, more and more people find that a problematic issue, namely racial bias, has always been concealed in the previous studies due to biased benchmarks but it explicitly degrades the performance in realistic FR systems. For example, Amazon's
Rekognition Tool incorrectly matched the photos of 28 U.S. congressmen with the faces of criminals, especially the error rate was up to 39% for non-Caucasian people. Although several studies have uncovered racial bias in non-deep FR algorithms, this field still remains to be vacant in deep learning era because so little testing information available makes it hard to measure the racial bias.
To facilitate the research towards this issue, in this work we construct a new Racial Faces in-the-Wild (RFW) database, as shown in Fig. 1 and Table 4, to fairly measure racial bias in deep FR. Based on experiments on RFW, we find that both commercial APIs and SOTA algorithms indeed suffer from racial bias: the error rates on African faces are about two times of Caucasians, as shown in Table
1. To investigate the biases caused by training data, we also collect a race-balanced training database, and validate that racial bias comes on both data and algorithm aspects. Some specific races are inherently more difficult to recognize even trained on the race-balanced training data. Further research efforts on algorithms are requested to eliminate racial bias.
Figure 1. Examples and average faces of RFW database. In rows top to bottom: Caucasian, Indian, Asian, African.
Unsupervised domain adaptation (UDA) is one of the promising methodologies to address algorithm biases, which can map two domains into a domain-invariant feature space and improve target performances in an unsupervised manner. Unfortunately, most UDA methods for object recognition are not applicable for FR because of two unique challenges. First, face identities (classes) of two domains are non-overlapping in FR, so that many skills in state-of-the-art (SOTA) methods based on sharing classes are inapplicable. Second, popular methods by the global alignment of source and target domain are insufficient to acquire the discriminating power for classification in FR.
How to meet these two challenges is meaningful but few works have been proposed in this community.
Model
RFW
Caucasian
Indian
Asian
African
Microsoft 
75.83 commercial
Face++ 
API
Baidu 
Amazon 
86.27 mean
Center-loss 
SOTA
Sphereface 
82.28 algorithm
Arcface1 
VGGface2 
83.38 mean
1 Arcface here is trained on CASIA-Webface using ResNet-34.
Table 1. Racial bias in deep FR systems. Verification accuracies(%) evaluated on 6000 difficult pairs of RFW database are given.
In this paper, we propose a new information maximization adaptation network (IMAN) to mitigate racial bias, which matches global distribution at domain-level, at the meantime, learns discriminative target distribution at cluster-level. To circumvent the non-overlapping classes between two domains, IMAN applies a spectral clustering algorithm to generate pseudo-labels, by which the network is pre-adapted with Softmax and the target performance is enhanced preliminarily. This clustering scheme of IMAN is fundamentally different from other UDA methods
 that are inapplicable to FR. Besides pseudo label based pre-adaptation, a novel mutual information (MI) based adaptation is proposed to further enhance the discriminative ability of the network output, which learns larger decision margins in an unsupervised way. Different from the common supervised losses and supervised MI methods, MI loss takes advantage of all unlabeled target data, no matter whether they are successfully assigned pseudolabels or not, in virtue of its unsupervised property.
Extensive experimental results show that IMAN conducted to transfer recognition knowledge from Caucasian(source) domain to other-race (target) domains. Its performance is much better than other UDA methods. Ablation study shows that MI loss has unique effect on reducing racial bias. In addition, IMAN is also helpful in adapting general deep model to a specific database, and achieved improved performance on GBU and IJB-A databases. The contributions of this work are three aspects. 1) A new RFW dataset is constructed and is released 1 for the study on racial bias. 2) Comprehensive experiments on RFW validate the existence and cause of racial bias in deep FR algorithms. 3)
A novel IMAN solution is introduced to address racial bias.
2. Related work
Racial bias in face recognition. Several studies have uncovered racial bias in non-deep
1http://www.whdeng.cn/RFW/index.html face recognition algorithms. The FRVT 2002 showed that recognition accuracies depend on demographic cohort.
Phillips et al. evaluated FR algorithms on the images of FRVT 2006 and found that algorithms performed better on natives.
Klare et al.
 collected mug shot face images of White, Black and Hispanic from the Pinellas County Sheriff's Office (PCSO) and concluded that the Black cohorts are more difficult to recognize. In deep learning era, existing racial bias databases are no longer suitable for deep FR algorithms due to their small scale and constrained conditions; commonly-used testing databases of deep FR, e.g. LFW, IJB-A, don't include significant racial diversity, as shown in Table 2. Although some studies, e.g. unequal-training and suppressing attributes, have made effort to mitigate racial and gender bias in several computer vision tasks, this study remains to be vacant in FR. Thus, we construct a new RFW database to facilitate the research towards this issue.
Train/
Database
Racial distribution (%)
Test
Caucasian
Asian
Indian
African train
CASIA-WebFace 
VGGFace2 
MS-Celeb-1M 
14.5 test
LFW 
IJB-A 
RFW
Table 2. The percentage of different race in commonly-used training and testing databases
Deep unsupervised domain adaptation. UDA utilizes labeled data in relevant source domains to execute new tasks in a target domain. However, the research of UDA is limited to object classification, very few studies have focused on UDA for FR task. Luo et al.
 integrated the maximum mean discrepancies (MMD) estimator to CNN to decrease domain discrepancy. Sohn et al. synthesized video frames from images by a set of transformations and applied a domain adversarial discriminator to align feature space of image and video domains.
Kan et al. utilized the sparse representation constraint to ensure that source domain shares similar distribution as target domain. In this paper, inspired by Inception Score
 used in Generative Adversarial Nets (GAN), we introduce MI as a regularization term to domain adaptation and propose a novel IMAN method to address this unique challenge of FR in an unsupervised way.
3. Racial Faces in-the-Wild: RFW
Instead of downloading images from websites, we collect them from MS-Celeb-1M. We use the "Nationality" attribute of FreeBase celebrities to directly select
Asians and Indians. For Caucasians and Africans, Face++
API is used to estimate race. An identity will be accept693 ed only if its most images are estimated as the same race, otherwise it will be abandoned. To avoid the negative effects caused by the biased Face++ tool, we manually check some images with low confidence scores from Face++.
Then we construct our RFW database with four testing subsets, namely Caucasian, Asian, Indian and African.
Each subset contains about 10K images of 3K individuals for face verification. All of these images have been carefully and manually cleaned. Besides, in order to exclude overlapping identities between RFW and commonly-used training datasets, we further remove the overlapping subjects by manual inspection, when the subject and its nearest neighbor in CASIA-Webface and VGGFace2 (based on Arcface feature) are found to be of the same identity.
For the performance evaluation, we recommend to use both the biometric receiver operating characteristic (ROC) curve and LFW-like protocol.
Specifically, ROC curve, which aims to report a comprehensive performance, evaluates algorithms on all pairs of 3K identities (about 14K positive vs. 50M negative pairs). In contrast, LFW-like protocol facilitates easy and fast comparison between algorithms with 6K pairs of images. Further, inspired by the ugly subset of GBU database, we have selected the "difficult" pairs (in term of cosine similarity) to avoid the saturated performance to be easily reported 2.
Positive pairs
Negative pairs
Figure 2.
Examples of pairs in RFW database. We select 6K difficult pairs according to cosine similarity to avoid saturated performance, these images challenge the recognizer by variations of same people and the similar appearance of different people.
In RFW, the images of each race are randomly collected from MS-Celeb-1M without any preference, and thus they are suitable to fairly measure racial bias. We have validated that, across varying races, their distributions of pose, age, and gender are similar. As evidence, the detailed distributions measured by Face++ API are show in Fig. 3(a)-3(d).
One can see from the figures that there is no significant difference between different races.
Moreover, the pose and age gap distributions of 3K difficult positive pairs are show in Fig. 3(e) and 3(f), which indicates that the selected difficult pairs are also fair across dif2All data and baseline code for evaluating will be publicly available for the research purpose. ferent races and contain larger intra-person variations. And
Fig. 2 presents some examples of the 6K selected pairs, and one can see from the figure that some pairs are very challenging even for human.
4. Information maximization adaptation network
In our study, source domain is a labeled training set, namely Ds = {xs i, ys i }M i=1 where xs i is the i-th source sample, ys i is its category label, and M is the number of source images. Target domain is an unlabeled training set, namely
Dt = {xt i}N i=1 where xt i is the i-th target sample and N is the number of target images. The data distributions of two domains are different, P(Xs, Ys) ̸= P(Xt, Yt). Our goal is to learn deep features invariant between domains and improve the performance of target images (faces of colored skin in our study) in an unsupervised manner. In the face recognition task, the identities (class) of two domains are non-overlapping, which poses a unique challenge different from other tasks.
Clustering-based pseudo labels for preadaptation
Previous UDA methods apply the source classifier to predict pseudo-labels in the target domain, by which the network can be fine-tuned using supervised losses.
Unfortunately, these well-established approaches are inapplicable in face recognition due to the nonoverlapping identities between two domains.
Therefore, we introduce a clustering algorithm into UDA to generate pseudo-labels for pre-adaptation training. The detailed steps of our clustering algorithm are given as following:
First, we feed unlabeled target data Xt into network and extract deep features F(Xt). Then, with these deep presentations, we construct a N × N adjacency matrix, where N is the number of faces in target domain and entry at (i, j), i.e. s(i, j), is the cosine similarity between target face xt i and xt j.
Second, we can build a clustering graph G(n, e) according to adjacency matrix, where the node ni represents i-th target image and edge e(ni, nj) signifies that two target images have larger cosine-similarity than the parameter λ: e(ni, nj) =
� 1, if s(i, j) > λ
0, otherwise
Then, we simply save each connected component with at least p nodes as a cluster (identity) and obtain pseudo-labels of these target images; the remaining images will be abandoned.
So, we only obtain pseudo-labels of partial images with higher confidence to alleviate negative influence caused by falsely-labeled samples. After that, we pre-adapt the network with the standard Softmax loss.(a) yaw(b) pitch(c) age(d) gender(e) pose gap(f) age gap
Figure 3. RFW statistics. We show the (a) yaw pose, (b) pitch pose, (c) age and (d) gender distribution of 3000 identities in RFW, as well as (e) Pose gap distribution and (f) age gap distribution of positive pairs in LFW and RFW.
Clustering method pseudo labels
MIadaptation
Pseudoadaptation
Clustering-based pseudo labels
Network output
Mutual information loss
Labeled source data
Unlabeled target data
Labeled source data
Anne Hathaway
Tom Cruise
…
…
…
…
MMD
…
…
MMD
…
…
Source
Classification loss unlabeled target data
…
Figure 4. Overview of IMAN architecture. Step-1: Pseudo-adaptation. Pseudo-labels of target images are generated by clustering algorithm and then are utilized to pre-adapt the network with supervision of Softmax to obtain preliminary improvement of target domain.
Step-2: MI-adaptation. With mutual information loss, the distribution of target classifier's output is further optimized and larger decision margins are learned without any label information.
4.2. Mutual information loss for discriminant adaptation
Although pre-adaptation has derived preliminary prediction of the target images, it is insufficient to boost the performance in target domain due to the imperfection of pseudolabels. How can we take full advantage of the full set of target images and learn more discriminative representations? Based on the preliminary prediction, we propose to further optimize the distribution of classifier's output without any label information. Our idea is to learn large decision margins in feature space through enlarging the classifier's output of one class while suppressing those of other classes in an unsupervised way. Different from supervised mutual information, our MI loss maximizes mutual information between unlabeled target data Xt and classifier's prediction Ot inspired by.
Based on the desideratum that an ideal conditional distribution of classifier's prediction p(Ot|xt i) should look like [0, 0,..., 1,..., 0], it's better to classify samples with large margin. Grandvalet proved that a entropy term
N
�N i=1 H(Ot|xt i) very effectively meets this requirement, because it is maximized when the distribution of classifier's prediction is uniform and vice versa. However, in the case of fully unsupervised learning, simply minimizing this entropy will cause that more decision boundaries are removed and most samples are assigned to the same class. Therefore, we prefer to uniform distribution of category. An estimate of the marginal distribution of classifier's prediction p(Ot) is given as follows: p(Ot) =
� p(xt i)p(Ot|xt i)dxt i = 1
N
N
� i=1 p
�
Ot|xt i
�(2) we suggest that maximizing the entropy of Ot can make samples assigned evenly across the categories of dataset.
In information theory, mutual information between X and Y, i.e. I(X; Y ), can be expressed as the difference of two entropy terms:
I(X; Y ) = H(X) − H(X|Y ) = H(Y ) − H(Y |X) (3)
If X and Y are related by a deterministic, invertible function, then maximal mutual information is attained. In our case, we combine the two entropy terms and obtain mutual information between data Xt and prediction Ot:
LM = 1
N
N
� i=1
H(Ot|xt i) − γH(Ot)
= 1
N
N
� i=1
NC
� j=1 p(ot j|xt i)logp(ot j|xt i) − γ
NC
� j=1 p(ot j)logp(ot j)
=
N
� i=1
NC
� j=1 p(xt i)p(ot j|xt i)logp(ot j|xt i) − γ
NC
� j=1 p(ot j)logp(ot j)
= H [Ot|Xt] − γH [Ot] ≈ −I(Xt; Ot)
695 where the first term is the entropy of conditional distribution of Ot which can enlarge the classifier's output of one class while suppressing those of other classes; and the second term is the entropy of marginal distribution of Ot which can avoid most samples being assigned to the same class. N is the number of target images, and NC is the number of target categories. But without groundtruth labels, how can we obtain NC and guarantee the accuracy of classifier's prediction? Benefiting from clustering-based pseudo labels, we utilize the number of clusters to substitute for NC, and obtain preliminary prediction through pre-adaptation to guarantee accuracy for mutual information loss.
4.3. Adaptation network
As shown in Fig. 4, the architecture of IMAN consists of a source and target CNN, with shared weights. Maximum mean discrepancy (MMD) estimator, which is a standard distribution distance metric to measure domain discrepancy, is adopted on higher layers of network which are called adaptation layers. We simply use a fork at the top of the network after the adaptation layer. The inputs of source CNN are source labeled images while those of target
CNN are target unlabeled data. The goal of training is to minimize the following loss:
L = LC(Xs, Ys) + α
� l∈L
MMD2(Dl s, Dl t) + βLM(Xt)(5) where α and β are the parameters for the trade-off between three terms. LM(Xt) is our mutual-information loss on unlabeled target data Xt. LC(Xs, Ys) denotes source classification loss on the source data Xs and the source labels Ys.
Dl
∗ is the l-th layer hidden representation for the source and target examples, and MMD2(Dl s, Dl t) is the MMD between the source and target evaluated on the lth layer representation. The empirical estimate of MMD between two domains is defined as MMD2(Ds, Dt) =
�����
M
M
� i=1 φ(xs i)− 1
N
N
� j=1 φ(xt j)
�����
H, where φ represents the function that maps the original data to a reproducing kernel Hilbert space.
The entire procedure of IMAN is depicted in Algorithm
1. Source classification loss supervises learning proceeds for source domain. MMD minimizes the domain discrepancy to learn domain-invariant representations. Additionally, in the pre-training stage, MMD provides more reliable underlying target representations for clustering leading to higher quality of pseudo-labels. Clustering-based pseudolabels can improve the performance of target domain preliminarily and guarantee the accuracy of network's prediction for unsupervised MI loss. MI loss can further take full advantage of all target data, no matter whether they are successfully clustered or not, to learn larger decision margins and enhance the discrimination ability of network for target domain.
Algorithm 1 Information Maximization Adaptation Network (IMAN).
Input:
Source domain labeled samples {xs i, ys i }M i=1, and target domain unlabeled samples {xt i}N i=1.
Output:
Network layer parameters Θ.
1: Stage-1: // Pre-training:
2: Pre-train network by MMD and source classification loss to minimize domain discrepancy and provide more reliable target representations for clustering;
3: Repeat:
4: Stage-2: // Pre-adaptation:
5: Adopt clustering algorithms to generate pseudo-labels of partial target images according to Eqn. (1); Pre-adapt the network on them with supervision of Softmax to obtain preliminary improvement of target domain;
6: Stage-3: // MI-adaptation:
7: Adapt the network with mutual information loss according to Eqn. (5) to further enhance the discrimination ability of network output;
8: Until convergence
5. Experiments on RFW
5.1. Racial bias experiment
Experimental Settings. We use the similar ResNet-34 architecture described in. It is trained with the guidance of Arcface loss on the CAISA-Webface, and is called Arcface(CASIA) model. CASIA-Webface consists of 0.5M images of 10K celebrities in which 85% of the photos are Caucasians. For preprocessing, we use five facial landmarks for similarity transformation, then crop and resize the faces to 112×112. Each pixel ( ) in RGB images is normalized by subtracting 127.5 and then being divided by 128. We set the batch size, momentum, and weight decay as 200, 0.9 and 5e−4, respectively. The learning rate is started from 0.1 and decreased twice with a factor of 10 when errors plateau.
Existence of racial bias. We extract features of 6000 pairs in RFW by our Arcface(CASIA) model and compare the distribution of cosine-distances, as shown in Fig.
5(c).
The distribution of Caucasian has a more distinct margin than that of other races, which visually proves the recognition errors of non-Caucasian subjects are much higher. Then, we also examine some SOTA algorithms, i.e.
Center-loss, Sphereface, VGGFace2 and ArcFace, as well as four commercial recognition APIs, i.e. Face++, Baidu, Amazon, Microsoft on our RFW. The(a) Center loss(b) Spereface(c) Arcface(d) VGGFace2
Figure 6. The ROC curves of (e) Center loss, (f) Spereface (g)
Arcface, (h) VGGFace2 evaluated on all pairs. biometric ROC curves evaluated on all pairs are presented in Fig. 6; the accuracies in LFW-like protocol are given in Table 1 and its ROC curves are given in the Supplementary Material. First, all SOTA algorithms and APIs perform the best on Caucasian testing subset, followed by Indian, and the worst on Asian and African. This is because that the learned representations predominantly trained on Caucasians will discard information useful for discerning nonCaucasian faces. Second, a phenomenon is found coincident with : APIs which are developed by East Asian companies perform better on Asians, while APIs developed in the Western hemisphere perform better on Caucasians.
Existence of domain gap. The visualization and quantitative comparisons are conducted at feature level.
The deep features of 1.2K images are extracted by our Arcface(CASIA) model and are visualized respectively using tSNE embeddings, as shown in Fig. 5(a). The features almost completely separate according to race. Moreover, we use the MMD to compute distribution discrepancy between the images of Caucasians and other races in Fig. 5(b).
From the figures, we make the same conclusions: the distribution discrepancies between Caucasians and other races are much larger than that between Caucasians themselves, which conforms that there is domain gap between races.
Cause of racial bias. We download more images of nonCaucasians from Website according to FreeBase celebrities, and construct an Equalizedface dataset. It contains
590K images from 14K celebrities which has the similar scale with CASIA-Webface database but is approximately race-balanced with 3.5K identities per race. Using Equalizedface as training data, we train an Arcface(Equal) model in the same way as Arcface(CASIA) model and compare their performances on 6000 difficult pairs of RFW, as shown in Table 3. Compared with Arcface(CASIA) model, Arcface(Equal) model trained equally on all races performs much better on non-Caucasians which proves that racial bias in databases will reflect in FR algorithm. However, even with balanced training, we see that non-Caucasians still perform poorly than Caucasians. The reason may be that faces of colored skin are more difficult to extract and preprocess feature information, especially in dark situations. Moreover, we also train specific models on 7K identities of the same race, its performance is a bit lower compared to balanced training (3.5K people for each race). We believe there exists cooperative relationships among different races due to similar low-level features so that this mixture of races would improve the recognition ability.
5.2. Domain adaptation experiment
Datasets. A training set with four race-subsets is also constructed according to RFW. One training subset consists of about 500K labeled images of 10k Caucasians and three other subsets contain 50K unlabeled images of nonCaucasians, respectively, as shown in Table 4. We use Caucasian as source domain and other races as target domains, and evaluate algorithms on 6000 pairs and all pairs of RFW.
Subsets
Train
Test
# Subjects
# Images
# Subjects
# Images
Caucasian
IndianAsianAfricanTable 4. Statistic of training and testing dataset.
Implementation detail. For preprocessing, we share the uniform alignment methods as Arcface(CASIA) model as mentioned above. For MMD, we follow the settings in DAN, and apply MMD to the last two fully-connected layers.
In all experiments, we use ResNet-34 as backbone and set the batch size, momentum, and weight decay as 200, 0.9 and 5e − 4, respectively. In pre-training stage, the learning rate is started from 0.1 and decreased twice with a factor of 10 when errors plateau. In pre-adaptation stage, we pre-adapt network on pseudo-labeled target samples and source samples using learning rate of 5e − 3. In MI-adaptation stage, we adapt the network with learning rate of 1e − 3 using all source and target data. In IMAN-A(Arcface), Arcface is used as source classification loss and the parameter α, β and γ are set to be 10, 5 and 0.2, respectively. In IMANS(Softmax), Softmax is used as source classification loss and the parameter α, β and γ are set to be 2, 5 and 0.2.
Experimental result. Three UDA tasks are performed, namely transferring knowledge from Caucasian to Indian, Asian and African. Due to the particularity of task, very few studies have focused on UDA in FR task. The latest work
Caucasian
African
Asian
Indian(a) T-SNE(b) MMD
-0.2
Positive Pairs
Negative Pairs
-0.2
Positive Pairs
Negative Pairs
-0.2
Positive Pairs
Negative Pairs
-0.2
Positive Pairs
Negative Pairs
Indian testing subset
Caucasian testing subset
African testing subset
Asian testing subset
Cosine distance
Cosine distance
Cosine distance
Cosine distance
Numbers of pairs
Numbers of pairs
Numbers of pairs
Numbers of pairs
TN
FN
FP
TP
TN
FN
FP
TP
TN
FN
FP
TP
TN
FN
FP
TP(c) Distribution of cosine-distances of 6000 pairs
Figure 5. (a) The feature space of four testing subsets. Each color dot represents a image belong to Caucasian, Indian, Asian or African.(b) The distribution discrepancy between Caucasians and other races measured by MMD. 'Ca', 'As', 'In' and 'Af' represent Caucasian, Asian, Indian and African, respectively. (c) Distribution of cosine-distances of 6000 pairs on Caucasian, Indian, Asian and African subset.
Training Databases
LFW
CFP-FP
AgeDB-30
Caucasian
Indian
Asian
African
CASIA-WebFace 
Equalizedface (ours)
Caucasian-7000Indian-7000Asian-7000African-7000Table 3. Verification accuracy (%) of ResNet-34 models trained with different training datasets. is performed by Luo et al. who utilizes MMD-based method, i.e. DDC and DAN, to perform scene adaptation. Therefore, we also compare our IMAN with these two UDA methods. DDC adopts single-kernel MMD on the last fully-connected layers; DAN adopts multi-kernel
MMD on the last two fully-connected layers.
Methods
Caucasian
Indian
Asian
African
Softmax
DDC-S DAN-S IMAN-S (ours)Arcface 
DDC-A DAN-A IMAN-A (ours)IMAN*-A (ours)Table 5. Verification accuracy (%) on 6000 pairs of RFW dataset.
"-S" represents the methods using Softmax as source classification loss; while "-A" represents the ones using Arcface.
From Table 5 and Fig. 7, we have the following observations. First, without adaptation, Arcface, which published in CVPR'19 and reported SOTA performance on the LFW and MegaFace challenges, can not obtain perfect performance on non-Caucasians due to race gap. Second, MMDbased methods, i.e. DDC and DAN, obtain limited improvement compared with Softmax and Arcface model, which confirms our thought that the popular methods by the global alignment of source and target domain are insufficient for face recognition. Third, we can find that our IMAN-S and IMAN-A both dramatically outperform all of the compared methods and IMAN-A achieves about 3% gains over(a) Indian set(b) Asian set(c) African set
Figure 7. The ROC curves of Arcface, DAN-A, and IMAN-A models evaluated on all pairs of non-Caucasian sets.
Arcface model. Furthermore, when pre-adapting network with supervision of Arcface loss instead of Softmax loss in the second stage, our IMAN-A (denoted as IMAN*-A) is further improved, and obtains the best performances with
94.15%, 91.15% and 91.42% for Indian, Asian and African set. Especially, we further optimize IMAN*-A by performing pre-adaptation and MI-adaptation alternatively and iteratively in task Caucasian→African, and show the accuracy at each iteration in Fig. 8. The performance gradually increases until convergence.
Figure 8.
Verification accuracy of IMAN*-A at each iteration when performing pre-adaptation and MI-adaptation alternatively in task Caucasian→African. The value at the 0-th iteration means accuracy of Arcface tested on 6K pairs of African set.
Ablation Study. IMAN consists of two main contributions comparing with existing UDA methods, i.e. pseudoadaptation and MI-adaptation.
To evaluate their effec698 before adaptation after adaptation
Caucasian
African
Caucasian
African(a) Feature visualization(b) Domain discrepancy
Figure 9. (a) Feature visualization in task Caucasian→African. (b)
Distribution discrepancy of source and target domain. tiveness, we perform ablation study using Arcface loss as source classification loss.
In Table 6, the results of IMAN w/o pseudo-labels are unsatisfactory because MI loss depends on pseudo-adaptation to guarantee the accuracy of classifier and only performing MI-adaptation with a randomly-initialized classifier is meaningless. To get a fair comparison, as we can see from the results of IMAN w/o
MI, pseudo-adaptation is superior to baseline by about 2.3% on average, and our IMAN outperforms pseudo-adaptation by about 1.1% benefiting from MI-adaptation. It shows that each component has unique effect on reducing racial bias.
Methods
Indian
Asian
African w/o pseudo-labels
85.52 w/o MI
IMAN-A (ours)
Table 6. Ablation study on 6000 pairs of RFW dataset.
Visualization.
To demonstrate the transferability of the IMAN learned features, the visualization comparisons are conducted at feature level. First, we randomly extract the deep features of 10K source and target images in task Caucasian→African with Arcface model and IMANA model, respectively. The features are visualized using tSNE, as shown in Fig. 9(a). After adaptation, more source and target data begin to mix in feature space so that there is no boundary between them. Second, we compute domain discrepancy between source and target domain using Arcface and IMAN-A activations respectively. Fig. 9(b) shows that discrepancy using IMAN-A features is much smaller than that using Arcface features. Therefore, we conclude that our IMAN does help to minimize domain discrepancy and align feature space between two domains benefited from MMD.
Additional experiments on IJB-A and GBU. Besides race gap, there are other domain gaps which make the learnt model degenerate in target domain, e.g. different lighting condition, pose and image quality. To validate our IMAN method, we further adopt it to reduce these domain gaps by using CASIA-Webface as source domain and using GBU or IJB-A as target domain. The images in CASIA-Webface are collected from Internet under unconstrained environment and most of the figures are celebrities
Method
Ugly
Bad
Good
LRPCA-face 
Fusion 
VGG 
Arcface(CASIA) 
DAN-A 
IMAN-A (ours)
Table 7. VR at FAR of 0.001 for GBU partitions.
Method
IJB-A: Verif.
IJB-A: Identif.
TAR@FAR's of Rank1
Rank10
Bilinear-CNN Face-Search Deep-Multipose Triplet-Similarity Joint Bayesian VGG 
Arcface(CASIA) 
DAN-A 
IMAN-A (ours)
Table 8. Verification performance (%) of IJB-A. "Verif" represents the 1:1 verification and "Identif." denotes 1:N identification. taken in ambient lighting. GBU is split into three partitions with face pairs of different recognition difficulty, i.e. Good, Bad and Ugly. Each partition consists of a target set and a query set, and both them contain 1085 images of 437 distinct people. The images are frontal and are taken outdoors or indoors in atriums and hallways with digital camera. IJBA contains 5,397 images and 2,042 videos of 500 subjects, and covers large pose variations and contains many blurry video frames. The results on GBU and IJB-A databases are shown in Table 7 and 8. After adaptation, our IMANA surpasses other compared methods, even better than Arcface(CASIA) model. In particular, it outperforms the SOTA counterparts by a large margin on the GBU, although it is only based on the unsupervised adaptation.
6. Conclusion
An ultimate face recognition algorithm should perform fairly on different races. We have done the first step and create a benchmark, i.e. RFW, to fairly evaluate racial bias.
Through experiments on our RFW, we first verify the existence of racial bias. Then, we address it in the viewpoint of domain adaptation and design a novel IMAN method to bridge the domain gap and transfer knowledge between races. The comprehensive experiments prove the potential and effectiveness of our IMAN to reduce racial bias.
7. Acknowledgments
This work was supported by Canon Information Technology (Beijing) Co., Ltd. under Grant No. OLA18001.
References
 Amazon's rekognition tool. https://aws.amazon. com/rekognition/.
 Are face recognition systems accurate? depends on your race. https://www.technologyreview.com/s/
 Baidu cloud vision api. http://ai.baidu.com.
 Face++ research toolkit. www.faceplusplus.com.
 Microsoft azure. https://www.azure.cn.
 Ms-celeb-1m challenge 3: Face feature test/trillion pairs. http://trillionpairs.deepglint.com/.
 Wael AbdAlmageed, Yue Wu, Stephen Rawls, Shai Harel, Tal Hassner, Iacopo Masi, Jongmoo Choi, Jatuporn Lekust, Jungyeon Kim, Prem Natarajan, et al. Face recognition using deep multi-pose representations. In Applications of Computer Vision (WACV), 2016 IEEE Winter Conference on, pages
1–9. IEEE, 2016.
 Mohsan Alvi, Andrew Zisserman, and Christoffer Nellaker.
Turning a blind eye: Explicit removal of biases and variation from deep neural network embeddings. arXiv preprint arXiv:1809.02169, 2018.
 Alexander Amini, Ava Soleimany, Wilko Schwarting, Sangeeta Bhatia, and Daniela Rus. Uncovering and mitigating algorithmic bias through learned latent structure. AIES, Shane Barratt and Rishi Sharma. A note on the inception score. arXiv preprint arXiv:1801.01973, 2018.
 J Ross Beveridge, Geof H Givens, P Jonathon Phillips, Bruce A Draper, and Yui Man Lui. Focus on quality, predicting frvt 2006 performance. In Automatic Face &
Gesture Recognition, 2008. FG'08. 8th IEEE International
Conference on, pages 1–8. IEEE, 2008.
 Karsten M Borgwardt, Arthur Gretton, Malte J Rasch, HansPeter Kriegel, Bernhard Sch¨olkopf, and Alex J Smola. Integrating structured biological data by kernel maximum mean discrepancy. Bioinformatics, 22(14):e49–e57, 2006.
 Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in commercial gender classification. In Proceedings of the 1st Conference on Fairness, Accountability and Transparency, volume 81, pages 77–91, Raffaele Cafiero, Andrea Gabrielli, Miguel A. Mu&Ntilde, and oz. Integrating structured biological data by kernel maximum mean discrepancy. Bioinformatics, 22(14):e49–e57, Qiong Cao, Li Shen, Weidi Xie, Omkar M Parkhi, and Andrew Zisserman. Vggface2: A dataset for recognising faces across pose and age. arXiv preprint arXiv:1710.08092, 2017.
 Chaoqi Chen, Weiping Xie, Tingyang Xu, Wenbing Huang, Yu Rong, Xinghao Ding, Yue Huang, and Junzhou Huang.
Progressive feature alignment for unsupervised domain adaptation. arXiv preprint arXiv:1811.08585, 2018.
 Jun-Cheng Chen, Vishal M Patel, and Rama Chellappa. Unconstrained face verification using deep cnn features. In Applications of Computer Vision (WACV), 2016 IEEE Winter
Conference on, pages 1–9. IEEE, 2016.
 Minmin Chen, Kilian Q Weinberger, and John Blitzer. Cotraining for domain adaptation. In Advances in neural information processing systems, pages 2456–2464, 2011.
 Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya
Sutskever, and Pieter Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. In Advances in neural information processing systems, pages 2172–2180, 2016.
 Aruni Roy Chowdhury, Tsung-Yu Lin, Subhransu Maji, and Erik Learned-Miller. One-to-many face recognition with bilinear cnns. In 2016 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 1–9. IEEE, 2016.
 Jiankang Deng, Jia Guo, and Stefanos Zafeiriou. Arcface:
Additive angular margin loss for deep face recognition. arXiv preprint arXiv:1801.07698, 2018.
 Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. In ICML, pages 647–655, 2014.
 Nicholas Furl, P Jonathon Phillips, and Alice J O'Toole.
Face recognition algorithms and the other-race effect: computational mechanisms for a developmental contact hypothesis. Cognitive Science, 26(6):797–815, 2002.
 Yaroslav Ganin. Unsupervised domain adaptation by backpropagation. In ICML, pages 1180–1189, 2015.
 Clare Garvie.
The perpetual line-up: Unregulated police face recognition in America. Georgetown Law, Center on
Privacy & Technology, 2016.
 Ryan Gomes, Andreas Krause, and Pietro Perona. Discriminative clustering by regularized information maximization.
In NIPS, pages 775–783, 2010.
 Google. Freebase data dumps. https://developers. google.com/freebase/data, 2015.
 Yves Grandvalet and Yoshua Bengio.
Semi-supervised learning by entropy minimization.
In Advances in neural information processing systems, pages 529–536, 2005.
 Patrick J Grother, George W Quinn, and P Jonathon Phillips.
Report on the evaluation of 2d still-image face recognition algorithms. NIST interagency report, 7709:106, 2010.
 Yandong Guo, Lei Zhang, Yuxiao Hu, Xiaodong He, and Jianfeng Gao.
Ms-celeb-1m: A dataset and benchmark for large-scale face recognition. In ECCV, pages 87–102.
Springer, 2016.
 Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition.
In CVPR, pages 770–778, 2016.
 Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. arXiv preprint arXiv:1709.01507, 2017.
 Gary B Huang, Manu Ramesh, Tamara Berg, and Erik
Learned-Miller. Labeled faces in the wild: A database for studying face recognition in unconstrained environments.
Technical report, Technical Report 07-49, University of Massachusetts, Amherst, 2007.
 Bongjin Jun, Taewan Kim, and Daijin Kim. A compact local binary pattern using maximization of mutual information for face analysis. Pattern Recognition, 44(3):532–543, 2011.
 Meina Kan, Shiguang Shan, and Xilin Chen.
Bi-shifting auto-encoder for unsupervised domain adaptation. In ICCV, pages 3846–3854, 2015.
 Brendan F Klare, Mark J Burge, Joshua C Klontz, Richard
W Vorder Bruegge, and Anil K Jain. Face recognition performance: Role of demographic information. IEEE Transactions on Information Forensics and Security, 7(6):1789–
 Brendan F Klare, Ben Klein, Emma Taborsky, Austin Blanton, Jordan Cheney, Kristen Allen, Patrick Grother, Alan
Mah, and Anil K Jain. Pushing the frontiers of unconstrained face detection and recognition: IARPA Janus Benchmark A.
In CVPR, pages 1931–1939, 2015.
 Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classification with deep convolutional neural networks. In NIPS, pages 1097–1105, 2012.
 Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha
Raj, and Le Song. Sphereface: Deep hypersphere embedding for face recognition. In CVPR, volume 1, 2017.
 Mingsheng Long, Yue Cao, Jianmin Wang, and Michael I.
Jordan. Learning transferable features with deep adaptation networks. In ICML, pages 97–105, 2015.
 Mingsheng Long, Jianmin Wang, and Michael I Jordan.
Deep transfer learning with joint adaptation networks. arXiv preprint arXiv:1605.06636, 2016.
 Vahid Mirjalili, Sebastian Raschka, Anoop Namboodiri, and Arun Ross. Semi-adversarial networks: Convolutional autoencoders for imparting privacy to face images. In 2018
International Conference on Biometrics (ICB), pages 82–89.
IEEE, 2018.
 Vahid Mirjalili, Sebastian Raschka, and Arun Ross. Gender privacy: An ensemble of semi adversarial networks for confounding arbitrary gender classifiers. arXiv preprint arXiv:1807.11936, 2018.
 Asem Othman and Arun Ross. Privacy of facial soft biometrics: Suppressing gender but retaining identity. In European
Conference on Computer Vision, pages 682–696. Springer, Sinno Jialin Pan, Xiaochuan Ni, Jian-Tao Sun, Qiang Yang, and Zheng Chen. Cross-domain sentiment classification via spectral feature alignment. In Proceedings of the 19th international conference on World wide web, pages 751–760.
ACM, 2010.
 Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, et al.
Deep face recognition. In BMVC, volume 1, page 6, 2015.
 P Jonathon Phillips.
A cross benchmark assessment of a deep convolutional neural network for face recognition. In
Automatic Face & Gesture Recognition (FG 2017), 2017 12th IEEE International Conference on, pages 705–
710. IEEE, 2017.
 P. Jonathon Phillips, J. Ross Beveridge, Bruce A. Draper, Geof Givens, Alice J. O'Toole, David Bolme, Joseph Dunlop, Yui Man Lui, Hassan Sahibzada, and Samuel Weimer. The good, the bad, and the ugly face challenge problem. Image
& Vision Computing, 30(3):177–185, 2012.
 P Jonathon Phillips, Patrick Grother, Ross Micheals, Duane M Blackburn, Elham Tabassi, and Mike Bone.
Face recognition vendor test 2002. In Analysis and Modeling of Faces and Gestures, 2003. AMFG 2003. IEEE International
Workshop on, page 44. IEEE, 2003.
 P Jonathon Phillips, Fang Jiang, Abhijit Narvekar, Julianne
Ayyad, and Alice J O'Toole. An other-race effect for face recognition algorithms. ACM Transactions on Applied Perception (TAP), 8(2):14, 2011.
 Kuniaki Saito, Yoshitaka Ushiku, and Tatsuya Harada.
Asymmetric tri-training for unsupervised domain adaptation. arXiv preprint arXiv:1702.08400, 2017.
 Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Advances in neural information processing systems, pages 2234–2242, 2016.
 Swami Sankaranarayanan, Azadeh Alavi, and Rama Chellappa.
Triplet similarity embedding for face verification. arXiv preprint arXiv:1602.03418, 2016.
 Florian Schroff, Dmitry Kalenichenko, and James Philbin.
Facenet: A unified embedding for face recognition and clustering. In CVPR, pages 815–823, 2015.
 Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.
 Maneet Singh, Shruti Nagpal, Mayank Vatsa, Richa Singh, and Afzel Noore.
Supervised cosmos autoencoder:
Learning beyond the Euclidean loss. arXiv preprint arXiv:1810.06221, 2018.
 Kihyuk Sohn, Sifei Liu, Guangyu Zhong, Xiang Yu, MingHsuan Yang, and Manmohan Chandraker. Unsupervised domain adaptation for face recognition in unlabeled videos. arXiv preprint arXiv:1708.02191, 2017.
 Yi Sun, Yuheng Chen, Xiaogang Wang, and Xiaoou Tang.
Deep learning face representation by joint identificationverification. In NIPS, pages 1988–1996, 2014.
 Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent
Vanhoucke, Andrew Rabinovich, et al. Going deeper with convolutions. CVPR, 2015.
 Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In CVPR, volume 1, page 4, 2017.
 Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, and Trevor Darrell.
Deep domain confusion: Maximizing for domain invariance. Computer Science, 2014.
 Dayong Wang, Charles Otto, and Anil K Jain. Face search at scale: 80 million gallery. arXiv preprint arXiv:1507.07242, Mei Wang and Weihong Deng. Deep face recognition: A survey. arXiv preprint arXiv:1804.06655, 2018.
 Mei Wang and Weihong Deng. Deep visual domain adaptation: A survey. Neurocomputing, 312:135 – 153, 2018.
 Yandong Wen, Kaipeng Zhang, Zhifeng Li, and Yu Qiao. A discriminative feature learning approach for deep face recognition. In ECCV, pages 499–515. Springer, 2016.
 Shaoan Xie, Zibin Zheng, Liang Chen, and Chuan Chen.
Learning semantic representations for unsupervised domain adaptation. In International Conference on Machine Learning, pages 5419–5428, 2018.
 Dong Yi, Zhen Lei, Shengcai Liao, and Stan Z Li. Learning face representation from scratch. arXiv preprint arXiv:1411.7923, 2014.
 Shi Yuan and Sha Fei. Information-theoretical learning of discriminative clusters for unsupervised domain adaptation.
In ICML, pages 1275–1282, 2012.
 Weichen Zhang, Wanli Ouyang, Wen Li, and Dong Xu.
Collaborative and adversarial network for unsupervised domain adaptation.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3801–
 Weihong Deng Haifeng Shen Zimeng Luo, Jiani Hu. Deep unsupervised domain adaptation for face recognition. In FG, pages 453–457. IEEE, 2018.Domain Adaptation for Visual Applications: A Comprehensive
Survey
Gabriela Csurka
Abstract The aim of this paper1 is to give an overview of domain adaptation and transfer learning with a specific view on visual applications. After a general motivation, we first position domain adaptation in the larger transfer learning problem. Second, we try to address and analyze briefly the state-of-the-art methods for different types of scenarios, first describing the historical shallow methods, addressing both the homogeneous and the heterogeneous domain adaptation methods. Third, we discuss the effect of the success of deep convolutional architectures which led to new type of domain adaptation methods that integrate the adaptation within the deep architecture. Fourth, we overview the methods that go beyond image categorization, such as object detection or image segmentation, video analyses or learning visual attributes. Finally, we conclude the paper with a section where we relate domain adaptation to other machine learning solutions.
1 Introduction
While huge volumes of unlabeled data are generated and made available in many domains, the cost of acquiring data labels remains high. To overcome the burden of annotation, alternative solutions have been proposed in the literature in order to exploit the unlabeled data (referred to as semi-supervised learning), or data and/or models available in similar domains (referred to as transfer learning). Domain Adaptation(DA) is a particular case of transfer learning (TL) that leverages labeled data in one or more related source domains, to learn a classifier for unseen or unlabeled data in a target domain. In general it is assumed that the task is the same, i.e. class labels are shared between domains. The source domains are assumed to be related to the target domain, but not identical, in which case, it becomes a standard machine learning (ML) problem where we assume that the test data is drawn from the same distribution as the training data. When this assumption is not verified, i.e. the distributions of training and test sets do not match, the performance at test time can be significantly degraded.
Xerox Research Center Europe (www.xrce.xerox.com), 6 chemin Maupertuis, 38240 Meylan, France, e-mail:
Gabriela.Csurka@xrce.xerox.com
1 Book chapter to appear in "Domain Adaptation in Computer Vision Applications", Springer Series: Advances in Computer
Vision and Pattern Recognition, Edited by Gabriela Csurka.
1 arXiv:1702.05374v2 [cs.CV] 30 Mar 2017
Gabriela Csurka
Fig. 1 Example scenarios with domain adaptation needs.
In visual applications, such distribution difference, called domain shift, are common in real-life applications. They can be consequences of changing conditions, i.e. background, location, pose changes, but the domain mismatch might be more severe when, for example, the source and target domains contain images of different types, such as photos, NIR images, paintings or sketches. Service provider companies are especially concerned since, for the same service (task), the distribution of the data may vary a lot from one customer to another. In general, machine learning components of service solutions that are re-deployed from a given customer or location to a new customer or location require specific customization to accommodate the new conditions. For example, in brand sentiment management it is critical to tune the models to the way users talk about their experience given the different products. In surveillance and urban traffic understanding, pretrained models on previous locations might need adjustment to the new environment. All these entail either acquisition of annotated data in the new field or the calibration of the pretrained models to achieve the contractual performance in the new situation. However, the former solution, i.e. data labeling, is expensive and time consuming due to the significant amount of human effort involved. Therefore, the second option is preferred when possible. This can be achieved either by adapting the pretrained models taking advantage of the unlabeled (and if available labeled) target set or, to build the target model, by exploiting both previously acquired labeled source data and the new unlabeled target data together.
Numerous approaches have been proposed in the last years to address adaptation needs that arise in different application scenarios (see a few examples in Figure 1). Examples include DA and TL solutions for named entity recognition and opinion extraction across different text corpora, multilingual text
Domain Adaptation for Visual Applications: A Comprehensive Survey
Fig. 2 An overview of different transfer learning approaches. (Image: Courtesy to S.J. Pan.) classification, sentiment analysis, WiFi-based localization, speech recognition across different speakers, object recognition in images acquired in different conditions, video concept detection, video event recognition, activity recognition, human motion parsing from videos, face recognition, facial landmark localization, facial action unit detection, 3D pose estimation, document categorization across different customer datasets, etc.
In this paper, we mainly focus on domain adaptation methods applied to visual tasks. For a broader review of the transfer learning literature as well as for approaches specifically designed to solve non-visual tasks, e.g. text or speech, please refer to.
The rest of the paper is organized as follows. In Section 2 we define more formally transfer learning and domain adaptation. In Section 3 we review shallow DA methods that can be applied on visual features extracted from the images, both in the homogeneous and heterogeneous case. Section 4 addresses more recent deep DA methods and Section 5 describes DA solutions proposed for computer vision applications beyond image classification. In Section 6 we relate DA to other transfer learning and standard machine learning approaches and in Section 7 we conclude the paper.
Gabriela Csurka
2 Transfer learning and domain adaptation
In this section, we follow the definitions and notation of. Accordingly, a domain D is composed of a d-dimensional feature space X ⊂ Rd with a marginal probability distribution P(X), and a task T defined by a label space Y and the conditional probability distribution P(Y|X), where X and Y are random variables.
Given a particular sample set X = {x1,... xn} of X, with corresponding labels Y = {y1,... yn} from Y, P(Y|X) can in general be learned in a supervised manner from these feature-label pairs {xi, yi}.
Let us assume that we have two domains with their related tasks: a source domain Ds = {X s, P(Xs)} with T s = {Ys, P(Ys|Xs)} and a target domain Dt = {X t, P(Xt)} with T t = {Yt, P(Yt|Xt)}. If the two domains corresponds, i.e. Ds = Dt and T s = T t, traditional ML methods can be used to solve the problem, where Ds becomes the training set and Dt the test set.
When this assumption does not hold, i.e. Dt ̸= Ds or T t ̸= T s, the models trained on Ds might perform poorly on Dt, or they are not applicable directly if T t ̸= T s. When the source domain is somewhat related to the target, it is possible to exploit the related information from {Ds, T s} to learn P(Yt|Xt). This process is known as transfer learning (TL).
We distinguish between homogeneous TL, where the source and target are represented in the same the feature space, X t = X s, with P(Xt) ̸= P(Xs) due to domain shift, and heterogeneous TL where the source and target data can have different representations, X t ̸= X s (or they can even be of different modalities such as image vs. text).
Based on these definitions, categorizes the TL approaches into three main groups depending on the different situations concerning source and target domains and the corresponding tasks. These are the inductive TL, transductive TL and unsupervised TL (see Figure 2). The inductive TL is the case where the target task is different but related to the source task, no matter whether the source and target domains are the same or not. It requires at least some labeled target instances to induce a predictive model for the target data. In the case of the transductive TL, the source and target tasks are the same, and either the source and target data representations are different (X t ̸= X s) or the source and target distributions are different due to selection bias or distribution mismatch. Finally, the unsupervised TL refers to the case where both the domains and the tasks are different but somewhat related. In general, labels are not available neither for the source nor for the target, and the focus is on exploiting the (unlabeled) information in the source domain to solve unsupervised learning task in the target domain. These tasks include clustering, dimensionality reduction and density estimation.
According to this classification, DA methods are transductive TL solutions, where it is assumed that the tasks are the same, i.e. T t = T s. In general they refer to a categorization task, where both the set of labels and the conditional distributions are assumed to be shared between the two domains, i.e. Ys = Yt and P(Y|Xt) = P(Y|Xs). However, the second assumption is rather strong and does not always hold in reallife applications. Therefore, the definition of domain adaptation is relaxed to the case where only the first assumption is required, i.e. Ys = Yt = Y.
In the DA community, we further distinguish between the unsupervised2 (US) case where the labels are available only for the source domain and the semi-supervised (SS) case where a small set of target examples are labeled.
2 Note also that the unsupervised DA is not related to the unsupervised TL, for which no source labels are available and in general the task to be solved is unsupervised.
Domain Adaptation for Visual Applications: A Comprehensive Survey
Fig. 3 Illustration of the effect of instance re-weighting samples on the source classifier. (Image: Courtesy to M. Long.)
3 Shallow domain adaptation methods
In this section, we review shallow DA methods that can be applied on vectorial visual features extracted from images. First, in Section 3.1 we survey homogeneous DA methods, where the feature representation for the source and target domains is the same, X t = X s with P(Xt) ̸= P(Xs), and the tasks shared, Ys = Yt. Then, in Section 3.2 we discuss methods that can exploit efficiently several source domains.
Finally in Section 3.3 we discuss the heterogeneous case, where the source and target data have different representations.
3.1 Homogeneous domain adaptation methods
Instance re-weighting methods. The DA case when we assume that the conditional distributions are shared between the two domains, i.e. P(Y|Xs) = P(Y|Xt), is often referred to as dataset bias or covariate shift. In this case, one could simply apply the model learned on the source to estimate P(Y|Xt). However, as
P(Xs) ̸= P(Xt), the source model might yield a poor performance when applied on the target set despite of the underlying P(Y|Xs) = P(Y|Xt) assumption. The most popular early solutions proposed to overcome this to happen are based on instance re-weighting (see Figure 3 for an illustration).
To compute the weight of an instance, early methods proposed to estimate the ratio between the likelihoods of being a source or target example. This can be done either by estimating the likelihoods independently using a domain classifier or by approximating directly the ratio between the densities with a Kullback-Leibler Importance Estimation Procedure. However, one of the most popular measure used to weight data instances, used for example in, is the Maximum Mean Discrepancy (MMD)
 computed between the data distributions in the two domains.
The method proposed in infers re-sampling weights through maximum entropy density estimation.
 improves predictive inference under covariate shift by weighting the log-likelihood function. The Importance Weighted Twin Gaussian Processes directly learns the importance weight function, without going through density estimation, by using the relative unconstrained least-squares importance fitting. The Selective Transfer Machine jointly optimizes the weights as well as the classifier's parameters to preserve the discriminative power of the new decision boundary.
Gabriela Csurka
Fig. 4 Illustration of the TrAdaBoost method where the idea is to decrease the importance of the misclassified source examples while focusing, as in AdaBoost, on the misclassified target examples. (Image: Courtesy to S. J. Pan).
The Transfer Adaptive Boosting (TrAdaBoost), is an extension to AdaBoost3, that iteratively re-weights both source and target examples during the learning of a target classifier. This is done by increasing the weights of miss-classified target instances as in the traditional AdaBoost, but decreasing the weights of miss-classified source samples in order to diminish their importance during the training process (see Figure 4). The TrAdaBoost was further extended by integrating dynamic updates in.
Parameter adaptation methods. Another set of early DA methods, but which does not necessarily assume
P(Y|Xs) = P(Y|Xt), investigates different options to adapt the classifier trained on the source domain, e.g. an SVM, in order to perform better on the target domain4. Note that these methods in general require at least a small set of labeled target examples per class, hence they can only be applied in the semi-supervised
DA scenario. As such, the Transductive SVM that aims at decreasing the generalization error of the classification, by incorporating knowledge about the target data into the SVM optimization process. The Adaptive SVM (A-SVM) progressively adjusts the decision boundaries of the source classifiers with the help of a set of so called perturbation functions built by exploiting predictions on the available labeled target examples (see Figure 5). The Domain Transfer SVM simultaneously reduces the mismatch in the distributions (MMD) between two domains and learns a target decision function. The Adaptive Multiple
Kernel Learning (A-MKL) generalizes this by learning an adapted classifier based on multiple base
3 Code at https://github.com/BoChen90/machine-learning-matlab/blob/master/TrAdaBoost.m
4 The code for several methods, such as A-SVM, A-MKL, DT-MKL can be downloaded from http://www.codeforge. com/article/248440
Domain Adaptation for Visual Applications: A Comprehensive Survey
Fig. 5 Illustration of the Adaptive SVM, where a set of so called perturbation functions ∆f are added to the source classifier fs to progressively adjusts the decision boundaries of fs for the target domain. (Courtesy to D. Xu). kernels and the pre-trained average classifier. The model minimizes jointly the structural risk functional and the mismatch between the data distributions (MMD) of the two domains.
The domain adaptation SVM (DASVM) exploits within the semi-supervised DA scenario both the transductive SVM and its extension, the progressive transductive SVM. The cross-domain SVM, proposed in, constrains the impact of source data to the k-nearest neighbors (similarly to the spirit of the Localized SVM ). This is done by down-weighting support vectors from the source data that are far from the target samples.
Feature augmentation. One of the simplest method for DA was proposed in, where the original representation x is augmented with itself and a vector of the same size filled with zeros as follows: the source features become
� xs xs
� and target features
� xt
0 xt
�. Then an SVM is trained on these augmented features to figure out which parts of the representation is shared between the domains and which are the domain specific ones.
The idea of feature augmentation is also behind the Geodesic Flow Sampling (GFS) and the Geodesic Flow Kernel (GFK), where the domains are embedded in d-dimensional linear subspaces that can be seen as points on the Grassman manifold corresponding to the collection of all d-dimensional subspaces. In the case of GFS, following the geodesic path between the source and target domains, representations, corresponding to intermediate domains, are sampled gradually and concatenated (see illustration in Figure 6). Instead of sampling, GFK5, extends GFS to the infinite case, proposing a kernel that makes the solution equivalent to integrating over all common subspaces lying on the geodesic path.
A more generic framework, proposed in, accommodates domain representations in high-dimensional
Reproducing Kernel Hilbert Space (RKHS) using kernel methods and low-dimensional manifold representations corresponding to Laplacian Eigenmaps. The approach described in was inspired by the manifoldbased incremental learning framework in. It generates a set of intermediate dictionaries which smoothly connect the source and target domains. This is done by decomposing the target data with the current intermediate domain dictionary updated with a reconstruction residue estimated on the target. Concatenating these
5 Code available at http://www-scf.usc.edu/˜boqinggo/domain_adaptation/GFK_v1.zip
Gabriela Csurka
Fig. 6 The GFS samples between source S1 and target S2 on the geodesic path intermediate domains S1,i that can be seen as cross-domain data representations. (Courtesy to R. Gopalan ).) intermediate representations enables learning a better cross domain classifier.
These methods exploit intermediate cross-domain representations that are built without the use of class labels. Hence, they can be applied in both, the US and SS, scenarios. These cross-domain representations are then used either to train a discriminative classifier using the available labeled set (only from the source or from both domains), or to label the target instances using nearest neighbor search in the kernel space.
Feature space alignment. Instead of of augmenting the features, other methods tries to align the source features with the target ones. As such, the Subspace Alignment (SA) learns an alignment between the source subspace obtained by PCA and the target PCA subspace, where the PCA dimensions are selected by minimizing the Bregman divergence between the subspaces. It advantage is its simplicity, as shown in Algorithm 1. Similarly, the linear Correlation Alignment (CORAL) can be written in few lines of MATLAB code as illustrated in Algorithm 2. The method minimizes the domain shift by using the second-order statistics of the source and target distributions. The main idea is a whitening of the source data using its covariance followed by a "re-coloring" using the target covariance matrix.
As an alternative to feature alignment, a large set of feature transformation methods were proposed with the objective to find a projection of the data into a latent space such that the discrepancy between the source and target distributions is decreased. Note that the projections can be shared between the domains or they can be domain specific projections. In the latter case we talk about asymmetric feature transformation. Furthermore, when the transformation learning procedure uses no class labels, the method is called unsupervised feature transformation and when the transformation is learned by exploiting class labels (only from the source or also from the target when available) it is referred to as supervised feature transformation.
Unsupervised feature transformation. One of the first such DA method is the Transfer Component Analysis (TCA) that proposes to discover common latent features having the same marginal distribution across the source and target domains, while maintaining the intrinsic structure (local geometry of the data manifold) of the original domain by a smoothness term.
Domain Adaptation for Visual Applications: A Comprehensive Survey
Algorithm 1: Subspace Alignment (SA) 
Input: Source data Xs, target data Xt, subspace dimension d
1: Ps ← PCA(Xs, d), Pt ← PCA(Xt, d) ;
2: Xs a = XsPsP⊤ s Pt, Xt a = XtPt ;
Output: Aligned source, Xs a and target, Xt a data.
Algorithm 2: Correlation Alignment (CORAL) 
Input: Source data Xs, target data Xt
1: Cs = cov(Xs) + eye(size(Xs, 2)), Ct = cov(Xt) + eye(size(Xt, 2))
2: Xs w = Xs ∗ C−1/2 s(whitening), Xs a = Xs w ∗ C−1/2 t(re-coloring)
Output: Source data Xs a adjusted to the target.
Instead of restricting the discrepancy to a simple distance between the sample means in the lowerdimensional space, Baktashmotlagh et al. propose the Domain Invariant Projection6 (DIP) approach that compares directly the distributions in the RKHS while constraining the transformation to be orthogonal.
They go a step further in and based on the fact that probability distributions lie on a Riemannian manifold, propose the Statistically Invariant Embedding7 (SIE) that uses the Hellinger distance on this manifold to compare kernel density estimates between of the source and target data. Both the DIP and SIE, involve non-linear optimizations and are solved with the conjugate gradient algorithm.
The Transfer Sparse Coding8 (TSC) learns robust sparse representations for classifying cross-domain data accurately. To bring the domains closer, the distances between the sample means for each dimensions of the source and the target is incorporated into the objective function to be minimized. The Transfer Joint
Matching9 (TJM) learns a non-linear transformation between the two domains by minimizing the distance between the empirical expectations of source and target data distributions integrated within a kernel embedding. In addition, to put less emphasis on the source instances that are irrelevant to classify the target data, instance re-weighing is employed.
The feature transformation proposed by in exploits the correlation between the source and target set to learn a robust representation by reconstructing the original features from their noised counterparts. The method, called Marginalized Denoising Autoencoder (MDA), is based on a quadratic loss and a drop-out noise level that factorizes over all feature dimensions. This allows the method to avoid explicit data corruption by marginalizing out the noise and to have a closed-form solution for the feature transformation. Note that it is straightforward to stack together several layers with optional non-linearities between layers to obtain a multi-layer network with the parameters for each layer obtained in a single forward pass (see Algorithm 3).
In general, the above mentioned methods learn the transformation without using any class label. After projecting the data in the new space, any classifier trained on the source set can be used to predict labels for the target data. The model often works even better if in addition a small set of the target examples are handlabeled (SS adaptation). The class labels can also be used to learn a better transformation. Such methods, 6 Code at https://drive.google.com/uc?export=download&id=0B9_PW9TCpxT0c292bWlRaWtXRHc
7 Code at https://drive.google.com/uc?export=download&id=0B9_PW9TCpxT0SEdMQ1pCNzdZekU
8 Code at http://ise.thss.tsinghua.edu.cn/˜mlong/doc/transfer-sparse-coding-cvpr13.zip
9 Code at http://ise.thss.tsinghua.edu.cn/˜mlong/doc/transfer-joint-matching-cvpr14.zip
Gabriela Csurka
Algorithm 3: Stacked Marginalized Denoising Autoencoder (sMDA).
Input: Source data Xs, target data Xt
Input: Parameters: p (noise level), ω (regularizer) and k (number of stacked layers)
1: X = [Xs, Xt], S = X⊤X, and X0 = X;
2: P = (1 − p)S and Q = (1 − p)2S + p(1 − p)diag(S)
3: W = (Q + ωID)−1P.
4: (Optionally), stack K layers with X(k) = tanh(X(k−1)W(k)).
Output: Denoised features Xk. called supervised feature transformation based DA methods, to learn the transformation exploit class labels, either only from the source or also from the target (when available). When only the source class labels are exploited, the method can still be applied to the US scenario, while methods using also target labels are designed for the SS case.
Supervised feature transformation. Several unsupervised feature transformation methods, cited above, have been extended to capitalize on class labels to learn a better transformation. Among these extensions, we can mention the Semi-Supervised TCA where the objective function that is minimized contains a label dependency term in addition to the distance between the domains and the manifold regularization term. The label dependency term has the role of maximizing the alignment of the projections with the source labels and, when available, target labels.
Similarly, in a quadratic regularization term, relying on the pretrained source classifier, is added into the MDA framework, in order to keep the denoised source data well classified. Moreover, the domain denoising and cross-domain classifier can be learned jointly by iteratively solving a Sylvester linear system to estimate the transformation and a linear system to get the classifier in closed form10.
To take advantage of class labels, the distance between each source sample and its corresponding class means is added as regularizer into the DIP respectively SIE model. This term encourages the source samples from the same class to be clustered in the latent space. The Adaptation Regularization based Transfer
Learning11 performs DA by optimizing simultaneously the structural risk functional, the joint distribution matching between domains and the manifold consistency. The Max-Margin Domain Transform12 optimizes both the transformation and classifier parameters jointly, by introducing an efficient cost function based on the misclassification loss.
Another set of methods extend marginal distribution discrepancy minimization to conditional distribution involving data labels from the source and class predictions from the target. Thus, proposes an adaptive kernel approach that maps the marginal distribution of the target and source sets into a common kernel space, and use a sample selection strategy to draw conditional probabilities between the two domains closer.
The Joint Distribution Adaptation13 jointly adapts the marginal distribution through a principled (PCA based) dimensionality reduction procedure and the conditional distribution between the domains.
10 Code at https://github.com/sclincha/xrce_msda_da_regularization
11 Code at http://ise.thss.tsinghua.edu.cn/˜mlong/doc/adaptation-regularization-tkde14. zip
12 Code at https://cs.stanford.edu/˜jhoffman/code/Hoffman_ICLR13_MMDT_v3.zip
13 Code at http://ise.thss.tsinghua.edu.cn/˜mlong/doc/joint-distribution-adaptation-iccv13. zip
Domain Adaptation for Visual Applications: A Comprehensive Survey
Fig. 7 The NBNN-DA adjusts the image-to-class distances by tuning the per class metrics and iteratively making the metric progressively more suitable for the target. (Image: Courtesy to T. Tommasi )
Metric learning based feature transformation. These methods are particular supervised feature transformation methods that involves that at least a limited set of target labels are available, and they use metric learning techniques to bridge the relatedness between the source and target domains. Thus, proposes distance metric learning with either log-determinant or manifold regularization to adapt face recognition models between subjects. uses the Information-Theoretic Metric Learning from to define a common distance metric across different domains. This method was further extended in by incorporating non-linear kernels, which enable the model to be applicable to the heterogeneous case (i.e. different source and target representations).
The metric learning for Domain Specific Class Means (DSCM) learns a transformation of the feature space which, for each instance minimizes the weighted soft-max distances to the corresponding domain specific class means. This allows in the projected space to decrease the intraclass and to increase the interclass distances (see also Figure 10). This was extended with an active learning component by the Self-adaptive
Metric Learning Domain Adaptation (SaML-DA) framework, where the target training set is iteratively increased with labels predicted with DSCM and used to refine the current metric. SaML-DA was inspired by the Naive Bayes Nearest Neighbor based Domain Adaptation14 (NBNN-DA) framework, which combines metric learning and NBNN classifier to adjust the instance-to-class distances by progressively making the metric more suitable for the target domain (see Figure 7). The main idea behind both methods, SaMLDA and NBNN-DA, is to replace at each iteration the most ambiguous source example of each class by the target example for which the classifier (DSCM respectively NNBA) is the most confident for the given class.
Local feature transformation. The previous methods learn a global transformation to be applied to each source and target example. In contrast, the Adaptive Transductive Transfer Machines (ATTM) complements the global transformation with a sample-based transformation to refine the probability density function of the source instances assuming that the transformation from the source to the target domain is locally lin14 Code at http://www.tatianatommasi.com/2013/DANBNNdemo.tar.gz
Gabriela Csurka
Fig. 8 The OTDA consider a local transportation plan for each sample in the source domain to transport the training samples close to the target examples. (Image: Courtesy to N. Courty.) ear. This is achieved by representing the target set by a Gaussian Mixture Model and learning an optimal translation parameter that maximizes the likelihood of the translated source as a posterior.
Similarly, the Optimal Transport for Domain Adaptation, considers a local transportation plan for each source example. The model can be seen as a graph matching problem, where the final coordinates of each sample are found by mapping the source samples to the target ones, whilst respecting the marginal distribution of the target domain (see Figure 8). To exploit class labels, a regularization term with group-lasso is added inducing, on one hand, group sparsity and, on another hand, constraining source samples of the same class to remain close during the transport.
Landmark selection. In order to improve the feature learning process, several methods have been proposed with the aim of selecting the most relevant instances from the source, so-called landmark examples, to be used to train the adaptation model (see examples in Figure 9). Thus, proposes to minimize a variant of the MMD to identify good landmarks by creating a set of auxiliary tasks that offer multiple views of the original problem15. The Statistically Invariant Sample Selection, uses the Hellinger distance on the statistical manifold instead of MMD. The selection is forced to keep the proportions of the source samples per class the same as in the original data. Contrariwise to these approaches, the Multi-scale Landmark Selection16 does not require any class labels. It takes each instance independently and considers it as being a good candidate if the Gaussian distributions of the source examples and of the target points centered on the instance are similar over a set of different scales (Gaussian variances).
Note that the landmark selection process, although strongly related to instance re-weighting methods with binary weights, can be rather seen as data preprocessing and hence complementary to the adaptation process.
15 Code at http://www-scf.usc.edu/˜boqinggo/domain_adaptation/landmark_v1.zip
16 Code at http://home.heeere.com/data/cvpr-2015/LSSA.zip
Domain Adaptation for Visual Applications: A Comprehensive Survey
Fig. 9 Landmarks selected for the task amazon versus webcam using the popular Office31 dataset with (a) MMD and (b) the Hellinger distance on the statistical manifold.
3.2 Multi-source domain adaptation
Most of the above mentioned methods were designed for a single source vs. target case. When multiple sources are available, they can be concatenated to form a single source set, but because the possible shift between the different source domains, this might not be always a good option. Alternatively, the models built for each source-target pair (or their results) can be combined to make a final decision. However, a better option might be to build multi-source DA models which, relying only on the a priori known domain labels, are able to exploit the specificity of each source domain.
Such methods are the Feature Augmentation (FA) and the A-SVM, already mentioned in Section 3.1, both exploiting naturally the multi-source aspect of the dataset. Indeed in the case of FA, extra feature sets, one for each source domain, concatenated to the representations, allow to learn source specific properties shared between a given source and the target. The A-SVM uses an ensemble of source specific auxiliary classifiers to adjust the parameters of the target classifier.
Similarly, the Domain Adaptation Machine l leverages a set of source classifiers by the integration of domain-dependent regularizer term which is based on a smoothness assumption. The model forces the target classifier to share similar decision values with the relevant source classifiers on the unlabeled target instances. The Conditional Probability based Multi-source Domain Adaptation (CP-MDA) approach extends the above idea by adding weight values for each source classifier based on conditional distributions.
The DSCM proposed in relies on domain specific class means both to learn the metric but also to predict the target class labels (see illustration in Figure 10). The domain regularization and classifier based regularization terms of the extended MDA are both sums of source specific components.
The Robust DA via Low-Rank Reconstruction (RDALRR) transforms each source domain into an intermediate representation such that the transformed samples can be linearly reconstructed from the target ones. Within each source domain, the intrinsic relatedness of the reconstructed samples is imposed by using a low-rank structure where the outliers are identified using sparsity constraints. By enforcing different source domains to have jointly low ranks, a compact source sample set is formed with a distribution close to the target domain (see Figure 11).
Gabriela Csurka
Fig. 10 Metric learning for the DSCM classifier, where µs ci and µs′ ci represent source specific class means and µt ci class means in the target domain. The feature transformation W is learned by minimizing for each sample the weighted soft-max distances to the corresponding domain specific class means in the projected space.
To better take advantage of having multiple source domains, extensions to methods previously designed for a single source vs. target case were proposed in. Thus, describes a multi-source version of the GFS, which was further extended in to the Subspaces by Sampling Spline Flow approach. The latter uses smooth polynomial functions determined by splines on the manifold to interpolate between different source and the target domain. combines17 constrained clustering algorithm, used to identify automatically source domains in a large data set, with a multi-source extension of the Asymmetric
Kernel Transform. efficiently extends the TrAdaBoost to multiple source domains.
Source domain weighting. When multiple sources are available, it is desired to select those domains that provide the best information transfer and to remove the ones that have more likely negatively impact on the final model. Thus, to down-weight the effect of less related source domains, in first the available labels are propagated within clusters obtained by spectral clustering and then to each source cluster a Supervised
Local Weight (SLW) is assigned based on the percentage of label matches between predictions made by a source model and those made by label propagation.
In the Locally Weighted Ensemble framework, the model weights are computed as a similarity between the local neighborhood graphs centered on source and target instances. The CP-MDA, mentioned above, uses a weighted combination of source learners, where the weights are estimated as a function of conditional probability differences between the source and target domains. The Rank of Domain value defined in measures the relatedness between each source and target domain as the KL divergences between data distributions once the data is projected into the latent subspace. The Multi-Model Knowledge Transfer minimizes the negative transfer by giving higher weights to the most related linear SVM source classifiers.
These weights are determined through a leave one out learning process.
17 Code at https://cs.stanford.edu/˜jhoffman/code/hoffman_latent_domains_release_v2.zip
Domain Adaptation for Visual Applications: A Comprehensive Survey
Fig. 11 The RDALRR transforms each source domain into an intermediate representation such that the transformed samples can be linearly reconstructed from the target samples. (Image: Courtesy to I.H. Jhuo.)
3.3 Heterogeneous domain adaptation
Heterogeneous transfer learning (HTL) refers to the setting where the representation spaces are different for the source and target domains (X t ̸= X s as defined in Section 2). As a particular case, when the tasks are assumed to be the same, i.e. Ys = Yt, we refer to it as heterogeneous domain adaptation (HDA).
Both HDA and HTL are strongly related to multi-view learning, where the presence of multiple information sources gives an opportunity to learn better representations (features) by analyzing the views simultaneously. This makes possible to solve the task when not all the views are available. Such situations appear when processing simultaneously audio and video, documents containing both image and text(e.g. web pages or photos with tags or comments), images acquired with depth information, etc. We can also have multi-view settings when the views have the same modalities (textual, visual, audio), such as in the case of parallel text corpora in different languages, photos of the same person taken across different poses, illuminations and expressions.
Multi-view learning assumes that at training time for the same data instance multiple views from complementary information sources are available (e.g. a person is identified by photograph, fingerprint, signature or iris). Instead, in the case of HTL and HDA, the challenge comes from the fact that we have one view at training and another one at test time. Therefore, one set of methods proposed to solve HDA relies on some multi-view auxiliary data18 to bridge the gap between the domains (see Figure 12).
Methods relying on auxiliary domains. These methods principally exploit feature co-occurrences (e.g. between words and visual features) in the multi-view auxiliary domain. As such, the Transitive Transfer
Learning selects an appropriate domain from a large data set guided by domain complexity and, the distribution differences between the original domains (source and target) and the selected one (auxiliary).
18 When the bridge is to be done between visual and textual representations, a common practice is to crawl the Web for pages containing both text and images in order to build such intermediate multi-view data.
Gabriela Csurka
Fig. 12 Heterogeneous DA through an intermediate domain allowing to bridge the gap between features representing the two domains. For example, when the source domain contains text and the target images, the intermediate domain can be built from a set of crawled Web pages containing both text and images. (Image courtesy B. Tan ).
Then, using Non-negative Matrix Tri-factorization, feature clustering and label propagation is performed simultaneously through the intermediate domain.
The Mixed-Transfer approach builds a joint transition probability graph of mixed instances and features, considering the data in the source, target and intermediate domains. The label propagation on the graph is done by a random walk process to overcome the data sparsity. In the representations of the target images are enriched with semantic concepts extracted from the intermediate data19 through a Collective Matrix Factorization.
 proposes to build a translator function20 between the source and target domain by learning directly the product of the two transformation matrices that map each domain into a common (hypothetical) latent topic built on the co-occurrence data. Following the principle of parsimony, they encode as few topics as possible in order to be able to match text and images. The semantic labels are propagated from the labeled text corpus to unlabeled new images by a cross-domain label propagation mechanism using the built translator. In the co-occurrence data is represented by the principal components computed in each feature space and a Markov Chain Monte Carlo is employed to construct a directed cyclic network where each node is a domain and each edge weight represents the conditional dependence between the corresponding domains defined by the transfer weights.
 studies online HDA, where offline labeled data from a source domain is transferred to enhance the online classification performance for the target domain. The main idea is to build an offline classifier based on heterogeneous similarity using labeled data from a source domain and unlabeled co-occurrence data collected from Web pages and social networks (see Figure 13). The online target classifier is combined with the offline source classifier using Hedge weighting strategy, used in Adaboost, to update their weights for ensemble prediction.
Instead of relying on external data to bridge the data representation gap, several HDA methods exploit directly the data distribution in the source and target domains willing to remove simultaneously the gap between the feature representations and minimizing the data distribution shift. This is done by learning either a 19 Code available at http://www.cse.ust.hk/%7Eyinz/htl4ic.zip
20 Code available at http://www.ifp.illinois.edu/%7Eqi4/TTI_release_v1.zip
Domain Adaptation for Visual Applications: A Comprehensive Survey
Fig. 13 Combining the online classifier with the offline classifier (right) and transfer the knowledge through co-occurrences data in the heterogeneous intermediate domain (left). (Image: Courtesy to Y. Yan ) projection for each domain into a domain-invariant common latent space, referred to as symmetric transformation based HDA21, or a transformation from the source space towards the target space, called asymmetric transformation based HDA. These approaches require at least a limited amount of labeled target examples(semi-supervised DA).
Symmetric feature transformation. The aim of symmetric transformation based HDA approaches is to learn projections for both the source and target spaces into a common latent (embedding) feature space better suited to learn the task for the target. These methods are related, on one hand, to the feature transformation based homogeneous DA methods described in Section 3.1 and, on another hand, to multi-view embedding, where different views are embedded in a common latent space. Therefore, several DA methods originally designed for the homogeneous case, have been inspired by the multi-view embedding approaches and extended to heterogeneous data.
As such, the Heterogeneous Feature Augmentation22 (HFA), prior to data augmentation, embeds the source and target into a common latent space (see Figure 15). In order to avoid the explicit projections, the transformation metrics are computed by the minimization of the structural risk functional of SVM expressed as a function of these projection matrices. The final target prediction function is computed by an alternating optimization algorithm to simultaneously solve the dual SVM and to find the optimal transformations. This model was further extended in, where each projection matrix is decomposed into a linear combination of a set of rank-one positive semi-definite matrices and they are combined within a Multiple Kernel Learning approach.
The Heterogeneous Spectral Mapping unifies different feature spaces using spectral embedding where the similarity between the domains in the latent space is maximized with the constraint to preserve the original structure of the data. Combined with a source sample selection strategy, a Bayesian-based approach is applied to model the relationship between the different output spaces.
21 These methods can be used even if the source and target data are represented in the same feature space, i.e. X t = X s.
Therefore, it is not surprising that several methods are direct extensions of homogeneous DA methods described in Section 3.1.
22 Code available at https://sites.google.com/site/xyzliwen/publications/HFA_release_0315. rar
Gabriela Csurka
Fig. 14 The SDDL proposes to learn a dictionary in a latent common subspace while maintaining the manifold structure of the data. (Image: Courtesy to S. Shekhar )
 present a semi-supervised subspace co-projection method, which addresses heterogeneous multiclass DA. It is based on discriminative subspace learning and exploit unlabeled data to enforce an MMD criterion across domains in the projected subspace. They use Error Correcting Output Codes (ECOC) to address the multi-class aspect and to enhance the discriminative informativeness of the projected subspace.
The Semi-supervised Domain Adaptation with Subspace Learning jointly explores invariant lowdimensional structures across domains to correct data distribution mismatch and leverages available unlabeled target examples to exploit the underlying intrinsic information in the target domain.
To deal with both domain shift and heterogeneous data, the Shared Domain-adapted Dictionary Learning23 (SDDL) learns a class-wise discriminative dictionary in the latent projected space (see Figure
14). This is done by jointly learning the dictionary and the projections of the data from both domains onto a common low-dimensional space, while maintaining the manifold structure of data represented by sparse linear combinations of dictionary atoms.
The Domain Adaptation Manifold Alignment (DAMA) models each domain as a manifold and creates a separate mapping function to transform the heterogeneous input space into a common latent space while preserving the underlying structure of each domain. This is done by representing each domains with a Laplacian that captures the closeness of the instances sharing the same label. The RDALRR, mentioned above (see also Figure 11), transforms each source domain into an intermediate representation such that the source samples linearly reconstructed from the target samples are enforced to be related to each other under a low-rank structure. Note that both DAMA and RDALRR are multi-source HDA approaches.
23 Code available at http://www.umiacs.umd.edu/˜pvishalm/Codes/DomainAdaptDict.zip
Domain Adaptation for Visual Applications: A Comprehensive Survey
Fig. 15 The HFA is seeking for an optimal common space while simultaneously learning a discriminative SVM classifier.(Image: Courtesy to Dong Xu.)
Asymmetric feature transformation. In contrast to symmetric transformation based HDA, these methods aim to learn a projection of the source features into the target space such that the distribution mismatch within each class is minimized. Such method is the Asymmetric Regularized Cross-domain Transformation24 that utilizes an objective function responsible for the domain invariant transformation learned in a non-linear
Gaussian RBF kernel space. The Multiple Outlook MAPping algorithm finds the transformation matrix by singular value decomposition process that encourage the marginal distributions within the classes to be aligned while maintaining the structure of the data. It requires a limited amount of labeled target data for each class to be paired with the corresponding source classes.
 proposes a sparse and class-invariant feature mapping that leverages the weight vectors of the binary classifiers learned in the source and target domains. This is done by considering the learning task as a Compressed Sensing problem and using the ECOC scheme to generate a sufficient number of binary classifiers given the set of classes.
4 Deep domain adaptation methods
With the recent progress in image categorization due to deep convolutional architectures - trained in a fully supervised fashion on large scale annotated datasets, in particular on part of ImageNet - allowed a significant improvement of the categorization accuracy over previous state-of-the art solutions. Furthermore, it was shown that features extracted from the activation layers of these deep convolutional networks can be re-purposed to novel tasks even when the new tasks differ significantly from the task originally used to train the model.
Concerning domain adaptation, baseline methods without adaptation obtained using features generated by deep models 25 on the two most popular benchmark datasets Office (OFF31) and Office+Caltech(OC10) outperform by a large margin the shallow DA methods using the SURFBOV features originally provided with these datasets. Indeed, the results obtained with such Deep Convolutional Activation
Features26 (DeCAF) even without any adaptation to the target are significantly better that the results
24 Code available at http://vision.cs.uml.edu/code/DomainTransformsECCV10_v1.tar.gz
25 Activation layers extracted from popular CNN models, such as AlexNet, VGGNET, ResNet or GoogleNet.
26 Code to extract features available at https://github.com/UCBAIR/decaf-releas
Gabriela Csurka
Fig. 16 Examples from the Cross-Modal Places Dataset (CMPlaces) dataset proposed in. (Image: Courtesy to L. Castrej´on.) obtained with any DA method based on SURFBOV. As shown also in, this suggests that deep neural networks learn more abstract and robust representations, encode category level information and remove, to a certain measure, the domain bias.
Note however that in OFF31 and OC10 datasets the images remain relatively similar to the images used to train these models (usually datasets from the ImageNet Large-Scale Visual Recognition Challenge ).
In contrast, if we consider category models between e.g. images and paintings, drawings, clip art or sketches(see see examples from the CMPlaces dataset27 in Figure 16), the models have more difficulties to handle the domain differences and alternative solutions are necessary.
Solutions proposed in the literature to exploit deep models can be grouped into three main categories. The first group considers the CNN models to extract vectorial features to be used by the shallow DA methods.
The second solution is to train or fine-tune the deep network on the source domain, adjust it to the new task, and use the model to predict class labels for target instances. Finally, the most promising methods are based on deep learning architectures designed for DA.
Shallow methods with deep features. The first, naive solution is to consider the deep network as feature extractor, where the activations of a layer or several layers of the deep architecture is considered as representation for the input image. These Deep Convolutional Activation Features (DeCAF) extracted from both source and target examples can then be used within any shallow DA method described in Section 3. For example, Feature Augmentation, Max-Margin Domain Transforms and Geodesic Flow Kernel 
27 Dataset available at http://projects.csail.mit.edu/cmplaces/
Domain Adaptation for Visual Applications: A Comprehensive Survey
Fig. 17 The DLID model aims in interpolating between domains based on the amount of source and target data used to train each model. (Image courtesy S. Chopra ). were applied to DECAF features in, Subspace Alignment and Correlation Alignment in. experiments with DeCAF features within the extended MDA framework, while explores various metric learning approaches to align deep features extracted from RGB face images (source) and NIR or sketches(target).
In general, these DA methods allow to further improve the classification accuracy compared to the baseline classifiers trained only on the source data with these DeCAF features. Note however that the gain is often relatively small and significantly lower than the gain obtained with the same methods when used with the SURFBOV features.
Fine-tuning deep CNN architectures. The second and most used solution is to fine-tune the deep network model on the new type of data and for the new task. But fine-tuning requires in general a relatively large amount of annotated data which is not available for the target domain, or it is very limited.
Therefore, the model is in general fine-tuned on the source - augmented with, when available, the few labeled target instances - which allows in a first place to adjust the deep model to the new task28, common between the source and target in the case of DA. This is fundamental if the targeted classes do not belong to the classes used to pretrain the deep model. However, if the domain difference between the source and target is important, fine-tuning the model on the source might over-fit the model for the source. In this case the performance of the fine-tuned model on the target data can be worse than just training the class prediction layer or as above, using the model as feature extractor and training a classifier29 with the corresponding
DeCAF features.
4.1 DeepDA architectures
Finally, the most promising are the deep domain adaptation (deepDA) methods that are based on deep learning architectures designed for domain adaptation. One of the first deep model used for DA is the Stacked
Denoising Autoencoders proposed to adapt sentiment classification between reviews of different products. This model aims at finding common features between the source and target collections relying on denoising autoencoders. This is done by training a multi-layer neural network to reconstruct input data from partial random corruptions with backpropagation. The Stacked Marginalized Denoising Autoencoders 
28 This is done by replacing the class prediction layer to correspond to the new set of classes.
29 Note that the two approaches are equivalent when the layer preceding the class prediction layer are extracted.
Gabriela Csurka
Fig. 18 Adversarial adaptation methods can be viewed as instantiations of the same framework with different choices regarding their properties (Image courtesy E. Tzeng).(see also in Section 3.1) is a variant of the SDA, where the random corruption is marginalized out and hence yields a unique optimal solution (feature transformation) computed in closed form between layers.
The Domain Adaptive Neural Network30 uses such denoising auto-encoder as a pretraining stage.
To ensure that the model pretrained on the source continue to adapt to the target, the MMD is embedded as a regularization in the supervised backpropagation process (added to the cross-entropy based classification loss of the labels source examples).
The Deep Learning for Domain Adaptation, inspired by the intermediate representations on the geodesic path, proposes a deep model based interpolation between domains. This is achieved by a deep nonlinear feature extractor trained in an unsupervised manner using the Predictive Sparse Decomposition on intermediate datasets, where the amount of source data is gradually replaced by target samples.
 proposes a light-weight domain adaptation method, which, by using only a few target samples, analyzes and reconstructs the output of the filters that were found affected by the domain shift. The aim of the reconstruction is to make the filter responses given a target image resemble to the response map of a source image. This is done by simultaneously selecting and reconstructing the response maps of the bad filters using a Lasso based optimization with a KL-divergence measure that guides the filter selection process.
Most DeedDA methods follow a Siamese architectures with two streams, representing the source and target models (see for example Figure 18), and are trained with a combination of a classification loss and a discrepancy loss or an adversarial loss. The classification loss depends on the labeled source data. The discrepancy loss aims to diminish the shift between the two domains while the adversarial loss tries to encourage a common feature space through an adversarial objective with respect to
30 Code available at https://github.com/ghif/mtae
Domain Adaptation for Visual Applications: A Comprehensive Survey
Fig. 19 The JAN minimizes a joint distribution discrepancy of several intermediate layers including the soft prediction one. (Image courtesy M. Long). a domain discriminator.
Discrepancy-based methods. These methods, inspired by the shallow feature space transformation approaches described in Section 3.1, uses in general a discrepancy based on MMD defined between corresponding activation layers of the two streams of the Siamese architecture. One of the first such method is the Deep Domain Confusion (DDC) where the layer to be considered for the discrepancy and its dimension is automatically selected amongst a set of fine-tuned networks based on linear MMD between the source and the target. Instead of using a single layer and linear MMD, Long et al. proposed the Deep Adaptation
Network31 (DAN) that consider the sum of MMDs defined between several layers, including the soft prediction layer too. Furthermore, DAN explore multiple kernels for adapting these deep representations, which substantially enhances adaptation effectiveness compared to a single kernel method used in and. This was further improved by the Joint Adaptation Networks, which instead of the sum of marginal distributions (MMD) defined between different layers, consider the joint distribution discrepancies of these features.
The Deep CORAL extends the shallow CORAL method described in Section 3 to deep architectures32. The main idea is to learn a nonlinear transformation that aligns correlations of activation layers between the two streams. This idea is similarly to DDC and DAN except that instead of MMD the CORAL loss33 (expressed by the distance between the covariances) is used to minimize discrepancy between the domains.
In contrast to the above methods, Rozantsev et al. consider the MMD between the weights of the source respectively target models of different layers, where an extra regularizer term ensures that the weights in the two models remains linearly related.
Adversarial discriminative models. The aim of these models is to encourage domain confusion through an adversarial objective with respect to a domain discriminator. proposes a unified view of existing adversarial DA methods by comparing them depending on the loss type, the weight sharing strategy between the two streams and, on whether they are discriminative or generative (see illustration in Figure 18).
Amongst the discriminative models we have the model proposed in using a confusion loss, the Ad31 Code available at https://github.com/thuml/transfer-caffe
32 Code available at https://github.com/VisionLearningGroup/CORAL
33 Note that this loss can be seen as minimizing the MMD with a polynomial kernel.
Gabriela Csurka
Fig. 20 The DANN architecture including a feature extractor (green) and a label predictor (blue), which together form a standard feed-forward architecture. Unsupervised DA is achieved by the gradient reversal layer that multiplies the gradient by a certain negative constant during the backpropagation-based training to ensures that the feature distributions over the two domains are made indistinguishable. (Image courtesy Y. Ganin ). versarial Discriminative Domain Adaptation that considers an inverted label GAN loss and the Domain-Adversarial Neural Network with a minimax loss. The generative methods, additionally to the discriminator, relies on a generator, which, in general, is a Generative Adversarial Network (GAN).
The domain confusion based model34 proposed in considers a domain confusion objective, under which the mapping is trained with both unlabeled and sparsely labeled target data using a cross-entropy loss function against a uniform distribution. The model simultaneously optimizes the domain invariance to facilitate domain transfer and uses a soft label distribution matching loss to transfer information between tasks.
The Domain-Adversarial Neural Networks35 (DANN), integrates a gradient reversal layer into the standard architecture to promote the emergence of features that are discriminative for the main learning task on the source domain and indiscriminate with respect to the shift between the domains (see Figure 20). This layer is left unchanged during the forward propagation and its gradient reversed during backpropagation.
The Adversarial Discriminative Domain Adaptation uses an inverted label GAN loss to split the optimization into two independent objectives, one for the generator and one for the discriminator. In contrast to the above methods, this model considers independent source and target mappings (unshared weights between the two streams) allowing domain specific feature extraction to be learned, where the target weights are initialized by the network pretrained on the source.
Adversarial generative models. These models combine the discriminative model with a generative component in general based on GANs. As such, the Coupled Generative Adversarial Networks consists of a tuple of GANs each corresponding to one of the domains. It learns a joint distribution of multi-domain images and enforces a weight sharing constraint to limit the network capacity.
34 Code available at https://github.com/erictzeng/caffe/tree/confusion
35 Code available at https://github.com/ddtm/caffe/tree/grl
Domain Adaptation for Visual Applications: A Comprehensive Survey
Fig. 21 The DSN architecture combines shared and domain specific encoders, which learns common and domain specific representation components respectively with a shared decoder that learns to reconstruct the input samples. (Image courtesy K.
Bousmalis ).
The model proposed in also exploit GANs with the aim to generate source-domain images such that they appear as if they were drawn from the target domain. Prior knowledge regarding the low-level image adaptation process, such as foreground-background segmentation mask, can be integrated in the model through content-similarity loss defined by a masked Pairwise Mean Squared Error between the unmasked pixels of the source and generated images. As the model decouples the process of domain adaptation from the task-specific architecture, it is able to generalize also to object classes unseen during the training phase.
Data reconstruction (encoder-decoder) based methods. In contrast to the above methods, the Deep Reconstruction Classification Network36 proposed in combines the standard convolutional network for source label prediction with a deconvolutional network for target data reconstruction. To jointly learn source label predictions and unsupervised target data reconstruction, the model alternates between unsupervised and supervised training. The parameters of the encoding are shared across both tasks, while the decoding parameters are separated. The data reconstruction can be viewed as an auxiliary task to support the adaptation of the label prediction.
The Domain Separation Networks (DSN) introduces the notion of a private subspace for each domain, which captures domain specific properties, such as background and low level image statistics. A shared subspace, enforced through the use of autoencoders and explicit loss functions, captures common features between the domains. The model integrates a reconstruction loss using a shared decoder, which learns to reconstruct the input sample by using both the private (domain specific) and source representations (see Figure 21).
36 Code available at https://github.com/ghif/drcn
Gabriela Csurka
Fig. 22 The DTN architecture with strongly-shared and weakly-shared parameter layers. (Image courtesy X. Shu ).
Heterogeneous deepDA. Concerning heterogeneous or multi-modal deep domain adaptation, we can mention the Transfer Neural Trees proposed to relate heterogeneous cross-domain data. It is a two stream network, one stream for each modality, where the weights in the latter stages of the network are shared.
As the prediction layer, a Transfer Neural Decision Forest (Transfer-NDF) is used that performs jointly adaptation and classification.
The weakly-shared Deep Transfer Networks for Heterogeneous-Domain Knowledge Propagation learns a domain translator function from multi-modal source data that can be used to predict class labels in the target even if only one of the modality is present. The proposed structure has the advantage to be flexible enough to represent both domain-specific features and shared features across domains (see Figure 22).
5 Beyond image classification
In the previous sections, we attempted to provide an overview of visual DA methods with emphasis on image categorization. Compared to this vast literature focused on object recognition, relatively few papers go beyond image classification and address domain adaptation related to other computer vision problems such as object detection, semantic segmentation, pose estimation, video event or action detection. One of the main reason is probably due to the fact that these problems are more complex and have often additional challenges and requirements (e.g. precision related to the localization in the case of detection, pixel level accuracy required for image segmentation, increased amount of annotation burden needed for videos, etc.)
Moreover, adapting visual representations such as contours, deformable and articulated 2-D or 3-D models, graphs, random fields or visual dynamics, is less obvious with classical vectorial DA techniques.
Therefore, when these tasks are addressed in the context of domain adaptation, the problem is generally rewritten as a classification problem with vectorial feature representations and a set of predefined class labels. In this case the main challenge becomes finding the best vectorial representation for the given the Domain Adaptation for Visual Applications: A Comprehensive Survey
Fig. 23 Virtual word examples: SYNTHIA (top), Virtual KITTI (bottom). task. When this is possible, shallow DA methods, described in the Section 3, can be applied to the problem.
Thereupon, we can find in the literature DA solutions such as Adaptive SVM, DT-SVM, A-MKL
 or Selective Transfer Machine applied to video concept detection, video event recognition, activity recognition, facial action unit detection, and 3D Pose Estimation.
When rewriting the problem into classification of vectorial representation is less obvious, as in the case of image segmentation, where the output is a structured output, or detection where the output is a set of bounding boxes, most often the target training set is simply augmented with the source data and traditionalsegmentation, detection, etc. - methods are used. To overcome the lack of labels in the target domain, source data is often gathered by crawling the Web (webly supervised) or the target set is enriched with synthetically generated data. The usage of the synthetic data became even more popular since the massive adoption of deep CNNs to perform computer vision tasks requiring large amount of annotated data.
Synthetic data based adaptation. Early methods use 3D CAD models to improve solutions for pose and viewpoint estimation, object and object part detection, segmentation and scene understanding. The recent progresses in computer graphics and modern high-level generic graphics platforms such as game engines enable to generate photo-realistic
Gabriela Csurka
Fig. 24 Illustration of the Cool-TSN deep multi-task learning architecture for end-to-end action recognition in videos.(Image courtesy C. De Souza). virtual worlds with diverse, realistic, and physically plausible events and actions. Popular virtual words are
SYNTHIA37, Virtual KITTI38 and GTA-V (see also Figure 23).
Such virtually generated and controlled environments come with different levels of labeling for free and therefore have great promise for deep learning across a variety of computer vision problems, including optical flow, object trackers, depth estimation from RGB, object detection semantic segmentation or human actions recognition.
In most cases, the synthetic data is used to enrich the real data for building the models. However, DA techniques can further help to adjust the model trained with virtual data (source) to real data (target) especially when no or few labeled examples are available in the real domain. As such, propose a deep spatial feature point architecture for visuomotor representation which, using synthetic examples and a few supervised examples, transfer the pretrained model to real imagery. This is done by combining a pose estimation loss, a domain confusion loss that aligns the synthetic and real domains, and a contrastive loss that aligns specific pairs in the feature space. All together, these three losses ensure that the representation is suitable to the pose estimation task while remaining robust to the synthetic-real domain shift.
The Cool Temporal Segment Network is an end-to-end action recognition model for real-world target categories that combines a few examples of labeled real-world videos with a large number of procedurally generated synthetic videos. The model uses a deep multi-task representation learning architecture, able to mix synthetic and real videos even if the action categories differ between the real and synthetic sets(see Figure 24).
37 Available at http://synthia-dataset.net
Available athttp://www.xrce.xerox.com/Research-Development/Computer-Vision/
Proxy-Virtual-Worlds
Domain Adaptation for Visual Applications: A Comprehensive Survey
Fig. 25 Online adaptation of the generic detector with tracked regions. (Image courtesy P. Sharma ).
5.1 Object detection
Concerning visual applications, after the image level categorization task, object detection received the most attention from the visual DA/TL community. Object detection models, until recently, were composed of a window selection mechanism and appearance based classifiers trained on the features extracted from labeled bounding boxes. At test time, the classifier was used to decide if a region of interest obtained by sliding windows or generic window selection models contains the object or not.
Therefore, considering the window selection mechanism as being domain independent, standard DA methods can be integrated with the appearance based classifiers to adapt to the target domain the models trained on the source domain. The Projective Model Transfer SVM (PMT-SVM) and the Deformable Adaptive SVM (DA-SVM) proposed in are such methods, which adapt HOG deformable source templates
 with labeled target bounding boxes (SS scenario), and the adapted template is used at test time to detect the presence or absence of an object class in sliding windows. In the PMT-SVM was further combined with MMDT to handle complex domain shifts. The detector is further improved by a smoothness constraints imposed on the classifier scores utilizing instance correspondences (e.g. the same object observed simultaneously from multiple views or tracked between video frames).
 uses the TCA to adapt image level HOG representation between source and target domains for object detection. proposes a Taylor Expansion Based Classifier Adaptation for either boosting or logistic regression to adapt person detection between videos acquired in different meeting rooms.
Online adaptation of the detector. Most early works related to object detector adaptation concern online adaptation of a generic detector trained on strongly labeled images (bounding boxes) to detect objects (in general cars or pedestrians) in videos. These methods exploit redundancies in videos to obtain prospective positive target examples (windows) either by background modeling/subtraction, or by combination of object tracking with regions proposed by the generic detector (see the main idea in Figure 25). Using these designated target samples in the new frame the model is updated involving semi-supervised approaches such as self-training or co-training.
For instance, proposes a non-parametric detector adaptation algorithm, which adjusts an offline frame-based object detector to the visual characteristic of a new video clip. The Structure-Aware Adaptive
Structural SVM (SA-SSVM) adapts online the deformable part-based model for pedestrian detection (see Figure 26). To handle the case when no target label is available, a strategy inspired by self-paced learning and supported by a Gaussian Process Regression is used to automatically label samples in the tarGabriela Csurka
Fig. 26 Domain Adaptation of DPM based on SA-SSVM (Image courtesy J. Xu). get domains. The temporal structure of the video is exploited through similarity constraints imposed on the adapted detector.
Multi-object tracking. Multi-object tracking aims at automatically detecting and tracking individual object (e.g. car or pedestrian) instances. These methods generally capitalizes on multi-task and multi-instance learning to perform category-to-instance adaptation. For instance, introduces a Multiple
Instance Learning (MIL) loss function for Real Adaboost, which is used within a tracking based unsupervised online sample collection mechanism to incrementally adjust the pretrained detector.
 propose an unsupervised, online and self-tuning learning algorithm to optimize a multi-task learning based convex objective involving a high-precision/low-recall off-the-shelf generic detector. The method exploits the data structure to jointly learn an ensemble of instance-level trackers, from which adapted categorylevel object detectors are derived. The main idea in is to jointly learn all detectors (the target instance models and the generic one) using an online adaptation via Bayesian filtering coupled with multi-task learning to efficiently share parameters and reduce drift, while gradually improving recall.
The transductive approach in re-trains the detector with automatically discovered target domain examples starting with the easiest first, and iteratively re-weighting labeled source samples by scoring trajectory tracks. introduces a multi-class random fern adaptive classifier where different categories of the positive samples (corresponding to different video tracks) are considered as different target classes, and all negative online samples are considered as a single negative target class. proposes a particle filtering framework for multi-person tracking-by-detection to predict the target locations.
Deep neural architectures. More recently, end-to-end deep learning object detection models were proposed that integrate and learn simultaneously the region proposals and the object appearance. In general, these models are initialized by deep models pretrained with image level annotations (often on the ILSVRC datasets
 ). In fact, the pretrained deep model combined with class-agnostic region of interest proposal, can
Domain Adaptation for Visual Applications: A Comprehensive Survey
31 already be used to predict the presence or absence of the target object in the proposed local regions. When strongly labeled target data is available, the model can further be fine-tuned using the labeled bounding boxes to improve both the recognition and the object localization. Thus, the Large
Scale Detection through Adaptation39 learns to transform an image classifier into an object detector by fine-tuning the CNN model, pretrained on images, with a set of labeled bounding boxes. The advantage of this model is that it generalizes well even for localization of classes for which there were no bounding box annotations during the training phase.
Instead fine-tuning, uses Subspace Alignment to adjust class specific representations of bounding boxes (BB) between the source and target domain. The source BBs are extracted from the strongly annotated training set, while the target BBs are obtained with the RCNN-detector trained on the source set. The detector is then re-trained with the target aligned source features and used to classify the target data projected into the target subspace.
6 Beyond domain adaptation: unifying perspectives
The aim of this section is to relate domain adaptation to other machine learning solutions. First in Section
6.1 we discuss how DA is related to other transfer learning (TL) techniques. Then, in Section 6.2 we connect
DA to several classical machine learning approaches illustrating how these methods are exploited in various
DA solutions. Finally, in Section 6.3 we examine the relationship between heterogeneous DA and multiview/multi-modal learning.
6.1 DA within transfer learning
As shown in Section 2, DA is a particular case of the transductive transfer learning aimed to solve a classification task common to the source and target, by simultaneously exploiting labeled source and unlabeled target examples (see also Figure 2). As such, DA is opposite to unsupervised TL, where both domains and tasks are different with labels available neither for source nor for target.
DA is also different from self-taught learning, which exploits a limited labeled target data for a classification task together with a large amount of unlabeled source data mildly related to the task. The main idea behind self-taught learning is to explore the unlabeled source data and to discover repetitive patterns that could be used for the supervised learning task.
On the other hand, DA is more closely related to domain generalization, multitask learning or few-shot learning discussed below.
Domain generalization. Similarly to multi-source DA, domain generalization methods aim to average knowledge from several related source domains, in order to learn a model for a new target domain. But, in contrast to DA where unlabeled target instances are available to adapt the model, in domain generalization, no target example is accessible at training time.
39 Code available at https://github.com/jhoffman/lsda/zipball/master
Gabriela Csurka
Multi-task learning. In multi-task learning different tasks (e.g. sets of the labels) are learned at the same time using a shared representation such that what is learned for each task can help in learning the other tasks. If we considering the tasks in DA as domain source and target) specific tasks, a semisupervised DA method can be seen as a sort of two-task learning problem where, in particular, learning the source specific task helps learning the target specific task. Furthermore, in the case of multi-source domain adaptation different source specific tasks are jointly exploited in the interest of the target task.
On the other hand, as we have seen in Section 5.1, multi-task learning techniques can be beneficial for online DA, in particular for multi-object tracking and detection, where the generic object detector(trained on source data) is adapted for each individual object instance.
Few-shot learning. Few-shot learning aims to learn information about object categories when only a few training images are available for training. This is done by making use of prior knowledge of related categories for which larger amount of annotated data is available. Existing solutions are the knowledge transfer through the reuse of model parameters, methods sharing parts or features or approaches relying on contextual information.
An extreme case of few-shot learning is the zero-shot learning, where the new task is deduced from previous tasks without using any training data for the current task. To address zero-shot learning, the methods rely either on nameable image characteristics and semantic concepts, or on latent topics discovered by the system directly from the data. In both cases, detecting these attributes can be seen as the common tasks between the training classes (source domains) and the new classes(target domains).
Unified DA and TL models. We have seen that the particularity of DA is the shared label space, in contrast to more generic TL approaches where the focus is on the task transfer between classes. However, in it is claimed that task transfer and domain shift can be seen as different declinations of learning to learn paradigm, i.e. the ability to leverage prior knowledge when attempting to solve a new task. Based on this observation, a common framework is proposed to leverage source data regardless of the origin of the distribution mismatch. Considering prior models as experts, the original features are augmented with the output confidence values of the source models and target classifiers are then learned with these features.
Similarly, the Transductive Prediction Adaptation (TPA) augments the target features with class predictions from source experts, before applying the MDA framework on these augmented features.
It is shown that MDA, exploiting the correlations between the target features and source predictions, can denoise the class predictions and improve classification accuracy. In contrast to the method in, TPA works also in the case when no label is available in the target domain (US scenario).
The Cross-Domain Transformation learns a regularized non-linear transformation using supervised data from both domains to map source examples closer to the target ones. It is shown that the models built in this new space generalize well not only to new samples from categories used to train the transformation (DA) but also to new categories that were not present at training time (task transfer). The Unifying Multi-Domain
Multi-Task Learning, is a Neural Network framework that can be flexibly applied to multi-task, multidomain and zero-shot learning and even to zero-shot domain adaptation.
Domain Adaptation for Visual Applications: A Comprehensive Survey
6.2 DA related to traditional ML methods
Semi-supervised learning. DA can be seen as a particular case of the semi-supervised learning, where, similarly to the majority of DA approaches, unlabeled data is exploited to remedy the lack of labeled data. Hence, ignoring the domain shift, traditional semi-supervised learning can be used as a solution for
DA, where the source instances form the supervised part, and the target domain provides the unlabeled data.
For this reason, DA methods often exploit or extend semi-supervised learning techniques such as transductive SVM, self-training, or co-training. When the domain shift is small, traditional semi-supervised methods can already bring a significant improvement over baseline methods obtained with the pretrained source model.
Active learning. Instance selection based DA methods exploit ideas from active learning to select instances with best potentials to help the training process. Thus, the Migratory-Logit algorithm explore, both the target and source data to actively select unlabeled target samples to be added to the training sets.
 describes an active learning method for relevant target data selection and labeling, which combines
TrAdaBoost with standard SVM., (see also Chapter 15), uses active learning and DA techniques to generalize semantic object parts (e.g. animal eyes or legs) to unseen classes (animals). The methods described in combine transfer learning and domain adaptation with the target sample selection and automatic sample labeling, based on the classifier confidence. These new samples are then used to iteratively update the target models.
Online learning. Online or sequential learning is strongly related to active learning; in both cases the model is iteratively and continuously updated using new data. However, while in active learning the data to be used for the update is actively selected, in online learning generally the new data is acquired sequentially. Domain adaptation can be combined with online learning too. As an example, we presented in Section 5.1 the online adaptation for incoming video frames of a generic object detector trained offline on labeled image sets. proposes online adaptation of image classifier to user generated content in social computing applications.
Furthermore, as discussed in Section 4, fine-tuning a deep model, pretrained on ImageNet (source), for a new dataset (target), can be seen as sort of semi-supervised domain adaptation.
Both, fine-tuning as well as training deepDA models, use sequential learning where data batches are used to perform the stochastic gradient updates. If we assume that these batches contain the target data acquired sequentially, the model learning process can be directly used for online DA adaptation of the original model.
Metric learning. In Section 3 we presented several metric learning based DA methods. where class labels from both domains are exploited to bridge the relatedness between the source and target. Thus, proposes a new distance metric for the target domain by using the existing distance metrics learned on the source domain. uses information-theoretic metric learning as a distance metric across different domains, which was extended to non-linear kernels in. proposes a metric learning adapted to the DSCM classifier, while defines a multi-task metric learning framework to learn relationships between source and target tasks. explores various metric learning approaches to align deep features extracted from RGB and NIR face images.
Gabriela Csurka
Fig. 27 Illustrating through an example the difference between TL to ML in the case of homogeneous data and between multiview and HTL/HDA when working with heterogeneous data. Image courtesy Q. Yang.
Classifier ensembles. Well studied in ML, classifier ensembles have also been considered for DA and TL.
As such, applies a bagging approach for transferring the learning capabilities of a model to different domains where a high number of trees is learned on both source and target data in order to build a pruned version of the final ensemble to avoid a negative transfer. uses random decision forests to transfer relevant features between domains. The optimization framework in takes as input several classifiers learned on the source domain as well as the results of a cluster ensemble operating solely on the target domain, yielding a consensus labeling of the data in the target domain. Boosting was extended to DA and TL in.
6.3 HDA related to multi-view/multi-modal learning
In many data intensive applications, such as video surveillance, social computing, medical health records or environmental sciences, data collected from diverse domains or obtained from various feature extractors exhibit heterogeneity. For example, a person can be identified by different facets e.g. face, fingerprint, signature or iris, or in video surveillance, an action or event can be recognized using multiple cameras. When working with such heterogeneous or multi-view data most, methods try to exploit simultaneously different modalities to build better final models.
Domain Adaptation for Visual Applications: A Comprehensive Survey
As such, multi-view learning methods are related to HDA/HTL as discussed also in Section 3.3. Nevertheless, while multi-view learning assumes that multi-view examples are available during training, in the case of HDA, this assumption rarely holds (see illustration in 27). On contrary, the aim of HDA is to transfer information from the source domain represented with one type of data (e.g. text) to the target domain represented with another type of data (e.g. images). While this assumption essentially differentiates the multi-view learning from HDA, we have seen in Section 3.3 that HDA methods often rely on an auxiliary intermediate multi-view domain. Hence, HDA/HTL can strongly benefit from multi-view learning techniques such as canonical correlation analysis, co-training, spectral embedding and multiple kernel learning.
Similarly to HDA/HTL relying on intermediate domains, cross-modal image retrieval methods depend on multi-view auxiliary data to define cross-modal similarities, or to perform semantic or multi-view embedding. Hence, HDA/HTL can strongly benefit from such cross-modal data representations.
In the same spirit, webly supervised approaches are also related to DA and HDA as is these approaches rely on collected Web data (source) data used to refine the target model. As such, uses multiple kernel learning to adapt visual events learned from the Web data for video clips.
 and propose domain transfer approaches from weakly-labeled Web images for action localization and event recognition tasks.
7 Conclusion
In this chapter we tried to provide an overview of different solutions for visual domain adaptation, including both shallow and deep methods. We grouped the methods both by their similarity concerning the problem(homogeneous vs. heterogeneous data, unsupervised vs. semi-supervised scenario) and the solutions proposed (feature transformation, instance reweighing, deep models, online learning, etc.). We also reviewed methods that solve DA in the case of heterogeneous data as well as approaches that address computer vision problems beyond the image classification, such as object detection or multi-object tracking. Finally, we positioned domain adaptation within a larger context by linking it to other transfer learning techniques as well as to traditional machine learning approaches.
Due to the lack of the space and the large amount of methods mentioned, we could only briefly depict each method; the interested reader can follow the reference for deeper reading. We also decided not to provide any comparative experimental results between these methods for the following reasons: (1) Even if many
DA methods were tested on the benchmark OFF31 and OC10 datasets, papers use often different experimental protocols (sampling the source vs. using the whole data, unsupervised vs. supervised) and different parameter tuning strategies (fix parameter sets, tuning on the source, cross validation or unknown).(2) Results reported in different papers given the same methods (e.g. GFK, TCA, SA) vary also a lot between different re-implementations. For all these reasons, making a fair comparison between all the methods based only on the literature review is rather difficult. (3) These datasets are rather small, some methods have published results only with the outdated SURFBOV features and relying only on these results is not sufficient to derive general conclusions about the methods. For a fair comparison, deep methods should be compared to shallow methods using deep features extracted from similar architectures, but both features extracted from the latest deep models and deep DA architectures build on these models perform extremely well on OFF31 and OC10 even without adaptation.
Gabriela Csurka
Most DA solutions in the literature are tested on these relatively small datasets (both in terms of number of classes and number of images). However, with the proliferation of sensors, large amount of heterogeneous data is collected, and hence there is a real need for solutions being able to efficiently exploit them. This shows a real need for more challenging datasets to evaluate and compare the performance of different methods. The few new DA datasets, such as the Testbed cross-dataset (TB) or datasets built for model adaptation between photos, paintings and sketches while more challenging than the popular OFF31, OC10 or MNIST vs. SVHN, they are only sparsely used. Moreover, except the cross-modal
Place dataset, they are still small scale and single modality datasets.
We have also seen that only relatively few papers address adaptation beyond recognition and detection.
Image and video understanding, semantic and instance level segmentation, human pose, event and action recognition, motion and 3D scene understanding, where trying to simply describe the problem with a vectorial representation and classical domain adaptation, even when it is possible, has serious limitations. Recently, these challenging problems are addressed with deep methods requiring large amount of labeled data.
How to adapt these new models between domains with no or very limited amount of data is probably one of the main challenge that should be addressed by the visual domain adaptation and transfer learning community in the next few years.
References
1. B. F. Klare, S. S. Bucak, A. K. Jain, and T. Akgul, "Towards automated caricature recognition," in International Conference on Biometrics (ICB), 2012.
2. E. J. Crowley and A. Zisserman, "In search of art," in ECCV Workshop on Computer Vision for Art Analysis, 2014.
3. L. Castrej´on, Y. Aytar, C. Vondrick, H. Pirsiavash, and A. Torralba, "Learning aligned cross-modal representations from weakly aligned data," in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.
4. S. Saxena and J. Verbeek, "Heterogeneous face recognition with cnns," in ECCV Workshop on Transferring and Adapting
Source Knowledge in Computer Vision (TASK-CV), 2016.
5. H. Daum´e III and D. Marcu, "Domain adaptation for statistical classifiers," Journal of Artificial Intelligence Research, vol. 26, no. 1, pp. 101–126, 2006.
6. S. J. Pan, X. Ni, J.-T. Sun, Q. Yang, and Z. Chen, "Cross-domain sentiment classification via spectral feature alignment," in International Conference on World Wide Web (WWW), 2010.
7. J. Blitzer, S. Kakade, and D. P. Foster, "Domain adaptation with coupled subspaces," in International Conference on
Artificial Intelligence and Statistics (AISTATS), 2011.
8. M. Zhou and K. C. Chang, "Unifying learning to rank and domain adaptation: Enabling cross-task document scoring," in ACM SIGKDD Conference on Knowledge Discovery and Data Mining (SIGKDD), 2014.
9. P. Prettenhofer and B. Stein, "Cross-language text classification using structural correspondence learning," in Annual
Meeting of the Association for Computational Linguistics(ACL), 2010.
10. J. T. Zhou, I. W. Tsang, S. J. Pan, and M. Tan, "Heterogeneous domain adaptation for multiple classes," in International
Conference on Artificial Intelligence and Statistics (AISTATS), 2014.
11. J. T. Zhou, S. J. Pan, I. W. Tsang, and Y. Yan, "Hybrid heterogeneous transfer learning through deep learning," in AAAI
Conference on Artificial Intelligence (AAAI), 2014.
12. M. Chen, Z. Xu, K. Q. Weinberger, and F. Sha, "Marginalized denoising autoencoders for domain adaptation," in International Conference on Machine Learning (ICML), 2012.
13. X. Glorot, A. Bordes, and Y. Bengio, "Domain adaptation for large-scale sentiment classification: A deep learning approach," in International Conference on Machine Learning (ICML), 2011.
14. S. J. Pan, J. T. Tsang, Ivor W.and Kwok, and Q. Yang, "Domain adaptation via transfer component analysis," Transactions on Neural Networks, vol. 22, no. 2, pp. 199 – 210, 2011.
15. C. J. Leggetter and P. C. Woodland, "Maximum likelihood linear regression for speaker adaptation of continuous density hidden markov models," Computer Speech and Language, vol. 9, no. 2, pp. 171–185, 1995.
Domain Adaptation for Visual Applications: A Comprehensive Survey
16. D. A. Reynolds, T. F. Quatieri, and R. B. Dunn, "Speaker verification using adapted Gaussian Mixture Models," Digital
Signal Processing, vol. 10, no. 1, pp. 19–41, 2000.
17. K. Saenko, B. Kulis, M. Fritz, and T. Darrell, "Adapting visual category models to new domains," in European Conference on Computer Vision (ECCV), 2010.
18. B. Gong, Y. Shi, F. Sha, and K. Grauman, "Geodesic flow kernel for unsupervised domain adaptation," in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2012.
19. B. Fernando, A. Habrard, M. Sebban, and T. Tuytelaars, "Unsupervised visual domain adaptation using subspace alignment," in IEEE International Conference on Computer Vision (ICCV), 2013.
20. M. Long, J. Wang, G. Ding, J. Sun, and P. S. Yu, "Transfer feature learning with joint distribution adaptation," in IEEE
International Conference on Computer Vision (ICCV), 2013.
21. B. Sun, J. Feng, and K. Saenko, "Return of frustratingly easy domain adaptation," in AAAI Conference on Artificial
Intelligence (AAAI), 2016.
22. J. Yang, R. Yan, and A. G. Hauptmann, "Cross-domain video concept detection using adaptive svms," in IEEE International Conference on Computer Vision (ICCV), 2013.
23. L. Duan, D. Xu, and S.-F. Chang, "Exploiting web images for event recognition in consumer videos: A multiple source domain adaptation approach," in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2012.
24. N. FarajiDavar, T. deCampos, D. Windridge, J. Kittler, and W. Christmas, "Domain adaptation in the context of sport video action recognition," in BMVA British Machine Vision Conference (BMVC), 2012.
25. F. Zhu and L. Shao, "Enhancing action recognition by cross-domain dictionary learning," in BMVA British Machine Vision
Conference (BMVC), 2013.
26. H. Shen, S.-I. Yu, Y. Yang, D. Meng, and A. Hauptmann, "Unsupervised video adaptation for parsing human motion," in European Conference on Computer Vision (ECCV), 2014.
27. M. Yang, L. Zhang, X. Feng, and D. Zhang, "Fisher discrimination dictionary learning for sparse representation," in IEEE
International Conference on Computer Vision (ICCV), 2011.
28. S. Shekhar, V. M. Patel, H. V. Nguyen, and R. Chellappa, "Generalized domain-adaptive dictionaries," in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2013.
29. A. Sharma and D. W. Jacobs, "Bypassing synthesis: PLS for face recognition with pose, low-resolution and sketch," in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2011.
30. B. Smith and L. Zhang, "Collaborative facial landmark localization for transferring annotations across datasets," in European Conference on Computer Vision (ECCV), 2014.
31. W.-S. Chu, F. D. l. Torre, and J. F. Cohn, "Selective transfer machine for personalized facial action unit detection," in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2013.
32. M. Yamada, L. Sigal, and M. Raptis, "No bias left behind: Covariate shift adaptation for discriminative 3d pose estimation," in European Conference on Computer Vision (ECCV), 2012.
33. B. Chidlovskii, S. Clinchant, and G. Csurka, "Domain adaptation in the absence of source domain data," in Joint European
Conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD), 2016.
34. G. Csurka, B. Chidlovskii, and S. Clinchant, "Adapted domain specific class means," in ICCV workshop on Transferring and Adapting Source Knowledge in Computer Vision (TASK-CV), 2015.
35. G. Csurka, D. Larlus, A. Gordo, and J. Almazan, "What is the right way to represent document images?," CoRR, vol. arXiv:1603.01076, 2016.
36. K. Weiss, T. M. Khoshgoftaar, and D. Wang, "A survey of transfer learning," Journal of Big Data, vol. 9, no. 3, 2016.
37. S. J. Pan and Q. Yang, "A survey on transfer learning," Transactions on Knowledge and Data Engineering, vol. 22, no. 10, pp. 1345–1359, 2010.
38. W. Dai, Q. Yang, G.-R. Xue, and Y. Yu, "Self-taught clustering," in International Conference on Machine Learning(ICML), 2008.
39. Z. Whang, Y. Song, and C. Zhang, "Transferred dimensionality reduction," in Joint European Conference on Machine
Learning and Knowledge Discovery in Databases (ECML PKDD), 2008.
40. M. Long, J. Wang, G. Ding, J. Sun, and P. S. Yu, "Transfer joint matching for unsupervised domain adaptation," in IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), 2014.
41. H. Shimodaira, "Improving predictive inference under covariate shift by weighting the log-likelihood function," Journal of Statistical Planning and Inference, vol. 90, no. 2, pp. 227–244, 2000.
42. B. Zadrozny, "Learning and evaluating classifiers under sample selection bias," in International Conference on Machine
Learning (ICML), 2004.
Gabriela Csurka
43. M. Sugiyama, S. Nakajima, H. Kashima, P. v. Buenau, and M. Kawanabe, "Direct importance estimation with model selection and its application to covariate shift adaptation," in Annual Conference on Neural Information Processing Systems(NIPS), 2008.
44. T. Kanamori, S. Hido, and M. Sugiyama, "Efficient direct density ratio estimation for non-stationarity adaptation and outlier detection.," Journal of Machine Learning Research, vol. 10, pp. 1391–1445, 2009.
45. J. Huang, A. Smola, A. Gretton, K. Borgwardt, and B. Sch¨olkopf, "Correcting sample selection bias by unlabeled data," in Annual Conference on Neural Information Processing Systems (NIPS), 2007.
46. A. Gretton, A. Smola, J. Huang, M. Schmittfull, K. Borgwardt, and B. Sch¨olkopf, "Covariate shift by kernel mean matching," in Dataset Shift in Machine Learning (J. Qui˜nonero Candela, M. Sugiyama, A. Schwaighofer, and N. D. Lawrence, eds.), The MIT Press, 2009.
47. K. M. Borgwardt, A. Gretton, M. J. Rasch, H.-P. Kriegel, B. Sch¨olkopf, and A. J. Smola, "Integrating structured biological data by kernel maximum mean discrepancy," Bioinformatics, vol. 22, pp. 49–57, 2006.
48. M. Dud´ık, R. E. Schapire, and S. J. Phillips, "Correcting sample selection bias in maximum entropy density estimation," in Annual Conference on Neural Information Processing Systems (NIPS), 2005.
49. W. Dai, Q. Yang, G.-R. Xue, and Y. Yu, "Boosting for transfer learning," in International Conference on Machine Learning(ICML), 2007.
50. Y. Freund and R. Schapire, "A decision-theoretic generalization of on-line learning and an application to boosting,"
Journal of Computer and System Sciences, vol. 55, no. 1, pp. 119–139, 1997.
51. S. Al-Stouhi and C. K. Reddy, "Adaptive boosting for transfer learning using dynamic updates," in Joint European Conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD), 2011.
52. B. Chidlovskii, G. Csurka, and S. Gangwar, "Assembling heterogeneous domain adaptation methods for image classification," in CLEF online Working Notes, 2014.
53. T. Joachims, "Transductive inference for text classification using support vector machines," in International Conference on Machine Learning (ICML), 1999.
54. J. Yang, R. Yan, and A. G. Hauptmann, "Cross-domain video concept detection using adaptive SVMs," in ACM Multimedia, 2007.
55. L. Duan, I. W. Tsang, D. Xu, and S. J. Maybank, "Domain transfer SVM for video concept detection," in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2009.
56. L. Bruzzone and M. Marconcini, "Domain adaptation problems: A dasvm classification technique and a circular validation strategy," Transactions of Pattern Recognition and Machine Analyses (PAMI), vol. 32, pp. 770–787, 2010.
57. Y. Chen, G. Wang, and S. Dong, "Learning with progressive transductive support vector machine," Pattern Recognition
Letters, vol. 24, no. 12, pp. 845–855, 2003.
58. W. Jiang, E. Zavesky, S.-F. Chang, and A. Loui, "Cross-domain learning methods for high-level visual concept classification," in International Conference on Image Processing (ICIP), 2008.
59. H. Cheng, P.-N. Tan, and R. Jin, "Localized support vector machine and its efficient algorithm," in SIAM International
Conference on Data Mining (SDM), 2007.
60. H. Daum´e III, "Frustratingly easy domain adaptation," CoRR, vol. arXiv:0907.1815, 2009.
61. R. Gopalan, R. Li, and R. Chellappa, "Domain adaptation for object recognition: An unsupervised approach," in IEEE
International Conference on Computer Vision (ICCV), 2011.
62. R. Gopalan, R. Li, and R. Chellappa, "Unsupervised adaptation across domain shifts by generating intermediate data representations," Transactions of Pattern Recognition and Machine Analyses (PAMI), vol. 36, no. 11, 2014.
63. B. Gong, K. Grauman, and F. Sha, "Connecting the dots with landmarks: Discriminatively learning domain invariant features for unsupervised domain adaptation," in International Conference on Machine Learning (ICML), 2013.
64. J. Ni, Q. Qiu, and R. Chellappa, "Subspace interpolation via dictionary learning for unsupervised domain sadaptation," in IEEE International Conference on Computer Vision (ICCV), 2013.
65. M. Baktashmotlagh, M. Harandi, B. Lovell, and M. Salzmann, "Unsupervised domain adaptation by domain invariant projection," in IEEE International Conference on Computer Vision (ICCV), 2013.
66. M. Baktashmotlagh, M. Harandi, B. Lovell, and M. Salzmann, "Domain adaptation on the statistical manifold," in IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), 2014.
67. A. Edelman, T. A. Arias, and S. T. Smith, "The geometry of algorithms with orthogonality constraints," Journal of Matrix
Analysis and Applications, vol. 20, no. 2, pp. 303–353, 1998.
68. M. Long, G. Ding, J. Wang, J. Sun, Y. Guo, and P. Yu, "Transfer sparse coding for robust image representation," in IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), 2013.
Domain Adaptation for Visual Applications: A Comprehensive Survey
69. G. Matasci, M. Volpi, M. Kanevski, L. Bruzzone, and D. Tuia, "Semi-supervised transfer component analysis for domain adaptation in remote sensing image classification," Transactions on Geoscience and Remote Sensing, vol. 53, no. 7, pp. 3550–3564, 2015.
70. G. Csurka, B. Chidlovskii, S. Clinchant, and S. Michel, "Unsupervised domain adaptation with regularized domain instance denoising," in ECCV workshop on Transferring and Adapting Source Knowledge in Computer Vision (TASK-CV), 71. M. Long, J. Wang, G. Ding, S. J. Pan, and P. Yu, "Adaptation regularization: a general framework for transfer learning,"
Transactions on Knowledge and Data Engineering, vol. 5, no. 26, pp. 1076–1089, 2014.
72. J. Hoffman, E. Rodner, J. Donahue, T. Darrell, and K. Saenko, "Efficient learning of domain-invariant image representations," in International Conference on Learning representations (ICLR), 2013.
73. E. Zhong, W. Fan, J. Peng, K. Zhang, J. Ren, D. Turaga, and O. Verscheure, "Cross domain distribution adaptation via kernel mapping," in ACM SIGKDD Conference on Knowledge Discovery and Data Mining (SIGKDD), 2009.
74. Z.-J. Zha, t. Mei, M. Wang, Z. Wang, and X.-S. Hua, "Robust distance metric learning with auxiliary knowledge," in AAAI International Joint Conference on Artificial Intelligence (IJCAI), 2009.
75. J. V. Davis, B. Kulis, P. Jain, S. Sra, and I. S. Dhillon, "Information-theoretic metric learning," in International Conference on Machine Learning (ICML), 2007.
76. B. Kulis, K. Saenko, and T. Darrell, "What you saw is not what you get: Domain adaptation using asymmetric kernel transforms," in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2011.
77. G. Csurka, B. Chidlovskii, and F. Perronnin, "Domain adaptation with a domain specific class means classifier," in ECCV
Workshop on Transferring and Adapting Source Knowledge in Computer Vision (TASK-CV), 2014.
78. T. Tommasi and B. Caputo, "Frustratingly easy NBNN domain adaptation," in IEEE International Conference on Computer Vision (ICCV), 2013.
79. N. Courty, R. Flamary, D. Tuia, and A. Rakotomamonjy, "Optimal transport for domain adaptation," CoRR, vol. arXiv:1507.00504, 2015.
80. N. FarajiDavar, T. deCampos, and J. Kittler, "Adaptive transductive transfer machines," in BMVA British Machine Vision
Conference (BMVC), 2014.
81. R. Aljundi, R. Emonet, D. Muselet, and M. Sebban, "Landmarks-based kernelized subspace alignment for unsupervised domain adaptation," in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.
82. L. Duan, D. Xu, and I. W. Tsang, "Domain adaptation from multiple sources: A domain-dependent regularization approach," Transactions on Neural Networks and Learning Systems, vol. 23, no. 3, pp. 504–518, 2012.
83. R. Chattopadhyay, J. Ye, S. Panchanathan, W. Fan, and I. Davidson, "Multi-source domain adaptation and its application to early detection of fatigue," in ACM SIGKDD Conference on Knowledge Discovery and Data Mining (SIGKDD), 2011.
84. I.-H. Jhuo, D. Liu, D. Lee, and S.-F. Chang, "Robust visual domain adaptation with low-rank reconstruction," in IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), 2012.
85. R. Caseiro, J. F. Henriques, P. Martins, and J. Batista, "Beyond the shortest path : Unsupervised domain adaptation by sampling subspaces along the spline flow," in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 86. J. Hoffman, B. Kulis, T. Darrell, and K. Saenko, "Discovering latent domains for multisource domain adaptation," in European Conference on Computer Vision (ECCV), 2012.
87. Y. Yao and G. Doretto, "Boosting for transfer learning with multiple sources," in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2010.
88. L. Ge, J. Gao, H. Ngo, K. Li, and A. Zhang, "On handling negative transfer and imbalanced distributions in multiple source transfer learning," in SIAM International Conference on Data Mining (SDM), 2013.
89. T. Tommasi and B. Caputo, "Safety in numbers: learning categories from few examples with multi model knowledge transfer," in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2010.
90. C. Xu, D. Tao, and C. Xu, "A survey on multi-view learning," CoRR, vol. arXiv:1304.5634, 2013.
91. W. Wang, R. Arora, K. Livescu, and J. Bilmes, "On deep multi-view representation learning," in International Conference on Machine Learning (ICML), 2015.
92. K. Chaudhuri, S. Kakade, K. Livescu, and K. S. Sridharan, "Multi-view clustering via canonical correlation analysis," in International Conference on Machine Learning (ICML), 2009.
93. D. Hardoon, S. Szedmak, and J. Shawe-Taylor, "Canonical correlation analysis: An overview with application to learning methods," Neurocomputing, vol. 16, no. 12, pp. 2639–2664, 2004.
94. R. Socher and F.-F. Li, "Connecting modalities: Semi-supervised segmentation and annotation of images using unaligned text corpora," in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2010.
Gabriela Csurka
95. F. Yan and K. Mikolajczyk, "Deep correlation for matching images and text," in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.
96. J. Hoffman, S. Gupta, and T. Darrell, "Learning with side information through modality hallucination," in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.
97. A. Vinokourov, N. Cristianini, and J. Shawe-Taylor, "Inferring a semantic representation of text via cross-language correlation analysis," in Annual Conference on Neural Information Processing Systems (NIPS), 2003.
98. M. Faruqui and C. Dyer, "Improving vector space word representations using multilingual correlation," in Conference of the European Chapter of the Association for Computational Linguistics (EACL), 2014.
99. A. Sharma, A. Kumar, H. Daum´e III, and D. Jacobs, "Generalized multiview analysis: A discriminative latent space," in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2012.
100. Q. Qiu, V. M. Patel, P. Turaga, and R. Chellappa, "Domain adaptive dictionary learning," in European Conference on
Computer Vision (ECCV), 2012.
101. b. Tan, Y. Song, E. Zhong, and Q. Yang, "Transitive transfer learning," in ACM SIGKDD Conference on Knowledge
Discovery and Data Mining (SIGKDD), 2015.
102. C. Ding, T. Li, W. Peng, and H. Park, "Orthogonal nonnegative matrix tri-factorizations for clustering," in ACM SIGKDD
Conference on Knowledge Discovery and Data Mining (SIGKDD), 2005.
103. B. Tan, E. Zhong, M. Ng, and K. Q. Yang, "Mixed-transfer: Transfer learning over mixed graphs," in SIAM International
Conference on Data Mining (SDM), 2014.
104. Y. Zhu, Y. Chen, Z. Lu, S. J. Pan, G.-R. Xue, Y. Yu, and Q. Yang, "Heterogeneous transfer learning for image classification," in AAAI Conference on Artificial Intelligence (AAAI), 2011.
105. A. Singh, P. Singh, and G. Gordon, "Relational learning via collective matrix factorization," in ACM SIGKDD Conference on Knowledge Discovery and Data Mining (SIGKDD), 2008.
106. G.-J. Qi, C. Aggarwal, and T. Huang, "Towards semantic knowledge propagation from text corpus to web images," in International Conference on World Wide Web (WWW), 2011.
107. L. Yang, L. Jing, J. Yu, and M. K. Ng, "Learning transferred weights from co-occurrence data for heterogeneous transfer learning," Transactions on Neural Networks and Learning Systems, vol. 27, no. 11, pp. 2187–2200, 2015.
108. C. Andrieu, N. Freitas, A. Doucet, and M. Jordan, "An introduction to mcmc for machine learning," Machine Learning, vol. 50, no. 1, pp. 5–43, 2003.
109. Y. Yan, Q. Wu, M. Tan, and H. Min, "Online heterogeneous transfer learning by weighted offline and online classifiers," in ECCV Workshop on Transferring and Adapting Source Knowledge in Computer Vision (TASK-CV), 2016.
110. J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y. Ng, "Multimodal deep learning," in International Conference on Machine Learning (ICML), 2011.
111. Y. Gong, Q. Ke, M. Isard, and S. Lazebnik, "A multi-view embedding space for modeling internet images, tags, and their semantics," International Journal of Computer Vision, vol. 106, no. 2, pp. 210–233, 2014.
112. G. Cao, A. Iosifidis, K. Chen, and M. Gabbouj, "Generalized multi-view embedding for visual recognition and crossmodal retrieval," CoRR, vol. arXiv:1605.09696, 2016.
113. L. Wang, Y. Li, and S. Lazebnik, "Learning deep structure-preserving image-text embeddings," in IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2016.
114. L. Duan, D. Xu, and I. W. Tsang, "Learning with augmented features for heterogeneous domain adaptation," Transactions of Pattern Recognition and Machine Analyses (PAMI), vol. 36, no. 6, pp. 1134–1148, 2012.
115. W. Li, L. Duan, D. Xu, and I. W. Tsang, "Learning with augmented features for supervised and semi-supervised heterogeneous domain adaptation," Transactions of Pattern Recognition and Machine Analyses (PAMI), vol. 36, no. 6, pp. 1134
– 1148, 2014.
116. X. Shi, Q. Liu, W. Fan, P. S. Yu, and R. Zhu, "Transfer learning on heterogeneous feature spaces via spectral transformation," in IEEE International Conference on Data Mining (ICDM), 2010.
117. M. Xiao and Y. Guo, "Semi-supervised subspace co-projection for multi-class heterogeneous domain adaptation," in Joint
European Conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD), 2015.
118. T. Yao, Y. Pan, C.-W. Ngo, H. Li, and T. Mei, "Semi-supervised domain adaptation with subspace learning for visual recognition," in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.
119. C. Wang and S. Mahadevan, "Heterogeneous domain adaptation using manifold alignment," in AAAI International Joint
Conference on Artificial Intelligence (IJCAI), 2011.
120. M. Harel and S. Mannor, "Learning from multiple outlooks," in International Conference on Machine Learning (ICML), 121. D. L. Donoho, "Compressed sensing," Transactions on Information Theory, vol. 52, pp. 1289–1306, 2006.
Domain Adaptation for Visual Applications: A Comprehensive Survey
122. O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C.
Berg, and L. Fei-Fei, "Imagenet large scale visual recognition challenge," International Journal of Computer Vision, vol. 115, no. 3, pp. 211–252, 2015.
123. J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and T. Darrell, "Decaf: A deep convolutional activation feature for generic visual recognition," in International Conference on Machine Learning (ICML), 2014.
124. A. Krizhevsky, I. Sutskever, and G. Hinton, "ImageNet classification with deep Convolutional Neural Networks," in Annual Conference on Neural Information Processing Systems (NIPS), 2012.
125. K. Simonyan and A. Zisserman, "Very deep convolutional networks for large-scale image recognition," CoRR, vol. arXiv:1409.1556, 2014.
126. K. He, X. Zhang, S. Ren, and J. Sun, "Deep residual learning for image recognition," CoRR, vol. arXiv:1512.03385, 2015.
127. C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich, "Going deeper with convolutions," in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.
128. S. Chopra, S. Balakrishnan, and R. Gopalan, "DLID: Deep learning for domain adaptation by interpolating between domains," in ICML Workshop on Challenges in Representation Learning (WREPL), 2013.
129. Y. Bengio, A. Courville, and P. Vincent, "Representation learning: A review and new perspectives," Transactions of Pattern Recognition and Machine Analyses (PAMI), vol. 35, no. 8, pp. 1798–1828, 2013.
130. J. Yosinski, J. Clune, Y. Bengio, and H. Lipson, "How transferable are features in deep neural networks?," in Annual
Conference on Neural Information Processing Systems (NIPS), 2014.
131. E. J. Crowley and A. Zisserman, "The state of the art: Object retrieval in paintings using discriminative regions," in BMVA
British Machine Vision Conference (BMVC), 2014.
132. M. D. Zeiler and R. Fergus, "Visualizing and understanding convolutional networks," CoRR, vol. arXiv:1311.2901, 2013.
133. M. Oquab, L. Bottou, I. Laptev, and J. Sivic, "Learning and transferring mid-level image representations using convolutional neural networks," in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014.
134. A. Babenko, A. Slesarev, A. Chigorin, and V. S. Lempitsky, "Neural codes for image retrieval," in European Conference on Computer Vision (ECCV), 2014.
135. B. Chu, V. Madhavan, O. Beijbom, J. Hoffman, and T. Darrell, "Best practices for fine-tuning visual classifiers to new domains," in ECCV Workshop on Transferring and Adapting Source Knowledge in Computer Vision (TASK-CV), 2016.
136. E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell, "Adversarial discriminative domain adaptation," in NIPS Workshop on
Adversarial Training, (WAT), 2016.
137. P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol, "Extracting and composing robust features with denoising autoencoders," in International Conference on Machine Learning (ICML), 2008.
138. M. Ghifary, W. B. Kleijn, M. Zhang, and D. Balduzzi, "Domain generalization for object recognition with multi-task autoencoders," in IEEE International Conference on Computer Vision (ICCV), 2015.
139. K. Kavukcuoglu, M. Ranzato, and Y. LeCun, "Fast inference in sparse coding algorithms with applications to object recognition," CoRR, vol. arXiv:1010.3467, 2010.
140. R. Aljundi and T. Tuytelaars, "Lightweight unsupervised domain adaptation by convolutional filter reconstruction," in ECCV Workshop on Transferring and Adapting Source Knowledge in Computer Vision (TASK-CV), 2016.
141. J. Bromley, J. W. Bentz, L. Bottou, I. Guyon, Y. LeCun, C. Moore, E. S¨ackinger, and R. Shah, "Signature verification using a "siamese" time delay neural network," International Journal of Pattern Recognition and Artificial Intelligence, vol. 7, no. 04, pp. 669–688, 1993.
142. E. Tzeng, J. Hoffman, N. Zhang, K. Saenko, and T. Darrell, "Deep domain confusion: Maximizing for domain invariance,"
CoRR, vol. arXiv:1412.3474, 2014.
143. M. Long, Y. Cao, J. Wang, and M. Jordan, "Learning transferable features with deep adaptation networks," in International
Conference on Machine Learning (ICML), 2015.
144. B. Sun and K. Saenko, "Deep coral: Correlation alignment for deep domain adaptation," in ECCV Workshop on Transferring and Adapting Source Knowledge in Computer Vision (TASK-CV), 2016.
145. M. Long, J. Wang, and M. I. Jordan, "Deep transfer learning with joint adaptation networks," CoRR, vol. arXiv:1605.06636, 2016.
146. A. Rozantsev, M. Salzmann, and P. Fua, "Beyond sharing weights for deep domain adaptation," CoRR, vol. arXiv:1603.06432, 2016.
147. Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette, M. Marchand, and V. Lempitsky, "Domainadversarial training of neural networks," Journal of Machine Learning Research, 2016.
148. E. Tzeng, J. Hoffman, T. Darrell, and K. Saenko, "Simultaneous deep transfer across domains and tasks," in IEEE International Conference on Computer Vision (ICCV), 2015.
Gabriela Csurka
149. I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, "Generative adversarial nets," in Annual Conference on Neural Information Processing Systems (NIPS), 2014.
150. M.-Y. Liu and O. Tuzel, "Coupled generative adversarial networks," in Annual Conference on Neural Information Processing Systems (NIPS), 2016.
151. K. Bousmalis, N. Silberman, D. Dohan,, D. Erhan, and D. Krishnan, "Unsupervised pixel-level domain adaptation with generative adversarial networks," CoRR, vol. arXiv:1612.05424, 2016.
152. D. Eigen, C. Puhrsch, and R. Fergus, "Depth map prediction from a single image using a multi-scale deep network," in Annual Conference on Neural Information Processing Systems (NIPS), 2014.
153. K. Bousmalis, G. Trigeorgis, N. Silberman, D. Erhan, and D. Krishnan, "Domain separation networks," in Annual Conference on Neural Information Processing Systems (NIPS), 2016.
154. M. Ghifary, W. B. Kleijn, M. Zhang, and D. Balduzzi, "Deep reconstruction-classification networks for unsupervised domain adaptation," in European Conference on Computer Vision (ECCV), 2016.
155. M. Zeiler, D. Krishnan, G. Taylor, and R. Fergus, "Deconvolutional networks," in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2010.
156. W.-Y. Chen, T.-M. H. Hsu, and Y.-H. H. Tsai, "Transfer neural trees for heterogeneous domain adaptation," in European
Conference on Computer Vision (ECCV), 2016.
157. X. Shu, G.-J. Qi, J. Tang, and W. Jingdong, "Weakly-shared deep transfer networks for heterogeneous-domain knowledge propagation," in ACM Multimedia, 2015.
158. S. Divvala, A. Farhadi, and C. Guestrin, "Learning everything about anything: Webly-supervised visual concept learning," in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014.
159. X. Chen and A. Gupta, "Webly supervised learning of convolutional networks," in IEEE International Conference on
Computer Vision (ICCV), 2015.
160. E. Crowley and A. Zisserman, "The art of detection," in ECCV Workshop on Computer Vision for Art Analysis, (CVAA), 161. A. Agarwal and B. Triggs, "A local basis representation for estimating human pose from cluttered images," in Asian
Conference on Computer Vision (ACCV), 2006.
162. J. Shotton, A. Fitzgibbon, M. Cook, T. Sharp, M. Finocchio, R. Moore, A. Kipman, and A. Blake, "Real-time human pose recognition in parts from single depth images," in IEEE Conference on Computer Vision and Pattern Recognition(CVPR), 2011.
163. P. Panareda-Busto, J. Liebelt, and J. Gall, "Adaptation of synthetic data for coarse-to-fine viewpoint refinement," in BMVA
British Machine Vision Conference (BMVC), 2015.
164. H. Su, C. Qi, Y. Yi, and L. Guibas, "Render for CNN: viewpoint estimation in images using CNNs trained with rendered
3D model views," in IEEE International Conference on Computer Vision (ICCV), 2015.
165. M. Stark, M. Goesele, and B. Schiele, "Back to the future: Learning shape models from 3D CAD data," in BMVA British
Machine Vision Conference (BMVC), 2010.
166. B. Pepik, M. Stark, P. Gehler, and B. Schiele, "Teaching 3D geometry to deformable part models," in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2012.
167. B. Sun and K. Saenko, "From virtual to reality: Fast adaptation of virtual object detectors to real domains," in BMVA
British Machine Vision Conference (BMVC), 2014.
168. A. Rozantsev, V. Lepetit, and P. Fua, "On rendering synthetic images for training an object detector," Computer Vision and Image Understanding, vol. 137, pp. 24–37, 2015.
169. X. Peng, B. Sun, K. Ali, and K. Saenko, "Learning deep object detectors from 3D models," in IEEE International Conference on Computer Vision (ICCV), 2015.
170. H. Hattori, V. Naresh Boddeti, K. M. Kitani, and T. Kanade, "Learning scene-specific pedestrian detectors without real data," in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.
171. F. Massa, B. Russell, and M. Aubry, "Deep exemplar 2D-3D detection by adapting from real to rendered views," in IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), 2016.
172. E. Bochinski, V. Eiselein, and T. Sikora, "Training a convolutional neural network for multi-class object detection using solely virtualworld data," in IEEE International Conference on Advanced Video and Signal-based Surveillance (AVSS), 173. S. Satkin, J. Lin, and M. Hebert, "Data-driven scene understanding from 3D models," in BMVA British Machine Vision
Conference (BMVC), 2012.
174. L.-C. Chen, S. Fidler, and R. Yuille, Alan L. Urtasun, "Beat the MTurkers: Automatic image labeling from weak 3D supervision," in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014.
Domain Adaptation for Visual Applications: A Comprehensive Survey
175. J. Papon and M. Schoeler, "Semantic pose using deep networks trained on synthetic RGB-D," in IEEE International
Conference on Computer Vision (ICCV), 2015.
176. G. Ros, L. Sellart, J. Materzy´nska, D. V´azquez, and A. L´opez, "The SYNTHIA dataset: A large collection of synthetic images for semantic segmentation of urban scenes," in IEEE Conference on Computer Vision and Pattern Recognition(CVPR), 2016.
177. A. Gaidon, Q. Wang, Y. Cabon, and E. Vig, "Virtual worlds as proxy for multi-object tracking analysis," in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.
178. S. Richter, V. Vineet, S. Roth, and K. Vladlen, "Playing for data: Ground truth from computer games," in European
Conference on Computer Vision (ECCV), 2016.
179. S. Meister and D. Kondermann, "Real versus realistically rendered scenes for optical flow evaluation," in ITG Conference on Electronic Media Technology (CEMT), 2011.
180. D. Butler, J. Wulff, G. Stanley, and M. Black, "A naturalistic open source movie for optical flow evaluation," in European
Conference on Computer Vision (ECCV), 2012.
181. N. Onkarappa and A. Sappa, "Synthetic sequences and ground-truth flow field generation for algorithm validation,"
Multimedia Tools and Applications, vol. 74, no. 9, pp. 3121–3135, 2015.
182. N. Mayer, E. Ilg, P. Hausser, P. Fischer, D. Cremers, A. Dosovitskiy, and T. Brox, "A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation," in IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2016.
183. G. Taylor, A. Chosak, and P. Brewer, "OVVV: Using virtual worlds to design and evaluate surveillance systems," in IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), 2007.
184. A. Shafaei, J. Little, and M. Schmidt, "Play and learn: Using video games to train computer vision models," in BMVA
British Machine Vision Conference (BMVC), 2016.
185. J. Mar´ın, D. V´azquez, D. Ger´onimo, and A. L´opez, L´opez, "Learning appearance in virtual scenarios for pedestrian detection," in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2010.
186. D. Vazquez, A. M. L´opez, J. Mar´ın, D. Ponsa, and D. Ger´onimo, "Virtual and real world adaptation for pedestrian detection," Transactions of Pattern Recognition and Machine Analyses (PAMI), vol. 36, no. 4, pp. 797–809, 2014.
187. J. Xu, D. V´azquez, A. L´opez, J. Mar´ın, and D. Ponsa, "Learning a part-based pedestrian detector in a virtual world,"
Transactions on Intelligent Transportation Systems, vol. 15, no. 5, pp. 2121–2131, 2014.
188. A. Handa, V. Patraucean, V. Badrinarayanan, S. Stent, and R. Cipolla, "Understanding real world indoor scenes with synthetic data," in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.
189. C. D. Souza, A. Gaidon, Y. Cabon, and A. L´opez, "Procedural generation of videos to train deep action recognition networks," CoRR, vol. arXiv:1612.00881, 2016.
190. E. Tzeng, C. Devin, J. Hoffman, C. Finn, X. Peng, S. Levine, K. Saenko, and T. Darrell, "Towards adapting deep visuomotor representations from simulated to real environments," CoRR, vol. arXiv:1511.07111, 2015.
191. J. Xu, S. Ramos, D. V´azquez, and A. L´opez, "Hierarchical adaptive structural SVM for domain adaptation," International
Journal of Computer Vision, vol. 119, no. 2, pp. 159–178, 2016.
192. X. Wang, M. Yang, S. Zhu, and Y. Lin, "Regionlets for generic object detection," in IEEE International Conference on
Computer Vision (ICCV), 2013.
193. C. L. Zitnick and P. Doll´ar, "Edge boxes: Locating object proposals from edges," in European Conference on Computer
Vision (ECCV), 2014.
194. J. R. Uijlings, K. E. van de Sande, T. Gevers, and A. W. Smeulders, "Selective search for object recognition," International
Journal of Computer Vision, vol. 104, no. 2, pp. 154–171, 2013.
195. Y. Aytar and A. Zisserman, "Tabula rasa: Model transfer for object category detection," in IEEE International Conference on Computer Vision (ICCV), 2011.
196. N. Dalal and B. Triggs, "Histograms of oriented gradients for human detection," in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2005.
197. P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan, "Object detection with discriminatively trained part-based models," Transactions of Pattern Recognition and Machine Analyses (PAMI), vol. 32, no. 9, pp. 1627–1645, 198. J. Donahue, J. Hoffman, E. Rodner, K. Saenko, and T. Darrell, "Semi-supervised domain adaptation with instance constraints," in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2013.
199. F. Mirrashed, V. I. Morariu, B. Siddiquie, R. S. Feris, and L. S. Davis, "Domain adaptive object detection," in Workshops on Application of Computer Vision (WACV), 2013.
200. C. Zhang, R. Hammid, and Z. Zhang, "Taylor expansion based classifier adaptation: Application to person detection," in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2008.
Gabriela Csurka
201. P. M. Roth, S. Sternig, H. Grabner, and H. Bischof, "Classifier grids for robust adaptive object detection," in IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), 2009.
202. S. Stalder, H. Grabner, and L. V. Gool, "Exploring context to learn scene specific object detectors.," in International
Workshop on Performance Evaluation of Tracking and Surveillance (PETS), 2009.
203. K. Tang, V. Ramanathan, L. Fei-Fei, and D. Koller, "Shifting weights: Adapting object detectors from image to video," in Annual Conference on Neural Information Processing Systems (NIPS), 2012.
204. P. Sharma and R. Nevatia, "Efficient detector adaptation for object detection in a video," in IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2013.
205. A. Gaidon, G. Zen, and J. A. Rodriguez-Serrano, "Self-learning camera: Autonomous adaptation of object detectors to unlabeled video streams.," CoRR, vol. arXiv:1406.4296, 2014.
206. A. Gaidon and E. Vig, "Online domain adaptation for multi-object tracking," in BMVA British Machine Vision Conference(BMVC), 2015.
207. C. Rosenberg, M. Hebert, and H. Schneiderman, "Semisupervised self-training of object detection models," in Workshops on Application of Computer Vision (WACV/MOTION), 2005.
208. B. Wu and R. Nevatia, "Improving part based object detection by unsupervised, online boosting," in IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2007.
209. O. Javed, S. Ali, and M. Shah, "Online detection and classification of moving objects using progressively improving detectors," in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2005.
210. A. Levin, P. Viola, and Y. Freund, "Unsupervised improvement of visual detectors using co-training," in IEEE International Conference on Computer Vision (ICCV), 2013.
211. X. Wang, G. Hua, and T. X. han, "Detection by detections: Non-parametric detector adaptation for a video," in IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), 2012.
212. J. Xu, S. Ramos, D. V´azquez, and A. L´opez, "Domain adaptation of deformable part-based models," Transactions of Pattern Recognition and Machine Analyses (PAMI), vol. 36, no. 12, pp. 2367–2380, 2014.
213. P. Doll´ar, C. Wojek, B. Schiele, and P. Perona, "Pedestrian detection: a benchmark," in IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2009.
214. P. Sharma, C. Huang, and R. Nevatia, "Unsupervised incremental learning for improved object detection in a video," in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2012.
215. M. D. Breitenstein, F. Reichlin, E. Koller-Meier, B. Leibe, and L. Van Gool, "Online multi-person tracking-by-detection from a single, uncalibrated camera," Transactions of Pattern Recognition and Machine Analyses (PAMI), vol. 31, no. 9, pp. 1820–1833, 2011.
216. P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun, "Overfeat: Integrated recognition, localization and detection using convolutional networks," CoRR, vol. arXiv:1312.6229, 2013.
217. R. Girshick, J. Donahue, T. Darrell, and J. Malik, "Rich feature hierarchies for accurate object detection and semantic segmentation," in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014.
218. J. Hoffman, S. Guadarrama, E. Tzeng, R. Hu, J. Donahue, R. Girshick, T. Darrell, and K. Saenko, "LSDA: Large scale detection through adaptation," in Annual Conference on Neural Information Processing Systems (NIPS), 2014.
219. A. Raj, V. P. N. Namboodiri, and T. Tuytelaars, "Subspace alignment based domain adaptation for rcnn detector," in BMVA British Machine Vision Conference (BMVC), 2015.
220. R. Raina, A. Battle, H. Lee, B. Packer, and A. Ng, "Self-taught learning: transfer learning from unlabeled data," in International Conference on Machine Learning (ICML), 2007.
221. K. Muandet, D. Balduzzi, and B. Sch¨olkopf, "Domain generalization via invariant feature representation," in International
Conference on Machine Learning (ICML), 2013.
222. Z. Xu, W. Li, L. Niu, and D. Xu, "Exploiting low-rank structure from latent domains for domain generalization," in European Conference on Computer Vision (ECCV), 2014.
223. C. Gan, T. Yang, and B. Gong, "Learning attributes equals multi-source domain generalization," in IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2016.
224. D. Novotny, D. Larlus, and A. Vedaldi, "I have seen enough: Transferring parts across categories," in BMVA British
Machine Vision Conference (BMVC), 2016.
225. R. Caruana, "Multitask learning: A knowledge-based source of inductive bias," Machine Learning, vol. 28, pp. 41–75, 226. T. Evgeniou and M. Pontil, "Regularized multi-task learning," in ACM SIGKDD Conference on Knowledge Discovery and Data Mining (SIGKDD), 2004.
227. B. Romera-Paredes, H. Aung, N. Bianchi-Berthouze, and M. Pontil, "Multilinear multitask learning," in International
Conference on Machine Learning (ICML), 2013.
Domain Adaptation for Visual Applications: A Comprehensive Survey
228. E. Miller, N. Matsakis, and P. Viola, "Learning from one example through shared densities on transforms," in IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), 2010.
229. L. Fei-Fei, R. Fergus, and P. Perona, "One-shot learning of object categories," Transactions of Pattern Recognition and Machine Analyses (PAMI), vol. 28, no. 4, pp. 594–611, 2006.
230. Y. Mansour, M. Mohri, and A. Rostamizadeh, "Domain adaptation with multiple sources," in Annual Conference on
Neural Information Processing Systems (NIPS), 2009.
231. L. Duan, I. W. Tsang, D. Xu, and T.-S. Chua, "Domain adaptation from multiple sources via auxiliary classifiers," in International Conference on Machine Learning (ICML), 2009.
232. Q. Sun, R. Chattopadhyay, S. Panchanathan, and J. Ye, "A two-stage weighting framework for multi-source domain adaptation," in Annual Conference on Neural Information Processing Systems (NIPS), 2011.
233. B. Gong, K. Grauman, and F. Sha, "Reshaping visual datasets for domain adaptation," in Annual Conference on Neural
Information Processing Systems (NIPS), 2013.
234. T. Tommasi and B. Caputo, "The more you know, the less you learn: from knowledge transfer to one-shot learning of object categories," in BMVA British Machine Vision Conference (BMVC), 2009.
235. M. Fink, "Object classification from a single example utilizing class relevance pseudo-metrics," in Annual Conference on
Neural Information Processing Systems (NIPS), 2004.
236. E. Bart and S. Ullman, "Cross-generalization: Learning novel classes from a single example by feature replacement," in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2005.
237. K. Murphy, A. Torralba, and W. Freeman, "Using the forest to see the trees: a graphical model relating features, objects, and scenes," in Annual Conference on Neural Information Processing Systems (NIPS), 2003.
238. V. Ferrari and A. Zisserman, "Learning visual attributes.," in Annual Conference on Neural Information Processing Systems (NIPS), 2007.
239. C. H. Lampert, H. Nickisch, and S. Harmeling, "Learning to detect unseen object classes by between-class attribute transfer," in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2009.
240. M. Palatucci, D. Pomerleau, G. Hinton, and T. M. Mitchell, "Zero-shot learning with semantic output codes," in Annual
Conference on Neural Information Processing Systems (NIPS), 2009.
241. Y. Fu, T. Hospedales, T. Xiang, Z. Fu, and S. Gong, "Transductive multi-view embedding for zero-shot recognition and annotation," in European Conference on Computer Vision (ECCV), 2014.
242. V. Sharmanska, N. Quadrianto, and C. Lampert, "Augmented attribute representations," in European Conference on Computer Vision (ECCV), 2012.
243. R. Layne, T. Hospedales, and S. Gong, "Re-id: Hunting attributes in the wild," in BMVA British Machine Vision Conference (BMVC), 2014.
244. Y. Fu, T. Hospedales, T. Xiang, and S. Gong, "Learning multimodal latent attributes," Transactions of Pattern Recognition and Machine Analyses (PAMI), vol. 36, no. 2, pp. 303–316, 2014.
245. N. Patricia and B. Caputo, "Learning to learn, from transfer learning to domain adaptation: A unifying perspective," in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014.
246. S. Clinchant, G. Csurka, and B. Chidlovskii, "Transductive adaptation of black box predictions," in Annual Meeting of the Association for Computational Linguistics(ACL), 2016.
247. Y. Yang and T. M. Hospedales, "A unified perspective on multi-domain and multi-task learning," in International Conference on Learning representations (ICLR), 2015.
248. O. Chapelle, B. Sch ˜A¶lkopf, and A. Zien, Semi-supervised learning. MIT Press, 2006.
249. X. Zhu, A. Goldberg, R. Brachman, and T. Dietterich, Introduction to semi-supervised learning. Morgan & Claypool
Publishers, 2009.
250. B. Settles, ""active learning literature survey," Tech. Rep. Computer Sciences Technical Report 1648, University of Wisconsin-Madison, 2010.
251. X. Liao, Y. Xue, and L. Carin, "Logistic regression with an auxiliary data source," in International Conference on Machine
Learning (ICML), 2005.
252. X. Shi, W. Fan, and J. Ren, "Actively transfer domain knowledge," in Joint European Conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD), 2008.
253. Y. Chan and H. Ng, "Domain adaptation with active learning for word sense disambiguation," in Annual Meeting of the Association for Computational Linguistics(ACL), 2007.
254. P. Rai, A. Saha, H. Daum´e III, and S. Venkatasubramanian, "Domain adaptation meets active learning," in ACL Workshop on Active Learning for Natural Language Processing (ALNLP), 2010.
255. A. Saha, P. Rai, H. Daum´e III, S. Venkatasubramanian, and S. DuVall, "Active supervised domain adaptation," in Joint
European Conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD), 2011.
Gabriela Csurka
256. X. Wang, T.-K. Huang, and J. Schneider, "Active transfer learning under model shift," in International Conference on
Machine Learning (ICML), 2014.
257. S. Shalev-Shwartz, Online Learning: Theory, Algorithms, and Applications. PhD thesis, Hebrew University, 7 2007.
258. L. Bottou, Online Algorithms and Stochastic Approximations. Cambridge University Press, 1998.
259. S. Shalev-Shwartz, "Online learning and online convex optimization," Foundations and Trends in Machine Learning, vol. 4, no. 2, pp. 107–194, 2011.
260. Y. Zhang and D.-Y. Yeung, "Transfer metric learning by learning task relationships," in ACM SIGKDD Conference on
Knowledge Discovery and Data Mining (SIGKDD), 2010.
261. T. Kamishima, M. Hamasaki, and S. Akaho, "Trbagg: A simple transfer learning method and its application to personalization in collaborative tagging," in IEEE International Conference on Data Mining (ICDM), 2009.
262. E. Rodner and J. Denzler, "Learning with few examples by transferring feature relevance," in BMVA British Machine
Vision Conference (BMVC), 2009.
263. A. Acharya, E. Hruschka, J. Ghosh, and S. Acharyya, "Transfer learning with cluster ensembles," in ICML Workshop on
Unsupervised and Transfer Learning (WUTL), 2012.
264. Q. Yang, Y. Chen, G.-R. Xue, W. Dai, and Y. Yong, "Heterogeneous transfer learning for image clustering via the socialweb," in Annual Meeting of the Association for Computational Linguistics(ACL), 2009.
265. M. Chen, K. Q. Weinberger, and J. Blitzer, "Co-training for domain adaptation," in Annual Conference on Neural Information Processing Systems (NIPS), 2011.
266. J. Ah-Pine, M. Bressan, S. Clinchant, G. Csurka, Y. Hoppenot, and J.-M. Renders, "Crossing textual and visual content in different application scenarios," Multimedia Tools and Applications, vol. 42, no. 1, pp. 31–56, 2009.
267. Y. Jia, M. Salzmann, and T. Darrell, "Learning cross-modality similarity for multinomial data," in IEEE International
Conference on Computer Vision (ICCV), 2011.
268. J. Weston, S. Bengio, and N. Usunier, "Large scale image annotation: learning to rank with joint word-image embeddings," Machine Learning, vol. 81, no. 1, pp. 21–35, 2010.
269. N. Rasiwasia, J. C. Pereira, E. Coviello, G. Doyle, G. R. G. Lanckriet, R. Levy, and N. Vasconcelos, "A new approach to cross-modal multimedia retrieval," in ACM Multimedia, 2010.
270. A. Frome, G. S. Corrado, J. Shlens, S. Bengio, J. Dean, M. Ranzato, and T. Mikolov, "Devise: A deep visual-semantic embedding model," in Annual Conference on Neural Information Processing Systems (NIPS), 2013.
271. R. Fergus, L. Fei-Fei, P. Perona, and A. Zisserman, "Learning object categories from google's image search," in IEEE
International Conference on Computer Vision (ICCV), 2005.
272. X.-J. Wang, L. Zhang, X. Li, and W.-Y. Ma, "Annotating images by mining image search results," Transactions of Pattern
Recognition and Machine Analyses (PAMI), vol. 30, pp. 1919–1932, 2008.
273. F. Schroff, A. Criminisi, and A. Zisserman, "Harvesting image databases from the web," in IEEE International Conference on Computer Vision (ICCV), 2007.
274. A. Bergamo and L. Torresani, "Exploiting weakly-labeled web images to improve object classification: a domain adaptation approach," in Annual Conference on Neural Information Processing Systems (NIPS), 2010.
275. C. Gan, C. Sun, L. Duan, and B. Gong, "Webly-supervised video recognition by mutually voting for relevant web images and web video frames," in European Conference on Computer Vision (ECCV), 2016.
276. L. Duan, D. Xu, I. W. Tsang, and J. Luo, "Visual event recognition in videos by learning from web data," Transactions of Pattern Recognition and Machine Analyses (PAMI), vol. 34, no. 9, pp. 1667–1680, 2012.
277. C. Sun, S. Shetty, R. Sukthankar, and R. Nevatia, "Temporal localization of fine-grained actions in videos by domain transfer from web images," in ACM Multimedia, 2015.
278. T. Tommasi and T. Tuytelaars, "A testbed for cross-dataset analysis," in ECCV Workshop on Transferring and Adapting
Source Knowledge in Computer Vision (TASK-CV), 2014.
279. Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, "Gradient-based learning applied to document recognition," Proceedings of the IEEE, vol. 86, no. 11, pp. 2278–2324, 1998.
280. Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng, "Reading digits in natural images with unsupervised feature learning," in NIPS Workshop on Deep Learning and Unsupervised Feature Learning (DLUFL), 2011.IEICE TRANS. INF. & SYST., VOL.E94–D, NO.10 OCTOBER 2011
INVITED PAPER
Special Section on Information-Based Induction Sciences and Machine Learning
A Short Introduction to Learning to Rank
Hang LI†a), Nonmember
SUMMARY
Learning to rank refers to machine learning techniques for training the model in a ranking task. Learning to rank is useful for many applications in Information Retrieval, Natural Language Processing, and Data Mining. Intensive studies have been conducted on the problem and significant progress has been made,. This short paper gives an introduction to learning to rank, and it specifically explains the fundamental problems, existing approaches, and future work of learning to rank. Several learning to rank methods using SVM techniques are described in details. key words: learning to rank, information retrieval, natural language processing, SVM
Ranking Problem
Learning to rank can be employed in a wide variety of applications in Information Retrieval (IR), Natural Language Processing (NLP), and Data Mining (DM).
Typical applications are document retrieval, expert search, definition search, collaborative filtering, question answering, keyphrase extraction, document summarization, and machine translation. Without loss of generality, we take document retrieval as example in this article.
Document retrieval is a task as follows (Fig. 1). The system maintains a collection of documents. Given a query, the system retrieves documents containing the query words from the collection, ranks the documents, and returns the top ranked documents. The ranking task is performed by using a ranking model f(q, d) to sort the documents, where q denotes a query and d denotes a document.
Traditionally, the ranking model f(q, d) is created without training. In the BM25 model, for example, it is assumed that f(q, d) is represented by a conditional probability distribution P(r|q, d) where r takes on 1 or 0 as value and denotes being relevant or irreverent, and q and d denote a query and a document respectively. In Language Model for IR (LMIR), f(q, d) is represented as a conditional probability distribution P(q|d). The probability models can be calculated with the words appearing in the query and document, and thus no training is needed (only tuning of a small number of parameters is necessary).
A new trend has recently arisen in document retrieval, particularly in web search, that is, to employ machine learning techniques to automatically construct the ranking model f(q, d). This is motivated by a number of facts. At web
Manuscript received December 31, 2010.
Manuscript revised April 15, 2011.
†The author is with Microsoft Research Asia, No.5 Dan Ling
St., Haidian, Beijing, 100080, China. a) E-mail: hangli@microsoft.com
DOI: 10.1587/transinf.E94.D.1854
Fig. 1
Document retrieval.
Fig. 2
Learning to rank for document retrieval. search, there are many signals which can represent relevance, for example, the anchor texts and PageRank score of a web page. Incorporating such information into the ranking model and automatically constructing the ranking model using machine learning techniques becomes a natural choice.
In web search engines, a large amount of search log data, such as click through data, is accumulated. This makes it possible to derive training data from search log data and automatically create the ranking model. In fact, learning to rank has become one of the key technologies for modern web search.
We describe a number of issues in learning for ranking, including training and testing, data labeling, feature construction, evaluation, and relations with ordinal classification.
Training and Testing
Learning to rank is a supervised learning task and thus has training and testing phases (see Fig. 2).
The training data consists of queries and documents.
Copyright c⃝ 2011 The Institute of Electronics, Information and Communication Engineers
LI: A SHORT INTRODUCTION TO LEARNING TO RANK
Each query is associated with a number of documents. The relevance of the documents with respect to the query is also given. The relevance information can be represented in several ways. Here, we take the most widely used approach and assume that the relevance of a document with respect to a query is represented by a label, while the labels denote several grades (levels). The higher grade a document has, the more relevant the document is.
Suppose that Q is the query set and D is the document set. Suppose that Y = {1, 2, · · ·, l} is the label set, where labels represent grades. There exists a total order between the grades l ≻ l − 1 ≻ · · · ≻ 1, where ≻ denotes the order relation. Further suppose that {q1, q2, · · ·, qm} is the set of queries for training and qi is the i-th query.
Di = {di,1, di,2, · · ·, di,ni} is the set of documents associated with query qi and yi = {yi,1, yi,2, · · ·, yi,ni} is the set of labels associated with query qi, where ni denotes the sizes of Di and yi; di, j denotes the j-th document in Di; and yi, j ∈ Y denotes the j-th grade label in yi, representing the relevance degree of di, j with respect to qi. The original training set is denoted as S = {(qi, Di), yi}m i=1.
A feature vector xi, j
= φ(qi, di, j) is created from each query-document pair (qi, di, j), i = 1, 2, · · ·, m; j =
1, 2, · · ·, ni, where φ denotes the feature functions. That is to say, features are defined as functions of a query document pair. For example, BM25 and PageRank are typical features. Letting xi = {xi,1, xi,2, · · ·, xi,ni}, we represent the training data set as S ′ = {(xi, yi)}m i=1. Here x ∈ X and X ⊆ ℜd.
We aim to train a (local) ranking model f(q, d) = f(x) that can assign a score to a given query document pair q and d, or equivalently to a given feature vector x. More generally, we can also consider training a global ranking model
F(q, D) = F(x). The local ranking model outputs a single score, while the global ranking model outputs a list of scores.
Let the documents in Di be identified by the integers
{1, 2, · · ·, ni}. We define a permutation (ranking list) πi on
Di as a bijection from {1, 2, · · ·, ni} to itself. We use Πi to denote the set of all possible permutations on Di, use πi( j) to denote the rank (or position) of the j-th document (i.e., di, j) in permutation πi. Ranking is nothing but to select a permutation πi ∈ Πi for the given query qi and the associated documents Di using the scores given by the ranking model f(qi, di).
The test data consists of a new query qm+1 and associated documents Dm+1. T = {(qm+1, Dm+1)}. We create feature vector xm+1, use the trained ranking model to assign scores to the documents Dm+1, sort them based on the scores, and give the ranking list of documents as output πm+1.
The training and testing data is similar to, but different from the data in conventional supervised learning such as classification and regression. Query and its associated documents form a group. The groups are i.i.d. data, while the instances within a group are not i.i.d. data. A local ranking model is a function of a query and a document, or equivalently, a function of a feature vector derived from a query and a document.
Data Labeling
Currently there are two ways to create training data. The first one is by human judgments and the second one is by derivation from search log data. We explain the first approach here. Explanations on the second approach can be found in. In the first approach, a set of queries is randomly selected from the query log of a search system. Suppose that there are multiple search systems. Then the queries are submitted to the search systems and all the top ranked documents are collected. As a result, each query is associated with multiple documents.
Human judges are then asked to make relevance judgments on all the query document pairs. Relevance judgments are usually conducted at five levels, for example, perfect, excellent, good, fair, and bad.
Human judges make relevance judgments from the viewpoint of average users. For example, if the query is 'Microsoft', and the web page is microsoft.com, then the label is 'perfect'. Furthermore, the Wikipedia page about
Microsoft is 'excellent', and so on. Labels representing relevance are then assigned to the query document pairs. Relevance judgment on a query document pair can be performed by multiple judges and then majority voting can be conducted. Benchmark data sets on learning to rank have also been released.
Evaluation
The evaluation on the performance of a ranking model is carried out by comparison between the ranking lists output by the model and the ranking lists given as the ground truth.
Several evaluation measures are widely used in IR and other fields. These include NDCG (Normalized Discounted Cumulative Gain), DCG (Discounted Cumulative Gain), MAP(Mean Average Precision), and Kendall's Tau.
Given query qi and associated documents Di, suppose that πi is the ranking list (permutation) on Di and yi is the set of labels (grades) of Di. DCG measures the goodness of the ranking list with the labels. Specifically, DCG at position k is defined as:
DCG(k) =
� j:πi(j)≤k
G( j)D(πi( j)), where Gi(·) is a gain function and Di(·) is a position discount function, and πi( j) is the position of di, j in πi. The summation is taken over the top k positions in the ranking list πi. DCG represents the cumulative gain of accessing the information from position one to position k with discounts on the positions. NDCG is normalized DCG and NDCG at position k is defined as:
NDCG(k) = G−1 max,i(k)
� j:πi(j)≤k
G( j)D(πi( j)), where Gmax,i(k) is the normalizing factor and is chosen such
IEICE TRANS. INF. & SYST., VOL.E94–D, NO.10 OCTOBER 2011
Table 1
Examples of NDCG calculation.
Perfect ranking
Formula
Explanation(3, 3, 2, 2, 1, 1, 1) grades: 3, 2, 1
Eq. (1) gains(1, 0.63, 0.5, · · ·)
Eq. (2) discounts(7, 11.41, 12.91, · · ·)
Eq. (3)
DCG(1/7, 1/11.41, 1/12.91,· · ·) normalizers(1,1,1,· · ·)
Eq. (4)
NDCG
Imperfect ranking
Formula
Explanation(2, 3, 2, 3, 1, 1, 1) grades: 3, 2, 1
Eq. (1) gains(1, 0.63, 0.5, · · ·)
Eq. (2) discounts(3, 7.41, 8.91, · · ·)
Eq. (3)
DCG(1/7, 1/11.41, 1/12.91,· · ·) normalizers(0.43, 0.65, 0.69, · · ·)
Eq. (4)
NDCG that a perfect ranking π∗ i 's NDCG score at position k is 1. In a perfect ranking, the documents with higher grades are always ranked higher. Note that there can be multiple perfect rankings for a query and associated documents.
The gain function is normally defined as an exponential function of grade. That is to say, the satisfaction of accessing information exponentially increases when the grade of relevance of information increases.
G( j) = 2yi,j − 1, (1) where yi, j is the label (grade) of document di, j in ranking list πi. The discount function is normally defined as a logarithmic function of position. That is to say, the satisfaction of accessing information logarithmically decreases when the position of access increases.
D(πi( j)) =
1 log2(1 + πi( j)), (2) where πi( j) is the position of document di, j in ranking list πi.
Hence, DCG and NDCG at position k become
DCG(k) =
� j:πi(j)≤k
2yi,j − 1 log2(1 + πi( j)), NDCG(k) = G−1 max,i(k)
� j:πi(j)≤k
2yi,j − 1 log2(1 + πi( j)).
In evaluation, DCG and NDCG values are further averaged over queries.
Table 1 gives examples of calculating NDCG values of two ranking lists. NDCG (DCG) has the effect of giving high scores to the ranking lists in which relevant documents are ranked high. For perfect rankings, the NDCG value at each position is always one, while for imperfect rankings, the NDCG values are usually less than one.
MAP is another measure widely used in IR. In MAP, it is assumed that the grades of relevance are at two levels:
1 and 0. Given query qi, associated documents Di, ranking list πi on Di, and labels yi of Di, Average Precision for qi is defined as:
AP =
�ni j=1 P( j) · yi, j
�ni j=1 yi, j, where yi, j is the label (grade) of di, j and takes on 1 or 0 as a value, representing being relevant or irrelevant. P( j) for query qi is defined as:
P( j) =
� k:πi(k)≤πi(j) yi,k πi( j), where πi( j) is the position of di, j in πi. P( j) represents the precision until the position of di, j for qi. Note that labels are either 1 or 0, and thus 'precision' can be defined. Average Precision represents averaged precision over all the positions of documents with label 1 for query qi.
Average Precision values are further averaged over queries to become Mean Average Precision (MAP).
Relation with Ordinal Classification
Ordinal classification (also known as ordinal regression) is similar to ranking, but is also different. The input of ordinal classification is a feature vector x and the output is a label y representing a grade, where the grades are classes in a total order. The goal of learning is to construct a model which can assign a grade label y to a given feature vector x. The model mainly consists of a scoring function f(x). The model first assigns a real number to x using f(x) and then determines the grade y of x using a number of thresholds. Specifically, it partitions the real number axis into intervals and aligns each interval to a grade. It takes the grade of the interval that f(x) falls into as the grade of x.
In ranking, one cares more about accurate ordering of objects, while in ordinal classification, one cares more about accurate ordered-categorization of objects. A typical example of ordinal classification is product rating. For example, given the features of a movie, we are to assign a number of stars (ratings) to the movie. In that case, correct assignment of the number of stars is critical. In contrast, in ranking such as document retrieval, given a query, the objective is to correctly sort related documents, although sometimes training data and testing data are labeled at multiple grades as in ordinal classification. The number of documents to be ranked can vary from query to query. There are queries for which more relevant documents are available in the collection, and there are also queries for which only weakly relevant documents are available.
Formulation
We formalize learning to rank as a supervised learning task.
Suppose that X is the input space (feature space) consisting of lists of feature vectors, and Y is the output space consisting of lists of grades. Further suppose that x is an element of X representing a list of feature vectors and y is an element of Y representing a list of grades. Let P(X, Y) be an unknown joint probability distribution where random variable X takes x as its value and random variable Y takes y as its value.
Assume that F(·) is a function mapping from a list of feature vectors x to a list of scores. The goal of the learning task is to automatically learn a function ˆF(x) given training
LI: A SHORT INTRODUCTION TO LEARNING TO RANK
1857 data (x1, y1), (x2, y2),..., (xm, ym).
Each training instance is comprised of feature vectors xi and the corresponding grades yi (i = 1, · · ·, m). Here m denotes the number of training instances.
F(x) and y can be further written as F(x)
=( f(x1), f(x2), · · ·, f(xn)) and y = (y1, y2, · · ·, yn). The feature vectors represent objects to be ranked. Here f(x) denotes the local ranking function and n denotes the number of feature vectors and grades.
A loss function L(·, ·) is utilized to evaluate the prediction result of F(·). First, feature vectors x are ranked according to F(x), then the top n results of the ranking are evaluated using their corresponding grades y. If the feature vectors with higher grades are ranked higher, then the loss will be small. Otherwise, the loss will be large. The loss function is specifically represented as L(F(x), y). Note that the loss function for ranking is slightly different from the loss functions in other statistical learning tasks, in the sense that it makes use of sorting.
We further define the risk function R(·) as the expected loss function with respect to the joint distribution P(X, Y), R(F) =
�
X×Y
L(F(x), y)dP(x, y).
Given training data, we calculate the empirical risk function as follows, ˆR(F) = 1 m m
� i=1
L(F(xi), yi).
The learning task then becomes the minimization of the empirical risk function, as in other learning tasks. The minimization of the empirical risk function could be difficult due to the nature of the loss function (it is not continuous and it uses sorting). We can consider using a surrogate loss function L′(F(x), y).
The corresponding empirical risk function is defined as follows.
ˆR′(F) = 1 m m
� i=1
L′(F(xi), yi).
We can also introduce a regularizer to conduct minimization of the regularized empirical risk. In such cases, the learning problem becomes minimization of the (regularized) empirical risk function based on the surrogate loss.
Note that we adopt a machine learning formulation here. In IR, the feature vectors x are derived from a query and its associated documents. The grades y represent the relevance degrees of the documents with respect to the query.
We make use of a global ranking function F(·). In practice, it can be a local ranking function f(·). The possible number of feature vectors in x can be very large, even infinite. The evaluation (loss function) is, however, only concerned with n results.
In IR, the true loss functions can be those defined based on NDCG (Normalized Discounted Cumulative Gain) and MAP (Mean Average Precision). For example, we can have
L(F(x), y) = 1.0 − NDCG.
Note that the true loss functions (NDCG loss and MAP loss) makes use of sorting based on F(x).
For the surrogate loss function, there are also different ways to define it, which lead to different approaches to learning to rank. For example, one can define pointwise loss, pairwise loss, and listwise loss functions.
The squared loss used in Subset Regression is a pointwise surrogate loss. We call it pointwise loss, because it is defined on single objects.
L′(F(x), y) = n
� i=1( f(xi) − yi)2.
It is actually an upper bound of 1.0 − NDCG.
Pairwise losses can be the hinge loss, exponential loss, and logistic loss on pairs of objects, which are used in Ranking SVM, RankBoost, and RankNet, respectively.
They are also upper bounds of 1.0 − NDCG.
L′(F(x), y) = n−1
� i=1 n
� j=i+1 φ(sign(yi − yj), f(xi) − f(xj)), where it is assumed that L′ = 0 when yi = yj and φ is the hinge loss, exponential loss, or logistic loss function.
Listwise loss functions are defined on lists of objects, just like the true loss functions, and thus are more directly related to the true loss functions.
Different listwise loss functions are exploited in the listwise methods. For example, the loss function in AdaRank is a listwise loss.
L′(F(x), y) = exp(−NDCG), where NDCG is calculated on the basis of F(x) and y. Obviously, it is also an upper bound of 1.0 − NDCG.
Pointwise Approach
In the pointwise approach, the ranking problem (ranking creation) is transformed to classification, regression, or ordinal classification, and existing methods for classification, regression, or ordinal classification are applied. Therefore, the group structure of ranking is ignored in this approach.
The pointwise approach includes Subset Ranking, McRank, Prank, and OC SVM. We take the last one as an example and describe it in detail.
SVM for Ordinal Classification
The method proposed by Shashua & Levin utilizes a number of parallel hyperplanes as a ranking model. Their method, referred to as OC SVM in this article, learns the parallel hyperplanes by the large margin principle. In one implementation, the method tries to maximize a fixed margin for all the adjacent classes (grades)†.
Suppose that X ⊆ ℜd and Y = {1, 2, · · ·, l} where there
†The other method maximizes the sum of all margins.
IEICE TRANS. INF. & SYST., VOL.E94–D, NO.10 OCTOBER 2011
Fig. 3
SVM for oridinal classification. exists a total order on Y. x ∈ X is an object (feature vector) and y ∈ Y is a label representing a grade. Given object x, we aim to predict its label (grade) y. That is to say, this is an ordinal classification problem. We employ a number of linear models (parallel hyperplanes) ⟨w, x⟩−br, (r = 1, · · ·, l−1) to make the prediction, where w ∈ ℜd is a weight vector and br ∈ ℜ, (r = 1, · · ·, l) are biases satisfying b1 ≤ · · · ≤ bl−1 ≤ bl = +∞. The models correspond to parallel hyperplanes
⟨w, x⟩−br = 0 separating grades r and r+1, (r = 1, · · ·, l−1).
Figure 3 illustrates the model. If x satisfies ⟨w, x⟩ − br−1 ≥ 0 and ⟨w, x⟩ − br < 0, then y = r, (r = 1, · · ·, l). We can write it as minr∈{1,···,l}{r|⟨w, x⟩ − br < 0}.
Suppose that the training data is given as follows. For each grade r = 1, · · ·, l, there are mr instances: xr,i, i =
1, · · ·, mr. The learning task is formalized as the following
Quadratic Programming (QP) problem. minw,b,ξ 1
2||w||2 + C �l−1 r=1
�mr i=1(ξr,i + ξ∗ r+1,i) s. t. ⟨w, xr,i⟩ + br ≥ 1 − ξr,i
⟨w, xr+1,i⟩ + br ≤ 1 − ξ∗ r+1,i ξr,i ≥ 0, ξ∗ r+1,i ≥ 0 i = 1, · · ·, mr, r = 1, · · ·, l − 1 m = m1 + · · · + ml, where xr,i denotes the i-th instance in the r-th grade, ξr+1,i and ξ∗ r+1,i denote the corresponding slack variables, || · || denotes L2 norm, m denotes the number of training instances, and C > 0 is a coefficient. The method tries to separate the instances in the neighboring grades with the same margin.
Pairwise Approach
In the pairwise approach, ranking is transformed into pairwise classification or pairwise regression.
In the former case, a classifier for classifying the ranking orders of document pairs is created and is employed in the ranking of documents. In the pairwise approach, the group structure of ranking is also ignored.
The pairwise approach includes Ranking SVM, RankBoost, RankNet, GBRank, IR SVM, Lambda Rank, and LambdaMART. We introduce
Ranking SVM and IR SVM in this article.
Ranking SVM
We can learn a classifier, such as SVM, for classifying the Fig. 4
Example of ranking problem.
Fig. 5
Transformation to pairwise classification. order of pairs of objects and utilize the classifier in the ranking task. This is the idea behind the Ranking SVM method proposed by Herbrich et al..
Figure 4 shows an example of the ranking problem.
Suppose that there are two groups of objects (documents associated with two queries) in the feature space. Further suppose that there are three grades (levels). For example, objects x1, x2, and x3 in the first group are at three different grades. The weight vector w corresponds to the linear function f(x) = ⟨w, x⟩ which can score and rank the objects.
Ranking objects with the function is equivalent to projecting the objects into the vector and sorting the objects according to the projections on the vector. If the ranking function is 'good', then there should be an effect that objects at grade 3 are ranked ahead of objects at grade 2, etc. Note that objects belonging to different groups are incomparable.
Figure 5 shows that the ranking problem in Fig. 4 can be transformed to Linear SVM classification. The differences between two feature vectors at different grades in the same group are treated as new feature vectors, e.g., x1 − x2, x1 − x3, and x2 − x3. Furthermore, labels are also assigned to the new feature vectors. For example, x1 − x2, x1 − x3, and x2 − x3 are positive. Note that feature vectors at the same grade or feature vectors from different groups are not utilized to create new feature vectors. One can train a Linear SVM classifier which separates the new feature vectors as shown in Fig. 5. Geometrically, the margin in the SVM model represents the closest distance between the projections of object pairs in two grades. Note that the hyperplane of the SVM classifier passes the original and the positive and negative instances form corresponding pairs. For example, x1 − x2 and x2 − x1 are positive and negative instances respectively. The weight vector w of the SVM classifier corresponds to the ranking function. In fact, we can discard the LI: A SHORT INTRODUCTION TO LEARNING TO RANK
Grade: 3, 2, 1
Documents are represented by their grades
Example 1: ranking-1: 2 3 2 1 1 1 1 ranking-2: 3 2 1 2 1 1 1
Example 2: ranking for query-1: 3 2 2 1 1 1 1 ranking for query-2: 3 3 2 2 2 1 1 1 1 1
Fig. 6
Example ranking lists. negative instances in learning, because they are redundant.
Training data is given as {((x(1) i, x(2) i ), yi)}, i = 1, · · ·, m where each instance consists of two feature vectors(x(1) i, x(2) i ) and a label yi ∈ {+1, −1} denoting which feature vector should be ranked ahead.
The learning of Ranking SVM is formalized as the following QP problem. minw,ξ 1
2||w||2 + C �m i=1 ξi s. t. yi⟨w, x(1) i
− x(2) i ⟩ ≥ 1 − ξi ξi ≥ 0 i = 1,..., m, where x(1) i and x(2) i denote the first and second feature vectors in a pair of feature vectors, || · || denotes L2 norm, m denotes the number of training instances, and C > 0 is a coefficient.
It is equivalent to the following non-constrained optimization problem, i.e., the minimization of the regularized hinge loss function. min w m
� i=1
[1 − yi⟨w, x(1) i
− x(2) i ⟩]+ + λ||w||2, (5) where [x]+ denotes function max(x, 0) and λ =
2C.
IR SVM
IR SVM proposed by Cao et al. is an extension of Ranking SVM for Information Retrieval (IR), whose idea can be applied to other applications as well.
Ranking SVM transforms ranking into pairwise classification, and thus it actually makes use of the 0-1 loss in the learning process. There exists a gap between the loss function and the IR evaluation measures. IR SVM attempts to bridge the gap by modifying 0-1 loss, that is, conducting cost sensitive learning of Ranking SVM.
We first look at the problems caused by straightforward application of Ranking SVM to document retrieval, using examples in Fig. 6.
One problem with the direction application of Ranking
SVM is that Ranking SVM equally treats document pairs across different grades. Example 1 indicates the problem.
There are two rankings for the same query. The documents at positions 1 and 2 are swapped in ranking-1 from the perfect ranking, while the documents at positions 3 and 4 are swapped in ranking-2 from the perfect ranking. There is only one error for each ranking in terms of the 0-1 loss, or difference in order of pairs. They have the same effect on the Fig. 7
Modified hinge loss functions. training of Ranking SVM, which is not desirable. Ranking2 should be better than ranking-1, from the viewpoint of IR, because the result on its top is better. Note that to have high accuracy on top-ranked documents is crucial for an IR system, which is reflected in the IR evaluation measures.
Another issue with Ranking SVM is that it equally treats document pairs from different queries. In example
2, there are two queries and the numbers of documents associated with them are different. For query-1 there are 2 document pairs between grades 3-2, 4 document pairs between grades 3-1, 8 document pairs between grades 2-1, and in total 14 document pairs. For query-2, there are 31 document pairs. Ranking SVM takes 14 instances (document pairs) from query-1 and 31 instances (document pairs) from query-2 for training. Thus, the impact on the ranking model from query-2 will be larger than the impact from query-1. In other words, the model learned will be biased toward query2. This is in contrast to the fact that in IR evaluation queries are evenly important. Note that the numbers of documents usually vary from query to query.
IR SVM addresses the above two problems by changing the 0-1 pairwise classification into a cost sensitive pairwise classification. It does so by modifying the hinge loss function of Ranking SVM.
Specifically, it sets different losses for document pairs across different grades and from different queries. To emphasize the importance of correct ranking on the top, the loss function heavily penalizes errors related to the top. To increase the influence of queries with less documents, the loss function heavily penalizes errors from the queries.
Figure 7 plots the shapes of different hinge loss functions with different penalty parameters. The x-axis represents yf(x) and the y-axis represents loss. When yf(x(1) i
− x(2) i ) ≥ 1, the losses are zero. When yf(x(1) i
− x(2) i ) < 1, the losses are represented by linearly decreasing functions with different slopes. If the slope equals −1, then the function is the normal hinge loss function. IR SVM modifies the hinge loss function, specifically modifies the slopes for different grade pairs and different queries. It assigns higher weights to document pairs across important grade pairs and assigns normalization weights to document pairs according to queries.
The learning of IR SVM is equivalent to the following optimization problem. Specifically, the minimization of the IEICE TRANS. INF. & SYST., VOL.E94–D, NO.10 OCTOBER 2011 modified regularized hinge loss function, min w m
� i=1 τk(i)μq(i)[1 − yi⟨w, x(1) i
− x(2) i ⟩]+ + λ||w||2, where [x]+ denotes max(x, 0), λ =
2C, and τk(i) and μq(i) are weights. See the loss function of Ranking SVM (5).
Here τk(i) represents the weight of instance (document pair) i whose label pair belongs to the k-th type. Xu et al. propose a heuristic method to determine the value of τk. The method takes the average drop in NDCG@1 when randomly changing the positions of documents belonging to the grade pair as the value of a grade pair τk. Moreover, μq(i) represents the weight of instance (document pair) i which is from query q. The value of μq(i) is simply determined by
|nq|, where nq is the number of document pairs for query q.
The equivalent QP problem is as below. minw,ξ 1
2||w||2 + Ci
�m i=1 ξi s. t. yi⟨w, x(1) i
− x(2) i ⟩ ≥ 1 − ξi, Ci = τk(i)μq(i)
2λ ξi ≥ 0, i = 1,..., m.
Listwise Approach
The listwise approach addresses the ranking problem in a more straightforward way. Specifically, it takes ranking lists as instances in both learning and prediction.
The group structure of ranking is maintained and ranking evaluation measures can be more directly incorporated into the loss functions in learning.
The listwise approach includes ListNet, ListMLE, AdaRank, SVM MAP, and Soft Rank.
SVM MAP and related methods are explained in this article.
SVM MAP
The algorithm SVM MAP developed by Yue et al. is designed to directly optimize MAP, but it can be easily extended to optimize NDCG. Xu et al. further generalize it to a group of algorithms.
In ranking, for query qi the ranking model f(xij) assigns a score to each associated document dij or feature vector xij where xij is the feature vector derived from qi and dij.
The documents di (feature vectors xi) are then sorted based on their scores and a permutation denoted as πi is obtained.
For simplicity, suppose that the ranking model f(xij) is a linear model: f(xij) = ⟨w, xij⟩, (6) where w denotes a weight vector.
Suppose that labels for the feature vectors xi are also given as yi. We consider using a scoring function S (xi, πi) to measure the goodness of ranking πi. S (xi, πi) is defined as
S (xi, πi) = ⟨w, σ(xi, πi)⟩, Objects: A, B, C fA = ⟨w, xA⟩, fB = ⟨w, xB⟩, fC = ⟨w, xC⟩
Suppose fA > fB > fC
For example:
Permutation1: ABC
Permutation2: ACB
S ABC = 1
6 ⟨w, ((xA − xB) + (xB − xC) + (xA − xC))⟩
S ACB = 1
6 ⟨w, ((xA − xC) + (xC − xB) + (xA − xB))⟩
S ABC > S ACB
Fig. 8
Example of scoring function. where w is still the weight vector and vector σ(xi, πi) is defined as σ(xi, πi) =
2 ni(ni − 1)
� k,l:k<l
[zkl(xik − xil)], where zkl = +1, if πi(k) < πi(l) (xik is ranked ahead of xil in πi), and zkl = −1, otherwise. Recall that ni is the number of documents associated with query qi.
For query qi, we can calculate S (xi, πi) for each permutation πi and select the permutation ˜πi with the largest score:
˜πi = arg max πi∈Πi S (xi, πi), (7) where Πi denotes the set of all possible permutations for xi.
It can be easily shown that the ranking ˜πi selected by
Eq. (7) is equivalent to the ranking created by the ranking model f(xij) (when both of them are linear functions). Figure 8 gives an example. It is easy to verify that both f(x) and S (xi, π) will output ABC as the most preferable ranking(permutation).
In learning, we would ideally create a ranking model that can maximize the accuracy in terms of a listwise evaluation measure on training data, or equivalently, minimizes the loss function defined below, L( f) = m
� i=1(E(π∗ i, yi) − E(πi, yi)), (8) where πi is the permutation on feature vector xi by ranking model f and yi is the corresponding list of grades. E(πi, yi) denotes the evaluation result of πi in terms of an evaluation measure (e.g., NDCG). Usually E(π∗ i, yi) = 1.
We view the problem of learning a ranking model as the following optimization problem in which the following loss function is minimized.
�m i=1 maxπ∗ i ∈Π∗ i ;πi∈Πi\Π∗ i
�
E(π∗ i, yi) − E(πi, yi)
�
·
�
[S (xi, π∗ i ) ≤ S (xi, πi)
�
], (9) where [[c]] is one if condition c is satisfied, otherwise it is zero. π∗ i ∈ Π∗ i ⊆ Πi denotes any of the perfect permutations for qi.
The loss function measures the loss when the most preferred ranking list by the ranking model is not the perfect ranking list. One can prove that the true loss function such as that in (8) is upper-bounded by the new loss function in LI: A SHORT INTRODUCTION TO LEARNING TO RANK
The loss function (9) is still not continuous and differentiable. We can consider using continuous, differentiable, and even convex upper bounds of the loss function (9).
1) The 0-1 function in (9) can be replaced with its upper bounds, for example, hinge functions, yielding
�m i=1 maxπ∗ i ∈Π∗ i,πi∈Πi\Π∗ i
�
E(π∗ i, yi) − E(πi, yi)
�
·
�
1 −
�
S (xi, π∗ i ) − S (xi, πi)
��
�m i=1
� maxπ∗ i ∈Π∗ i,πi∈Πi\Π∗ i
��
E(π∗ i, yi) − E(πi, yi)
�
−
�
S (xi, π∗ i ) − S (xi, πi)
���
+, 2) The max function can also be replaced with its upper bound, the sum function. This is because � i xi ≥ maxi xi if xi ≥ 0 holds for all i.
3) Relaxations 1 and 2 can be applied simultaneously.
For example, using the hinge function and taking the true loss as 1.0 − MAP, we obtain SVM MAP. More precisely, SVM MAP solves the following QP problem: minw;ξ≥0 1
2||w||2 + C m
�m i=1 ξi s.t.
∀i, ∀π∗ i ∈ Π∗ i, ∀πi ∈ Πi \ Π∗ i :
S (xi, π∗ i ) − S (xi, πi) ≥ E(π∗ i, yi) − E(πi, yi) − ξi, (10) where C is a coefficient and ξi is the maximum loss among all the losses for permutations of query qi.
Equivalently, SVM MAP minimizes the following regularized hinge loss function
�m i=1
� maxπ∗ i ∈Π∗ i ;πi∈Πi\Π∗ i (E(π∗ i, yi) − E(πi, yi))
− (S (xi, π∗ i ) − S (xi, πi))
�
+ + λ||w||2.
Intuitively, the first term calculates the total maximum loss when selecting the best permutation for each of the queries.
Specifically, if the difference between the permutations
S (xi, π∗ i ) − S (xi, πi) is less than the difference between the corresponding evaluation measures E(π∗ i, yi)−E(πi, yi), then there will be a loss, otherwise not. Next, the maximum loss is selected for each query and they are summed up over all the queries.
Since c · [[x ≤ 0]] < [c − x]+ holds for all c ∈ ℜ+ and x ∈ ℜ, it is easy to see that the loss in (11) also bounds the true loss function in (8).
Ongoing and Future Work
It is still necessary to develop more advanced technologies for learning to rank. There are also many open questions with regard to theory and applications of learning to rank,. Current and future research directions include
• training data creation
• semi-supervised learning and active learning
• feature learning
• scalable and efficient training
• domain adaptation and multi-task learning
• ranking by ensemble learning
• global ranking
• ranking of nodes in a graph.
References
 T.Y. Liu, "Learning to rank for information retrieval," Foundations and Trends in Information Retrieval, vol.3, no.3, pp.225–331, 2009.
 H. Li, "Learning to rank for information retrieval and natural language processing," Synthesis Lectures on Human Language Technologies, Morgan & Claypool, 2011.
 W.B. Croft, D. Metzler, and T. Strohman, Search Engines - Information Retrieval in Practice, Pearson Education, 2009.
 T.Y. Liu, J. Xu, T. Qin, W. Xiong, and H. Li, "LETOR: Benchmark dataset for research on learning to rank for information retrieval,"
Proc. SIGIR 2007 Workshop on Learning to Rank for Information
Retrieval, 2007.
 K. J¨arvelin and J. Kek¨al¨ainen, "IR evaluation methods for retrieving highly relevant documents," Proc. 23rd annual international ACM
SIGIR conference on Research and development in information retrieval, pp.41–48, SIGIR '00, New York, NY, USA, 2000.
 D. Cossock and T. Zhang, "Subset ranking using regression," COLT
'06: Proc. 19th Annual Conference on Learning Theory, pp.605–
 R. Herbrich, T. Graepel, and K. Obermayer, Large Margin rank boundaries for ordinal regression, MIT Press, Cambridge, MA, Y. Freund, R.D. Iyer, R.E. Schapire, and Y. Singer, "An efficient boosting algorithm for combining preferences," J. Machine Learning
Research, vol.4, pp.933–969, 2003.
 C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender, "Learning to rank using gradient descent,"
ICML '05: Proc. 22nd international conference on Machine learning, pp.89–96, 2005.
 W. Chen, T.Y. Liu, Y. Lan, Z.M. Ma, and H. Li, "Ranking measures and loss functions in learning to rank," NIPS '09, 2009.
 P. Li, C. Burges, and Q. Wu, "McRank: Learning to rank using multiple classification and gradient boosting," in Advances in Neural Information Processing Systems 20, ed. J. Platt, D. Koller, Y. Singer, and S. Roweis, pp.897–904, MIT Press, Cambridge, MA, 2008.
 K. Crammer and Y. Singer, "Pranking with ranking," NIPS, pp.641–
 A. Shashua and A. Levin, "Ranking with large margin principle:
Two approaches," in Advances in Neural Information Processing
Systems 15, ed. S.T.S. Becker and K. Obermayer, MIT Press, 2003.
 Z. Zheng, H. Zha, T. Zhang, O. Chapelle, K. Chen, and G. Sun, "A general boosting method and its application to learning ranking functions for web search," in Advances in Neural Information Processing Systems 20, ed. J. Platt, D. Koller, Y. Singer, and S. Roweis, pp.1697–1704, MIT Press, Cambridge, MA, 2008.
 Y. Cao, J. Xu, T.Y. Liu, H. Li, Y. Huang, and H.W. Hon, "Adapting ranking SVM to document retrieval," SIGIR' 06, pp.186–193, 2006.
 C. Burges, R. Ragno, and Q. Le, "Learning to rank with nonsmooth cost functions," in Advances in Neural Information Processing Systems 18, pp.395–402, MIT Press, Cambridge, MA, 2006.
 Q. Wu, C.J.C. Burges, K.M. Svore, and J. Gao, "Adapting boosting for information retrieval measures," Inf. Retr., vol.13, no.3, pp.254–
 Z. Cao, T. Qin, T.Y. Liu, M.F. Tsai, and H. Li, "Learning to rank:
From pairwise approach to listwise approach," ICML '07: Proc.
24th international conference on Machine learning, pp.129–136, F. Xia, T.Y. Liu, J. Wang, W. Zhang, and H. Li, "Listwise approach to learning to rank: Theory and algorithm," ICML '08: Proc. 25th international conference on Machine learning, pp.1192–1199, New
York, NY, USA, 2008.
 J. Xu and H. Li, "AdaRank: A boosting algorithm for information retrieval," SIGIR '07: Proc. 30th annual international ACM SIGIR conference on Research and development in information retrieval, pp.391–398, New York, NY, USA, 2007.
IEICE TRANS. INF. & SYST., VOL.E94–D, NO.10 OCTOBER 2011
 Y. Yue, T. Finley, F. Radlinski, and T. Joachims, "A support vector method for optimizing average precision," Proc. 30th annual international ACM SIGIR conference, pp.271–278, 2007.
 M. Taylor, J. Guiver, S. Robertson, and T. Minka, "SoftRank: Optimizing non-smooth rank metrics," WSDM '08: Proc. international conference on Web search and web data mining, pp.77–86, New
York, NY, USA, 2008.
 J. Xu, T.Y. Liu, M. Lu, H. Li, and W.Y. Ma, "Directly optimizing evaluation measures in learning to rank," SIGIR '08: Proc. 31st annual international ACM SIGIR conference on Research and development in information retrieval, pp.107–114, New York, NY, USA, O. Chapelle, Y. Chang, and T.Y. Liu, "Future directions in learning to rank," J. Machine Learning Research - Proceedings Track, vol.14, pp.91–100, 2011.
Hang Li is senior researcher and research manager in Web Search and Mining Group at
Microsoft Research Asia. He joined Microsoft
Research in June 2001. Prior to that, He worked at the Research Laboratories of NEC Corporation. He obtained a B.S. in Electrical Engineering from Kyoto University in 1988 and a M.S. in Computer Science from Kyoto University in 1990. He earned his Ph.D. in Computer Science from the University of Tokyo in 1998. He is interested in statistical learning, information retrieval, data mining, and natural language processing.Learning with Marginalized Corrupted Features
Laurens van der Maaten lvdmaaten@gmail.com
Delft University of Technology, Mekelweg 4, 2628 CD Delft, THE NETHERLANDS
Minmin Chen mc15@cec.wustl.edu
Stephen Tyree swtyree@wustl.edu
Kilian Q. Weinberger kilian@wustl.edu
Washington University, St. Louis, MO 63130, USA
Abstract
The goal of machine learning is to develop predictors that generalize well to test data.
Ideally, this is achieved by training on very large (infinite) training data sets that capture all variations in the data distribution.
In the case of finite training data, an effective solution is to extend the training set with artificially created examples—which, however, is also computationally costly. We propose to corrupt training examples with noise from known distributions within the exponential family and present a novel learning algorithm, called marginalized corrupted features (MCF), that trains robust predictors by minimizing the expected value of the loss function under the corrupting distribution— essentially learning with infinitely many (corrupted) training examples. We show empirically on a variety of data sets that MCF classifiers can be trained efficiently, may generalize substantially better to test data, and are more robust to feature deletion at test time.
1. Introduction
In the hypothetical scenario with infinite data drawn from the data distribution P, even a simple classifier such as nearest neighbor (Cover & Hart, 1967) becomes close to optimal (viz. its error is twice the Bayes error).
In the more realistic scenario with a finite training set, some variations in the data distribution will not be captured and the learned classifier performs worse at test time than during training. In
Proceedings of the 30 th International Conference on Machine Learning, Atlanta, Georgia, USA, 2013.
JMLR:
W&CP volume 28. Copyright 2013 by the author(s). this paper, we propose an algorithm to train a classifier from infinite training data, by corrupting the existing finite training examples with a fixed noise distribution.
So instead of approximating the exact statistics of P with finite data, we approximate a slightly modified data distribution P′ with infinite data.
Our augmented data distribution P′ follows a simple stochastic rule: pick one of the finite training examples uniformly at random and transform it with some predefined corrupting distribution. Many corrupting distributions are possible, but we focus on: i) Poisson corruption and ii) blankout / dropout corruption (random deletion of features). The Poisson corruption model is of interest when the data comprises count vectors, e.g., in document classification. It is particularly appealing as it introduces no additional hyper-parameters and, in our results, improves the test accuracy on almost all data sets and loss functions. Robustness to blankout corruption is of interest in settings with heavy-tailed feature distributions, and in the "nightmare at test time" scenario (Globerson & Roweis, 2006) in which some of the features are blanked out during testing(e.g., due to sensors failing or because the feature computation exceeds a time budget).
Previous work (Burges & Sch¨olkopf, 1997) explicitly augments the training set with additional examples that are corrupted through similar transformations.
Although the simplicity of such an approach is appealing, it lacks elegance and the computational cost of processing the additional corrupted training examples is prohibitive for most real-world problems. We show that it is efficient to train predictors on an infinite amount of corrupted copies of the training data by marginalizing out the corrupting distribution. In particular, we focus on empirical risk minimization and derive analytical solutions for a large family of corrupting distributions and a variety of loss functions.
Learning with Marginalized Corrupted Features
In summary, we make the following contributions: i) we introduce learning with marginalized corrupted features (MCF), a framework that regularizes classifiers by marginalizing out feature corruptions; ii) we derive analytical solutions for quadratic, exponential, and logistic loss functions for a range of corrupting distributions; and iii) on several real-world data sets, we show that training with MCF may lead to better classifiers than training with common l1 or l2-norm regularizers.
2. Related work
Next to work that explicitly corrupts training data (Burges & Sch¨olkopf, 1997), several prior studies consider implicit approaches to classifying objects that are subject to corruptions. Most of these studies minimize the loss under an adversarial worst-case scenario.
In particular, Globerson & Roweis (2006) and Dekel &
Shamir (2008) propose minimax formulations in which the maximum loss of an example over all
�D
K
� possible corrupted examples that blank out K features is minimized. Others (Bhattacharyya et al., 2004; Shivaswamy et al., 2006; Trafalis & Gilbert, 2007; Xu et al., 2009) also use minimax approaches that minimize the loss under a worst-case scenario, but corrupt the data by adding a constant that is uniformly drawn from
[−u, u] to each of the features.
Chechik et al. (2008) propose an algorithm that maximizes the margin in the subspace of the observed features for each training instance to deal with randomly deleted features. Teo et al. (2008) generalize the worstcase scenario to obtain invariances to transformations such as image rotations or translations. Their framework incorporates several prior formulations on learning with invariants as special cases; e.g., Herbrich &
Graepel (2004), who generalized SVMs to be invariant under polynomial input transformations.
Br¨uckner et al. (2012) study a worst-case scenario in which an adversary changes the data distribution to minimize a function that may be antagonistic to the loss.
Prior work differs from MCF in that the corruption is not computed analytically in expectation. In particular, existing approaches have two disadvantages: i) they are complex and computationally expensive and ii) they minimize the loss of a worst-case scenario that is unlikely to be encountered in practice. By contrast, MCF scales linearly in the number of training examples and considers an average-case instead of a worstcase scenario.
Moreover, MCF can readily be used with a variety of loss functions and corruption models.
Bishop (1995) and Webb (1994) proposed approaches that can be viewed as approximations to MCF for additive noise with small variance. By contrast, MCF is exact and can be used with noise distributions with potentially very high variance as well. MCF was inspired by recent successes of denoising autoencoders (Glorot et al., 2011; Vincent et al., 2008). Since autoencoders are non-linear, the marginalization over the corrupting distribution cannot be performed analytically in such models. Linear denoising autoencoders (Chen et al., 2012) can be viewed as a special case of MCF that aim to minimize the expected value of the reconstruction error under blankout corruption.
3. Learning with Marginalized
Corrupted Features (MCF)
To derive the marginalized corrupted features (MCF) framework, we start by defining a corrupting distribution that specifies how training observations x are transformed into corrupted versions ˜x.
We assume that the corrupting distribution factorizes over dimensions1 and that each individual distribution is a member of the natural exponential family: p(˜x|x)=
D
� d=1
PE(˜xd|xd; ηd), (1) where ηd indicates (user-defined) parameters of the corrupting distribution on dimension d. We also assume that the corrupting distribution is unbiased, i.e. that E[˜x]p(˜x|x) = x. This assumption is necessary because biases in the corrupting distribution may lead to undesired scale changes in the parameters ηd. Most corrupting distributions of interest are unbiased or can easily be made so; examples for PE are the unbiased "blankout" noise (Vincent et al., 2008), Gaussian noise, Laplace noise, and Poisson noise.
Explicit corruption. A simple approach to improving the generalization of a classifier using a corrupting distribution is to follow the spirit of Burges &
Sch¨olkopf (1997) by selecting each element of the training set D = {(xn,yn)}N n=1 and corrupting it M times, following (1). For each xn, this results in corresponding corrupted observations ˜xnm (with m = 1,..., M).
This leads to the construction of a new data set ˜D of size | ˜D| = MN. This extended data set can be used for training by minimizing:
L( ˜D; Θ) =
N
� n=1
M
M
� m=1
L(˜xnm, yn; Θ), with ˜xnm ∼ p(˜xnm|xn), Θ the set of model parameters, and L(x, y; Θ) the loss function of the model.
1For Gaussian noise models, this assumption is unnecessary: we can also work with non-isotropic Gaussian noise.
Learning with Marginalized Corrupted Features
Distribution
PDF
E[˜xnd]p(˜xnd|xnd)
V [˜xnd]p(˜xnd|xnd)
Blankout noise p(˜xnd = 0) = qd p(˜xnd =
1−qd xnd) = 1−qd xnd qd
1−qd x2 nd
Gaussian noise p(˜xnd|xnd) = N(˜xnd|xnd, σ2) xnd σ2
Laplace noise p(˜xnd|xnd) = Lap(˜xnd|xnd, λ) xnd
2λ2
Poisson noise p(˜xnd|xnd) = Poisson(˜xnd|xnd) xnd xnd
Table 1. The probability density function (PDF), mean, and variance of corrupting distributions of interest.
These quantities can be plugged into Eq. (3) to obtain the expected value under the corrupting distribution of the quadratic loss.
Implicit corruption. Although such an approach is effective, it lacks elegance and comes with high computational costs, as the minimization of L( ˜D; Θ) scales linearly in the number of corrupted observations. It is, however, of interest to consider the limiting case in which M →∞. In this case, we can apply the weak law of large numbers and rewrite
M
�M m=1 L(˜xm, ym; Θ) as its expectation (Duda et al., 2001, §2.10.2):
L(D; Θ) =
N
� n=1
E[L(˜xn, yn; Θ)]p(˜xn|xn).
Minimizing the expected value of the loss under the corruption model leads to a new approach for training predictors that we refer to as learning with marginalized corrupted features (MCF).
3.1. Specific loss functions
The tractability of (2) depends on the choice of loss function and corrupting distribution PE. In this section, we show that for linear predictors that employ a quadratic or exponential loss function, the required expectations under p(˜x|x) in (2) can be computed analytically for all corrupting distributions in the natural exponential family. For linear predictors with logistic loss, we derive a practical upper bound on the expected loss under p(˜x|x), which serves as surrogate loss.
Quadratic loss.
Assuming2 a label variable y∈{−1, +1}, the expected value of the quadratic loss under corrupting distribution p(˜x|x) is given by:
L(D; w) =
N
� n=1
E
�� wT˜xn − yn
�2� p(˜xn|xn)
= wT
�
N
� n=1
E[˜xn]E[˜xn]T + V [˜xn]
� w
− 2
�
N
� n=1 ynE [˜xn]
�T w + N, 2The same derivations are applicable to regression settings in which y is a continuous variable. where V [x] is a diagonal D×D matrix storing the variances of x, and all expectations are under p(˜xn|xn).
Eq. (3) is convex irrespective of what corruption model is used; the optimal solution w∗ is given by: w∗ =
�
N
� n=1
E[˜xn]E[˜xn]T + V [˜xn]
�−1�
N
� n=1 ynE [˜xn]
�
To minimize the expected quadratic loss under the corruption model, we only need to compute the variance of the corrupting distribution, which is practical for all exponential-family distributions. The mean is always xnd because our corrupting distributions are unbiased.
Table 1 gives an overview of the variances of corrupting distributions of interest. (In blankout corruption, we scale the value of "preserved" features by
1−qd to ensure that the corrupting distribution is unbiased.)
An interesting setting of MCF with quadratic loss occurs when the corrupting distribution p(˜x|x) is an isotropic Gaussian distribution with mean x and variance σ2I. For such a Gaussian corruption model, we obtain as special case (Chapelle et al., 2000):
L(D; w) = wT
�
N
� n=1 xnxT n
� w
− 2
�
N
� n=1 ynxn
�T w + σ2NwTw + N, which is the standard l2-regularized quadratic loss with regularization parameter σ2N. Interestingly, using MCF with Laplace noise also leads to ridge regression (with regularization parameter 2λ2N).
Exponential loss. The expected value of the exponential loss under corruption model p(˜x|x) is:
L(D; w) =
N
� n=1
E
� e−ynwT˜xn� p(˜xn|xn)
=
N
� n=1
D
� d=1
E
� e−ynwd˜xnd� p(˜xnd|xnd), Learning with Marginalized Corrupted Features
Distribution
E[exp(−ynwd˜xnd)]p(˜xnd|xnd)
Blankout noise qd + (1 − qd) exp(−ynwd
1−qd xnd)
Gaussian noise exp(−ynwdxnd + 1
2σ2y2 nw2 d)
Laplace noise(1 − λ2y2 nw2 d)−1 exp(−ynwdxnd)
Poisson noise exp(xnd(exp(−ynwd) − 1))
Table 2. Moment-generating functions (MGFs) of various corrupting distributions. These quantities can be plugged into equations (4) and (5) to obtain the expected value of the loss (or surrogate) under the corrupting distribution of the exponential and logistic loss functions, respectively. where we used the assumption that the corruption is independent across features. The above equation can be recognized as a product of moment-generating functions E[exp(tnd˜xnd)] with tnd =−ynwd. The momentgenerating function (MGF) can be computed for all corrupting distributions in the natural exponential family.
An overview of these MGFs is given in Table 2. Because the expected exponential loss is a convex combination of convex functions, it is itself convex irrespective of what corruption model is used.
The derivation above can readily be extended to multiclass exponential loss (Zhu et al., 2006) (with K classes) by replacing the weight vector w by a D×K weight matrix W, and by replacing the labels y by label vectors y = {1, −
K−1}K with �K k=1 yk = 0.
Logistic loss.
In the case of the logistic loss, the solution to (2) cannot be computed in closed form.
Instead, we derive an upper bound, which can be minimized as a surrogate loss:
L(D; w) =
N
� n=1
E
� log
�
1 + e−ynwT˜xn
�� p(˜xn|xn)
≤
N
� n=1 log
�
D
� d=1
E
� e−ynwd˜xnd� p(˜xnd|xnd)
�
Herein, we have made use of Jensen's inequality to upper-bound E[log z]. In the upper bound, we again recognize a product of MGFs, which can be computed in closed-form for corrupting distributions in the natural exponential family (see Table 2). The upper bound on the expected logistic loss is convex whenever the moment-generating function is log-linear in wd, e.g., for blankout corruption.
Again, the above derivation can readily be extended to multi-class logistic loss (by redefining the labels y to be label vectors y ∈ {0, 1}K with �K k=1 yk = 1; and by defining the loss as the logarithm of the softmaxprobability of making the correct prediction).
Case study. As an illustrative example, we show the upper bound of the logistic loss (5), where inputs are corrupted with the Poisson distribution:
L(D; w) ≤
N
� n=1 log
�
1 + exp
� D
� d=1 xnd(e−ynwd − 1)
��
Remarkably, this regularized loss does not have any additional hyper-parameters. Further, the sum over all features can still be computed efficiently for sparse data by summing only over non-zero entries in xn.
Computational complexity. The use of MCF does not impact the computational complexity of training: the complexity of the training algorithms remains linear in N. The additional training time for minimizing quadratic loss with MCF is negligible, because the computation time is dominated by the inversion of a D ×D-matrix. For exponential and logistic loss, we empirically found that computing the gradient of the MCF loss was 2× and 10× slower than computing the gradient of the "normal" loss, respectively.
4. Experiments
We perform experiments comparing MCF predictors with standard predictors on three tasks: i) document classification based on word-count features using MCF with blankout and Poisson corruption; ii) image classification based on bag-of-visual-word features using MCF with blankout and Poisson corruption; and iii) classification of objects in the "nightmare at test time" scenario using MCF with blankout corruption. The three experiments are described separately below.
Code to reproduce the results of our experiments is available from http://homepage. tudelft.nl/19j49/mcf.
4.1. Document classification
We first test MCF predictors with blankout and Poisson corruption on document classification tasks.
Specifically, we focus on three data sets: the Dmoz data set, the Reuters data set, and the Amazon review benchmark set (Blitzer et al., 2007).
Data sets. The Dmoz open directory (http://www. dmoz.org) contains a large collection of webpages arranged into a tree hierarchy. The subset we use contains N = 8, 980 webpages from the K = 16 categories in the top level of the hierarchy. Each webpage is represented by a bag-of-words representation with
D=16, 498 words. The Reuters data set is a collection of documents that appeared on the Reuters newswire in 1987. Documents with multiple labels were removed from the data, resulting in a set of N = 8, 293 documents from K = 65 categories.
The bag-of-words
Learning with Marginalized Corrupted Features
Classification error
DMOZ
Reuters
Blankout corruption level q
Quadratic loss
Exponential loss
Logistic loss
Blankout MCF (Qua.)
Blankout MCF (Exp.)
Blankout MCF (Log.)
Poisson MCF (Qua.)
Poisson MCF (Exp.)
Poisson MCF (Log.)
Amazon / Kitchen
Amazon / DVD
Amazon / Electronics
Amazon / Books
Figure 1. Classification errors of MCF predictors using blankout and Poisson corruption – as a function of the blankout corruption level q – on the Amazon, Dmoz, and Reuters data sets for l2-regularized quadratic, exponential, and quadratic loss functions. Classification errors are represented on the y-axis, whereas the blankout corruption level q is represented on the x-axis. The case of MCF with blankout corruption and q = 0 corresponds to a standard l2-regularized classifier.
Figure best viewed in color. representation contains D = 18, 933 words for each document. The four Amazon data sets consist of approximately N = 6, 000 reviews of four types of products: books, DVDs, electronics, and kitchen appliances. Each review is represented by a bag-of-words representation of the D=20, 000 most common words.
On the Dmoz and Reuters data sets, the task is to classify the documents into one of the predefined categories. On the Amazon data set, the task is to decide whether a review is positive or negative.
Setup. On the Dmoz and Reuters data sets, we use a fixed training set of 75% of the data, and evaluate the performance of our predictors on a fixed test set of 25% of the data. On the Amazon data set, we follow the experimental setup of Blitzer et al. (2007) by using a fixed division of the data into approximately 2, 000 training examples and about 4, 000 test examples (the exact numbers vary slightly between tasks). All experiments are performed using linear classifiers that are trained with l2-regularization; the amount of l2regularization is determined via cross-validation. We consider quadratic, exponential, and logistic loss functions (both with and without MCF). The minimization of the (expected) exponential and logistic losses is performed by running Mark Schmidt's minFuncimplementation of L-BFGS until convergence or until a predefined maximum number of iterations is reached.
All predictors included a bias term that is neither regularized nor corrupted. In our experiments with MCF using blankout corruption, we use the same noise level for each feature, i.e. we assume that ∀d : qd = q. On all data sets, we first investigate the performance of MCF as a function of the corruption level q (but we still cross-validate over the l2-regularizer). In a second set of experiments, we cross-validate over the blankout corruption parameter q and study to what extent the performance (improvements) of MCF depend on the amount of available training data. (MCF with Poisson corruption has no additional hyper-parameters, as a result of which it requires no extra cross-validations.)
Results. Figure 1 shows the test error of our MCF predictors on all data sets as a function of the blankout corruption level q. Herein, corruption level q = 0 corresponds to the baseline predictors, i.e. to predictors that do not employ MCF. The results show: i) that MCF improves over standard predictors for both blankout corruption (for all corruption levels q) and Poisson corruption on five out of six tasks; ii) that MCF with Poisson corruption leads to signifiLearning with Marginalized Corrupted Features cant performance improvements over standard classifiers whilst introducing no additional hyperparameters; and iii) that the best performance tends to be achieved by MCF with blankout corruption with high corruption levels, i.e. when q is in the order of 0.8. The best-performing MCF classifiers reduce the test errors by up to 22% on the Amazon data if q is properly set.
In many of the experiments with MCF-trained losses(in particular, when blankout corruption is used), we also observe that the optimal level of l2-regularization is 0. This shows that MCF has a regularizing effect in itself, rendering additional regularization superfluous.
Figure 2 presents the results of a second set of experiments on Dmoz and Reuters in which we study how the performance of MCF depends on the amount of training data. For each training set size, we repeat the experiment five times with randomly sub-sampled training sets; the figure reports the mean test errors and the corresponding standard deviations. The results show that classifiers trained with MCF (solid curves) significantly outperform their counterparts without MCF(dashed curves).
The performance improvement is consistent irrespective of the training set size, viz. up to 25% on the Dmoz data set.
Explicit vs. implicit feature corruption.
Figure 3 shows the classification error on Amazon (books) when a classifier without MCF is trained on the data set with additional explicitly corrupted samples, as formulated in (3). Specifically, we use the blankout corruption model with q set by cross-validation for each setting, and we train the classifiers with quadratic loss and l2-regularization. The graph shows a clear trend that the error decreases when the training set contains more corrupted versions of the original training data, i.e. with higher M in eq. (3). The graph illustrates that the best performance is obtained as M approaches infinity, which is equivalent to MCF with blankout corruption (marker in the bottom right; q=0.9).
4.2. Image classification
We performed image-classification experiments on the CIFAR-10 data set (Krizhevsky, 2009), which is a subset of the 80 million tiny images (Torralba et al., 2008).
The data set contains RGB images with 10 classes of size 32×32, and consists of a fixed training and test set of 50, 000 and 10, 000 images, respectively.
Setup. We follow the experimental setup of Coates et al. (2011): we whiten the training images and extract a set of 7 × 7 image patches on which we apply k-means clustering (with k =2048) to construct a codebook. Next, we slide a 7×7 pixel window over each image and identify the nearest prototype in the # of labeled training data
Blankout / DMOZ
Poisson / DMOZ
Blankout / Reuters
Poisson / Reuters
Quadratic loss (L2)
Exponential loss (L2)
Logistic loss (L2)
Blankout MCF (qua.)
Blankout MCF (exp.)
Blankout MCF (log.)
Quadratic loss (L2)
Exponential loss (L2)
Logistic loss (L2)
Poisson MCF (qua.)
Poisson MCF (exp.)
Poisson MCF (log.)
Classification error
Figure 2. The performance of standard and MCF classifiers with blankout and Poisson corruption models as a function of training set size on the Dmoz and Reuters data sets. Both the standard and MCF predictors employ l2regularization. Figure best viewed in color.
Learning with Marginalized Corrupted Features
64 128 256 inf
Noise
Error
Explicit corruption
Implicit corruption (MCF)
# of corrupted copies
Classification error
…... 11
Figure 3. Comparison between MCF and explicitly adding corrupted examples to the training set (for quadratic loss) on the Amazon (books) data using blankout corruption.
Training with MCF is equivalent to using infinitely many corrupted copies of the training data. codebook for each window location. We construct an image descriptor3 by subdividing the image into four equally sized quadrants and counting the number of times each prototype occurs in each quadrant. This leads to a descriptor of dimensionality D = 4×2048.
We do not normalize the descriptors, because all images have the same size.
We train MCF predictors with blankout and Poisson corruption on the full set of training images, cross-validating over a range of l2regularization parameters. The generalization error is evaluated on the test set.
Results. The results are reported in Table 3. The baseline classifiers (without MCF) are comparable to the 68.8% accuracy reported by Coates et al. (2011) with exactly the same experimental setup (except for exponential loss). The results illustrate the potential of MCF classifiers to improve the prediction performance on bag-of-visual-words features, in particular, when using quadratic or logistic loss in combination with a Poisson corruption model. Although our focus in this section is to merely illustrate the potential of MCF on image classification tasks, it is worth noting that the best results in Table 3 match those of a highly non-linear mean-covariance RBMs trained on the same data (Ranzato & Hinton, 2010), despite our use of very simple visual features and of linear classifiers.
4.3. Nightmare at test time
To test the performance of our MCF predictors with blankout corruption under the "nightmare at test
3This way of extracting the image features is referred to by Coates et al. (2011) as k-means with hard assignment, average pooling, patch size 7×7, and stride 1.
Quadr.
Expon.
Logist.
No MCF
Poisson MCF
Blankout MCF
Table 3. Classification errors obtained on the CIFAR-10 data set with MCF classifiers trained on simple spatialpyramid bag-of-visual-words features. time" scenario, we perform experiments on the MNIST handwritten digits data set. The MNIST data set contains N = 60, 000 training and 10, 000 test images of size D=28×28=784 pixels with K =10 classes.
Setup. We train our predictors on the full training set, and evaluate their performance on versions of the test set in which a certain percentage of the pixels are randomly blanked out, i.e. set to zero. We compare the performance of our MCF-predictors (using blankout corruption) with that of standard predictors that use l1 or l2-regularized quadratic, exponential, logistic, and hinge loss. As before, we use cross-validation to determine the optimal value of the regularization parameter. For MCF predictors, we also cross-validate over the blankout corruption level q (again, we use the same noise level for each feature, i.e. ∀d : qd = q).
In addition to the comparisons with standard predictors, we also compare the performance of MCF with that of FDROP (Globerson & Roweis, 2006), which is a state-of-the-art algorithm for the "nightmare at test time" setting that minimizes the hinge loss under an adversarial worst-case scenario.
The performances are reported as a function of the feature-deletion percentage in the test set, i.e. as a function of the probability with which a pixel in the test set is switched off.
Following the experimental setting of Globerson & Roweis (2006), we perform the cross-validation for each deletion percentage independently, i.e. we create a small validation set with the same feature-deletion level and use it to determine the best regularization parameters and blankout corruption level q for that percentage of feature deletions.
Results. Figure 4 shows the performance of our predictors as a function of the percentage of deletions in the test images.
The figure shows the performance for all three loss functions with MCF (solid lines) and without MCF (dashed lines). The performance of a standard predictor using hinge loss is shown as a red dashed line; the performance of FDROP is shown as a black dashed line. The results presented in Figure 4 clearly illustrate the ability of MCF with blankout corruption to produce predictors that are robust to the "nightmare at test time" scenario: MCF improves the performance substantially for all three loss funcLearning with Marginalized Corrupted Features
Quadratic loss (L2)
Exponential loss (L2)
Logistic loss (L2)
Hinge loss (L2)
Hinge loss (FDROP)
MCF quadratic loss
MCF exponential loss
MCF logistic loss
Figure 4. Classification errors of standard and MCF predictors with a blankout corruption model – trained using three different losses – and of FDROP (Globerson &
Roweis, 2006) on the MNIST data set using the "nightmare at test time" scenario. Classification errors are represented on the y-axis, whereas the percent of features that are deleted at test time is represented on the x-axis. Figure best viewed in color. tions considered. For instance, in the case in which
50% of the pixels in the test images is deleted, the performance improvements obtained using MCF for quadratic, exponential, and logistic loss are 40%, 47%, and 60%, respectively. Further, the results also indicate that MCF-losses may outperform FDROP: our
MCF logistic loss outperforms FDROP's worst-case hinge loss across the board4. This is particularly impressive as the standard hinge loss performs surprisingly better than standard logistic loss on this data set(the improvement of FDROP over the generic hingeloss is relatively modest). This result suggests that it is better to consider an average-case than a worst-case scenario in the "nightmare at test time" setting.
5. Discussion and Future Work
We presented an approach to learn classifiers by marginalizing corrupted features (MCF). Specifically, MCF trains predictors by introducing corruption on the training examples, which is marginalized out in the expectation of the loss function. We minimize the expected loss with respect to the model parameters.
Our experimental results show that MCF predictors with blankout and Poisson corruption perform very well in the context of bag-of-words features.
MCF with Poisson corruption is particularly interesting for such count features, as it improves classification per4Quadratic and exponential losses perform somewhat worse because they are less appropriate for linear classifiers, but even they outperform FDROP for large numbers of feature deletions in the test data. formances without introducing any additional hyperparameters. As a disclaimer, care must be taken when applying MCF with Poisson corruption on data sets with outliers. Poisson corruption may emphasize outliers in the expected loss because the variance of a Poisson distribution is equal to its mean, and because our loss functions are not robust to outliers. A solution to this problem may be to redefine the corruption distribution to p(˜xd|xd) = Pois(˜xnd| min{xnd, u}) for some cutoff parameter u≥0.
In most of our experiments with MCF, the l2regularizer parameter(which was set by crossvalidation) ended up very close to zero. This implies that MCF in itself has a regularizing effect. At the same time, MCF with blankout corruption also appears to prevent weight undertraining (Sutton et al., 2005): it encourages the weight on each feature to be non-zero, in case this particular feature survives the corruption.
Our experiments also reveal that MCF with blankout corruption produces predictors that are more robust to the "nightmare at test time" scenario, making it useful in learning settings in which features in the test data may be missing. Learning with MCF is quite different from previous approaches for this setting (Dekel & Shamir, 2008; Globerson & Roweis, 2006): it does not learn under a worst-case scenario, but the (arguably) more common average-case scenario by considering all possible corrupted observations. This has the advantage that it is computationally much cheaper and that it allows for incorporating prior knowledge. For instance, if the data is generated by a collection of unreliable sensors, knowledge on the sensor reliability may be used to set the qd-parameters.
In future work, we intend to explore extensions of MCF to regression and structured prediction, as well as to investigate if MCF can be employed for kernel machines. We also plan to explore in more detail what corruption models p(˜x|x) are useful for what types of data, and how these corruption models regularize classifiers (Ng, 2004). Further, MCF could be used in the training of neural networks with a single layer of hidden units: blankout noise on the hidden nodes can improve the performance of the networks (LeCun et al., 1990; Sietsma & Dow, 1991; Hinton et al., 2012) and can be marginalized out analytically. A final interesting direction is to investigate the effect of marginalizing corrupted labels (Lawrence & Sch¨olkopf, 2001).
Acknowledgements
LvdM is supported by FP7 Social Signal Processing (SSPNet). KQW, MC, and ST are supported by NIH grant U01
1U01NS073457-01 and NSF grants 1149882 and 1137211.
The authors thank Fei Sha for helpful discussions.
Learning with Marginalized Corrupted Features
References
Bhattacharyya, C., Pannagadatta, K.S., and Smola, A.J.
A second order cone programming formulation for classifying missing data. In Advances in Neural Information
Processing Systems, pp. 153–160, 2004.
Bishop, C.M. Training with noise is equivalent to tikhonov regularization. Neural Computation, 7(1):108–116, 1995.
Blitzer, J., Dredze, M., and Pereira, F. Biographies, Bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. In Association for Computational Linguistics, volume 45, pp. 440, 2007.
Br¨uckner, M., Kanzow, C., and Scheffer, T. Static prediction games for adversarial learning problems. Journal of Machine Learning Research, 12:2617–2654, 2012.
Burges, C.J.C. and Sch¨olkopf, B. Improving the accuracy and speed of support vector machines. Advances in Neural Information Processing Systems, 9:375–381, 1997.
Chapelle, O., Weston, J., Bottou, L., and Vapnik, V. Vicinal risk minimization. In Advances in Neural Information Processing Systems, pp. 416–422, 2000.
Chechik, G., Heitz, G., Elidan, G., Abbeel, P., and Koller, D. Max-margin classification of data with absent features. Journal of Machine Learning Research, 9(Jan):
1–21, 2008.
Chen, M., Xu, Z., Weinberger, K.Q., and Sha, F. Marginalized denoising autoencoders for domain adaptation. In
Proceedings of the International Conference on Machine
Learning, pp. 767–774, 2012.
Coates, A., Lee, H., and Ng, A.Y. An analysis of singlelayer networks in unsupervised feature learning. In Proceedings of the International Conference on Artificial Intelligence & Statistics, JMLR W&CP 15, pp. 215–223, Cover, T. and Hart, P. Nearest neighbor pattern classification. IEEE Transactions on Information Theory, 13(1):21–27, 1967.
Dekel, O. and Shamir, O. Learning to classify with missing and corrupted features. In Proceedings of the International Conference on Machine Learning, pp. 216–223, Duda, R.O., Hart, P.E., and Stork, D.G. Pattern Classification. Wiley Interscience Inc., 2001.
Globerson, A. and Roweis, S. Nightmare at test time: Robust learning by feature deletion. In Proceedings of the International Conference on Machine Learning, pp. 353–
Glorot, X., Bordes, A., and Bengio, Y. Domain adaptation for large-scale sentiment classification: A deep learning approach. In Proceedings of the International Conference on Machine Learning, pp. 513–520, 2011.
Herbrich, R. and Graepel, T. Invariant pattern recognition by semidefinite programming machines. In Advances in Neural Information Processing Systems, volume 16, pp.
Hinton, G.E., Srivastava, N., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R.R. Improving neural networks by preventing co-adaptation of feature detectors, 2012.
Krizhevsky, A. Learning multiple layers of features from tiny images.
Technical report, University of Toronto, Lawrence, N.D. and Sch¨olkopf, B.
Estimating a kernel
Fisher discriminant in the presence of label noise.
In
Proceedings of the International Conference in Machine
Learning, pp. 306–313, 2001.
LeCun, Y., Denker, J.S., and Solla, S.A. Optimal brain damage. In Advances in Neural Information Processing
Systems, pp. 598–605, 1990.
Ng, A.Y. Feature selection, l1 vs. l2 regularization, and rotational invariance.
In Proceedings of International
Conference on Machine Learning, pp. 78–85, 2004.
Ranzato, M. and Hinton, G.E. Modeling pixel means and covariances using factorized third-order Boltzmann machines. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2551–2558, Shivaswamy, P.K., Bhattacharyya, C., and Smola, A.J.
Second order cone programming approaches for handling missing and uncertain data. Journal of Machine Learning Research, 7:1283–1314, 2006.
Sietsma, J. and Dow, R.J.F. Creating artificial neural networks that generalize. Neural Networks, 4:67–79, 1991.
Sutton, C., Sindelar, M., and McCallum, A. Feature bagging: Preventing weight undertraining in structured discriminative learning. Technical Report IR-402, University of Massachusetts, 2005.
Teo, C.H., Globerson, A., Roweis, S., and Smola, A. Convex learning with invariances. Advances in Neural Information Processing Systems, 20:1489–1496, 2008.
Torralba, A., Fergus, R., and Freeman, W.T. 80 million tiny images: A large dataset for non-parametric object and scene recognition. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 30(11):1958–1970, Trafalis, T. and Gilbert, R. Robust support vector machines for classification and computational issues. Optimization Methods and Software, 22(1):187–198, 2007.
Vincent, P., Larochelle, H., Bengio, Y., and Manzagol, P.A.
Extracting and composing robust features with denoising autoencoders.
In Proceedings of the International
Conference on Machine Learning, pp. 1096–1103, 2008.
Webb, A.R. Functional approximation by feed-forward networks: a least-squares approach to generalization. IEEE
Transactions on Neural Networks, 5(3):363–371, 1994.
Xu, H., Caramanis, C., and Mannor, S. Robustness and regularization of support vector machines.
Journal of Machine Learning Research, 10:1485–1510, 2009.
Zhu, J., Rosset, S., Zou, H., and Hastie, T. Multi-class AdaBoost. Technical Report 430, Department of Statistics, University of Michigan, 2006.Published as a conference paper at ICLR 2017
ADVERSARIAL FEATURE LEARNING
Jeff Donahue jdonahue@cs.berkeley.edu
Computer Science Division
University of California, Berkeley
Philipp Krähenbühl philkr@utexas.edu
Department of Computer Science
University of Texas, Austin
Trevor Darrell trevor@eecs.berkeley.edu
Computer Science Division
University of California, Berkeley
ABSTRACT
The ability of the Generative Adversarial Networks (GANs) framework to learn generative models mapping from simple latent distributions to arbitrarily complex data distributions has been demonstrated empirically, with compelling results showing that the latent space of such generators captures semantic variation in the data distribution. Intuitively, models trained to predict these semantic latent representations given data may serve as useful feature representations for auxiliary problems where semantics are relevant. However, in their existing form, GANs have no means of learning the inverse mapping – projecting data back into the latent space. We propose Bidirectional Generative Adversarial Networks (BiGANs) as a means of learning this inverse mapping, and demonstrate that the resulting learned feature representation is useful for auxiliary supervised discrimination tasks, competitive with contemporary approaches to unsupervised and self-supervised feature learning.
INTRODUCTION
Deep convolutional networks (convnets) have become a staple of the modern computer vision pipeline.
After training these models on a massive database of image-label pairs like ImageNet (Russakovsky et al., 2015), the network easily adapts to a variety of similar visual tasks, achieving impressive results on image classification (Donahue et al., 2014; Zeiler & Fergus, 2014; Razavian et al., 2014) or localization (Girshick et al., 2014; Long et al., 2015) tasks. In other perceptual domains such as natural language processing or speech recognition, deep networks have proven highly effective as well (Bahdanau et al., 2015; Sutskever et al., 2014; Vinyals et al., 2015; Graves et al., 2013). However, all of these recent results rely on a supervisory signal from large-scale databases of hand-labeled data, ignoring much of the useful information present in the structure of the data itself.
Meanwhile, Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) have emerged as a powerful framework for learning generative models of arbitrarily complex data distributions. The GAN framework learns a generator mapping samples from an arbitrary latent distribution to data, as well as an adversarial discriminator which tries to distinguish between real and generated samples as accurately as possible. The generator's goal is to "fool" the discriminator by producing samples which are as close to real data as possible. When trained on databases of natural images, GANs produce impressive results (Radford et al., 2016; Denton et al., 2015).
Interpolations in the latent space of the generator produce smooth and plausible semantic variations, and certain directions in this space correspond to particular semantic attributes along which the data distribution varies. For example, Radford et al. (2016) showed that a GAN trained on a database of human faces learns to associate particular latent directions with gender and the presence of eyeglasses.
A natural question arises from this ostensible "semantic juice" flowing through the weights of generators learned using the GAN framework: can GANs be used for unsupervised learning of rich feature representations for arbitrary data distributions? An obvious issue with doing so is that the 1 arXiv:1605.09782v7 [cs.LG] 3 Apr 2017
Published as a conference paper at ICLR 2017 features data z
G
G(z) x
E
E(x)
G(z), z x, E(x)
D
P(y)
Figure 1: The structure of Bidirectional Generative Adversarial Networks (BiGAN). generator maps latent samples to generated data, but the framework does not include an inverse mapping from data to latent representation.
Hence, we propose a novel unsupervised feature learning framework, Bidirectional Generative
Adversarial Networks (BiGAN). The overall model is depicted in Figure 1. In short, in addition to the generator G from the standard GAN framework (Goodfellow et al., 2014), BiGAN includes an encoder E which maps data x to latent representations z. The BiGAN discriminator D discriminates not only in data space (x versus G(z)), but jointly in data and latent space (tuples (x, E(x)) versus(G(z), z)), where the latent component is either an encoder output E(x) or a generator input z.
It may not be obvious from this description that the BiGAN encoder E should learn to invert the generator G. The two modules cannot directly "communicate" with one another: the encoder never
"sees" generator outputs (E(G(z)) is not computed), and vice versa. Yet, in Section 3, we will both argue intuitively and formally prove that the encoder and generator must learn to invert one another in order to fool the BiGAN discriminator.
Because the BiGAN encoder learns to predict features z given data x, and prior work on GANs has demonstrated that these features capture semantic attributes of the data, we hypothesize that a trained
BiGAN encoder may serve as a useful feature representation for related semantic tasks, in the same way that fully supervised visual models trained to predict semantic "labels" given images serve as powerful feature representations for related visual tasks. In this context, a latent representation z may be thought of as a "label" for x, but one which came for "free," without the need for supervision.
An alternative approach to learning the inverse mapping from data to latent representation is to directly model p(z|G(z)), predicting generator input z given generated data G(z). We'll refer to this alternative as a latent regressor, later arguing (Section 4.1) that the BiGAN encoder may be preferable in a feature learning context, as well as comparing the approaches empirically.
BiGANs are a robust and highly generic approach to unsupervised feature learning, making no assumptions about the structure or type of data to which they are applied, as our theoretical results will demonstrate. Our empirical studies will show that despite their generality, BiGANs are competitive with contemporary approaches to self-supervised and weakly supervised feature learning designed specifically for a notoriously complex data distribution – natural images.
Dumoulin et al. (2016) independently proposed an identical model in their concurrent work, exploring the case of a stochastic encoder E and the ability of such models to learn in a semi-supervised setting.
PRELIMINARIES
Let pX(x) be the distribution of our data for x ∈ ΩX (e.g. natural images). The goal of generative modeling is capture this data distribution using a probabilistic model. Unfortunately, exact modeling of this probability density function is computationally intractable (Hinton et al., 2006; Salakhutdinov
& Hinton, 2009) for all but the most trivial models. Generative Adversarial Networks (GANs) (GoodPublished as a conference paper at ICLR 2017 fellow et al., 2014) instead model the data distribution as a transformation of a fixed latent distribution pZ(z) for z ∈ ΩZ. This transformation, called a generator, is expressed as a deterministic feed forward network G : ΩZ → ΩX with pG(x|z) = δ (x − G(z)) and pG(x) = Ez∼pZ [pG(x|z)]. The goal is to train a generator such that pG(x) ≈ pX(x).
The GAN framework trains a generator, such that no discriminative model D : ΩX �→ can distinguish samples of the data distribution from samples of the generative distribution. Both generator and discriminator are learned using the adversarial (minimax) objective min
G max
D V (D, G), where V (D, G) := Ex∼pX [log D(x)] + Ex∼pG [log (1 − D(x))]
�
��
�
Ez∼pZ[log(1−D(G(z)))]
Goodfellow et al. (2014) showed that for an ideal discriminator the objective C(G)
:= maxD V (D, G) is equivalent to the Jensen-Shannon divergence between the two distributions pG and pX.
The adversarial objective 1 does not directly lend itself to an efficient optimization, as each step in the generator G requires a full discriminator D to be learned. Furthermore, a perfect discriminator no longer provides any gradient information to the generator, as the gradient of any global or local maximum of V (D, G) is 0. To provide a strong gradient signal nonetheless, Goodfellow et al. (2014) slightly alter the objective between generator and discriminator updates, while keeping the same fixed point characteristics. They also propose to optimize (1) using an alternating optimization switching between updates to the generator and discriminator. While this optimization is not guaranteed to converge, empirically it works well if the discriminator and generator are well balanced.
Despite the empirical strength of GANs as generative models of arbitrary data distributions, it is not clear how they can be applied as an unsupervised feature representation. One possibility for learning such representations is to learn an inverse mapping regressing from generated data G(z) back to the latent input z. However, unless the generator perfectly models the data distribution pX, a nearly impossible objective for a complex data distribution such as that of high-resolution natural images, this idea may prove insufficient.
BIDIRECTIONAL GENERATIVE ADVERSARIAL NETWORKS
In Bidirectional Generative Adversarial Networks (BiGANs) we not only train a generator, but additionally train an encoder E : ΩX → ΩZ. The encoder induces a distribution pE(z|x) = δ(z − E(x)) mapping data points x into the latent feature space of the generative model. The discriminator is also modified to take input from the latent space, predicting PD(Y |x, z), where Y = 1 if x is real (sampled from the real data distribution pX), and Y = 0 if x is generated (the output of G(z), z ∼ pZ).
The BiGAN training objective is defined as a minimax objective min
G,E max
D V (D, E, G)(2) where V (D, E, G) := Ex∼pX
�
Ez∼pE(·|x) [log D(x, z)]
�
��
� log D(x,E(x))
�
+ Ez∼pZ
�
Ex∼pG(·|z) [log (1 − D(x, z))]
�
��
� log(1−D(G(z),z))
�
We optimize this minimax objective using the same alternating gradient based optimization as
Goodfellow et al. (2014). See Section 3.4 for details.
BiGANs share many of the theoretical properties of GANs (Goodfellow et al., 2014), while additionally guaranteeing that at the global optimum, G and E are each other's inverse. BiGANs are also closely related to autoencoders with an ℓ0 loss function. In the following sections we highlight some of the appealing theoretical properties of BiGANs.
Definitions
Let pGZ(x, z) := pG(x|z)pZ(z) and pEX(x, z) := pE(z|x)pX(x) be the joint distributions modeled by the generator and encoder respectively. Ω := ΩX × ΩZ is the joint latent and Published as a conference paper at ICLR 2017 data space. For a region R ⊆ Ω, PEX(R) :=
�
Ω pEX(x, z)1[(x,z)∈R] d(x, z) =
�
ΩX pX(x)
�
ΩZ pE(z|x)1[(x,z)∈R] dz dx
PGZ(R) :=
�
Ω pGZ(x, z)1[(x,z)∈R] d(x, z) =
�
ΩZ pZ(z)
�
ΩX pG(x|z)1[(x,z)∈R] dx dz are probability measures over that region. We also define
PX(RX) :=
�
ΩX pX(x)1[x∈RX] dx
PZ(RZ) :=
�
ΩZ pZ(z)1[z∈RZ] dz as measures over regions RX ⊆ ΩX and RZ ⊆ ΩZ. We refer to the set of features and data samples in the support of PX and PZ as ˆΩX := supp(PX) and ˆΩZ := supp(PZ) respectively. DKL (P || Q) and DJS (P || Q) respectively denote the Kullback-Leibler (KL) and Jensen-Shannon divergences between probability measures P and Q. By definition, DKL (P || Q) := Ex∼P [log fP Q(x)]
DJS (P || Q) := 1
�
DKL
�
P
���
��� P +Q
�
+ DKL
�
Q
���
��� P +Q
��, where fP Q := dP dQ is the Radon-Nikodym (RN) derivative of measure P with respect to measure Q, with the defining property that P(R) =
�
R fP Q dQ. The RN derivative fP Q : Ω �→ R≥0 is defined for any measures P and Q on space Ω such that P is absolutely continuous with respect to Q: i.e., for any R ⊆ Ω, P(R) > 0 =⇒ Q(R) > 0.
OPTIMAL DISCRIMINATOR, GENERATOR, & ENCODER
We start by characterizing the optimal discriminator for any generator and encoder, following Goodfellow et al. (2014). This optimal discriminator then allows us to reformulate objective (3), and show that it reduces to the Jensen-Shannon divergence between the joint distributions PEX and PGZ.
Proposition 1 For any E and G, the optimal discriminator D∗
EG := arg maxD V (D, E, G) is the Radon-Nikodym derivative fEG := dPEX d(PEX+PGZ) : Ω �→ of measure PEX with respect to measure PEX + PGZ.
Proof. Given in Appendix A.1.
This optimal discriminator now allows us to characterize the optimal generator and encoder.
Proposition 2 The encoder and generator's objective for an optimal discriminator C(E, G) := maxD V (D, E, G) = V (D∗
EG, E, G) can be rewritten in terms of the Jensen-Shannon divergence between measures PEX and PGZ as C(E, G) = 2 DJS (PEX || PGZ ) − log 4.
Proof. Given in Appendix A.2.
Theorem 1 The global minimum of C(E, G) is achieved if and only if PEX = PGZ. At that point, C(E, G) = − log 4 and D∗
EG = 1
Proof. From Proposition 2, we have that C(E, G) = 2 DJS (PEX || PGZ ) − log 4. The JensenShannon divergence DJS (P || Q) ≥ 0 for any P and Q, and DJS (P || Q) = 0 if and only if P = Q.
Therefore, the global minimum of C(E, G) occurs if and only if PEX = PGZ, and at this point the value is C(E, G) = − log 4. Finally, PEX = PGZ implies that the optimal discriminator is chance:
D∗
EG = dPEX d(PEX+PGZ) = dPEX
2 dPEX = 1
2. □
The optimal discriminator, encoder, and generator of BiGAN are similar to the optimal discriminator and generator of the GAN framework (Goodfellow et al., 2014). However, an important difference is that BiGAN optimizes a Jensen-Shannon divergence between a joint distribution over both data X and latent features Z. This joint divergence allows us to further characterize properties of G and E, as shown below.
OPTIMAL GENERATOR & ENCODER ARE INVERSES
We first present an intuitive argument that, in order to "fool" a perfect discriminator, a deterministic
BiGAN encoder and generator must invert each other. (Later we will formally state and prove this
Published as a conference paper at ICLR 2017 property.) Consider a BiGAN discriminator input pair (x, z). Due to the sampling procedure, (x, z) must satisfy at least one of the following two properties:(a) x ∈ ˆΩX ∧ E(x) = z(b) z ∈ ˆΩZ ∧ G(z) = x
If only one of these properties is satisfied, a perfect discriminator can infer the source of (x, z) with certainty: if only (a) is satisfied, (x, z) must be an encoder pair (x, E(x)) and D∗
EG(x, z) = 1; if only (b) is satisfied, (x, z) must be a generator pair (G(z), z) and D∗
EG(x, z) = 0.
Therefore, in order to fool a perfect discriminator at (x, z) (so that 0 < D∗
EG(x, z) < 1), E and G must satisfy both (a) and (b). In this case, we can substitute the equality E(x) = z required by (a) into the equality G(z) = x required by (b), and vice versa, giving the inversion properties x = G(E(x)) and z = E(G(z)).
Formally, we show in Theorem 2 that the optimal generator and encoder invert one another almost everywhere on the support ˆΩX and ˆΩZ of PX and PZ.
Theorem 2 If E and G are an optimal encoder and generator, then E = G−1 almost everywhere; that is, G(E(x)) = x for PX-almost every x ∈ ΩX, and E(G(z)) = z for PZ-almost every z ∈ ΩZ.
Proof. Given in Appendix A.4.
While Theorem 2 characterizes the encoder and decoder at their optimum, due to the non-convex nature of the optimization, this optimum might never be reached. Experimentally, Section 4 shows that on standard datasets, the two are approximate inverses; however, they are rarely exact inverses. It is thus also interesting to show what objective BiGAN optimizes in terms of E and G. Next we show that BiGANs are closely related to autoencoders with an ℓ0 loss function.
RELATIONSHIP TO AUTOENCODERS
As argued in Section 1, a model trained to predict features z given data x should learn useful semantic representations. Here we show that the BiGAN objective forces the encoder E to do exactly this: in order to fool the discriminator at a particular z, the encoder must invert the generator at that z, such that E(G(z)) = z.
Theorem 3 The encoder and generator objective given an optimal discriminator C(E, G) := maxD V (D, E, G) can be rewritten as an ℓ0 autoencoder loss function
C(E, G) = Ex∼pX
�
1[E(x)∈ˆΩZ∧G(E(x))=x] log fEG(x, E(x))
�
Ez∼pZ
�
1[G(z)∈ˆΩX∧E(G(z))=z] log (1 − fEG(G(z), z))
� with log fEG ∈ (−∞, 0) and log (1 − fEG) ∈ (−∞, 0) PEX-almost and PGZ-almost everywhere.
Proof. Given in Appendix A.5.
Here the indicator function 1[G(E(x))=x] in the first term is equivalent to an autoencoder with ℓ0 loss, while the indicator 1[E(G(z))=z] in the second term shows that the BiGAN encoder must invert the generator, the desired property for feature learning. The objective further encourages the functions
E(x) and G(z) to produce valid outputs in the support of PZ and PX respectively. Unlike regular autoencoders, the ℓ0 loss function does not make any assumptions about the structure or distribution of the data itself; in fact, all the structural properties of BiGAN are learned as part of the discriminator.
LEARNING
In practice, as in the GAN framework (Goodfellow et al., 2014), each BiGAN module D, G, and E is a parametric function (with parameters θD, θG, and θE, respectively). As a whole, BiGAN can be optimized using alternating stochastic gradient steps. In one iteration, the discriminator parameters θD are updated by taking one or more steps in the positive gradient direction ∇θDV (D, E, G), then the encoder parameters θE and generator parameters θG are together updated by taking a step in the negative gradient direction −∇θE,θGV (D, E, G). In both cases, the expectation terms of Published as a conference paper at ICLR 2017
V (D, E, G) are estimated using mini-batches of n samples {x(i) ∼ pX}n i=1 and {z(i) ∼ pZ}n i=1 drawn independently for each update step.
Goodfellow et al. (2014) found that an objective in which the real and generated labels Y are swapped provides stronger gradient signal to G. We similarly observed in BiGAN training that an "inverse" objective provides stronger gradient signal to G and E. For efficiency, we also update all modules
D, G, and E simultaneously at each iteration, rather than alternating between D updates and G, E updates. See Appendix B for details.
GENERALIZED BIGAN
It is often useful to parametrize the output of the generator G and encoder E in a different, usually smaller, space Ω′
X and Ω′
Z rather than the original ΩX and ΩZ. For example, for visual feature learning, the images input to the encoder should be of similar resolution to images used in the evaluation. On the other hand, generating high resolution images remains difficult for current generative models. In this situation, the encoder may take higher resolution input while the generator output and discriminator input remain low resolution.
We generalize the BiGAN objective V (D, G, E) (3) with functions gX : ΩX �→ Ω′
X and gZ : ΩZ �→
Ω′
Z, and encoder E : ΩX �→ Ω′
Z, generator G : ΩZ �→ Ω′
X, and discriminator D : Ω′
X ×Ω′
Z �→ :
Ex∼pX
�
Ez′∼pE(·|x) [log D(gX(x), z′)]
�
��
� log D(gX(x),E(x))
�
+ Ez∼pZ
�
Ex′∼pG(·|z) [log (1 − D(x′, gZ(z)))]
�
��
� log(1−D(G(z),gZ(z)))
�
An identity gX(x) = x and gZ(z) = z (and Ω′
X = ΩX, Ω′
Z = ΩZ) yields the original objective. For visual feature learning with higher resolution encoder inputs, gX is an image resizing function that downsamples a high resolution image x ∈ ΩX to a lower resolution image x′ ∈ Ω′
X, as output by the generator. (gZ is identity.)
In this case, the encoder and generator respectively induce probability measures PEX′ and PGZ′ over regions R
⊆
Ω′ of the joint space Ω′
:=
Ω′
X × Ω′
Z, with PEX′(R)
:=
�
ΩX
�
Ω′
X
�
Ω′
Z pEX(x, z′)1[(x′,z′)∈R]δ(gX(x) − x′) dz′ dx′ dx =
�
ΩX pX(x)1[(gX(x),E(x))∈R] dx, and PGZ′ defined analogously. For optimal E and G, we can show PEX′ = PGZ′: a generalization of Theorem 1. When E and G are deterministic and optimal, Theorem 2 – that E and G invert one another – can also be generalized: ∃z∈ˆΩZ{E(x) = gZ(z) ∧ G(z) = gX(x)} for PX-almost every x ∈ ΩX, and ∃x∈ˆΩX{E(x) = gZ(z) ∧ G(z) = gX(x)} for PZ-almost every z ∈ ΩZ.
EVALUATION
We evaluate the feature learning capabilities of BiGANs by first training them unsupervised as described in Section 3.4, then transferring the encoder's learned feature representations for use in auxiliary supervised learning tasks. To demonstrate that BiGANs are able to learn meaningful feature representations both on arbitrary data vectors, where the model is agnostic to any underlying structure, as well as very high-dimensional and complex distributions, we evaluate on both permutation-invariant
MNIST (LeCun et al., 1998) and on the high-resolution natural images of ImageNet (Russakovsky et al., 2015).
In all experiments, each module D, G, and E is a parametric deep (multi-layer) network. The BiGAN discriminator D(x, z) takes data x as its initial input, and at each linear layer thereafter, the latent representation z is transformed using a learned linear transformation to the hidden layer dimension and added to the non-linearity input.
BASELINE METHODS
Besides the BiGAN framework presented above, we considered alternative approaches to learning feature representations using different GAN variants.
Discriminator
The discriminator D in a standard GAN takes data samples x ∼ pX as input, making its learned intermediate representations natural candidates as feature representations for related tasks.
Published as a conference paper at ICLR 2017
BiGAN
D
LR
JLR
AE (ℓ2)
AE (ℓ1)
Table 1: One Nearest Neighbors (1NN) classification accuracy (%) on the permutation-invariant
MNIST (LeCun et al., 1998) test set in the feature space learned by BiGAN, Latent Regressor (LR), Joint Latent Regressor (JLR), and an autoencoder (AE) using an ℓ1 or ℓ2 distance.
G(z) x
G(E(x))
Figure 2: Qualitative results for permutation-invariant MNIST BiGAN training, including generator samples G(z), real data x, and corresponding reconstructions G(E(x)).
This alternative is appealing as it requires no additional machinery, and is the approach used for unsupervised feature learning in Radford et al. (2016). On the other hand, it is not clear that the task of distinguishing between real and generated data requires or benefits from intermediate representations that are useful as semantic feature representations. In fact, if G successfully generates the true data distribution pX(x), D may ignore the input data entirely and predict P(Y = 1) = P(Y = 1|x) = 1
2 unconditionally, not learning any meaningful intermediate representations.
Latent regressor
We consider an alternative encoder training by minimizing a reconstruction loss
L(z, E(G(z))), after or jointly during a regular GAN training, called latent regressor or joint latent regressor respectively. We use a sigmoid cross entropy loss L as it naturally maps to a uniformly distributed output space. Intuitively, a drawback of this approach is that, unlike the encoder in a BiGAN, the latent regressor encoder E is trained only on generated samples G(z), and never "sees" real data x ∼ pX. While this may not be an issue in the theoretical optimum where pG(x) = pX(x) exactly – i.e., G perfectly generates the data distribution pX – in practice, for highly complex data distributions pX, such as the distribution of natural images, the generator will almost never achieve this perfect result. The fact that the real data x are never input to this type of encoder limits its utility as a feature representation for related tasks, as shown later in this section.
PERMUTATION-INVARIANT MNIST
We first present results on permutation-invariant MNIST (LeCun et al., 1998). In the permutationinvariant setting, each 28×28 digit image must be treated as an unstructured 784D vector (Goodfellow et al., 2013). In our case, this condition is met by designing each module as a multi-layer perceptron(MLP), agnostic to the underlying spatial structure in the data (as opposed to a convnet, for example).
See Appendix C.1 for more architectural and training details. We set the latent distribution pZ =
[U(−1, 1)]50 – a 50D continuous uniform distribution.
Table 1 compares the encoding learned by a BiGAN-trained encoder E with the baselines described in Section 4.1, as well as autoencoders (Hinton & Salakhutdinov, 2006) trained directly to minimize either ℓ2 or ℓ1 reconstruction error. The same architecture and optimization algorithm is used across all methods. All methods, including BiGAN, perform at roughly the same level. This result is not overly surprising given the relative simplicity of MNIST digits. For example, digits generated by
G in a GAN nearly perfectly match the data distribution (qualitatively), making the latent regressor(LR) baseline method a reasonable choice, as argued in Section 4.1. Qualitative results are presented in Figure 2.
IMAGENET
Next, we present results from training BiGANs on ImageNet LSVRC (Russakovsky et al., 2015), a large-scale database of natural images. GANs trained on ImageNet cannot perfectly reconstruct
Published as a conference paper at ICLR 2017
D
E
Noroozi & Favaro (2016)
G
AlexNet-based D
Krizhevsky et al. (2012)
Figure 3: The convolutional filters learned by the three modules (D, G, and E) of a BiGAN (left, top-middle) trained on the ImageNet (Russakovsky et al., 2015) database. We compare with the filters learned by a discriminator D trained with the same architecture (bottom-middle), as well as the filters reported by Noroozi & Favaro (2016), and by Krizhevsky et al. (2012) for fully supervised
ImageNet training (right).
G(z) x
G(E(x)) x
G(E(x)) x
G(E(x))
Figure 4: Qualitative results for ImageNet BiGAN training, including generator samples G(z), real data x, and corresponding reconstructions G(E(x)). the data, but often capture some interesting aspects. Here, each of D, G, and E is a convnet. In all experiments, the encoder E architecture follows AlexNet (Krizhevsky et al., 2012) through the fifth and last convolution layer (conv5). We also experiment with an AlexNet-based discriminator D as a baseline feature learning approach. We set the latent distribution pZ = [U(−1, 1)]200 – a 200D continuous uniform distribution. Additionally, we experiment with higher resolution encoder input images – 112 × 112 rather than the 64 × 64 used elsewhere – using the generalization described in Section 3.5. See Appendix C.2 for more architectural and training details.
Qualitative results
The convolutional filters learned by each of the three modules are shown in Figure 3. We see that the filters learned by the encoder E have clear Gabor-like structure, similar to those originally reported for the fully supervised AlexNet model (Krizhevsky et al., 2012). The filters also have similar "grouping" structure where one half (the bottom half, in this case) is more color sensitive, and the other half is more edge sensitive. (This separation of the filters occurs due to the AlexNet architecture maintaining two separate filter paths for computational efficiency.)
In Figure 4 we present sample generations G(z), as well as real data samples x and their BiGAN reconstructions G(E(x)). The reconstructions, while certainly imperfect, demonstrate empirically that
Published as a conference paper at ICLR 2017 conv1 conv2 conv3 conv4 conv5
Random (Noroozi & Favaro, 2016)
Wang & Gupta (2015)
Doersch et al. (2015)
Noroozi & Favaro (2016)*
BiGAN (ours)
BiGAN, 112 × 112 E (ours)
Table 2: Classification accuracy (%) for the ImageNet LSVRC (Russakovsky et al., 2015) validation set with various portions of the network frozen, or reinitialized and trained from scratch, following the evaluation from Noroozi & Favaro (2016). In, e.g., the conv3 column, the first three layers
– conv1 through conv3 – are transferred and frozen, and the last layers – conv4, conv5, and fully connected layers – are reinitialized and trained fully supervised for ImageNet classification. BiGAN is competitive with these contemporary visual feature learning methods, despite its generality. (*Results from Noroozi & Favaro (2016) are not directly comparable to those of the other methods as a different base convnet architecture with larger intermediate feature maps is used.) the BiGAN encoder E and generator G learn approximate inverse mappings, as shown theoretically in Theorem 2. In Appendix C.2, we present nearest neighbors in the BiGAN learned feature space.
ImageNet classification
Following Noroozi & Favaro (2016), we evaluate by freezing the first
N layers of our pretrained network and randomly reinitializing and training the remainder fully supervised for ImageNet classification. Results are reported in Table 2.
VOC classification, detection, and segmentation
We evaluate the transferability of BiGAN representations to the PASCAL VOC (Everingham et al., 2014) computer vision benchmark tasks, including classification, object detection, and semantic segmentation. The classification task involves simple binary prediction of presence or absence in a given image for each of 20 object categories.
The object detection and semantic segmentation tasks go a step further by requiring the objects to be localized, with semantic segmentation requiring this at the finest scale: pixelwise prediction of object identity. For detection, the pretrained model is used as the initialization for Fast R-CNN (Girshick, 2015) (FRCN) training; and for semantic segmentation, the model is used as the initialization for Fully Convolutional Network (Long et al., 2015) (FCN) training, in each case replacing the AlexNet (Krizhevsky et al., 2012) model trained fully supervised for ImageNet classification. We report results on each of these tasks in Table 3, comparing BiGANs with contemporary approaches to unsupervised (Krähenbühl et al., 2016) and self-supervised (Doersch et al., 2015; Agrawal et al., 2015; Wang & Gupta, 2015; Pathak et al., 2016) feature learning in the visual domain, as well as the baselines discussed in Section 4.1.
DISCUSSION
Despite making no assumptions about the underlying structure of the data, the BiGAN unsupervised feature learning framework offers a representation competitive with existing self-supervised and even weakly supervised feature learning approaches for visual feature learning, while still being a purely generative model with the ability to sample data x and predict latent representation z. Furthermore, BiGANs outperform the discriminator (D) and latent regressor (LR) baselines discussed in Section 4.1, confirming our intuition that these approaches may not perform well in the regime of highly complex data distributions such as that of natural images. The version in which the encoder takes a higher resolution image than output by the generator (BiGAN 112 × 112 E) performs better still, and this strategy is not possible under the LR and D baselines as each of those modules take generator outputs as their input.
Although existing self-supervised approaches have shown impressive performance and thus far tended to outshine purely unsupervised approaches in the complex domain of high-resolution images, purely unsupervised approaches to feature learning or pre-training have several potential benefits.
Published as a conference paper at ICLR 2017
FRCN
FCN
Classification
Detection
Segmentation(% mAP)(% mAP)(% mIU) trained layers fc8 fc6-8 all all all sup.
ImageNet (Krizhevsky et al., 2012)
48.0 self-sup.
Agrawal et al. (2015)Pathak et al. (2016)
Wang & Gupta (2015)Doersch et al. (2015)
51.1 unsup. k-means (Krähenbühl et al., 2016)
Discriminator (D)Latent Regressor (LR)Joint LRAutoencoder (ℓ2)BiGAN (ours)
BiGAN, 112 × 112 E (ours)
Table 3: Classification and Fast R-CNN (Girshick, 2015) detection results for the PASCAL VOC
2007 (Everingham et al., 2014) test set, and FCN (Long et al., 2015) segmentation results on the PASCAL VOC 2012 validation set, under the standard mean average precision (mAP) or mean intersection over union (mIU) metrics for each task. Classification models are trained with various portions of the AlexNet (Krizhevsky et al., 2012) model frozen. In the fc8 column, only the linear classifier (a multinomial logistic regression) is learned – in the case of BiGAN, on top of randomly initialized fully connected (FC) layers fc6 and fc7. In the fc6-8 column, all three FC layers are trained fully supervised with all convolution layers frozen. Finally, in the all column, the entire network is "fine-tuned". BiGAN outperforms other unsupervised (unsup.) feature learning approaches, including the GAN-based baselines described in Section 4.1, and despite its generality, is competitive with contemporary self-supervised (self-sup.) feature learning approaches specific to the visual domain.
BiGAN and other unsupervised learning approaches are agnostic to the domain of the data. The self-supervised approaches are specific to the visual domain, in some cases requiring weak supervision from video unavailable in images alone. For example, the methods are not applicable in the permutation-invariant MNIST setting explored in Section 4.2, as the data are treated as flat vectors rather than 2D images.
Furthermore, BiGAN and other unsupervised approaches needn't suffer from domain shift between the pre-training task and the transfer task, unlike self-supervised methods in which some aspect of the data is normally removed or corrupted in order to create a non-trivial prediction task. In the context prediction task (Doersch et al., 2015), the network sees only small image patches – the global image structure is unobserved. In the context encoder or inpainting task (Pathak et al., 2016), each image is corrupted by removing large areas to be filled in by the prediction network, creating inputs with dramatically different appearance from the uncorrupted natural images seen in the transfer tasks.
Other approaches (Agrawal et al., 2015; Wang & Gupta, 2015) rely on auxiliary information unavailable in the static image domain, such as video, egomotion, or tracking. Unlike BiGAN, such approaches cannot learn feature representations from unlabeled static images.
We finally note that the results presented here constitute only a preliminary exploration of the space of model architectures possible under the BiGAN framework, and we expect results to improve significantly with advancements in generative image models and discriminative convolutional networks alike.
ACKNOWLEDGMENTS
The authors thank Evan Shelhamer, Jonathan Long, and other Berkeley Vision labmates for helpful discussions throughout this work. This work was supported by DARPA, AFRL, DoD MURI award
N000141110688, NSF awards IIS-1427425 and IIS-1212798, and the Berkeley Artificial Intelligence
Research laboratory. The GPUs used for this work were donated by NVIDIA.
Published as a conference paper at ICLR 2017
REFERENCES
Pulkit Agrawal, Joao Carreira, and Jitendra Malik. Learning to see by moving. In ICCV, 2015.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In ICLR, 2015.
Emily L. Denton, Soumith Chintala, Arthur Szlam, and Rob Fergus. Deep generative image models using a Laplacian pyramid of adversarial networks. In NIPS, 2015.
Carl Doersch, Abhinav Gupta, and Alexei A. Efros. Unsupervised visual representation learning by context prediction. In ICCV, 2015.
Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor
Darrell. DeCAF: A deep convolutional activation feature for generic visual recognition. In ICML, Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Alex Lamb, Martin Arjovsky, Olivier Mastropietro, and Aaron Courville. Adversarially learned inference. arXiv:1606.00704, 2016.
Mark Everingham, S. M. Ali Eslami, Luc Van Gool, Christopher K. I. Williams, John Winn, and Andrew Zisserman. The PASCAL Visual Object Classes challenge: A retrospective. IJCV, 2014.
Ross Girshick. Fast R-CNN. In ICCV, 2015.
Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014.
Ian Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. Maxout networks. In ICML, 2013.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, 2014.
Alex Graves, Abdel-rahman Mohamed, and Geoffrey E. Hinton. Speech recognition with deep recurrent neural networks. In ICASSP, 2013.
Geoffrey E. Hinton and Ruslan R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 2006.
Geoffrey E. Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm for deep belief nets. Neural Computation, 2006.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, 2015.
Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio
Guadarrama, and Trevor Darrell. Caffe: Convolutional architecture for fast feature embedding. arXiv:1408.5093, 2014.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.
Philipp Krähenbühl, Carl Doersch, Jeff Donahue, and Trevor Darrell. Data-dependent initializations of convolutional neural networks. In ICLR, 2016.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. ImageNet classification with deep convolutional neural networks. In NIPS, 2012.
Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proc. IEEE, 1998.
Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In CVPR, 2015.
Andrew L. Maas, Awni Y. Hannun, and Andrew Y. Ng. Rectifier nonlinearities improve neural network acoustic models. In ICML, 2013.
Published as a conference paper at ICLR 2017
Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In ECCV, 2016.
Deepak Pathak, Philipp Krähenbühl, Jeff Donahue, Trevor Darrell, and Alexei A. Efros. Context encoders: Feature learning by inpainting. In CVPR, 2016.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. In ICLR, 2016.
Ali Razavian, Hossein Azizpour, Josephine Sullivan, and Stefan Carlsson. CNN features off-the-shelf: an astounding baseline for recognition. In CVPR Workshops, 2014.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Fei-Fei Li. ImageNet large scale visual recognition challenge. IJCV, 2015.
Ruslan Salakhutdinov and Geoffrey E. Hinton. Deep Boltzmann machines. In AISTATS, 2009.
Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks.
In NIPS, 2014.
Theano Development Team. Theano: A Python framework for fast computation of mathematical expressions. arXiv:1605.02688, 2016.
Oriol Vinyals, Łukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoffrey E. Hinton.
Grammar as a foreign language. In NIPS, 2015.
Xiaolong Wang and Abhinav Gupta. Unsupervised learning of visual representations using videos.
In ICCV, 2015.
Matthew D. Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In ECCV, Published as a conference paper at ICLR 2017
APPENDIX A
ADDITIONAL PROOFS
A.1
PROOF OF PROPOSITION 1 (OPTIMAL DISCRIMINATOR)
Proposition 1 For any E and G, the optimal discriminator D∗
EG := arg maxD V (D, E, G) is the Radon-Nikodym derivative fEG := dPEX d(PEX+PGZ) : Ω �→ of measure PEX with respect to measure PEX + PGZ.
Proof. For measures P and Q on space Ω, with P absolutely continuous with respect to Q, the RN derivative fP Q := dP dQ exists, and we have
Ex∼P [g(x)] =
�
Ω g dP =
�
Ω g dP dQ dQ =
�
Ω gfP Q dQ = Ex∼Q [fP Q(x)g(x)].
Let the probability measure PEG :=
PEX+PGZ
2 denote the average of measures PEX and PGZ.
Both PEX and PGZ are each absolutely continuous with respect to PEG. Hence the RN derivatives fEG := dPEX d(PEX+PGZ) = 1
2 dPEX dPEG and fGE := dPGZ d(PEX+PGZ) = 1
2 dPGZ dPEG exist and sum to 1: fEG + fGE = dPEX d(PEX+PGZ) + dPGZ d(PEX+PGZ) = d(PEX+PGZ) d(PEX+PGZ) = 1.
We use (4) and (5) to rewrite the objective V (3) as a single expectation under measure PEG:
V (D, E, G) = E(x,z)∼PEX [log D(x, z)] + E(x,z)∼PGZ [log (1 − D(x, z))]
= E(x,z)∼PEG[2fEG
� �� � dPEX dPEG(x, z) log D(x, z)] + E(x,z)∼PEG[2fGE
� �� � dPGZ dPEG(x, z) log (1 − D(x, z))]
= 2 E(x,z)∼PEG [fEG(x, z) log D(x, z) + fGE(x, z) log (1 − D(x, z))]
= 2 E(x,z)∼PEG [fEG(x, z) log D(x, z) + (1 − fEG(x, z)) log (1 − D(x, z))].
Note that arg maxy {a log y + (1 − a) log(1 − y)} = a for any a ∈. Thus, D∗
EG = fEG. □
A.2
PROOF OF PROPOSITION 2 (ENCODER AND GENERATOR OBJECTIVE)
Proposition 2 The encoder and generator's objective for an optimal discriminator C(E, G) := maxD V (D, E, G) = V (D∗
EG, E, G) can be rewritten in terms of the Jensen-Shannon divergence between measures PEX and PGZ as C(E, G) = 2 DJS (PEX || PGZ ) − log 4.
Proof. Using Proposition 1 along with (5) (1 − D∗
EG = 1 − fEG = fGE) we rewrite the objective
C(E, G) = maxDV (D, E, G) = V (D∗
EG, E, G)
= E(x,z)∼PEX [log D∗
EG(x, z)] + E(x,z)∼PGZ [log (1 − D∗
EG(x, z))]
= E(x,z)∼PEX [log fEG(x, z)] + E(x,z)∼PGZ [log fGE(x, z)]
= E(x,z)∼PEX [log (2fEG(x, z))] + E(x,z)∼PGZ [log (2fGE(x, z))] − log 4
= DKL (PEX || PEG ) + DKL (PGZ || PEG ) − log 4
= DKL
�
PEX
���� PEX+PGZ
�
+ DKL
�
PGZ
���� PEX+PGZ
�
− log 4
= 2 DJS (PEX || PGZ ) − log 4. □
A.3
MEASURE DEFINITIONS FOR DETERMINISTIC E AND G
While Theorem 1 and Propositions 1 and 2 hold for any encoder pE(z|x) and generator pG(x|z), stochastic or deterministic, Theorems 2 and 3 assume the encoder E and generator G are deterministic functions; i.e., with conditionals pE(z|x) = δ(z − E(x)) and pG(x|z) = δ(x − G(z)) defined as δ functions.
Published as a conference paper at ICLR 2017
For use in the proofs of those theorems, we simplify the definitions of measures PEX and PGZ given in Section 3 for the case of deterministic functions E and G below:
PEX(R) =
�
ΩX pX(x)
�
ΩZ pE(z|x)1[(x,z)∈R] dz dx
=
�
ΩX pX(x)
��
ΩZ δ(z − E(x))1[(x,z)∈R] dz
� dx
=
�
ΩX pX(x)1[(x,E(x))∈R] dx
PGZ(R) =
�
ΩZ pZ(z)
�
ΩX pG(x|z)1[(x,z)∈R] dx dz
=
�
ΩZ pZ(z)
��
ΩX δ(x − G(z))1[(x,z)∈R] dx
� dz
=
�
ΩZ pZ(z)1[(G(z),z)∈R] dz
A.4
PROOF OF THEOREM 2 (OPTIMAL GENERATOR AND ENCODER ARE INVERSES)
Theorem 2 If E and G are an optimal encoder and generator, then E = G−1 almost everywhere; that is, G(E(x)) = x for PX-almost every x ∈ ΩX, and E(G(z)) = z for PZ-almost every z ∈ ΩZ.
Proof. Let R0
X := {x ∈ ΩX : x ̸= G(E(x))} be the region of ΩX in which the inversion property x = G(E(x)) does not hold. We will show that, for optimal E and G, R0
X has measure zero under
PX (i.e., PX(R0
X) = 0) and therefore x = G(E(x)) holds PX-almost everywhere.
Let R0 := {(x, z) ∈ Ω : z = E(x) ∧ x ∈ R0
X} be the region of Ω such that (x, E(x)) ∈ R0 if and only if x ∈ R0
X. We'll use the definitions of PEX and PGZ for deterministic E and G (Appendix A.3), and the fact that PEX = PGZ for optimal E and G (Theorem 1).
PX(R0
X) =
�
ΩX pX(x)1[x∈R0
X] dx
=
�
ΩX pX(x)1[(x,E(x))∈R0] dx
= PEX(R0)
= PGZ(R0)
=
�
ΩZ pZ(z)1[(G(z),z)∈R0] dz
=
�
ΩZ pZ(z)1[z=E(G(z)) ∧ G(z)∈R0
X] dz
=
�
ΩZ pZ(z)
1[z=E(G(z)) ∧ G(z)̸=G(E(G(z)))]
�
��
�
=0 for any z, as z=E(G(z)) =⇒ G(z)=G(E(G(z))) dz
= 0.
Hence region R0
X has measure zero (PX(R0
X) = 0), and the inversion property x = G(E(x)) holds
PX-almost everywhere.
An analogous argument shows that R0
Z := {z ∈ ΩZ : z ̸= E(G(z))} has measure zero on PZ (i.e., PZ(R0
Z) = 0) and therefore z = E(G(z)) holds PZ-almost everywhere. □
A.5
PROOF OF THEOREM 3 (RELATIONSHIP TO AUTOENCODERS)
As shown in Proposition 2 (Section 3), the BiGAN objective is equivalent to the Jensen-Shannon divergence between PEX and PGZ. We now go a step further and show that this Jensen-Shannon divergence is closely related to a standard autoencoder loss. Omitting the 1
2 scale factor, a KL divergence term of the Jensen-Shannon divergence is given as
DKL
�
PEX
���� PEX+PGZ
�
= log 2 +
�
Ω log dPEX d(PEX + PGZ) dPEX
= log 2 +
�
Ω log f dPEX, (6) where we abbreviate as f the Radon-Nikodym derivative fEG := dPEX d(PEX+PGZ) ∈ defined in Proposition 1 for most of this proof.
Published as a conference paper at ICLR 2017
We'll make use of the definitions of PEX and PGZ for deterministic E and G found in Appendix A.3.
The integral term of the KL divergence expression given in (6) over a particular region R ⊆ Ω will be denoted by
F(R) :=
�
R log dPEX d (PEX + PGZ) dPEX =
�
R log f dPEX.
Next we will show that f > 0 holds PEX-almost everywhere, and hence F is always well defined and finite. We then show that F is equivalent to an autoencoder-like reconstruction loss function.
Proposition 3 f > 0 PEX-almost everywhere.
Proof. Let Rf=0 := {(x, z) ∈ Ω : f(x, z) = 0} be the region of Ω in which f = 0. Using the definition of the Radon-Nikodym derivative f, the measure PEX(Rf=0) =
�
Rf=0 f d(PEX +
PGZ) =
�
Rf=0 0 d(PEX + PGZ) = 0 is zero. Hence f > 0 PEX-almost everywhere. □
Proposition 3 ensures that log f is defined PEX-almost everywhere, and F(R) is well-defined. Next we will show that F(R) mimics an autoencoder with ℓ0 loss, meaning F is zero for any region in which G(E(x)) ̸= x, and non-zero otherwise.
Proposition 4 The KL divergence F outside the support of PGZ is zero: F(Ω \ supp(PGZ)) = 0.
We'll first show that in region RS := Ω \ supp(PGZ), we have f = 1 PEX-almost everywhere.
Let Rf<1 := {(x, z) ∈ RS : f(x, z) < 1} be the region of RS in which f < 1. Let's assume that
PEX(Rf<1) > 0 has non-zero measure. Then, using the definition of the Radon-Nikodym derivative, PEX(Rf<1) =
�
Rf<1 f d(PEX + PGZ) =
�
Rf<1 f
����
≤ε<1 dPEX +
�
Rf<1 f dPGZ
�
��
�
≤ εPEX(Rf<1)
< PEX(Rf<1), where ε is a constant smaller than 1. But PEX(Rf<1) < PEX(Rf<1) is a contradiction; hence
PEX(Rf<1) = 0 and f = 1 PEX-almost everywhere in RS, implying log f = 0 PEX-almost everywhere in RS. Hence F(RS) = 0. □
By definition, F(Ω \ supp(PEX)) = 0 is also zero. The only region where F might be non-zero is R1 := supp(PEX) ∩ supp(PGZ).
Proposition 5 f < 1 PEX-almost everywhere in R1.
Let Rf=1 :=
�(x, z) ∈ R1 : f(x, z) = 1
� be the region in which f = 1. Let's assume the set
Rf=1 ̸= ∅ is not empty. By definition of the support1, PEX(Rf=1) > 0 and PGZ(Rf=1) > 0. The Radon-Nikodym derivative on Rf=1 is then given by
PEX(Rf=1) =
�
Rf=1 f d(PEX + PGZ) =
�
Rf=1 1 d(PEX + PGZ) = PEX(Rf=1) + PGZ(Rf=1), which implies PGZ(Rf=1) = 0 and contradicts the definition of support. Hence Rf=1 = ∅ and f < 1 PEX-almost everywhere on R1, implying log f < 0 PEX-almost everywhere. □
Theorem 3 The encoder and generator objective given an optimal discriminator C(E, G) := maxD V (D, E, G) can be rewritten as an ℓ0 autoencoder loss function
C(E, G) = Ex∼pX
�
1[E(x)∈ˆΩZ∧G(E(x))=x] log fEG(x, E(x))
�
Ez∼pZ
�
1[G(z)∈ˆΩX∧E(G(z))=z] log (1 − fEG(G(z), z))
� with log fEG ∈ (−∞, 0) and log (1 − fEG) ∈ (−∞, 0) PEX-almost and PGZ-almost everywhere.
Proof. Proposition 4 (F(Ω \ supp(PGZ)) = 0) and F(Ω \ supp(PEX)) = 0 imply that R1 := supp(PEX) ∩ supp(PGZ) is the only region of Ω where F may be non-zero; hence F(Ω) = F(R1).
1We use the definition U ∩ C ̸= ∅ =⇒ µ(U ∩ C) > 0 here.
Published as a conference paper at ICLR 2017
Note that supp(PEX) = {(x, E(x)) : x ∈ ˆΩX} supp(PGZ) = {(G(z), z) : z ∈ ˆΩZ}
=⇒ R1 := supp(PEX) ∩ supp(PGZ) = {(x, z) : E(x) = z ∧ x ∈ ˆΩX ∧ G(z) = x ∧ z ∈ ˆΩZ}
So a point (x, E(x)) is in R1 if x ∈ ˆΩX, E(x) ∈ ˆΩZ, and G(E(x)) = x. (We can omit the x ∈ ˆΩX condition from inside an expectation over PX, as PX-almost all x /∈ ˆΩX have 0 probability.)
Therefore, DKL
�
PEX
���� PEX+PGZ
�
− log 2 = F(Ω) = F(R1)
=
�
R1 log f(x, z) dPEX
=
�
Ω 1[(x,z)∈R1] log f(x, z) dPEX
= E(x,z)∼PEX
�
1[(x,z)∈R1] log f(x, z)
�
= Ex∼pX
�
1[(x,E(x))∈R1] log f(x, E(x))
�
= Ex∼pX
�
1[E(x)∈ˆΩZ∧G(E(x))=x] log f(x, E(x))
�
Finally, with Propositions 3 and 5, we have f ∈ (0, 1) PEX-almost everywhere in R1, and therefore log f ∈ (−∞, 0), taking a finite and strictly negative value PEX-almost everywhere.
An analogous argument (along with the fact that fEG + fGE = 1) lets us rewrite the other KL divergence term
DKL
�
PGZ
���� PEX+PGZ
�
− log 2 = Ez∼pZ
�
1[G(z)∈ˆΩX∧E(G(z))=z] log fGE(G(z), z)
�
= Ez∼pZ
�
1[G(z)∈ˆΩX∧E(G(z))=z] log (1 − fEG(G(z), z))
�
The Jensen-Shannon divergence is the mean of these two KL divergences, giving C(E, G):
C(E, G) = 2 DJS (PEX || PGZ ) − log 4
= DKL
�
PEX
���� PEX+PGZ
�
+ DKL
�
PGZ
���� PEX+PGZ
�
− log 4
= Ex∼pX
�
1[E(x)∈ˆΩZ∧G(E(x))=x] log fEG(x, E(x))
�
Ez∼pZ
�
1[G(z)∈ˆΩX∧E(G(z))=z] log (1 − fEG(G(z), z))
�
□
APPENDIX B
LEARNING DETAILS
In this section we provide additional details on the BiGAN learning protocol summarized in Section 3.4. Goodfellow et al. (2014) found for GAN training that an objective in which the real and generated labels Y are swapped provides stronger gradient signal to G. We similarly observed in BiGAN training that an "inverse" objective Λ (with the same fixed point characteristics as V ) provides stronger gradient signal to G and E, where Λ(D, G, E) = Ex∼pX
�
Ez∼pE(·|x) [log (1 − D(x, z))]
�
��
� log(1−D(x,E(x)))
�
+ Ez∼pZ
�
Ex∼pG(·|z) [log D(x, z)]
�
��
� log D(G(z),z)
�
In practice, θG and θE are updated by moving in the positive gradient direction of this inverse objective ∇θE,θGΛ, rather than the negative gradient direction of the original objective.
We also observed that learning behaved similarly when all parameters θD, θG, θE were updated simultaneously at each iteration rather than alternating between θD updates and θG, θE updates, so we took the simultaneous updating (non-alternating) approach for computational efficiency. (For standard GAN training, simultaneous updates of θD, θG performed similarly well, so our standard
GAN experiments also follow this protocol.)
Published as a conference paper at ICLR 2017
APPENDIX C
MODEL AND TRAINING DETAILS
In the following sections we present additional details on the models and training protocols used in the permutation-invariant MNIST and ImageNet evaluations presented in Section 4.
Optimization
For unsupervised training of BiGANs and baseline methods, we use the Adam optimizer (Kingma & Ba, 2015) to compute parameter updates, following the hyperparameters (initial step size α = 2 × 10−4, momentum β1 = 0.5 and β2 = 0.999) used by Radford et al. (2016).
The step size α is decayed exponentially to α = 2 × 10−6 starting halfway through training. The mini-batch size is 128. ℓ2 weight decay of 2.5 × 10−5 is applied to all multiplicative weights in linear layers (but not to the learned bias β or scale γ parameters applied after batch normalization).
Weights are initialized from a zero-mean normal distribution with a standard deviation of 0.02, with one notable exception: BiGAN discriminator weights that directly multiply z inputs to be added to spatial convolution outputs have initializations scaled by the convolution kernel size – e.g., for a 5 × 5 kernel, weights are initialized with a standard deviation of 0.5, 25 times the standard initialization.
Software & hardware
We implement BiGANs and baseline feature learning methods using the Theano (Theano Development Team, 2016) framework, based on the convolutional GAN implementation provided by Radford et al. (2016). ImageNet transfer learning experiments (Section 4.3) use the Caffe (Jia et al., 2014) framework, per the Fast R-CNN (Girshick, 2015) and FCN (Long et al., 2015) reference implementations. Most computation is performed on an NVIDIA Titan X or Tesla
K40 GPU.
C.1
PERMUTATION-INVARIANT MNIST
In all permutation-invariant MNIST experiments (Section 4.2), D, G, and E each consist of two hidden layers with 1024 units. The first hidden layer is followed by a non-linearity; the second is followed by (parameter-free) batch normalization (Ioffe & Szegedy, 2015) and a non-linearity. The second hidden layer in each case is the input to a linear prediction layer of the appropriate size. In D and E, a leaky ReLU (Maas et al., 2013) non-linearity with a "leak" of 0.2 is used; in G, a standard
ReLU non-linearity is used. All models are trained for 400 epochs.
C.2
IMAGENET
In all ImageNet experiments (Section 4.3), the encoder E architecture follows AlexNet (Krizhevsky et al., 2012) through the fifth and last convolution layer (conv5), with local response normalization(LRN) layers removed and batch normalization (Ioffe & Szegedy, 2015) (including the learned scaling and bias) with leaky ReLU non-linearity applied to the output of each convolution at unsupervised training time. (For supervised evaluation, batch normalization is not used, and the pre-trained scale and bias is merged into the preceding convolution's weights and bias.)
In most experiments, both the discriminator D and generator G architecture are those used by Radford et al. (2016), consisting of a series of four 5 × 5 convolutions (or "deconvolutions" – fractionallystrided convolutions – for the generator G) applied with 2 pixel stride, each followed by batch normalization and rectified non-linearity.
The sole exception is our discriminator baseline feature learning experiment, in which we let the discriminator D be the AlexNet variant described above. Generally, using AlexNet (or similar convnet architecture) as the discriminator D is detrimental to the visual fidelity of the resulting generated images, likely due to the relatively large convolutional filter kernel size applied to the input image, as well as the max-pooling layers, which explicitly discard information in the input. However, for fair comparison of the discriminator's feature learning abilities with those of BiGANs, we use the same architecture as used in the BiGAN encoder.
Preprocessing
To produce a data sample x, we first sample an image from the database, and resize it proportionally such that its shorter edge has a length of 72 pixels. Then, a 64 × 64 crop is randomly selected from the resized image. The crop is flipped horizontally with probability 1
2. Finally, the crop is scaled to [−1, 1], giving the sample x.
Published as a conference paper at ICLR 2017
Query
#1
#2
#3
#4
Figure 5: For the query images used in Krähenbühl et al. (2016) (left), nearest neighbors (by minimum cosine distance) from the ImageNet LSVRC (Russakovsky et al., 2015) training set in the fc6 feature space of the ImageNet-trained BiGAN encoder E. (The fc6 weights are set randomly; this space is a random projection of the learned conv5 feature space.)
Timing
A single epoch (one training pass over the 1.2 million images) of BiGAN training takes roughly 40 minutes on a Titan X GPU. Models are trained for 100 epochs, for a total training time of under 3 days.
Nearest neighbors
In Figure 5 we present nearest neighbors in the feature space of the BiGAN encoder E learned in unsupervised ImageNet training.Bottom-up Segmentation for Top-down Detection
Sanja Fidler1
Roozbeh Mottaghi2
Alan Yuille2
Raquel Urtasun1
1TTI Chicago, 2UCLA
{fidler, rurtasun}@ttic.edu, {roozbehm@cs, yuille@stat}.ucla.edu
Abstract
In this paper we are interested in how semantic segmentation can help object detection.
Towards this goal, we propose a novel deformable part-based model which exploits region-based segmentation algorithms that compute candidate object regions by bottom-up clustering followed by ranking of those regions. Our approach allows every detection hypothesis to select a segment (including void), and scores each box in the image using both the traditional
HOG filters as well as a set of novel segmentation features.
Thus our model "blends" between the detector and segmentation models. Since our features can be computed very efficiently given the segments, we maintain the same complexity as the original DPM. We demonstrate the effectiveness of our approach in PASCAL VOC 2010, and show that when employing only a root filter our approach outperforms
Dalal & Triggs detector on all classes, achieving 13% higher average AP. When employing the parts, we outperform the original DPM in 19 out of 20 classes, achieving an improvement of 8% AP. Furthermore, we outperform the previous state-of-the-art on VOC'10 test by 4%.
1. Introduction
Over the past few years, we have witnessed a push towards holistic approaches that try to solve multiple recognition tasks jointly. This is important as information from multiple sources should facilitate scene understanding as a whole. For example, knowing which objects are present in the scene should simplify segmentation and detection tasks. Similarly, if we can detect where an object is, segmentation should be easier as only figure-ground segmentation is necessary. Existing approaches typically take the output of a detector and refine the regions inside the boxes to produce image segmentations.
An alternative approach is to use the candidate detections produced by state-of-the-art detectors as additional features for segmentation. This simple approach has proven very successful in standard benchmarks.
In contrast, in this paper we are interested in exploiting semantic segmentation in order to improve object detection. While bottom-up segmentation has been often believed to be inferior to top-down object detectors due to its frequent over- and under- segmentation, recent approaches
 have shown impressive results in difficult datasets such as PASCAL VOC challenge. Here, we take advantage of region-based segmentation approaches, which compute a set of candidate object regions by bottom-up clustering, and produce a segmentation by ranking those regions using class specific rankers. Our goal is to make use of these candidate object segments to bias sliding window object detectors to agree with these regions. Importantly, unlike the aforementioned holistic approaches, we reason about all possible object bounding boxes (not just candidates) to not limit the expressiveness of our model.
Deformable part-based models (DPM) and its variants, are arguably the leading technique to object detection 1. However, so far, there has not been many attempts to incorporate segmentation into DPMs. In this paper we propose a novel deformable part-based model, which exploits region-based segmentation by allowing every detection hypothesis to select a segment (including void) from a small pool of segment candidates. Towards this goal, we derive simple features, which can capture the essential information encoded in the segments. Our detector scores each box in the image using both the traditional HOG filters as well as the set of novel segmentation features. Our model
"blends" between the detector and the segmentation models by boosting object hypotheses on the segments. Furthermore, it can recover from segmentation mistakes by exploiting a powerful appearance model. Importantly, as given the segments we can compute our features very efficiently, our approach has the same computational complexity as the original DPM.
We demonstrate the effectiveness of our approach in PASCAL VOC 2010, and show that when employing only a root filter our approach outperforms Dalal & Triggs detector by 13% AP, and when employing parts, we outperform the original DPM by 8%. Furthermore, we outperform the previous state-of-the-art on VOC2010 by 4%.
1Poselets can be shown to be very similar in spirit to DPMs
2013 IEEE Conference on Computer Vision and Pattern Recognition
1063-6919/13 $26.00 © 2013 IEEE
DOI 10.1109/CVPR.2013.423
2013 IEEE Conference on Computer Vision and Pattern Recognition
1063-6919/13 $26.00 © 2013 IEEE
DOI 10.1109/CVPR.2013.423
2013 IEEE Conference on Computer Vision and Pattern Recognition
1063-6919/13 $26.00 © 2013 IEEE
DOI 10.1109/CVPR.2013.423
We believe that these results will encourage new research on bottom-up segmentation as well as hybrid segmentationdetection approaches, as our paper clearly demonstrates the importance of segmentation for object detection.
In the remainder of the paper, we first review related work and then introduce our novel deformable part-based model, which we call segDPM. We then show our experimental evaluation and conclude with future work.
2. Related Work
Deformable part-based model and its variants have been proven to be very successful in difficult object detection benchmarks such as PASCAL VOC challenge. Several approaches have tried to augment the level of supervision in these models. Azizpour et al. use part annotations to help clustering different poses as well as to model occlusions. Hierarchical versions of these models have also been proposed, where each part is composed of a set of sub-parts. The relative rigidity of DPMs has been alleviated in by leveraging a dictionary of shape masks.
This allows a better treatment of variable object shape. Desai et al. proposed a structure prediction approach to perform non-maxima suppression in DPMs which exploits pairwise relationships between multi-class candidates. The tree structure of DPMs allows for fast inference but can suffer from problems such as double counting observations.
To mitigate this, consider lateral connections between high resolution parts.
In the past few years, a wide variety of segmentation algorithms that employ object detectors as top-down cues have been proposed. This is typically done in the form of unary features for an MRF, or as candidate bounding boxes for holistic MRFs. Complex features based on shape masks were exploited in to parse the scene holistically in terms of the objects present in the scene, their spatial location as well as semantic segmentation. In, heads of cats and dogs are detected with a DPM, and segmentation is performed using a GrabCut-type method. By combining top-down shape information from DPM parts and bottom-up color and boundary cues, tackle segmentation and detection task simultaneously and provide shape and depth ordering for the detected objects. Dai et al.
 exploit a DPM to find a rough location for the object of interest and refine the detected bounding box according to occlusion boundaries and color information. find silhouettes for objects by extending or refining DPM boxes corresponding to a reliably detectable part of an object.
DPMs provide object-specific cues, which can be exploited to learn object segmentations. In, masks for detected objects are found by employing a group of segments corresponding to the foreground region. Other object detectors have been used in the literature to help segmenting object regions. For instance, while finds segmentations for people by aligning the masks obtained for each Poselet, integrates low level segmentation cues with Poselets in a soft manner.
There are a few attempts to use segments/regions to improve object detection. Gu et al. apply hough transform for a set of regions to cast votes on the location of the object. More recently, learn object shape model from a set of contours and use the learned model of contours for detection. In contrast, in this paper we proposed a novel deformable-part based model, which allows each detection hypothesis to select candidate segments. Simple features express the fact that the detections should agree with the segments. Importantly, these features can be computed very efficiently, and thus our approach has the same computational complexity as DPM.
3. Semantic Segmentation for Object Detection
We are interested in utilizing semantic segmentation to help object detection.
In particular, we take advantage of region-based segmentation approaches, which compute candidate object regions by bottom-up clustering, and rank those regions to estimate a score for each class. Towards this goal we frame detection as an inference problem, where each detection hypothesis can select a segment from a pool of candidates (those returned from the segmentation as well as void). We derive simple features, which can be computed very efficiently while capturing most information encoded in the segments. In the remainder of the section, we first discuss our novel DPM formulation. We then define our new segment-based features and discuss learning and inference in our model.
3.1. A Segmentation-Aware DPM
Following, let p0 be a random variable encoding the location and scale of a bounding box in an image pyramid as well as the mixture component id. As shown in a mixture model is necessary in order to cope with variability in appearance as well as the different aspect ratios of the training examples. Let {pi}i=1,···,P be a set of parts which encode bounding boxes at double the resolution of the root. Denote with h the index over the set of candidate segments returned by the segmentation algorithm. We frame the detection problem as inference in a Markov Random Field (MRF), where each root filter hypothesis can select a segment from a pool of candidates. We thus write the score of a configuration as
E(p, h)
=
P
� i=0 wT i · φ(x, pi) +
P
� i=1 wT i,def · φ(x, p0, pi) +
+wT segφ(x, h, p0)(1) where h ∈ {0, 1, · · ·, H(x)}, with H(x) the total number of segments for this class in image x. Note that h = 0 im3295 segment rootfilter s filt φseg−in(x, h, p0) h p0 segment rootfilter egm filt h p0 φseg−out(x, h, p0) segment rootfilter egm filt h p0 φback−in(x, h, p0) φback−out(x, h, p0) segment h rootfilter p0
Figure 1. The box-segment features: φseg−in and φseg−out, encourage the box to contain as many segment pixels as possible. This pair of features alone could result in box hypotheses that "overshoot" the segment. The purpose of the second pair of features, φback−in and φback−out, is the opposite: it tries to minimize the number of background pixels inside the box and maximize its number outside. In synchrony these features would try to tightly place a box around the segment. plies that no segment is selected. We will use S(h) to denote the segment that h indexes. As in, we employ a HOG pyramid to compute φ(x, p0), and use double resolution to compute the part features φ(x, pi). The features φ(x, h, p0) link segmentation and detection. In this paper, we define features at the level of the root, but our formulation can be easily extended to include features at the part level.
3.2. Segmentation Features
Given a set of candidate segments, we would like to encode features linking segmentation and detection while remaining computationally efficient. We would also like to be robust to over- and under- object segmentations, as well as false positive or missing segments. Towards this goal, we derive simple features which encourage the selected segment to agree with the object detection hypothesis. Most of our features employ integral images which makes them extremely efficient, as this computation can be done in constant time. We now describe the features in more details.
Segment-In:
Given a segment S(h), our first feature counts the percentage of pixels in S(h) that fall inside the bounding box defined by p0. Thus φseg−in(x, h, p0) =
|S(h)|
� p∈B(p0)
{p ∈ S(h)} where |S(h)| is the size of the segment indexed by h, and B(p0) is the set of pixels contained in the bounding box defined by p0. This feature encourages the bounding box to contain the segment.
Segment-Out:
Our second feature counts the percentage of segment pixels that are outside the bounding box, φseg−out(x, h, p0) =
|S(h)|
� p/∈B(p0)
{p ∈ S(h)}
This feature discourages boxes that do not contain all segment pixels.
Background-In:
We additionally compute a feature counting the amount of background inside the bounding box as follows φback−in(x, h, p0) =
N − |S(h)|
� p∈B(p0)
{p /∈ S(h)} with N the size of the image. This feature captures the statistics of how often the segments leak outside the true bounding box vs how often they are too small.
Background-Out:
This feature counts the amount of background outside the bounding box φback−out(x, h, p0) =
N − |S(h)|
� p/∈B(p0)
{p /∈ S(h)}
It tries to discourage bounding boxes that are too big and do not tightly fit the segments.
Overlap:
This feature penalizes bounding boxes which do not overlap well with the segment.
In particular, it computes the intersection over union between the candidate bounding box defined by p0 and the tighter bounding box around the segment S(h). It is defined as follows φoverlap(x, h, p0) = B(p0) ∩ B(S(h))
B(p0) ∪ B(S(h)) − λ with B(S(h)) the tighter bounding box around S(h), B(p0) the bounding box defined by p0, and λ a constant, which is the intersection over union level that defines a true positive.
We employ in practice λ = 0.7.
Background bias:
The value of all of the above features is 0 when h = 0. We incorporate an additional feature to learn the bias for the background segment (h = 0). This puts the scores of the HOG filters and the segmentation potentials into a common referential. We thus simply define
3296 φbias(x, h, p0) =
�
1 if h = 0
0 otherwise.
Fig. 1 depicts our features computed for a specific bounding box p0 and segment S(h). Note that the first two features, φseg−in and φseg−out, encourage the box to contain as many segment pixels as possible. This pair of features alone could result in box hypotheses that "overshoot" the segment. The purpose of the second pair of features, φback−in and φback−out, is the opposite: it tries to minimize the number of background pixels inside the box and maximize its number outside. In synchrony these features would try to tightly place a box around the segment. The overlap feature has a similar purpose, but helps us better tune the model to the VOC IOU evaluation setting.
3.3. Efficient Computation
Given the segments, all of our proposed features can be computed very efficiently. Note that the features have to be computed for each segment h, but this is not a problem as there are typically only a few segments per image. We start our discussion with the first four features, which can be computed in constant time using a single integral image per segment. This is both computationally and memory efficient. Let φint(h) be the integral image for segment h, which, at each point (u, v), counts the % of pixels that belong to this segment and are contained inside the subimage defined by the domain [0, u] × [0, v]. This is illustrated in Fig. 2. Given the integral image φint(h) for the h-segment, we compute the features as follows φseg−in(x, h, p0)
= φbr(h, p0) − φtr(h, p0)
−φbl(h, p0) + φtl(h, p0) φseg−out(x, h, p0)
=
|S(h)| − φseg−in(x, h, p0) φback−in(x, h, p0)
=
|B(p0)| − φseg−in(x, h, p0) φback−out(x, h, p0)
=(N − |S(h)|) − φback−in(x, h, p0) where as shown in Fig. 2, (φtl, φtr, φbl, φbr) indexes the integral image of segment S(h) at the four corners, i.e., topleft, top-right, bottom-left, bottom-right, of the bounding box defined by p0.
The overlap feature between a hypothesis p0 and a segment S(h) can also be computed very efficiently. First, we compute the intersection as:
B(p0) ∩ B(S(h)) = max
�
0, min(x0,right, xS(h),right) − max(x0,left, xS(h),left)
�
· max
�
0, min(y0,bottom, yS(h),bottom) − max(y0,top, yS(h),top)
�
Note that the overlap will be non-zero only when each of the terms is larger than 0. Given that the segment bounding box
B(S(h)) is fixed and the width and height of p0 at a particular level of the pyramid are fixed as well, we can quickly φint(h) segment S(h) integral image segment rootfilter s filte h p0 φseg−in(x, h, p0) = φbr(h, p0) − φtr(h, p0)
− φbl(h, p0) + φtl(h, p0) φbr(h, p0) φtr(h, p0) φbl(h, p0) φtl(h, p0)(h
) φ
Figure 2. Segment feature computation via integral image. compute the bounds of where in the image the feature needs to be computed (i.e., when the feature is different than 0).
The denominator, B(p0) ∪ B(S(h)), can then be simply computed as the sum of the box areas minus the overlap.
3.4. Inference
Inference in our model can be done by solving the following optimization problem max p0
�
P
� i=0 wT i · φ(x, pi) +
P
� i=1 max pi (wT i,def · φ(x, p0, pi)) +
+ max h (wT seg · φ(x, h, p0))
�
Note that this can be done efficiently using dynamic programming as the structure of the graphical model forms a tree.
The algorithm works as follows:
First, we compute maxh wT segφ(x, h, p0) as well as maxpi(wT i,def · φ(x, p0, pi)) for each root filter hypothesis p0.
We then compute the score as the sum of the HOG and segment score for each mixture component at the root level. Finally, we compute the maximum over the mixture components to get the score of an object hypothesis.
3.5. Learning
We learn a different weight for each feature using a latent structured-SVM. Allowing different weights for the different segmentation features is important in order to learn how likely is for each class to have segments that undershoot or overshoot the detection bounding box. We employ as loss the intersection over the union of the root filters. As in DPM, we initialize the model by first training only the root filters, followed by training a root mixture model. Finally we add the parts and perform several additional iterations of stochastic gradient descent.
Note that we expect the weights for φseg−in(x, h, p0), φback−out(x, h, p0) and φoverlap(x, h, p0) to be positive, as
3297 we would like to maximize the overlap, the amount of foreground inside the bounding box and background outside the bounding box. Similarly, the weights for φseg−out(x, h, p0) and φback−in(x, h, p0) are expected to be negative as we would like to minimize the amount of background inside the bounding box as well as the amount of foreground segment outside. In practice, as the object's shape can be far from rectangular, and the segments are noisy, the sign of the weights can vary to best capture the statistics of the data.
3.6. Implementation Details
We use CPMC to get the candidate segments. In particular, for most experiments we use the final segmentation output of. For each class, we find all connected components in the segmentation output, and remove those that do not exceed 1500 pixels. Unless otherwise noted, we do not use the score of the segments. On average, this gives us one segment per image. We also provide one experiment where we used more segments (5 on average per image), which we describe in Sec. 4.
4. Experimental Evaluation
We first evaluate our detection performance on val subset of PASCAL VOC 2010 detection dataset, and compare it to the baselines. We train all methods, including the baselines on the train subset. We use the standard PASCAL criterion for detection (50% IOU overlap) and report average precision (AP) as the measure of evaluation.
As baselines we use the Dalal&Triggs detector (which for fairness we compare to our detector when only using the root filters), the DPM, as well as CPMC when used as a detector. To compute the latter, we find all the connected components in the final segmentation output of CPMC, and place the tightest bounding box around each component. To compute the score of the box we utilize the CPMC ranking scores for the segments.
The comparison with and our approach (segDPM) without parts is shown in the top Table 1, while the bottom table compares CPMC-based detector, DPM and our approach with parts. We significantly outperform the baselines: Dalal & Triggs detector by 13% and the CPMC baseline by 10%. Our model also achieves a significant boost of 8% AP over the DPM, which is a well established and difficult baseline to beat. Importantly, we outperform DPM in 19 out of 20 classes. The main power of our approach is that it blends between DPM (appearance) and segmentation(CPMC). When there is no segment, the method just scores a regular DPM. When there is a segment, our approach is encouraged to tightly fit a box around it. However, in cases of under- or over- segmentation, the appearance part of our model can still correctly position the box. Note that our results well demonstrate the effectiveness of using blended detection and segmentation models for object detection.
Fig. 4 depicts examples illustrating the performance of our approach. Note that our approach is able to both retrieve detections where there is no segment as well as position the bounding box correctly where there is segment evidence.
We evaluate our approach on VOC 2010 test in Table 2. Here, we trained CPMC, as well as our model on
VOC trainval. We compare segDPM with DPM without the post-processing steps, i.e., bounding box prediction and context-rescoring, in the top of Table 2. In the bottom of Table 2 we compare our full approach with existing top scoring approaches. For the full approach, we show results when typical context-rescoring approach is used (same as in DPM), which we refer to as segDPM+rescore. We also show results when we rescored the detections by using the classification scores for each class, kindly provided to us by.
The classification (presence/absence of class in an image) accuracy measured by mean AP on VOC2010 is 76.2%.
We refer to this approach with segDPM+rescore+classif. We outperform the competitors by 3.6%, and achieve the best result in 13 out of 20 classes.
We also experimented with using more segments, on the VOC 2010 train / val split. In particular, among 150 segments per image returned by, we selected a topranking subset for each class, so that there was an average of 5 segments per image. The results are reported in Table 3.
We compare it to CPMC when using the same set of segments. One can see that with more segments our approach improves by 1.5%. As such, it outperforms DPM by 10%.
5. Conclusion
In this paper, we have proposed a novel deformable partbased model, which exploits region-based segmentation by allowing every detection hypothesis to select a segment (including void) from a pool of segment candidates. We derive simple yet very efficient features, which can capture the essential information encoded in the segments. Our detector scores each box in the image using both the HOG filters as in original DPM, as well as a set of novel segmentation features. This way, our model "blends" between the detector and the segmentation model, by boosting object hypotheses on the segments, while recovering from making mistakes by exploiting a powerful appearance model. We demonstrated the effectiveness of our approach in PASCAL VOC
2010, and show that when employing only a root filter our approach outperforms Dalal & Triggs detector by 13%
AP and when employing parts, we outperform the original
DPM by 8%. We believe that this is just the beginning of a new and exciting direction. We expect a new generation of object detectors which are able to exploit semantic segmentation yet to come.
Acknowledgments
R.M. was supported in part by NSF
0917141 and ARL 62250-CS.
3298 plane bike bird boat bottle bus car cat chair cow table dog horse motor person plant sheep sofa train tv
Avg.
VOC 2010 val, no parts
Dalal 
18.1 segDPM (no parts)
VOC 2010 val, with parts
CPMC (no score) 
CPMC (score) 
DPM 
26.6 segDPM (parts)
Table 1. AP performance (in %) on VOC 2010 val for our detector with parts, the DPM, and the CPMC-based detector. plane bike bird boat bottle bus car cat chair cow table dog horse motor person plant sheep sofa train tv
Avg.
VOC 2010 test, no post-processing
DPM no postproc. 
17.3 38.5 34.3 29.9 segDPM no postproc.
VOC 2010 test, with post-processing segDPM+rescore+classif
14.8 38.7 35.0 52.8 43.1 40.4 segDPM+rescore
NLPR HOGLBP 
MITUCLA HIERARCHY 
NUS HOGLBP CTX 
14.8 27.9 49.5 38.4 34.2 van de Sande et al. 
UOCTTI LSVM MDPM 
Gu et al. 
UVA DETMONKEY 
UVA GROUPLOC 
BONN FGT SEGM 
Table 2. AP performance (in %) on VOC 2010 test for our detector with parts and the DPM, without post processing (top table), and comparison with existing methods (only top 11 shown), with post-processing (table below).
References
 P. Arbelaez, B. Hariharan, C. Gu, S. Gupta, L. Bourdev, and J. Malik.
Finding animals: Semantic segmentation using regions and parts. In
CVPR, 2012. 1
 H. Azizpour and I. Laptev.
Object detection using stronglysupervised deformable part models. In ECCV, 2012. 1, 2
 L. Bertelli, T. Yu, D. Vu, and B. Gokturk. Kernelized structural svm learning for supervised object segmentation. In CVPR, 2011. 2
 L. Bourdev, S. Maji, T. Brox, and J. Malik. Detecting people using mutually consistent poselet activations. In ECCV, 2010. 1, 2
 T. Brox, L. Bourdev, S. Maji, and J. Malik. Object segmentation by alignment of poselet activations to image contours. In CVPR'11. 1
 G. Cardinal, X. Boix, J. van de Weijer, A. D. Bagdanov, J. Serrat, and J. Gonzalez. Harmony potentials for joint classification and segmentation. In CVPR, 2010. 1
 J. Carreira, R. Caseiroa, J. Batista, and C. Sminchisescu. Semantic segmentation with second-order pooling. In ECCV, 2012. 1, 5, 6, 7
 J. Carreira, F. Li, and C. Sminchisescu. Object Recognition by Sequential Figure-Ground Ranking. IJCV, 2011. 1, 5, 6
 Q. Chen, Z. Song, Y. Hua, Z. Huang, and S. Yan. Hierarchical matching with side information for image classification. In CVPR, 2012.
 Y. Chen, L. Zhu, and A. Yuille. Active mask hierarchies for object detection. In ECCV, 2010. 1, 2
 Q. Dai and D. Hoiem.
Learning to localize detected objects.
In
CVPR, 2012. 2
 N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In CVPR, pages I: 886–893, 2005. 1, 5, 6
 C. Desai, D. Ramanan, and C. Fowlkes. Discriminative models for multi-class object layout. In ICCV, 2009. 2
 P. Felzenszwalb, R. Girshick, D. McAllester, and D. Ramanan. Object detection with discriminatively trained part based models. PAMI, R. Girshick, P. Felzenszwalb, and D. McAllester. Object detection with grammar models. In NIPS, 2009. 4
 C. Gu, P. Arbelaez, Y. Lin, K. Yu, and J. Malik. Multi-component models for object detection. In ECCV, 2012. 6
 C. Gu, J. Lim, P. Arbelaez, and J. Malik. Recognition using regions.
In CVPR, 2009. 2
 G. Heitz, S. Gould, A. Saxena, and D. Koller. Cascaded classification models: Combining models for holistic scene understanding. In
NIPS, 2008. 1
 P. Kr¨ahenb¨uhl and V. Koltun. Efficient inference in fully connected crfs with gaussian edge potentials. In NIPS, 2011. 1, 2
 L. Ladicky, C. Russell, P. Kohli, and P. H. Torr. Graph cut based inference with co-occurrence statistics. In ECCV, 2010. 1
 L. Ladicky, P. Sturgess, K. Alahari, C. Russell, and P. H. Torr. What, where and how many? combining object detectors and crfs.
In
ECCV, 2010. 2
 V. Lempitsky, P. Kohli, C. Rother, and B. Sharp. Image segmentation with a bounding box prior. In ICCV, 2009. 1
 M. Maire, S. X. Yu, and P. Perona. Object detection and segmentation from joint embedding of parts and pixels. In ICCV, 2011. 2
 A. Monroy and B. Ommer. Beyond bounding-boxes: Learning object shape by model-driven grouping. In ECCV12. 2
 R. Mottaghi. Augmenting deformable part models with irregularshaped object patches. In CVPR, 2012. 2
 O. Parkhi, A. Vedaldi, C. V. Jawahar, and A. Zisserman. The truth about cats and dogs. In ICCV, 2011. 2
 M. Pedersoli, A. Vedaldi, and J. Gonzlez. A coarse-to-fine approach for fast deformable object detection. In CVPR, 2011. 2
 P. Srinivasan, Q. Zhu, and J. Shi. Many-to-one contour matching for describing and discriminating object shape. In CVPR, 2010. 2
 E. Sudderth, A. Torralba, W. T. Freeman, and A. Wilsky. Learning hierarchical models of scenes, objects, and parts. In ICCV, 2005. 1
1 recall precision class = aeroplane val 2010
Dalal, AP=29.1
CMPC, AP=53.3
DPM, AP=46.3
Ours−wo parts, AP=52.4
Ours−parts, AP=55.7
1 recall precision class = bicycle val 2010
Dalal, AP=36.9
CMPC, AP=19.5
DPM, AP=49.5
Ours−wo parts, AP=43.1
Ours−parts, AP=50
1 recall precision class = bird val 2010
Dalal, AP=2.89
CMPC, AP=22.8
DPM, AP=4.79
Ours−wo parts, AP=20.9
Ours−parts, AP=23.3
1 recall precision class = boat val 2010
Dalal, AP=3.39
CMPC, AP=15.7
DPM, AP=6.4
Ours−wo parts, AP=15.7
Ours−parts, AP=16
1 recall precision class = bottle val 2010
Dalal, AP=15.6
CMPC, AP=8.1
DPM, AP=22.6
Ours−wo parts, AP=18.6
Ours−parts, AP=28.5
1 recall precision class = bus val 2010
Dalal, AP=47.1
CMPC, AP=42.7
DPM, AP=53.5
Ours−wo parts, AP=55.8
Ours−parts, AP=57.4
1 recall precision class = car val 2010
Dalal, AP=27.1
CMPC, AP=22.1
DPM, AP=38.7
Ours−wo parts, AP=33.2
Ours−parts, AP=43.2
1 recall precision class = cat val 2010
Dalal, AP=11.4
CMPC, AP=51.3
DPM, AP=24.8
Ours−wo parts, AP=43.9
Ours−parts, AP=49.3
1 recall precision class = chair val 2010
Dalal, AP=9.83
CMPC, AP=4.27
DPM, AP=14.2
Ours−wo parts, AP=10.7
Ours−parts, AP=14.3
1 recall precision class = cow val 2010
Dalal, AP=5.81
CMPC, AP=18.9
DPM, AP=10.5
Ours−wo parts, AP=22
Ours−parts, AP=23.5
1 recall precision class = diningtable val 2010
Dalal, AP=6.05
CMPC, AP=10.5
DPM, AP=11
Ours−wo parts, AP=14.8
Ours−parts, AP=17.7
1 recall precision class = dog val 2010
Dalal, AP=5.03
CMPC, AP=28.1
DPM, AP=13
Ours−wo parts, AP=31.1
Ours−parts, AP=32.4
1 recall precision class = horse val 2010
Dalal, AP=24.8
CMPC, AP=30.5
DPM, AP=36.4
Ours−wo parts, AP=40.9
Ours−parts, AP=42.5
1 recall precision class = motorbike val 2010
Dalal, AP=28.4
CMPC, AP=38.3
DPM, AP=38.7
Ours−wo parts, AP=45.1
Ours−parts, AP=47.3
1 recall precision class = person val 2010
Dalal, AP=27.5
CMPC, AP=20.9
DPM, AP=42.7
Ours−wo parts, AP=33.6
Ours−parts, AP=42.1
1 recall precision class = pottedplant val 2010
Dalal, AP=2.19
CMPC, AP=6.03
DPM, AP=3.61
Ours−wo parts, AP=11.1
Ours−parts, AP=11.9
1 recall precision class = sheep val 2010
Dalal, AP=18.4
CMPC, AP=19.2
DPM, AP=26.9
Ours−wo parts, AP=27.3
Ours−parts, AP=32.5
1 recall precision class = sofa val 2010
Dalal, AP=9.21
CMPC, AP=18.6
DPM, AP=22.7
Ours−wo parts, AP=22
Ours−parts, AP=25.5
1 recall precision class = train val 2010
Dalal, AP=27.4
CMPC, AP=35.4
DPM, AP=34.2
Ours−wo parts, AP=42.5
Ours−parts, AP=43.9
1 recall precision class = tvmonitor val 2010
Dalal, AP=23.2
CMPC, AP=21.1
DPM, AP=31.2
Ours−wo parts, AP=31.7
Ours−parts, AP=39.7
Figure 3. Precision-recall curves on PASCAL VOC 2010 val. Note that our approach significantly outperforms all baselines. plane bike bird boat bottle bus car cat chair cow table dog horse motor person plant sheep sofa train tv
Avg.
VOC 2012 val, more segments
CPMC (1 seg) 
CPMC (5 seg) 
31.4 segDPM (1 seg)
34.7 segDPM (5 seg)
Table 3. AP performance (in %) on VOC 2010 val for our detector when using more segments.
 K. E. A. van de Sande, J. R. R. Uijlings, T. Gevers, and A. W. M.
Smeulders. Segmentation as selective search for object recognition.
In ICCV, 2011. 6
 A. Vedaldi, V. Gulshan, M. Varma, and A. Zisserman. Multiple ker3300 aeroplane aeroplane aeroplane car car car car car car car boat boat boat boat boat boat boat bird bird bird bird cat chair cat cat chair cat chair dog dog dog dog(a) GT(b) CPMC(c) DPM(d) segDPM
Figure 4. For each method, we show top k detections for each class, where k is the number of boxes for that class in GT. For example, for an image with a chair and a cat GT box, we show the top scoring box for chair and the top scoring box for cat. nels for object detection. In ICCV, 2009. 6
 Y. Yang, S. Hallman, D. Ramanan, and C. Fowlkes. Layered object models for image segmentation. PAMI, 2011. 2
 J. Yao, S. Fidler, and R. Urtasun. Describing the scene as a whole:
Joint object detection, scene classification and semantic segmentation. In CVPR, 2012. 1, 2
 Y. Yu, J. Zhang, Y. Huang, S. Zheng, W. Ren, C. Wang, K. Huang, and T. Tan. Object detection by context and boosted hog-lbp. In
ECCV w. on PASCAL, 2010. 6
 L. Zhu, Y. Chen, A. Yuille, and W. Freeman. Latent hierarchical structural learning for object detection. In CVPR, 2010. 1, 2, 6DeepWalk: Online Learning of Social Representations
Bryan Perozzi
Stony Brook University
Department of Computer
Science
Rami Al-Rfou
Stony Brook University
Department of Computer
Science
Steven Skiena
Stony Brook University
Department of Computer
Science
{bperozzi, ralrfou, skiena}@cs.stonybrook.edu
ABSTRACT
We present DeepWalk, a novel approach for learning latent representations of vertices in a network. These latent representations encode social relations in a continuous vector space, which is easily exploited by statistical models. DeepWalk generalizes recent advancements in language modeling and unsupervised feature learning (or deep learning) from sequences of words to graphs.
DeepWalk uses local information obtained from truncated random walks to learn latent representations by treating walks as the equivalent of sentences. We demonstrate DeepWalk's latent representations on several multi-label network classification tasks for social networks such as BlogCatalog, Flickr, and YouTube. Our results show that DeepWalk outperforms challenging baselines which are allowed a global view of the network, especially in the presence of missing information. DeepWalk's representations can provide F1 scores up to 10% higher than competing methods when labeled data is sparse. In some experiments, DeepWalk's representations are able to outperform all baseline methods while using 60% less training data.
DeepWalk is also scalable. It is an online learning algorithm which builds useful incremental results, and is trivially parallelizable. These qualities make it suitable for a broad class of real world applications such as network classification, and anomaly detection.
Categories and Subject Descriptors
H.2.8 [Database Management]: Database ApplicationsData Mining; I.2.6 [Artificial Intelligence]: Learning; I.5.1
[Pattern Recognition]: Model - Statistical
Keywords social networks; deep learning; latent representations; learning with partial labels; network classification; online learning
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.
KDD'14, August 24–27, 2014, New York, NY, USA.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-2956-9/14/08...$15.00. http://dx.doi.org/10.1145/2623330.2623732(a) Input: Karate Graph(b) Output: Representation
Figure 1: Our proposed method learns a latent space representation of social interactions in Rd. The learned representation encodes community structure so it can be easily exploited by standard classification methods. Here, our method is used on Zachary's Karate network to generate a latent representation in R2. Note the correspondence between community structure in the input graph and the embedding.
Vertex colors represent a modularity-based clustering of the input graph.
INTRODUCTION
The sparsity of a network representation is both a strength and a weakness. Sparsity enables the design of efficient discrete algorithms, but can make it harder to generalize in statistical learning. Machine learning applications in networks (such as network classification, content recommendation, anomaly detection, and missing link prediction ) must be able to deal with this sparsity in order to survive.
In this paper we introduce deep learning (unsupervised feature learning) techniques, which have proven successful in natural language processing, into network analysis for the first time. We develop an algorithm (DeepWalk) that learns social representations of a graph's vertices, by modeling a stream of short random walks. Social representations are latent features of the vertices that capture neighborhood similarity and community membership. These latent representations encode social relations in a continuous vector space with a relatively small number of dimensions. DeepWalk generalizes neural language models to process a special language composed of a set of randomly-generated walks.
These neural language models have been used to capture the semantic and syntactic structure of human language, and even logical analogies.
DeepWalk takes a graph as input and produces a latent representation as an output.
The result of applying our method to the well-studied Karate network is shown in Figure 1. The graph, as typically presented by force-directed layouts, is shown in Figure 1a. Figure 1b shows the output of our method with 2 latent dimensions. Beyond the striking similarity, we note that linearly separable portions of (1b) correspond to clusters found through modularity maximization in the input graph (1a) (shown as vertex colors).
To demonstrate DeepWalk's potential in real world scenarios, we evaluate its performance on challenging multi-label network classification problems in large heterogeneous graphs.
In the relational classification problem, the links between feature vectors violate the traditional i.i.d. assumption. Techniques to address this problem typically use approximate inference techniques to leverage the dependency information to improve classification results. We distance ourselves from these approaches by learning label-independent representations of the graph. Our representation quality is not influenced by the choice of labeled vertices, so they can be shared among tasks.
DeepWalk outperforms other latent representation methods for creating social dimensions, especially when labeled nodes are scarce. Strong performance with our representations is possible with very simple linear classifiers (e.g. logistic regression). Our representations are general, and can be combined with any classification method (including iterative inference methods). DeepWalk achieves all of that while being an online algorithm that is trivially parallelizable.
Our contributions are as follows:
• We introduce deep learning as a tool to analyze graphs, to build robust representations that are suitable for statistical modeling. DeepWalk learns structural regularities present within short random walks.
• We extensively evaluate our representations on multilabel classification tasks on several social networks. We show significantly increased classification performance in the presence of label sparsity, getting improvements
5%-10% of Micro F1, on the sparsest problems we consider. In some cases, DeepWalk's representations can outperform its competitors even when given 60% less training data.
• We demonstrate the scalability of our algorithm by building representations of web-scale graphs, (such as
YouTube) using a parallel implementation. Moreover, we describe the minimal changes necessary to build a streaming version of our approach.
The rest of the paper is arranged as follows. In Sections 2 and 3, we discuss the problem formulation of classification in data networks, and how it relates to our work. In Section 4 we present DeepWalk, our approach for Social Representation
Learning. We outline ours experiments in Section 5, and present their results in Section 6. We close with a discussion of related work in Section 7, and our conclusions.
PROBLEM DEFINITION
We consider the problem of classifying members of a social network into one or more categories. Let G = (V, E), where V represent the members of the network, E are their connections, E ⊆ (V × V ), and GL = (V, E, X, Y ) is a partially labeled social network, with attributes X ∈ R|V |×S where S is the size of the feature space for each attribute vector, and Y ∈ R|V |×|Y|, Y is the set of labels.
In a traditional machine learning classification setting, we aim to learn a hypothesis H that maps elements of X to the labels set Y. In our case, we can utilize the significant information about the dependence of the examples embedded in the structure of G to achieve superior performance.
In the literature, this is known as the relational classification (or the collective classification problem ). Traditional approaches to relational classification pose the problem as an inference in an undirected Markov network, and then use iterative approximate inference algorithms (such as the iterative classification algorithm, Gibbs Sampling, or label relaxation ) to compute the posterior distribution of labels given the network structure.
We propose a different approach to capture the network topology information. Instead of mixing the label space as part of the feature space, we propose an unsupervised method which learns features that capture the graph structure independent of the labels' distribution.
This separation between the structural representation and the labeling task avoids cascading errors, which can occur in iterative methods. Moreover, the same representation can be used for multiple classification problems concerning that network.
Our goal is to learn XE ∈ R|V |×d, where d is small number of latent dimensions. These low-dimensional representations are distributed; meaning each social phenomena is expressed by a subset of the dimensions and each dimension contributes to a subset of the social concepts expressed by the space.
Using these structural features, we will augment the attributes space to help the classification decision.
These features are general, and can be used with any classification algorithm (including iterative methods). However, we believe that the greatest utility of these features is their easy integration with simple machine learning algorithms. They scale appropriately in real-world networks, as we will show in Section 6.
LEARNING SOCIAL REPRESENTATIONS
We seek to learn social representations with the following characteristics:
• Adaptability - Real social networks are constantly evolving; new social relations should not require repeating the learning process all over again.
• Community aware - The distance between latent dimensions should represent a metric for evaluating social similarity between the corresponding members of the network. This allows generalization in networks with homophily.
• Low dimensional - When labeled data is scarce lowdimensional models generalize better, and speed up convergence and inference.
• Continuous - We require latent representations to model partial community membership in continuous space.
In addition to providing a nuanced view of community membership, a continuous representation has smooth decision boundaries between communities which allows more robust classification.
Our method satisfies these requirements by learning representation for vertices from a stream of short random walks, using optimization techniques originally designed for language modeling. Here, we review the basics of both random walks and language modeling, and describe how their combination satisfies our requirements.
Vertexmvisitationmcount
5 k mofmVertices
FrequencymofmVertexmOccurrenceminmShortmRandommWalks(a) YouTube Social Graph
Word mention count
# of Words
Frequency of Word Occurrence in Wikipedia(b) Wikipedia Article Text
Figure 2: The distribution of vertices appearing in short random walks (2a) follows a power-law, much like the distribution of words in natural language (2b).
Random Walks
We denote a random walk rooted at vertex vi as Wvi. It is a stochastic process with random variables W1 vi, W2 vi,..., Wk vi such that Wk+1 vi is a vertex chosen at random from the neighbors of vertex vk. Random walks have been used as a similarity measure for a variety of problems in content recommendation and community detection. They are also the foundation of a class of output sensitive algorithms which use them to compute local community structure information in time sublinear to the size of the input graph.
It is this connection to local structure that motivates us to use a stream of short random walks as our basic tool for extracting information from a network. In addition to capturing community information, using random walks as the basis for our algorithm gives us two other desirable properties.
First, local exploration is easy to parallelize.
Several random walkers (in different threads, processes, or machines) can simultaneously explore different parts of the same graph. Secondly, relying on information obtained from short random walks make it possible to accommodate small changes in the graph structure without the need for global recomputation. We can iteratively update the learned model with new random walks from the changed region in time sub-linear to the entire graph.
Connection: Power laws
Having chosen online random walks as our primitive for capturing graph structure, we now need a suitable method to capture this information. If the degree distribution of a connected graph follows a power law (i.e. scale-free), we observe that the frequency which vertices appear in the short random walks will also follow a power-law distribution.
Word frequency in natural language follows a similar distribution, and techniques from language modeling account for this distributional behavior. To emphasize this similarity we show two different power-law distributions in Figure 2. The first comes from a series of short random walks on a scale-free graph, and the second comes from the text of 100,000 articles from the English Wikipedia.
A core contribution of our work is the idea that techniques which have been used to model natural language (where the symbol frequency follows a power law distribution (or Zipf's law)) can be re-purposed to model community structure in networks. We spend the rest of this section reviewing the growing work in language modeling, and transforming it to learn representations of vertices which satisfy our criteria.
Language Modeling
The goal of language modeling is to estimate the likelihood of a specific sequence of words appearing in a corpus. More formally, given a sequence of words W n
1 = (w0, w1, · · ·, wn), where wi ∈ V (V is the vocabulary), we would like to maximize the Pr(wn|w0, w1, · · ·, wn−1) over all the training corpus. Recent work in representation learning has focused on using probabilistic neural networks to build general representations of words which extend the scope of language modeling beyond its original goals.
In this work, we present a generalization of language modeling to explore the graph through a stream of short random walks. These walks can be thought of as short sentences and phrases in a special language; the direct analog is to estimate the likelihood of observing vertex vi given all the previous vertices visited so far in the random walk, i.e.
Pr
� vi | (v1, v2, · · ·, vi−1)
�
Our goal is to learn a latent representation, not only a probability distribution of node co-occurrences, and so we introduce a mapping function Φ: v ∈ V �→ R|V |×d. This mapping Φ represents the latent social representation associated with each vertex v in the graph. (In practice, we represent Φ by a |V |×d matrix of free parameters, which will serve later on as our XE). The problem then, is to estimate the likelihood:
Pr
� vi |
�
Φ(v1), Φ(v2), · · ·, Φ(vi−1)
��
However, as the walk length grows, computing this conditional probability becomes unfeasible.
A recent relaxation in language modeling turns the prediction problem on its head. First, instead of using the context to predict a missing word, it uses one word to predict the context. Secondly, the context is composed of the words appearing to both the right and left of the given word. Finally, it removes the ordering constraint on the problem, instead, requiring the model to maximize the probability of any word appearing in the context without the knowledge of its offset from the given word. In terms of vertex representation modeling, this yields the optimization problem: minimize
Φ
− log Pr
�
{vi−w, · · ·, vi+w} \ vi | Φ(vi)
�
We find these relaxations are particularly desirable for social representation learning. First, the order independence assumption better captures a sense of 'nearness' that is provided by random walks. Moreover, this relaxation is quite useful for speeding up the training time by building small models as one vertex is given at a time.
Solving the optimization problem from Eq. 3 builds representations that capture the shared similarities in local graph structure between vertices. Vertices which have similar neighborhoods will acquire similar representations (encoding co-citation similarity), allowing generalization on machine learning tasks.
By combining both truncated random walks and language models we formulate a method which satisfies all of our desired properties. This method generates representations of social networks that are low-dimensional, and exist in a continuous vector space. Its representations encode latent forms of community membership, and because the method
Algorithm 1 DeepWalk(G, w, d, γ, t)
Input: graph G(V, E) window size w embedding size d walks per vertex γ walk length t
Output: matrix of vertex representations Φ ∈ R|V |×d
1: Initialization: Sample Φ from U|V |×d
2: Build a binary Tree T from V
3: for i = 0 to γ do
O = Shuffle(V )
5: for each vi ∈ O do
Wvi = RandomWalk(G, vi,t)
SkipGram(Φ, Wvi, w)
8: end for
9: end for outputs useful intermediate representations, it can adapt to changing network topology.
METHOD
In this section we discuss the main components of our algorithm. We also present several variants of our approach and discuss their merits.
Overview
As in any language modeling algorithm, the only required input is a corpus and a vocabulary V. DeepWalk considers a set of short truncated random walks its own corpus, and the graph vertices as its own vocabulary (V = V ). While it is beneficial to know V and the frequency distribution of vertices in the random walks ahead of the training, it is not necessary for the algorithm to work as we will show in 4.2.2.
Algorithm: DeepWalk
The algorithm consists of two main components; first a random walk generator, and second, an update procedure.
The random walk generator takes a graph G and samples uniformly a random vertex vi as the root of the random walk Wvi. A walk samples uniformly from the neighbors of the last vertex visited until the maximum length (t) is reached. While we set the length of our random walks in the experiments to be fixed, there is no restriction for the random walks to be of the same length. These walks could have restarts (i.e. a teleport probability of returning back to their root), but our preliminary results did not show any advantage of using restarts. In practice, our implementation specifies a number of random walks γ of length t to start at each vertex.
Lines 3-9 in Algorithm 1 shows the core of our approach.
The outer loop specifies the number of times, γ, which we should start random walks at each vertex. We think of each iteration as making a 'pass' over the data and sample one walk per node during this pass. At the start of each pass we generate a random ordering to traverse the vertices. This is not strictly required, but is well-known to speed up the convergence of stochastic gradient descent.
In the inner loop, we iterate over all the vertices of the graph.
For each vertex vi we generate a random walk
|Wvi| = t, and then use it to update our representations(Line 7). We use the SkipGram algorithm to update
Algorithm 2 SkipGram(Φ, Wvi, w)
1: for each vj ∈ Wvi do
2: for each uk ∈ Wvi[j − w : j + w] do
J(Φ) = − log Pr(uk | Φ(vj))
Φ = Φ − α ∗ ∂J
∂Φ
5: end for
6: end for these representations in accordance with our objective function in Eq. 3.
SkipGram
SkipGram is a language model that maximizes the cooccurrence probability among the words that appear within a window, w, in a sentence. It approximates the conditional probability in Equation 3 using an independence assumption as the following
Pr
�
{vi−w, · · ·, vi+w} \ vi | Φ(vi)
�
= i+w
� j=i−w j̸=i
Pr(vj|Φ(vi))
Algorithm 2 iterates over all possible collocations in random walk that appear within the window w (lines 1-2). For each, we map each vertex vj to its current representation vector Φ(vj) ∈ Rd (See Figure 3b). Given the representation of vj, we would like to maximize the probability of its neighbors in the walk (line 3). We can learn such a posterior distribution using several choices of classifiers. For example, modeling the previous problem using logistic regression would result in a huge number of labels (that is equal to
|V |) which could be in millions or billions. Such models require vast computational resources which could span a whole cluster of computers. To avoid this necessity and speed up the training time, we instead use the Hierarchical
Softmax to approximate the probability distribution.
Hierarchical Softmax
Given that uk ∈ V, calculating Pr(uk | Φ(vj)) in line 3 is not feasible. Computing the partition function (normalization factor) is expensive, so instead we will factorize the conditional probability using Hierarchical softmax. We assign the vertices to the leaves of a binary tree, turning the prediction problem into maximizing the probability of a specific path in the hierarchy (See Figure 3c). If the path to vertex uk is identified by a sequence of tree nodes (b0, b1,..., b⌈log |V |⌉), (b0 = root, b⌈log |V |⌉ = uk) then
Pr(uk | Φ(vj)) =
⌈log |V |⌉
� l=1
Pr(bl | Φ(vj))
Now, Pr(bl | Φ(vj)) could be modeled by a binary classifier that is assigned to the parent of the node bl as Equation 6 shows, Pr(bl | Φ(vj) = 1/(1 + e−Φ(vj)·Ψ(bl))(6) where Ψ(bl) ∈ Rd is the representation assigned to tree node bl's parent. This reduces the computational complexity of calculating Pr(uk | Φ(vj)) from O(|V |) to O(log |V |).
We can speed up the training process further, by assigning shorter paths to the frequent vertices in the random walks.
Huffman coding is used to reduce the access time of frequent elements in the tree.(a) Random walk generation.(b) Representation mapping.(c) Hierarchical Softmax.
Figure 3: Overview of DeepWalk. We slide a window of length 2w + 1 over the random walk Wv4, mapping the central vertex v1 to its representation Φ(v1). Hierarchical Softmax factors out Pr(v3 | Φ(v1)) and Pr(v5 | Φ(v1)) over sequences of probability distributions corresponding to the paths starting at the root and ending at v3 and v5. The representation Φ is updated to maximize the probability of v1 co-occurring with its context {v3, v5}.
Optimization
The model parameter set is θ = {Φ, Ψ} where the size of each is O(d|V |). Stochastic gradient descent (SGD) is used to optimize these parameters (Line 4, Algorithm 2).
The derivatives are estimated using the back-propagation algorithm. The learning rate α for SGD is initially set to
2.5% at the beginning of the training and then decreased linearly with the number of vertices that are seen so far.
Parallelizability
As shown in Figure 2 the frequency distribution of vertices in random walks of social network and words in a language both follow a power law. This results in a long tail of infrequent vertices, therefore, the updates that affect Φ will be sparse in nature. This allows us to use asynchronous version of stochastic gradient descent (ASGD), in the multi-worker case. Given that our updates are sparse and we do not acquire a lock to access the model shared parameters, ASGD will achieve an optimal rate of convergence. While we run experiments on one machine using multiple threads, it has been demonstrated that this technique is highly scalable, and can be used in very large scale machine learning.
Figure 4 presents the effects of parallelizing DeepWalk. It shows the speed up in processing BlogCatalog and Flickr networks is consistent as we increase the number of workers to 8 (Figure 4a). It also shows that there is no loss of predictive performance relative to the running DeepWalk serially(Figure 4b).
Algorithm Variants
Here we discuss some variants of our proposed method, which we believe may be of interest.
Streaming
One interesting variant of this method is a streaming approach, which could be implemented without knowledge of the entire graph. In this variant small walks from the graph are passed directly to the representation learning code, and the model is updated directly. Some modifications to the learning process will also be necessary. First, using a decaying learning rate may no longer be desirable as it assumes the knowledge of the total corpus size. Instead, we can initialize
# of Workers
2-3
2-2
2-1
Relative Time
BlogCatalog
Flickr(a) Running Time
# of Workers
Relative Change in Micro F1
BlogCatalog
Flickr(b) Performance
Figure 4: Effects of parallelizing DeepWalk the learning rate α to a small constant value. This will take longer to learn, but may be worth it in some applications.
Second, we cannot necessarily build a tree of parameters any more. If the cardinality of V is known (or can be bounded), we can build the Hierarchical Softmax tree for that maximum value. Vertices can be assigned to one of the remaining leaves when they are first seen. If we have the ability to estimate the vertex frequency a priori, we can also still use Huffman coding to decrease frequent element access times.
Non-random walks
Some graphs are created as a by-product of agents interacting with a sequence of elements (e.g. users' navigation of pages on a website). When a graph is created by such a stream of non-random walks, we can use this process to feed the modeling phase directly. Graphs sampled in this way will not only capture information related to network structure, but also to the frequency at which paths are traversed.
In our view, this variant also encompasses language modeling. Sentences can be viewed as purposed walks through an appropriately designed language network, and language models like SkipGram are designed to capture this behavior.
This approach can be combined with the streaming variant(Section 4.4.1) to train features on a continually evolving network without ever explicitly constructing the entire graph.
Maintaining representations with this technique could enable web-scale classification without the hassles of dealing with a web-scale graph.
Name
BlogCatalog
Flickr
YouTube
|V |
|E|
|Y|
Labels
Interests
Groups
Groups
Table 1: Graphs used in our experiments.
EXPERIMENTAL DESIGN
In this section we provide an overview of the datasets and methods which we will use in our experiments. Code and data to reproduce our results will be available at the first author's website.1
Datasets
An overview of the graphs we consider in our experiments is given in Figure 1.
• BlogCatalog is a network of social relationships provided by blogger authors. The labels represent the topic categories provided by the authors.
• Flickr is a network of the contacts between users of the photo sharing website. The labels represent the interest groups of the users such as 'black and white photos'.
• YouTube is a social network between users of the popular video sharing website. The labels here represent groups of viewers that enjoy common video genres (e.g. anime and wrestling).
Baseline Methods
To validate the performance of our approach we compare it against a number of baselines:
• SpectralClustering : This method generates a representation in Rd from the d-smallest eigenvectors of �L, the normalized graph Laplacian of G. Utilizing the eigenvectors of �L implicitly assumes that graph cuts will be useful for classification.
• Modularity : This method generates a representation in Rd from the top-d eigenvectors of B, the Modularity matrix of G. The eigenvectors of B encode information about modular graph partitions of G.
Using them as features assumes that modular graph partitions will be useful for classification.
• EdgeCluster : This method uses k-means clustering to cluster the adjacency matrix of G. Its has been shown to perform comparably to the Modularity method, with the added advantage of scaling to graphs which are too large for spectral decomposition.
• wvRN : The weighted-vote Relational Neighbor is a relational classifier. Given the neighborhood Ni of vertex vi, wvRN estimates Pr(yi|Ni) with the (appropriately normalized) weighted mean of its neighbors (i.e
Pr(yi|Ni) =
Z
� vj∈Ni wij Pr(yj | Nj)). It has shown surprisingly good performance in real networks, and has been advocated as a sensible relational classification baseline.
• Majority: This na¨ıve method simply chooses the most frequent labels in the training set.
1http://bit.ly/deepwalk
EXPERIMENTS
In this section we present an experimental analysis of our method. We thoroughly evaluate it on a number of multilabel classification tasks, and analyze its sensitivity across several parameters.
Multi-Label Classification
To facilitate the comparison between our method and the relevant baselines, we use the exact same datasets and experimental procedure as in. Specifically, we randomly sample a portion (TR) of the labeled nodes, and use them as training data. The rest of the nodes are used as test. We repeat this process 10 times, and report the average performance in terms of both Macro-F1 and Micro-F1. When possible we report the original results here directly.
For all models we use a one-vs-rest logistic regression implemented by LibLinear extended to return the most probable labels as in.
We present results for DeepWalk with (γ = 80, w = 10, d = 128). The results for(SpectralClustering, Modularity, EdgeCluster) use Tang and Liu's preferred dimensionality, d = 500.
BlogCatalog
In this experiment we increase the training ratio (TR) on the BlogCatalog network from 10% to 90%. Our results are presented in Table 2. Numbers in bold represent the highest performance in each column.
DeepWalk performs consistently better than EdgeCluster, Modularity, and wvRN. In fact, when trained with only 20% of the nodes labeled, DeepWalk performs better than these approaches when they are given 90% of the data. The performance of SpectralClustering proves much more competitive, but DeepWalk still outperforms when labeled data is sparse on both Macro-F1 (TR ≤ 20%) and Micro-F1 (TR ≤ 60%).
This strong performance when only small fractions of the graph are labeled is a core strength of our approach. In the following experiments, we investigate the performance of our representations on even more sparsely labeled graphs.
Flickr
In this experiment we vary the training ratio (TR) on the Flickr network from 1% to 10%. This corresponds to having approximately 800 to 8,000 nodes labeled for classification in the entire network. Table 3 presents our results, which are consistent with the previous experiment. DeepWalk outperforms all baselines by at least 3% with respect to MicroF1. Additionally, its Micro-F1 performance when only 3% of the graph is labeled beats all other methods even when they have been given 10% of the data. In other words, DeepWalk can outperform the baselines with 60% less training data. It also performs quite well in Macro-F1, initially performing close to SpectralClustering, but distancing itself to a 1% improvement.
YouTube
The YouTube network is considerably larger than the previous ones we have experimented on, and its size prevents two of our baseline methods (SpectralClustering and Modularity) from running on it. It is much closer to a real world graph than those we have previously considered.
The results of varying the training ratio (TR) from 1% to
10% are presented in Table 4. They show that DeepWalk significantly outperforms the scalable baseline for creating
% Labeled Nodes
DeepWalk
SpectralClustering
EdgeCluster
Micro-F1(%)
Modularity
38.18 wvRN
Majority
DeepWalk
SpectralClustering
EdgeCluster
Macro-F1(%)
Modularity
24.97 wvRN
Majority
Table 2: Multi-label classification results in BlogCatalog
% Labeled Nodes
DeepWalk
SpectralClustering
Micro-F1(%)
EdgeCluster
Modularity
29.2 wvRN
Majority
DeepWalk
SpectralClustering
Macro-F1(%)
EdgeCluster
Modularity
17.12 wvRN
Majority
Table 3: Multi-label classification results in Flickr
% Labeled Nodes
DeepWalk
SpectralClustering
—
—
—
—
—
—
—
—
—
—
Micro-F1(%)
EdgeCluster
Modularity
—
—
—
—
—
—
—
—
—
— wvRN
Majority
DeepWalk
SpectralClustering
—
—
—
—
—
—
—
—
—
—
Macro-F1(%)
EdgeCluster
Modularity
—
—
—
—
—
—
—
—
—
— wvRN
Majority
Table 4: Multi-label classification results in YouTube
28 d
Micro F1
Training(a1) Flickr, γ = 30
28 d
Micro F1 γ(a2) Flickr, TR = 0.05
28 d
Micro F1
Training(a3) BlogCatalog, γ = 30
28 d
Micro F1 γ(a4) BlogCatalog, TR = 0.5(a) Stability over dimensions, d
27 γ
Micro F1 d(b1) Flickr, TR = 0.05
27 γ
Micro F1
Training(b2) Flickr, d = 128
27 γ
Micro F1 d(b3) BlogCatalog, TR = 0.5
27 γ
Micro F1
Training(b4) BlogCatalog, d = 128(b) Stability over number of walks, γ
Figure 5: Parameter Sensitivity Study graph representations, EdgeCluster. When 1% of the labeled nodes are used for test, the Micro-F1 improves by 14%.
The Macro-F1 shows a corresponding 10% increase. This lead narrows as the training data increases, but DeepWalk ends with a 3% lead in Micro-F1, and an impressive 5% improvement in Macro-F1.
This experiment showcases the performance benefits that can occur from using social representation learning for multilabel classification. DeepWalk, can scale to large graphs, and performs exceedingly well in such a sparsely labeled environment.
Parameter Sensitivity
In order to evaluate how changes to the parameterization of DeepWalk effect its performance on classification tasks, we conducted experiments on two multi-label classifications tasks (Flickr, and BlogCatalog). In the interest of brevity, we have fixed the window size and the walk length to emphasize local structure (w = 10, t = 40). We then vary the number of latent dimensions (d), the number of walks started per vertex (γ), and the amount of training data available (TR) to determine their impact on the network classification performance.
Effect of Dimensionality
Figure 5a shows the effects of increasing the number of latent dimensions available to our model.
Figures 5a1 and 5a3 examine the effects of varying the dimensionality and training ratio. The performance is quite consistent between both Flickr and BlogCatalog and show that the optimal dimensionality for a model is dependent on the number of training examples. (Note that 1% of Flickr has approximately as many labeled examples as 10% of BlogCatalog).
Figures 5a2 and 5a4 examine the effects of varying the dimensionality and number of walks per vertex. The relative performance between dimensions is relatively stable across different values of γ.
These charts have two interesting observations. The first is that there is most of the benefit is accomplished by starting γ = 30 walks per node in both graphs. The second is that the relative difference between different values of γ is quite consistent between the two graphs. Flickr has an order of magnitude more edges than
BlogCatalog, and we find this behavior interesting.
These experiments show that our method can make useful models of various sizes. They also show that the performance of the model depends on the number of random walks it has seen, and the appropriate dimensionality of the model depends on the training examples available.
Effect of sampling frequency
Figure 5b shows the effects of increasing γ, the number of random walks that we start from each vertex.
The results are very consistent for different dimensions(Fig. 5b1, Fig. 5b3) and the amount of training data (Fig.
5b2, Fig. 5b4). Initially, increasing γ has a big effect in the results, but this effect quickly slows (γ > 10). These results demonstrate that we are able to learn meaningful latent representations for vertices after only a small number of random walks.
RELATED WORK
The main differences between our proposed method and previous work can be summarized as follows:
1. We learn our latent social representations, instead of computing statistics related to centrality or partitioning.
2. We do not attempt to extend the classification procedure itself (through collective inference or graph kernels ).
3. We propose a scalable online method which uses only local information. Most methods require global information and are offline [17,39–41].
4. We apply unsupervised representation learning to graphs.
In this section we discuss related work in network classification and unsupervised feature learning.
Relational Learning
Relational classification (or collective classification) methods use links between data items as part of the classification process. Exact inference in the collective classification problem is NP-hard, and solutions have focused on the use of approximate inference algorithm which may not be guaranteed to converge.
The most relevant relational classification algorithms to our work incorporate community information by learning clusters, by adding edges between nearby nodes, by using PageRank, or by extending relational classification to take additional features into account. Our work takes a substantially different approach. Instead of a new approximation inference algorithm, we propose a procedure which learns representations of network structure which can then be used by existing inference procedure (including iterative ones).
A number of techniques for generating features from graphs have also been proposed [13,17,39–41]. In contrast to these methods, we frame the feature creation procedure as a representation learning problem.
Graph Kernels have been proposed as a way to use relational data as part of the classification process, but are quite slow unless approximated. Our approach is complementary; instead of encoding the structure as part of a kernel function, we learn a representation which allows them to be used directly as features for any classification method.
Unsupervised Feature Learning
Distributed representations have been proposed to model structural relationship between concepts. These representations are trained by the back-propagation and gradient descent. Computational costs and numerical instability led to these techniques to be abandoned for almost a decade.
Recently, distributed computing allowed for larger models to be trained, and the growth of data for unsupervised learning algorithms to emerge. Distributed representations usually are trained through neural networks, these networks have made advancements in diverse fields such as computer vision, speech recognition, and natural language processing.
CONCLUSIONS
We propose DeepWalk, a novel approach for learning latent social representations of vertices. Using local information from truncated random walks as input, our method learns a representation which encodes structural regularities. Experiments on a variety of different graphs illustrate the effectiveness of our approach on challenging multi-label classification tasks.
As an online algorithm, DeepWalk is also scalable. Our results show that we can create meaningful representations for graphs which are too large for standard spectral methods.
On such large graphs, our method significantly outperforms other methods designed to operate for sparsity. We also show that our approach is parallelizable, allowing workers to update different parts of the model concurrently.
In addition to being effective and scalable, our approach is also an appealing generalization of language modeling.
This connection is mutually beneficial.
Advances in language modeling may continue to generate improved latent representations for networks. In our view, language modeling is actually sampling from an unobservable language graph. We believe that insights obtained from modeling observable graphs may in turn yield improvements to modeling unobservable ones.
Our future work in the area will focus on investigating this duality further, using our results to improve language modeling, and strengthening the theoretical justifications of the method.
Acknowledgements
The authors thank the reviewers for their helpful comments. This research was partially supported by NSF Grants DBI-1060572 and IIS-1017181, and a Google Faculty Research Award.
REFERENCES
 R. Al-Rfou, B. Perozzi, and S. Skiena. Polyglot:
Distributed word representations for multilingual nlp.
In Proceedings of the Seventeenth Conference on
Computational Natural Language Learning, pages
183–192, Sofia, Bulgaria, August 2013. ACL.
 R. Andersen, F. Chung, and K. Lang. Local graph partitioning using pagerank vectors. In Foundations of Computer Science, 2006. FOCS'06. 47th Annual IEEE
Symposium on, pages 475–486. IEEE, 2006.
 Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives. 2013.
 Y. Bengio, R. Ducharme, and P. Vincent. A neural probabilistic language model. Journal of Machine
Learning Research, 3:1137–1155, 2003.
 L. Bottou. Stochastic gradient learning in neural networks. In Proceedings of Neuro-Nˆımes 91, Nimes, France, 1991. EC2.
 V. Chandola, A. Banerjee, and V. Kumar. Anomaly detection: A survey. ACM Computing Surveys (CSUR), R. Collobert and J. Weston. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th
ICML, ICML '08, pages 160–167. ACM, 2008.
 G. E. Dahl, D. Yu, L. Deng, and A. Acero.
Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition. Audio, Speech, and Language Processing, IEEE Transactions on, 20(1):30–42, 2012.
 J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, Q. Le, M. Mao, M. Ranzato, A. Senior, P. Tucker, K. Yang, and A. Ng. Large scale distributed deep networks. In P. Bartlett, F. Pereira, C. Burges, L. Bottou, and K. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages
1232–1240. 2012.
 D. Erhan, Y. Bengio, A. Courville, P.-A. Manzagol, P. Vincent, and S. Bengio. Why does unsupervised pre-training help deep learning? The Journal of Machine Learning Research, 11:625–660, 2010.
 R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research, 9:1871–1874, 2008.
 F. Fouss, A. Pirotte, J.-M. Renders, and M. Saerens.
Random-walk computation of similarities between nodes of a graph with application to collaborative recommendation. Knowledge and Data Engineering, IEEE Transactions on, 19(3):355–369, 2007.
 B. Gallagher and T. Eliassi-Rad. Leveraging label-independent features for classification in sparsely labeled networks: An empirical study. In Advances in Social Network Mining and Analysis, pages 1–19.
Springer, 2010.
 B. Gallagher, H. Tong, T. Eliassi-Rad, and C. Faloutsos. Using ghost edges for classification in sparsely labeled networks. In Proceedings of the 14th
ACM SIGKDD, KDD '08, pages 256–264, New York, NY, USA, 2008. ACM.
 S. Geman and D. Geman. Stochastic relaxation, gibbs distributions, and the bayesian restoration of images.
Pattern Analysis and Machine Intelligence, IEEE
Transactions on, (6):721–741, 1984.
 L. Getoor and B. Taskar. Introduction to statistical relational learning. MIT press, 2007.
 K. Henderson, B. Gallagher, L. Li, L. Akoglu, T. Eliassi-Rad, H. Tong, and C. Faloutsos. It's who you know: Graph mining using recursive structural features.
In Proceedings of the 17th ACM SIGKDD, KDD '11, pages 663–671, New York, NY, USA, 2011. ACM.
 G. E. Hinton. Learning distributed representations of concepts. In Proceedings of the eighth annual conference of the cognitive science society, pages 1–12. Amherst, MA, 1986.
 R. A. Hummel and S. W. Zucker. On the foundations of relaxation labeling processes. Pattern Analysis and Machine Intelligence, IEEE Transactions on, (3):267–287, 1983.
 U. Kang, H. Tong, and J. Sun. Fast random walk graph kernel. In SDM, pages 828–838, 2012.
 R. I. Kondor and J. Lafferty. Diffusion kernels on graphs and other discrete input spaces. In ICML, volume 2, pages 315–322, 2002.
 A. Krizhevsky, I. Sutskever, and G. E. Hinton.
Imagenet classification with deep convolutional neural networks. In NIPS, volume 1, page 4, 2012.
 D. Liben-Nowell and J. Kleinberg. The link-prediction problem for social networks. Journal of the American society for information science and technology, 58(7):1019–1031, 2007.
 F. Lin and W. Cohen. Semi-supervised classification of network data using very few labels. In Advances in Social Networks Analysis and Mining (ASONAM), 2010 International Conference on, pages 192–199, Aug
 S. A. Macskassy and F. Provost. A simple relational classifier. In Proceedings of the Second Workshop on
Multi-Relational Data Mining (MRDM-2003) at
KDD-2003, pages 64–76, 2003.
 S. A. Macskassy and F. Provost. Classification in networked data: A toolkit and a univariate case study.
The Journal of Machine Learning Research, 8:935–983, T. Mikolov, K. Chen, G. Corrado, and J. Dean.
Efficient estimation of word representations in vector space. CoRR, abs/1301.3781, 2013.
 T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems 26, pages
3111–3119. 2013.
 T. Mikolov, W.-t. Yih, and G. Zweig. Linguistic regularities in continuous space word representations.
In Proceedings of NAACL-HLT, pages 746–751, 2013.
 A. Mnih and G. E. Hinton. A scalable hierarchical distributed language model. Advances in neural information processing systems, 21:1081–1088, 2009.
 F. Morin and Y. Bengio. Hierarchical probabilistic neural network language model. In Proceedings of the international workshop on artificial intelligence and statistics, pages 246–252, 2005.
 J. Neville and D. Jensen. Iterative classification in relational data. In Proc. AAAI-2000 Workshop on
Learning Statistical Models from Relational Data, pages
13–20, 2000.
 J. Neville and D. Jensen. Leveraging relational autocorrelation with latent group models. In
Proceedings of the 4th International Workshop on
Multi-relational Mining, MRDM '05, pages 49–55, New
York, NY, USA, 2005. ACM.
 J. Neville and D. Jensen. A bias/variance decomposition for models using collective inference.
Machine Learning, 73(1):87–106, 2008.
 M. E. Newman. Modularity and community structure in networks. Proceedings of the National Academy of Sciences, 103(23):8577–8582, 2006.
 B. Recht, C. Re, S. Wright, and F. Niu. Hogwild: A lock-free approach to parallelizing stochastic gradient descent. In Advances in Neural Information Processing
Systems 24, pages 693–701. 2011.
 P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi-Rad. Collective classification in network data. AI magazine, 29(3):93, 2008.
 D. A. Spielman and S.-H. Teng. Nearly-linear time algorithms for graph partitioning, graph sparsification, and solving linear systems. In Proceedings of the thirty-sixth annual ACM symposium on Theory of computing, pages 81–90. ACM, 2004.
 L. Tang and H. Liu. Relational learning via latent social dimensions. In Proceedings of the 15th ACM
SIGKDD, KDD '09, pages 817–826, New York, NY, USA, 2009. ACM.
 L. Tang and H. Liu. Scalable learning of collective behavior based on sparse social dimensions. In
Proceedings of the 18th ACM conference on
Information and knowledge management, pages
1107–1116. ACM, 2009.
 L. Tang and H. Liu. Leveraging social media networks for classification. Data Mining and Knowledge
Discovery, 23(3):447–478, 2011.
 S. Vishwanathan, N. N. Schraudolph, R. Kondor, and K. M. Borgwardt. Graph kernels. The Journal of Machine Learning Research, 99:1201–1242, 2010.
 X. Wang and G. Sukthankar. Multi-label relational neighbor classification using social context features. In
Proceedings of the 19th ACM SIGKDD, pages 464–472.
ACM, 2013.
 W. Zachary. An information flow model for conflict and fission in small groups1. Journal of anthropological research, 33(4):452–473, 1977.DeepRank: A New Deep Architecture for Relevance Ranking in Information Retrieval
Liang Pang†∗, Yanyan Lan†∗, Jiafeng Guo†∗, Jun Xu†∗, Jingfang Xu‡, Xueqi Cheng†∗ pl8787@gmail.com,{lanyanyan,guojiafeng,junxu,cxq}@ict.ac.cn,xujingfang@sogou-inc.com
†CAS Key Lab of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China
∗University of Chinese Academy of Sciences, Beijing, China
‡Sogou Inc, Beijing, China
ABSTRACT
Tis paper concerns a deep learning approach to relevance ranking in information retrieval (IR). Existing deep IR models such as DSSM and CDSSM directly apply neural networks to generate ranking scores, without explicit understandings of the relevance. According to the human judgement process, a relevance label is generated by the following three steps: 1) relevant locations are detected; 2) local relevances are determined; 3) local relevances are aggregated to output the relevance label. In this paper we propose a new deep learning architecture, namely DeepRank, to simulate the above human judgment process. Firstly, a detection strategy is designed to extract the relevant contexts. Ten, a measure network is applied to determine the local relevances by utilizing a convolutional neural network (CNN) or two-dimensional gated recurrent units (2D-GRU).
Finally, an aggregation network with sequential integration and term gating mechanism is used to produce a global relevance score.
DeepRank well captures important IR characteristics, including exact/semantic matching signals, proximity heuristics, query term importance, and diverse relevance requirement. Experiments on both benchmark LETOR dataset and a large scale clickthrough data show that DeepRank can signifcantly outperform learning to ranking methods, and existing deep learning methods.
CCS CONCEPTS
•Information systems →Retrieval models and ranking;
KEYWORDS
Deep Learning; Ranking; Text Matching; Information Retrieval
INTRODUCTION
Relevance ranking is a core problem of information retrieval. Given a query and a set of candidate documents, a scoring function is usually utilized to determine the relevance degree of a document with respect to the query. Ten a ranking list is produced by sorting in descending order of the relevance score. Modern learning to
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proft or commercial advantage and that copies bear this notice and the full citation on the frst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permited. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specifc permission and/or a fee. Request permissions from permissions@acm.org.
CIKM'17, November 6–10, 2017, Singapore.
© 2017 ACM. ISBN 978-1-4503-4918-5/17/11...$15.00
DOI: htp://dx.doi.org/10.1145/3132847.3132914 rank approach applies machine learning techniques to the ranking function, which combines diferent kinds of human knowledge(i.e. relevance features such as BM25 and PageRank ) and therefore has achieved great improvements on the ranking performances. However, a successful learning to rank algorithm usually relies on efective handcrafed features for the learning process. Te feature engineering work is usually time-consuming, incomplete and over-specifed, which largely hinder the further development of this approach.
Recently, deep learning approach has shown great success in many machine learning applications such as speech recognition, computer vision, and natural language processing (NLP), owing to their ability of automatically learning the efective data representations (features). Terefore, a new direction of Neural IR is proposed to resort to deep learning for tackling the feature engineering problem of learning to rank, by directly using only automatically learned features from raw text of query and document. Tere have been some pioneer work, including DSSM, CDSSM, and DRMM. Both DSSM and CDSSM directly apply deep neural networks to obtain the semantic representations of query and document, and the ranking score is produced by computing their cosine similarity. Guo et al. argued that DSSM and CDSSM only consider the semantic matching between query and document, but ignore the more important relevance matching characteristics, such as exact matching signals, query term importance, and diverse matching requirement. Tus they proposed another deep architecture, i.e. DRMM, to solve this problem. However, DRMM does not explicitly model the relevance generation process, and fails to capture important IR characteristics such as passage retrieval intrinsics and proximity heuristics.
Inspired by the human judgement process, we propose a new deep learning architecture, namely DeepRank, to beter capture the relevance intrinsics. According to the illustration in, the human judgement process can be divided into three steps. Human annotators frst scan the whole document to detect the relevant locations. Ten the local relevance for each detected location is decided. Finally, those local relevances are combined to form the global relevance of the entire document. Consequently, DeepRank contains three parts to simulate the human judgement process, by tackling the following three problems:
Where does the relevance occur? According to the query-centric assumption proposed in, the relevant information for a query arXiv:1710.05649v2 [cs.IR] 22 Jul 2019 only locates in the contexts around query terms. Terefore, the context with a query term at the center position, namely query-centric context, is recognized as the relevant location in the detection step.
How to measure the local relevance? Afer the detection step, a measure network is utilized to determine the local relevance between query and each query-centric context. Firstly, a tensor is constructed to incorporate both the word representations of query/query-centric context, and the interactions between them.
Ten a CNN or 2D-GRU is applied on the tensor to output the representation of the local relevance. In this way, important IR characteristics such as exact/semantic matching signals, passage retrieval intrinsics, and proximity heuristics can be well captured.
How to aggregate such local relevances to determine the global relevance score? As shown by, two factors are important for user's complex principles of aggregating local relevances, i.e. query term importance and diverse relevance requirement. Terefore we propose to frst aggregate local relevances at query term level, and then make the combination by considering weights of diferent terms, via a term gating network. To obtain the term level relevance, we frst group the query-centric contexts with the same central word together. Ten a recurrent neural network (RNN) such as
GRU or LSTM is utilized to aggregate such local relevances sequentially, by further considering the position information of these query-centric contexts in the whole document.
Above all, DeepRank is a new architecture composed of three components, i.e. a detection strategy, a measure network with
CNN/2D-GRU, and an aggregation network with term gating and RNN inside. Terefore, DeepRank can be trained end-to-end with the pairwise ranking loss via stochastic gradient descent. We conduct experiments on both benchmark LETOR4.0 data and a large scale clickthrough data collected from a commercial search engine.
Te experimental results show that: 1) Existing deep IR methods such as DSSM, CDSSM, and DRMM perform much worse, if using only automatically learned features, than the pairwise and listwise learning to rank methods. 2) DeepRank signifcantly outperforms not only all the existing deep IR models but also all the pairwise and listwise learning to rank baseline methods. 3) If we incorporate handcrafed features into the model, as did in SQA, DeepRank will be further improved, and the performance is beter than SQA.
We also conduct a detailed experimental analysis on DeepRank to investigate the infuences of diferent setings.
To the best of our knowledge, DeepRank is the frst deep IR model to outperform existing learning to rank models.
RELATED WORK
We frst review related work on relevance ranking for IR, including learning to rank methods and deep learning methods.
Learning to Rank Methods
In the past few decades, machine learning techniques have been applied to IR, and gained great improvements to this area. Tis direction is called learning to rank. Major learning to rank methods can be grouped into three categories: pointwise, pairwise and listwise approach. Diferent approaches defne diferent input and output spaces, use diferent hypotheses, and employ diferent loss functions. Pointwise approach, such as logistic regression, inputs a feature vector of each single document and outputs the relevance degree of each single document. Pairwise approach, such as RankSVM and RankBoost, inputs pairs of documents, both represented by feature vectors and outputs the pairwise preference between each pair of documents. Listwise approach, such as ListNet, AdaRank and LambdaMart, inputs a set of document features associated with query and outputs the ranked list. All these approaches focus on learning the optimal way of combining features through discriminative training. However, a successful learning to rank algorithm relies on efective handcrafed features for the learning process. Te feature engineering work is usually time-consuming, incomplete and over-specifed, which largely hinder the further development of this direction.
Deep Learning Methods
Recently, deep learning techniques have been applied to IR, for automatically learning efective ranking features. Examples include DSSM, CDSSM, and DRMM. DSSM uses a deep neural network (DNN) to map both query and document to a common semantic space. Ten the relevance score is calculated as the cosine similarity between these two vectors. Rather than using
DNN, CDSSM is proposed to use CNN to beter preserve the local word order information when capturing contextual information of query/document. Ten max-pooling strategies are adopted to flter the salient semantic concepts to form a sentence level representation. However, DSSM and CDSSM view IR as a semantic matching problem, and focus on generating a good sentence level representations for query and document. Tey ignore the important intrinsics of relevance ranking in IR. Guo et al. frst point out the diferences between semantic matching and relevance matching.
Tey propose a new deep learning architecture DRMM to model
IR's own characteristics, including exact matching signals, query terms importance, and diverse matching requirement. Specifcally, DRMM frst builds local interactions between each pair of words from query and document based on word embeddings, and then maps the local interactions to a matching histogram for each query term. Ten DRMM employs DNN to learn hierarchical matching paterns. Finally, the relevance score is generated by aggregating the term level scores via a term gating network. Tough DRMM has made the frst step to design deep learning model specially for
IR, it does not explicitly model the relevance generation process of human. It also fails to model the important IR intrinsics such as passage retrieval strategies and proximity heuristics.
Another related sort of deep models, fourishing in NLP, provide a new way of thinking if we treat IR task as a general text matching task, i.e. query matches document. Tese work can be mainly categorized as representation focused models and interaction focused models. Te representation focused models try to build a good representation for each single text with a neural network, and then conduct matching between the two vectors. Te DSSM and CDSSM mentioned above belong to this category. Tere are also some other ones such as ARC-I model, which builds on word embeddings and makes use of convolutional layers and pooling layers to extract compositional text representation. Te interaction focused models frst build the local interactions between two texts, and then use neural networks to learn the more complicated interaction paterns for matching. Typical examples include ARC-II and MatchPyramid, and Match-SRNN. Tese models have been shown efective in text matching tasks such as paraphrase identifcation and question answering. DRMM can also be viewed as an interaction focused model.
MOTIVATION
Te motivation of our deep architecture comes from the process of human relevance judgement. As illustrated by, a human annotator will frst examine the whole document for local relevance information. Scanning involves iterating through every document location, and deciding for each whether local relevance information is found. As suggested by, if one can fnd relevant information in a document, the relevant information must locate around the query terms inside the document, namely query-centric assumption. Finally, the local relevance for each query-centric context is combined to form the relevance of the entire document. Figure ? gives an example to describe the process of human relevance judgement, where user's information need is represented by the query 'Hubble Telescope Achievements'. For a given document, annotators will directly extract several contexts containing the keyword 'Telescope' and 'Hubble', and determine whether they are relevant to the query. Ten these local relevance information will be considered together to determine the global relevance label of the document with respect to the query.
Terefore, the relevance label is generated following three steps in the human judgement process: 1) detection of relevant locations; 2) measurement of local relevance; 3) aggregation of local relevances. Consequently, three problems are going to be tackled if we want to well capture the relevance: (1) Where does the relevance occur? (2) How to measure the local relevance? (3) How to aggregate such local relevance to determine the fnal relevance label? Accordingly, we design our deep learning architecture, namely
DeepRank, to model the above relevance process.
DEEPRANK
In this section, we demonstrate a new deep learning architecture for
IR, namely DeepRank. DeepRank includes three parts to tackle the above three problems: a Detection Strategy, a Measure Network, and an Aggregation Network. In the detection step, the query-centric contexts are extracted to represent where the relevance occur. In the measurement step, CNN or 2D-GRU is adopted to measure the local relevance between query and each query-centric context.
Finally in the aggregation step, RNN and a term gating network are utilized to aggregate those local relevances to a global one for ranking. Figure 1 gives an illustration of DeepRank.
We will frst give some mathematical notations. Each query and document are represented as a sequence of word q = (w1,...,wM) and d = (v1,...,vN ), where wi denotes the i-th word in the query and vj denotes the j-th word in the document, respectively. Consequently, we can use d(k)[p] = (vp−k, · · ·,vp, · · ·,vp+k) to denote a piece of continuous text within a document, centered on the p-th word with the sequence length 2k + 1. If we use word2vec technique to represent each word to a vector, each word sequence can be represented as a matrix. Taking query q = (w1,...,wM) for example, if the word embedding of wi is xi, the query will be represented as Q = [x1,..., xM], where each column stands for a word vector.
Detection Strategy
According to the query-centric assumption, the relevance usually occurs at the locations where the query terms appear in the documents. Similar observations have also been obtained by the eye-tracking studies, showing that annotators focus more on the location of query terms when they are scanning the whole document for relevance judgment. Terefore, we defne the querycentric context as the relevant location, which is a context with a query term at the center position. Mathematically, given a query q and document d, if query term wu appears at the p-th position in the document, i.e. wu = vp, the query-centric context centered on this query term is represented as sp u(k) = dk[p]. Afer the detection step, we can obtain a set of word sequences {sp u(k)}, with each one represents a local relevant location.
Measure Network
Te goal of the measurement step is to determine the local relevance, i.e. relevance between query and each query-centric context.
As reviewed in Section 2, previous deep learning models for text matching can be divided into representation focused methods and interaction focused methods. Te representation focused methods focus on extracting high level semantic representations of texts, starting from basic word representations. While the interaction focused methods turn to directly model the interactions between the two texts, starting from word-level interaction signals. In order to combine the best of both approaches, we propose a new measure network, as shown in Figure 2. Firstly a tensor is constructed as the input. Ten CNN or 2D-GRU is applied on the tensor to output a vector, which stands for the representation of local relevance. In this way, the important IR characteristics such as exact/semantic matching signals, passage retrieval intrinsics, and proximity heuristics can be well captured in the measurement step.
Input Tensor. Te key idea of this layer is to feed both the word representations of query/query-centric context and the interactions between them into the input of the measure network.
Specifcally for a given query q and query-centric context sp u(k) with wu = vp, we denote the word-level interaction matrix used in MatchPyramid and Match-SRNN as S, where each element
Sij is defned as the similarity of corresponding words wi and vj. For example, indicator function or cosine similarity can be used to capture the word-level exact or semantic matching signals, respectively. Te mathematical formulas are shown as follows.
Sind ij
= 1 if wi = vj, Sind ij
= 0 otherwise, Scos ij
= xiT yj/(∥xi∥ · ∥yj∥), (2) where xi and yj denote the word embeddings of wi and vj, respectively. To further incorporate the word representations of query/query-centric context to the input, we extend each element of Sij to a three-dimensional vector ˜Sij = [xi,yj,Sij]T. Terefore, the original matrix S will become a three-order tensor, denoted as
S. In this way, the input tensor can be viewed as a combination of Figure 1: An illustration of DeepRank.
Figure 2: An illustration of measure network for a query and a query-centric context: (a) Input Tensor; (b) CNN; (c)
2D-GRU. three matrices, i.e. query matrix, query-centric context matrix, and word-level interaction matrix.
Based on the input tensor S, various neural networks can be directly applied to obtain the representations for the local relevance between query and query-centric context. In this paper, we choose the CNN architecture in MatchPyramid and 2D-GRU architecture in Match-SRNN, mainly because they have the ability to capture important proximity heuristics for IR.
Convolutional Neural Network. In this paper, we use a onelayer CNN in the measurement step, which includes a convolution operation and a max-pooling operation defned as follows. Te convolution operation can extract various matching paterns from the input tensor S, by using diferent kinds of kernels, as shown in Figure 2. Ten a max-pooling operation is used to flter signifcant matching paterns for further relevance determination. h(κ) i,j =
� l=1 γ −1
� s=0 γ −1
� t=0 w(κ) s,t ·S(l) i+s,j+t +b(κ), h(κ) = max i,j h(κ) i,j, κ = 1, · · ·,K(3) where l denotes the l-th slide of the tensor, γ denotes the fxed size of K diferent kernels, S(l) i+s,j+t denote the (i +s, j +t) element of the l-th matrix of the input tensor S, wκ s,t and bκ denotes parameters.
Finally, all the signifcant matching paterns obtained from diferent kernels are concatenated to form a vector, i.e. h = [h(1), · · ·,h(K)]T, to represent the local relevance. Tis vector will be treated as the input to the aggregation network.
Two-Dimensional Gated Recurrent Units. Rather than using a hierarchical structure to capture the matching paterns, 2DGRU in Match-SRNN adopts a diferent sequential model to accumulate the matching signals. It is an extension of GRU (a typical variant of RNN) to two-dimensional data like matrix or tensor1. Specifcally, 2D-GRU scans from top-lef to botom-right(or in a bidirectional way) recursively. At each position, the hidden representation depends on the representations of the top, lef, diagonal and current positions in the matrix. Te mathematical formulas are shown as follows. c = [hT i−1,j, hT i,j−1, hT i−1,j−1, ST ij]T, rθ = σ(W(rθ )c + b(rθ )), θ = l,t,d, z′ ϕ = W(zϕ)c + b(zϕ), ϕ = m,l,t,d, r = [rT l, rT t, rT d ]T, [zm, zl, zt, zd] = RMax([z′ m, z′ l, z′ t, z′ d]), h′ ij = ψ(WSij + U(r ⊙ [hT i,j−1, hT i−1,j, hT i−1,j−1]T ) + b), hij = zl ⊙ hi,j−1 + zt ⊙ hi−1,j + zd ⊙ hi−1,j−1 + zm ⊙ h′ ij, (4) where hij stands for the hidden representation at the (i, j)-th position, zm, zl, zt, zd are the four gates, U, W, and b are parameters, σ and ψ stand for sigmoid and tanh function respectively, and RMax is a function to conduct sofmax on each dimension across gates, [zϕ]j = e[z′ϕ]j e[z′m]j + e[z′l ]j + e[z′t ]j + e[z′d ]j, ϕ = m,l,t,d.
Te last hidden representation of 2D-GRU will be treated as the output h, which is the botom right one at the matrix/tensor. If you use a bi-directional 2D-GRU, both the top lef one −→h and botom right one ←−h can be concatenated together to form the output vector, i.e. h = [−→hT, ←−hT ]T.
1Strictly speaking, tensor is not a two-dimensional representation. However, 2D-GRU can be directly applied on tensor by treating each element of the matrix as a vector.
Please note that both CNN and 2D-GRU well capture the proximity heuristics in IR. Proximity heuristic rewards a document where the matched query terms occur close to each other, which is an important factor for a good retrieval model. For CNN, if the matched query terms occur close to each other, appropriate kernels can be utilized to extract such signifcant matching paterns and infuence the relevance score. In this way, CNN well captures the proximity heuristics. 2D-GRU can also model proximity. When there is a document where the matched query terms occur close to each other, the representation h will be strengthened by appropriately seting gates and other parameters. As a result, the relevance score of the document will be increased.
Aggregation Network
Afer the measurement step, we obtain a vector h to represent the local relevance between query and each query-centric context.
Terefore, we need a further aggregation step to output a global relevance score. In this process, two IR principles are going to be considered in our deep architecture. One is query term importance: query terms are critical to express user's information need and some terms are more important than others. Te other one is diverse matching requirement: the distribution of matching paterns can be quite diferent in a relevant document. For example, the Verbosity Hypothesis assumes that the relevance matching might be global. On the contrary, the Scope Hypothesis assumes that the relevance matching could happen in any part of a relevant document, and we do not require the document as a whole to be relevant to a query. In order to capture the two IR principles, we frst conduct a query term level aggregation, in which the diverse matching requirement is taken into account. Ten a term gating network is applied to capture the importance of diferent terms when producing the global relevance score.
Qery Term Level Aggregation. In order to capture the principle of diverse relevance requirement, we need to consider the position of the corresponding query-centric context when conducting query term level aggregation. Terefore, we append each vector h with the position indicator to encode the position information of the corresponding query-centric context. Specifcally, diferent position functions д(p) are utilized in our aggregation network:
Constant Function: д(p) = C, C ∈ R, Linear Function: д(p) = (L − p)/L, L ∈ R, Reciprocal Function: д(p) = a/(p + b), a,b ∈ R, (6)
Exponential Function: д(p) = a · exp(−p/b), a,b ∈ R, where p stands for the position of the query-centric context, determined by the central word vp. Afer this appending operation, the representation of local relevance for a query-centric context centered at word vp (denoted as h(p)) becomes [h(p)T,д(p)]T.
To conduct query term level aggregation, we frst group h(p) with the same central word together, which stands for all the local relevances with respect to a same query term. Ten RNN is used to integrate such local relevances by considering position information into consideration. Tat is to say, we can obtain the global relevance representation T(wu) for each query term wu as follows.
T(wu) = pn
RNN p=p1
� hT (p),д(p)
�, p1,...pn ∈ P(wu), (7) where P(wu) denotes the position set of all the query-centric contexts centered on query term wu. For example, you can use GRU to capture the sequential information, which is a typical variant of RNN. In the experimental analysis, we show the comparisons among diferent position functions.
Term Gating Network for Global Aggregation. Based on query term level global relevance representations T(wu), we use a term gating network (similar to that used in DRMM) to obtain the fnal global relevance score, by considering importances of diferent query terms. Specifcally, we defne a weight parameter
Ewu for each query term, and linear combine all the query term level relevances as follows.
F (q, d) =
� wu ∈q(Ewu I)T · T(wu), (8) where I is an vector with each element set to be 1, and the dimension is set to be the same as T(wu).
Model Training
DeepRank is an end-to-end deep neural network, which can be trained using stochastic gradient decent (SGD) methods, such as
Adam. L2 regularization and early stopping strategy are also used in our implementation to deal with overfting. More implementation details will be given in the experiments.
In our experiments, we use the following pairwise hinge loss for training, since we are considering a ranking problem. For future work, we are also willing to try other pairwise losses and listwise loss functions to conduct the training process.
L(q, d+, d−) = max(0, 1 − F (q, d+) + F (q, d−)), (9) where L(q, d+, d−) denotes the pairwise loss between a pair of positive and negative samples d+ and d−, and F (q, d) denotes the relevance score produced by DeepRank.
EXPERIMENTS
In this section, we conduct extensive experiments to evaluate DeepRank against state-of-the-art models, including learning to rank methods, and existing deep learning methods. Te experimental results on both LETOR4.0 benchmark and a large scale clickthrough data show that our model can signifcantly outperform all the baselines, especially when other existing deep learning methods perform much worse than learning to rank methods. Furthermore, we give detailed experimental analysis to show more insights on our model.
Experimental Settings
We frst introduce our experimental setings, including datasets, baseline methods/implementations, and evaluation measures.
Data Sets. Since most deep models need to learn many parameters, it is not appropriate to evaluate them with small traditional retrieval dataset, such as Robust04 and ClueWeb-09-Cat-B used in, which have only less than 300 queries. In our experiments, we use two datasets for evaluation, i.e. LETOR4.0 and a large scale clickthrough data. Te LETOR4.0 data is mainly used for comparing our model with other deep models, and the state-of-the-art learning to rank methods. Te clickthrough data is larger, and we use it to compare diferent deep models.
LETOR4.0 dataset contains two separate data sampled from the.GOV2 corpus using the TREC 2007 and TREC 2008 Million
Qery track queries, denoted as MQ2007 and MQ2008, respectively.
MQ2007 is a bit larger, which contains 1692 queries and 65,323 documents. While MQ2008 only contains 784 queries and 14,384 documents. Since the query number in MQ2008 is too small, which will cause the serious insufcient training problem for deep learning models, we propose to merge the training set of MQ2007 to that of MQ2008. Te validation and testing set are kept unchanged.
Ten we form a new large data set, still denoted as MQ2008. In total, MQ2007 and MQ2008 contains 69,623 and 84,834 query-document pairs, respectively. All the baselines are conducted fairly on this new dataset for comparison. In original LETOR4.0, each query and document pair is represented as a vector containing 46 diferent features, which is easy for implementations of learning to rank methods. While most deep IR models (except for SQA ) do not use any handcrafed features, the raw text of query and document are used for implementation.
Te large scale clickthrough data, namely ChineseClick, is collected from a commercial Chinese search engine. In the data collection process, the user is given the top 10 results for each proposed query. Clicked documents are viewed to be relevant, and the other ones are viewed as irrelevant. Since this is a Chinese dataset, we frst conduct word segmentation for queries and documents. Ten we apply some typical data preprocessing techniques, such as navigational queries fltering, stopping words and low frequency words(less than 50) removing. Afer these preprocessing, the fnal dataset contains 12,520 queries, 115,562 documents, and 118,835 querydocument pairs. It is further divided into training/validation/testing set according to the proportion 3:1:1 of query numbers.
Baseline Methods. We adopt two types of baseline methods for comparison, including learning to rank methods and deep learning methods.
For learning to rank approach, we compare both pairwise and listwise ranking methods. Te pairwise baselines include RankSVM and RankBoost, which apply SVM and boosting techniques to the pairwise ranking problem, respectively. Te listwise baselines include AdaRank and LambdaMart, where AdaRank proposes to directly optimizing IR evaluation measures by boosting to obtain a ranking list, and LamdaMart uses gradient boosting for optimizing a listwise ranking loss, which is the winner of YahooLearning to Rank Challenge. Tough the public results on
LETOR4.0 have included RankSVM, RankBoost and AdaRank as the baselines, we are not able to conduct signifcant testing since the ranking list and relevance scores are missing. Terefore, we implement them on our own. Most of our results are comparable with those on LETOR4.02. For RankSVM, we directly use the implementation in SVMrank. RankBoost, AdaRank and LambdaMart are implemented using RankLib3, which is a widely used tool in 2htp://research.microsof.com/en-us/um/beijing/projects/letor/letor4baseline.aspx
3htps://sourceforge.net/p/lemur/wiki/RankLib/ the area of learning to rank. For LETOR4.0 dataset, we use 46 dimensional standard features provided for public to evaluate the performance of learning to rank approaches. BM25-Title which calculate BM25 score between query and document title, is one of the powerful feature among these features.
For deep learning approach, we compare three existing deep
IR models, i.e. DSSM, CDSSM, and DRMM. We also compare some popular deep methods for text matching, including one representation focused method, i.e. ARC-I, and three interaction focused methods, i.e. ARC-II, MatchPyramid, and Match-SRNN. Implementation details are listed as follows.
Firstly, all the word embeddings in these methods are learned with the Continuous Bag-of-Words (CBOW) model from Wikipedia corpus, and the dimension is set to 50. In general, most deep learning baselines are applied following the original implementations.
We only reduce the parameter numbers due to the relative small size of our data. For example, we use a three-layer DNN as that in the original paper of DSSM, and the node number of each layer is reduced to 100, 100, and 50. For CDSSM, we use a one-layer CNN with
50 (1 × 3) kernels. Terefore, a 50-dimensional vector is obtained afer global pooling strategy. DRMM is directly implemented using the best confguration and the code released by. For ARC-I, we use a two-layer CNN with each one containing 16 (1 × 3) kernels.
Te size of pooling in the frst layer is set to 2, while the size of pooling in the last layer is set to be 2 for query representation, and 20 for document representation, respectively. For ARC-II, we use a two-layer CNN, where there are 8 kernels in each layer. Te size of kernels and pooling in both layers are set to (1 × 3)/(3 × 3) and(2 × 2)/(2 × 20), respectively. For MatchPyramid, we use cosine similarity to construct the word-level interaction matrix. Ten a one-layer CNN is applied on the matrix with 8 (3 × 3) kernels, and the size of dynamic pooling is set to (3 × 10). For Match-SRNN, 2D-GRU is directly applied on the same interaction matrix, and the dimension of hidden node is set to 2.
SQA model combines handcraf features in the learning process, therefore it is not appropriate to directly compare it with
DeepRank, which only uses automatically learned feature from raw text for ranking. For fair comparison, we delete the handcrafed features in SQA and obtain a pure deep SQA model, denoted as
SQA-noFeat. Furthermore, we incorporate the handcrafed features(46 default features in LETOR4.0) into the last layer of DeepRank to obtain DeepRank-Feat, which is used to compare with SQA. For both SQA-noFeat and SQA, one-layer CNN with 50 (1 × 3) kernels is used in the deep architecture.
Te DeepRank4 for performance comparison is implemented using the following setings: the window size of query-centric context is set to 15; cosine similarity is adopted to construct the input tensor; both CNN and 2D-GRU are used in the measurement step, therefore we have two versions of DeepRank, denoted as DeepRankCNN and DeepRank-2DGRU; the reciprocal function is used as the positional function, and GRU is adopted in the aggregation step. We also compare diferent setings of DeepRank for detailed analysis.
Evaluation Measures. For the evaluation on LETOR4.0, we follow the data partitions on this dataset (5-fold) and the average results are reported. While for the evaluation on ChineseClick, 4Te source code: htps://github.com/pl8787/textnet-release. we train the model on the training set, tune hyper-parameters on the validation set and report the results on the testing set for comparison. Tree evaluation measures are used in this paper, i.e. Precision, NDCG, and MAP. Furthermore, we conduct a pairwise t-test for signifcance testing with p-value lower than 0.05(i.e. p-value≤ 0.05).
Performance Comparison
Te performance comparison results of DeeRank against baseline models are shown in Table 1.
Performance Comparison on LETOR 4.0. From the results on MQ2007 and MQ2008, we can see that: 1) None of existing deep learning models could perform comparably with learning to rank methods. Some of them are even worse than BM25. Te results tell us that the automatically learned features in existing deep learning models are not beter than traditional extracted ones, though they are using more complex models for training. Someone may argue that the experimental fndings are inconsistent with previous studies that DSSM and CDSSM can signifcantly outperform traditional retrieval models, as stated in and. Te reason lies in that
LETOR4.0 is much smaller than the clickthrough data used in and. In the following experiments on ChineseClick, we can see that all the deep models perform beter than BM25. Terefore, deep models usually need more data for optimization, which is also the reason why we do not use Robust04 and ClueWeb-09-CAtB used in for evaluation. 2) As for the comparisons between these deep models, interaction focused ones such as DRMM, ARC-II, MatchPyramid and Match-SRNN perform much beter than representation focused ones such as DSSM, CDSSM, ARC-I, and SQAnoFeat. Tis is consistent with the understanding that interaction signals are much more important than the semantic representation of query/document in IR, as described in. Furthermore, DRMM performs the best among all the deep learning baseline methods.
Tis is because DRMM further incorporate IR characteristics into their architecture, which indicate the importance of capturing IR intrinsics in the architecture design process. 3) Our DeepRank not only signifcantly outperforms the deep learning baselines, but also signifcantly improves the results of learning to rank methods, even only use the query and document raw text data. For example, the improvement of DeepRank-CNN against the best deep learning baseline (i.e. DRMM) on MQ2007 is 16.1% w.r.t. NDCG@1, 12.9% w.r.t. P@1, and 6.4% w.r.t. MAP, respectively; while the improvement of DeepRank-CNN against the best learning to rank method(i.e. LambdaMart) on MQ2007 is 7.0% w.r.t. NDCG@1, 5.6% w.r.t.
P@1, and 6.2% w.r.t. MAP, respectively. Te results indicate that by appropriately modeling relevance, deep learning approach can signifcantly outperform learning to rank approach for IR application. 4) Tough SQA has used both automatically learned features and handcrafed features, the performance cannot compare with
DeepRank by using only automatically learned features for ranking.
If we incorporate handcrafed features into DeepRank, the performance will be further improved, as shown in DeepRank-CNN-Feat.
Te results demonstrate the superiority of our deep architecture.
Performance Comparison on ChineseClick. Te ChineseClick data is used to compare DeepRank with other deep learning methods. We do not include the DSSM and CDSSM baselines. Tat is because DSSM and CDSSM are specially designed for English data, and leter-trigram is used as the input of neural network, which is not applicable for Chinese data. If we are using the whole word embedding as the input, CDSMM will become the same as ARC-I.
Terefore, we omit CDSSM and directly report the results of ARC-I for comparison. Te results show that deep learning baselines perform comparably with BM25, some are even beter. Tat is because we are using a larger data, the training of deep models become more sufcient and the performances are improved. Our DeepRank still performs the best. Te improvement against the BM25 is about
21.0% w.r.t. NDCG@1, and 11.5% w.r.t. MAP. While the improvement against the best deep learning baseline (i.e. Match-SRNN) is about 11.0% w.r.t. NDCG@1, and 4.3% w.r.t. MAP.
From the above results, we conclude that DeepRank signifcantly improves the results of relevance ranking, with architecture specially designed to model human's relevance generation process.
Detailed Analysis of DeepRank
DeepRank is such a fexible deep architecture that diferent parameter setings and neural networks can be used in the detection, measurement, and aggregation steps. Some of these setings may largely infuence the fnal ranking performances. Terefore, we conduct a detailed analysis on MQ2007 to show the comparisons of DeepRank with diferent setings, with expect to give some insights for implementation. Specifcally, we analyze four factors, i.e. window size of query-centric context in the detection step, input tensor in the measurement step, neural network in the measurement step, positional function in the aggregation step. We change one factor of the above DeepRank-CNN each time to conduct the comparisons.
Impact of Diferent Window Sizes of Qery-Centric Context.
Te window size of query-centric context determines the scope of local relevance in the human judgment process. With a small window size, users would determine local relevance with less efort since contexts are short, but it is easy to introduce ambiguity due to limited context information. When window size is large, there are sufcient contexts to facilitate the precise local relevance judgment, but the cost is also increased and many noises may infuence the judgment. We conduct an experiment to compare diferent window sizes of query-centric context, varying in the range of 1, 7, 11, 15, 19 and 23. Te results listed at the top of Table 2 show that the performances of DeepRank frst increase and then become stable, with the increase of window size. Te best performance is obtained with window size up to 11/15 (w.r.t diferent evaluation measures). Terefore, with considering the computational complexity, we suggest to use a comparable medium window size in real application, and the exact number need to be tuned considering averaged document length, query length, and data size.
Impact of Diferent Input Tensors. In order to capture both word representations of query/query-centric context and their interactions, we propose to construct a three-order tensor S as the input of the measure network. Here, we compare four diferent setings of tensor. Sind
I and Scos
I stand for the case when we use
Table 1: Performance comparison of diferent models on MQ2007, MQ2008 and ChineseClick. Signifcant performance degradation with respect to DeepRank-CNN is denoted as (-) with p-value ≤ 0.05.
MQ2007
Model
NDCG@1
NDCG@3
NDCG@5
NDCG@10
P@1
P@3
P@5
P@10
MAP
BM25-Title
0.358−
0.372−
0.384−
0.414−
0.427−
0.404−
0.388−
0.366−
0.450−
RankSVM
0.408−
0.405−
0.414−
0.442−
0.472−
0.432−
0.413−
0.381−
0.464−
RankBoost
0.401−
0.404−
0.410−
0.436−
0.462−
0.428−
0.405−
0.374−
0.457−
AdaRank
0.400−
0.410−
0.415−
0.439−
0.461−
0.431−
0.408−
0.373−
0.460−
LambdaMart
0.412−
0.418−
0.421−
0.446−
0.481−
0.444−
0.418−
0.384−
0.468−
DSSM
0.290−
0.319−
0.335−
0.371−
0.345−
0.359−
0.359−
0.352−
0.409−
CDSSM
0.288−
0.288−
0.297−
0.325−
0.333−
0.309−
0.301−
0.291−
0.364−
Arc-I
0.310−
0.334−
0.348−
0.386−
0.376−
0.377−
0.370−
0.364−
0.417−
SQA-noFeat
0.309−
0.333−
0.348−
0.386−
0.375−
0.373−
0.372−
0.364−
0.419−
DRMM
0.380−
0.396−
0.408−
0.440−
0.450−
0.430−
0.417−
0.388−
0.467−
Arc-II
0.317−
0.338−
0.354−
0.390−
0.379−
0.378−
0.377−
0.366−
0.421−
MatchPyramid
0.362−
0.364−
0.379−
0.409−
0.428−
0.404−
0.397−
0.371−
0.434−
Match-SRNN
0.392−
0.402−
0.409−
0.435−
0.460−
0.436−
0.413−
0.384−
0.456−
DeepRank-2DGRU
DeepRank-CNN
SQA
DeepRank-CNN-Feat
MQ2008
Model
NDCG@1
NDCG@3
NDCG@5
NDCG@10
P@1
P@3
P@5
P@10
MAP
BM25-Title
0.344−
0.420−
0.461−
0.220−
0.408−
0.381−
0.337−
0.245−
0.465−
RankSVM
0.375−
0.431−
0.479−
0.441−
0.390−
0.348−
0.478−
RankBoost
0.436−
0.477−
0.392−
0.347−
0.481−
AdaRank
0.360−
0.422−
0.462−
0.430−
0.384−
0.339−
0.247−
0.468−
LambdaMart
0.437−
0.477−
0.446−
0.348−
0.478−
DSSM
0.286−
0.336−
0.378−
0.178−
0.341−
0.307−
0.284−
0.221−
0.391−
CDSSM
0.283−
0.331−
0.376−
0.175−
0.335−
0.302−
0.279−
0.222−
0.395−
Arc-I
0.295−
0.363−
0.413−
0.187−
0.361−
0.336−
0.311−
0.229−
0.424−
SQA-noFeat
0.291−
0.350−
0.401−
0.184−
0.366−
0.332−
0.309−
0.231−
0.416−
DRMM
0.368−
0.427−
0.468−
0.220−
0.437−
0.392−
0.344−
0.245−
0.473−
Arc-II
0.299−
0.340−
0.394−
0.181−
0.366−
0.326−
0.305−
0.229−
0.413−
MatchPyramid
0.351−
0.401−
0.442−
0.211−
0.408−
0.365−
0.329−
0.239−
0.449−
Match-SRNN
0.369−
0.426−
0.465−
0.223−
0.432−
0.383−
0.335−
0.239−
0.466−
DeepRank-2DGRU
DeepRank-CNN
SQA
DeepRank-CNN-Feat
ChineseClick
Model
NDCG@1
NDCG@3
NDCG@5
NDCG@10
P@1
P@3
P@5
P@10
MAP
BM25
0.200−
0.320−
0.412−
0.280−
0.200−
0.174−
0.169−
0.373−
Arc-I
0.208−
0.359−
0.451−
0.286−
0.208−
0.193−
0.180−
0.393−
SQA-noFeat
0.232−
0.368−
0.458−
0.292−
0.232−
0.194−
0.180−
0.153−
0.403−
DRMM
0.218−
0.346−
0.442−
0.288−
0.218−
0.185−
0.177−
0.392−
Arc-II
0.190−
0.329−
0.430−
0.283−
0.190−
0.180−
0.177−
0.373−
MatchPyramid
0.204−
0.342−
0.436−
0.285−
0.204−
0.184−
0.178−
0.384−
Match-SRNN
0.218−
0.360−
0.456−
0.295−
0.218−
0.190−
0.181−
0.399−
DeepRank-2DGRU
DeepRank-CNN
Table 2: Performance comparisons of DeepRank with diferent settings on MQ2007.
Model
NDCG@1
NDCG@5
MAP
DeepRank-W1
DeepRank-W7
DeepRank-W11
DeepRank-W15
DeepRank-W19
DeepRank-W23
DeepRank-Sind
I
DeepRank-Scos
I
DeepRank-SR
DeepRank-Scos
IR
DeepRank-DNN
DeepRank-2DGRU
DeepRank-CNN
DeepRank-Const
DeepRank-Linear
DeepRank-Exp
DeepRank-Recip
0.497 indicator or cosine function to construct the interaction matrix, and omit the other two matrices in the tensor. SR stands for the case that only word representations of query and query-centric context is considered in the tensor, i.e. interaction matrix is ignored. Scos
IR stands for the case when we use the three-order tensor, which is exactly the DeepRank we used in the performance comparisons.
From the results listed in the second row of Table 2, we can see the performances are improved when more information is modeled in the tensor. Terefore, both word representations of query/querycentric context and word-level interactions are important to the relevance judgement.
Impact of Diferent Measure Networks. Te measure network is adopted to determine the relevance between query and a detected query-centric context. In the model section, we demonstrate how to use CNN and 2D-GRU to conduct such measurement, mainly because these two kinds of neural networks have the ability to capture the proximity heuristics. Of course, you can also use other deep learning architectures, such as DNN. In this section, we conduct experiments on MQ2007 to compare the three diferent versions of DeepRank, denoted as DeepRank-DNN, DeepRank-CNN, and DeepRank-2DGRU. Te experimental results in the third row of Table 2 show that DeepRank-CNN and DeepRank-2DGRU perform much beter than DeepRank-DNN. Te reason lies in that CNN and 2D-GRU both have the ability to model the proximity heuristics, while DNN cannot because it is position sensitive, which is contradict with the position independent proximity heuristic.
Impact of Diferent Position Functions. As described in the aggregation network, diferent kinds of position functions can be used to model the position importance. Here we compare DeepRank with four diferent position functions, i.e. Constant, Linear, Reciprocal, and Exponential functions, denoted as DeepRank-Const, DeepRank-Linear, DeepRank-Recip and DeepRank-Exp, respectively. Te results listed in the fourth row of Table 2 show that
DeepRank-Recip is the best, while DeepRank-Const is the worst.
As for the other two functions, DeepRank-Exp perform comparable with DeepRank-Recip, and DeepRank-Linear is a litle worse than
DeepRank-Recip and DeepRank-Exp. Te results indicate that top positions are more important, which is consistent with many previous studies for relevance ranking in IR. As for the reason why reciprocal and exponential function performs beter than linear function, we think this is because MQ2007 is extracted from GOV data, where title and abstraction information may play a dominant role in determining the relevance. Terefore, the functions with a long tail, as that in reciprocal or exponential function, will be favored. To sum up, the position function plays an important role in DeepRank, and users should pay more atention to the choice, which need to be conducted by considering the characteristics of diferent applications.
Relations to Previous Models. We also would like to point out that DeepRank has a close relationship with previous models, such as BM25, MatchPyramid, and Match-SRNN. With some simplifcation, DeepRank can reduce to (or approximate) these models.
BM25 is a bag-of-words retrieval model that ranks a set of documents based on the query terms appearing in each document, regardless of the inter-relationships between the query terms within a document. Te most common form is given as follows.
BM25(q, d) =
� w ∈q
IDF(w) · f (w, d) · (k1 + 1) f (w, d) + k1 · (1 − b + b |d| avgdl)(10) where k1 and b are the hyper parameters, f (w, d) represents term frequency of w in document d, IDF(w) represents inverse document frequency of w, |d| denotes the document length and avgdl denotes the averaged document length.
We are going to show that a simplifed DeepRank has the ability to approximate BM25 function. Firstly, the window size of querycentric context is set to 1. Ten the indicator function is used to construct the input tensor, therefore word representations of query/query-centric context are ignored. In this way, exact matching signals are naturally captured, like in BM25. We can see that the output of tensor will be a 0-1 vector, with only the elements at the matched positions will be 1. If CNN is used in the measurement step, we can omit the convolution layer and directly use the pooling strategy to output the value 1; while if 2D-GRU is used, we can also output the value 1 by appropriately seting gates and parameters.
As a consequence, the output of the measure network will be 1 for each query-centric context. At the aggregation step, we set the position function as a constant д(p) = 1/|d|, and term weight as the IDF value, i.e. Ewu = IDF(wu). Terefore, the output of this
DeepRank can be viewed as the following function:
DeepRank(q, d) =
� w ∈q
IDF(w) · RNN p ∈P(w)
�
1, 1/|d|
�
=
� w ∈q
IDF(w) · G(f (w, d), |d|), (11) where the second equation is obtained because RNN is an accumulative process, and the function G is determined by the parameters in RNN, learned from the training data. Te function G has high capacities to approximate the functions in the formula of BM25 since there are many parameters. Terefore, a simplifed version of DeepRank can well approximate the BM25 model.
In addition, DeepRank has closer relationships with MatchPyramid and Match-SRNN. If we set the window size of query-centric context to be k = |d| and the weights of query term wu to be
1/f (wu, d), DeepRank reduces to MatchPyramid or Match-SRNN, by using CNN or 2D-GRU as the measure network, respectively.
CONCLUSIONS AND FUTURE WORK
In this paper, we propose a new deep learning architecture, namely
DeepRank. Firstly, a detection strategy is designed to extract querycentric contexts. A measure network is then applied to determine the local relevance between query and each query-centric context, by using CNN or 2D-GRU. Finally, an aggregation network is used to produce the global relevance score, via RNN and a term gating network. DeepRank not only well simulates the relevance generation process in human judgement, but also captures important
IR characteristics, i.e. exact/semantic matching signals, proximity heuristics, query term importance, and diverse relevance requirement. We conduct experiments on both benchmark LETOR4.0 data and a large clickthrough data. Te results show that DeepRank signifcantly outperform learning to rank methods and existing deep IR models, when most existing deep IR models perform much worse than learning to rank methods. To the best of our knowledge, DeepRank is the frst deep IR model to outperform existing learning to rank models. We also give a detailed analysis on DeepRank to show insights on parameter setings for implementation.
For future work, we plan to investigate the diferences between the automatically learned representations of DeepRank and efective features used in learning to rank, which may introduce some insights for architecture design of more powerful deep IR models.
ACKNOWLEDGMENTS
Tis work was funded by the 973 Program of China under Grant No.
2014CB340401, the National Natural Science Foundation of China(NSFC) under Grants No. 61232010, 61433014, 61425016, 61472401, and 61203298, and the Youth Innovation Promotion Association
CAS under Grants No. 20144310 and 2016102. Te authors would like to thank Chengxiang Zhai (UIUC) and Yixing Fan (ICT, CAS) for their valuable suggestions on this work, and Weipeng Chen (Sogou
Inc.) for providing helps on the data processing of ChineseClick.
REFERENCES
 Christopher JC Burges. 2010. From ranknet to lambdarank to lambdamart: An overview. Learning 11 (2010), 23–581.
 Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li. 2007. Learning to rank: from pairwise approach to listwise approach. In ICML. ACM, 129–136.
 Olivier Chapelle and Yi Chang. 2011. Yahoo! learning to rank challenge overview.
In Proceedings of the Learning to Rank Challenge. 1–24.
 Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using RNN encoder-decoder for statistical machine translation.
In EMNLP. 1724–1734.
 Carsten Eickhof, Sebastian Dungs, and Vu Tran. 2015. An eye-tracking study of query reformulation. In SIGIR. ACM, 13–22.
 Hui Fang, Tao Tao, and ChengXiang Zhai. 2004. A formal study of information retrieval heuristics. In SIGIR. ACM, 49–56.
 Yoav Freund, Raj Iyer, Robert E Schapire, and Yoram Singer. 2003. An efcient boosting algorithm for combining preferences. JMLR 4, Nov (2003), 933–969.
 Fredric C Gey. 1994. Inferring probability of relevance using the method of logistic regression. In SIGIR. Springer, 222–231.
 Rich Caruana Steve Lawrence Lee Giles. 2001. Overfting in Neural Nets: Backpropagation, Conjugate Gradient, and Early Stopping. In NIPS, Vol. 13. MIT Press, Alan Graves, Abdel-rahman Mohamed, and Geofrey Hinton. 2013. Speech recognition with deep recurrent neural networks. In ICASSP. IEEE, 6645–6649.
 Jiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce Crof. 2016. A deep relevance matching model for ad-hoc retrieval. In CIKM. ACM, 55–64.
 Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai Chen. 2014. Convolutional neural network architectures for matching natural language sentences. In NIPS.
2042–2050.
 Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry
Heck. 2013. Learning deep structured semantic models for web search using clickthrough data. In CIKM. ACM, 2333–2338.
 Torsten Joachims. 2002. Optimizing search engines using clickthrough data. In
SIGKDD. ACM, 133–142.
 Torsten Joachims. 2006. Training linear SVMs in linear time. In SIGIR. ACM, 217–226.
 Diederik Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014).
 Yann LeCun, Yoshua Bengio, and Geofrey Hinton. 2015. Deep learning. Nature
521, 7553 (2015), 436–444.
 Tie-Yan Liu. 2009. Learning to rank for information retrieval. Foundations and Trends in Information Retrieval 3, 3 (2009), 225–331.
 Yuanhua Lv and ChengXiang Zhai. 2009. Positional language models for information retrieval. In SIGIR. ACM, 299–306.
 Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jef Dean. 2013.
Distributed representations of words and phrases and their compositionality. In
NIPS. 3111–3119.
 Shuzi Niu, Jiafeng Guo, Yanyan Lan, and Xueqi Cheng. 2012. Top-k learning to rank: labeling, ranking and evaluation. In SIGIR. ACM, 751–760.
 Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1999. Te
PageRank citation ranking: Bringing order to the web. Technical Report. Stanford
InfoLab.
 Liang Pang, Yanyan Lan, Jiafeng Guo, Jun Xu, and Xueqi Cheng. 2016. A study of matchpyramid models on ad-hoc retrieval. In Neu-IR f16 SIGIR Workshop on
Neural Information Retrieval.
 Liang Pang, Yanyan Lan, Jiafeng Guo, Jun Xu, Shengxian Wan, and Xueqi Cheng.
2016. Text matching as image recognition. In AAAI. AAAI Press, 2793–2799.
 Tao Qin, Tie-Yan Liu, Jun Xu, and Hang Li. 2010. LETOR: A benchmark collection for research on learning to rank for information retrieval. Information Retrieval
13, 4 (2010), 346–374.
 Stephen Robertson. 2000. Evaluation in information retrieval. In Lectures on information retrieval. Springer, 81–92.
 Stephen E Robertson and Steve Walker. 1994. Some simple efective approximations to the 2-poisson model for probabilistic weighted retrieval. In SIGIR.
Springer-Verlag New York, Inc., 232–241.
 Aliaksei Severyn and Alessandro Moschiti. 2015. Learning to rank short text pairs with convolutional deep neural networks. In Proceedings of SIGIR. ACM, 373–382.
 Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng, and Gr´egoire Mesnil. 2014.
Learning semantic representations using convolutional neural networks for web search. In WWW. WWW, 373–374.
 Mark D Smucker, James Allan, and Ben Carterete. 2007. A comparison of statistical signifcance tests for information retrieval evaluation. In CIKM. ACM, 623–632.
 Tao Tao and ChengXiang Zhai. 2007. An exploration of proximity measures in information retrieval. In SIGIR. ACM, 295–302.
 Shengxian Wan, Yanyan Lan, Jiafeng Guo, Jun Xu, Liang Pang, and Xueqi Cheng.
2016. Match-SRNN: Modeling the Recursive Matching Structure with Spatial
RNN. In IJCAI. 2922–2928.
 Ho Chung Wu, Robert WP Luk, Kam-Fai Wong, and KL Kwok. 2007. A retrospective study of a hybrid document-context based retrieval model. Information processing & management 43, 5 (2007), 1308–1331.
 Jun Xu and Hang Li. 2007. Adarank: a boosting algorithm for information retrieval. In SIGIR. ACM, 391–398.
A
DATA PREPROCESSING
For pre-processing, all the words in documents and queries are white-space tokenized, lower-cased, and stemmed using the Krovetz stemmer. Stopword removal is performed on query and document words using the INQUERY stop list. Words occurred less than 5 times in the collection are removed from all the document.
B
QUERY MATRIX & CONTEXT MATRIX
Te constructions of query matrix and context matrix are described in detail below.
Sind ij
= 1 if wi = vj, Sind ij
= 0 otherwise, Scos ij
= xiT yj/(∥xi∥ · ∥yj∥), where xi and yj denote the word embeddings of wi and vj, respectively. To further incorporate the word representations of query/query-centric context to the input, we extend each element of Sij to a three-dimensional vector ˜Sij.
˜Sij = [xi,yj,Sij]T = [(WQxi)T, (WDyj)T,Sij]T where W Q and W D are the linear transformations that reducing the higher dimensions of word embeddings into lower ones, for example, from 50 dimensions to 2 dimensions.
Terefore, the original matrix S will become a three-order tensor, denoted as S. In this way, the input tensor can be viewed as a combination of three matrices, i.e. query matrix, query-centric context matrix, and word-level interaction matrix.
C
CODE
We have released two versions of DeepRank. Te original one is TextNet (htps://github.com/pl8787/textnet-release), the newer one is implemented in PyTorch (htps://github.com/pl8787/DeepRank
PyTorch).Learning Latent Vector Spaces for Product Search
Christophe Van Gysel cvangysel@uva.nl
Maarten de Rijke derijke@uva.nl
Evangelos Kanoulas e.kanoulas@uva.nl
University of Amsterdam, Amsterdam, The Netherlands
ABSTRACT
We introduce a novel latent vector space model that jointly learns the latent representations of words, e-commerce products and a mapping between the two without the need for explicit annotations.
The power of the model lies in its ability to directly model the discriminative relation between products and a particular word. We compare our method to existing latent vector space models (LSI, LDA and word2vec) and evaluate it as a feature in a learning to rank setting. Our latent vector space model achieves its enhanced performance as it learns better product representations. Furthermore, the mapping from words to products and the representations of words benefit directly from the errors propagated back from the product representations during parameter estimation. We provide an in-depth analysis of the performance of our model and analyze the structure of the learned representations.
Keywords
Entity retrieval; Latent space models; Representation learning
INTRODUCTION
Retail through online channels has become an integral part of consumers' lives. In addition to using these online platforms that generate hundreds of billions of dollars in revenue, consumers increasingly participate in multichannel shopping where they research items online before purchasing them in brick-andmortar stores. Search engines are essential for consumers to be able to make sense of these large collections of products available online. In the case of directed searching (in contrast to exploratory browsing), users formulate queries using characteristics of the product they are interested in (e.g., terms that describe the product's category). However, it is widely known that there exists a mismatch between queries and product representations where both use different terms to describe the same concepts. Thus, there is an urgent need for better semantic matching methods.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.
CIKM'16, October 24 - 28, 2016, Indianapolis, IN, USA c⃝ 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ISBN 978-1-4503-4073-1/16/10...$15.00
DOI: http://dx.doi.org/10.1145/2983323.2983702
Product search is a particular example of the more general entity finding task that is increasingly being studied. Other entity finding tasks considered recently include searching for people, books and groups. Products are retrievable entities where every product is associated with a description and one or more user reviews. Therefore, we use the terms "product" and "entity" interchangeably in this paper. However, there are two important differences between product search and the entity finding task as defined by de Vries et al.. First, in entity finding one retrieves entities of a particular type from large broad coverage multi-domain knowledge bases such as Wikipedia. In contrast, product search engines operate within a single domain which can greatly vary in size. Second, user queries in product search consist of free-form text, as opposed to the semi-structured queries with additional type or relational constraints being used in entity finding.
In this paper we tackle the problem of discriminating between products based on the language (i.e., descriptions and reviews) they are associated with. Existing methods that are aimed at discriminating between entities based on textual data learn word representations using a language modeling objective or heuristically construct entity representations. Our approach directly learns two things: a unidirectional mapping between words and entities, as well as distributed representations of both words and entities. It does so in an unsupervised and automatic manner such that words that are strongly evidential for particular products are projected nearby those products. While engineering of representations is important in information retrieval, unsupervised joint representation learning of words and entities has not received much attention. We fill this gap. Our focus on learning representations for an end-to-end task such as product search is in contrast to the large volume of recent literature on word representation learning that has a strong focus on upstream components such as distributional semantics, parsing and information extraction. In addition, our focus on unsupervised representation learning is in contrast to recent entity representation learning methods that heavily depend on precomputed entity relationships and cannot be applied in their absence.
In recent years, significant progress has been made concerning semantic representations of entities. We point out three key insights on which we build:(1) Distributed representations learned by discriminative neural networks reduce the curse of dimensionality and improve generalization. Latent features encapsulated by the model are shared by different concepts and, consequently, knowledge about one concept influences knowledge about others. (2) Discriminative approaches outperform generative models if enough training data is available as discriminative models solve the classification problem directly instead of solving a more general problem first. (3) Recently proposed unsuarXiv:1608.07253v1 [cs.IR] 25 Aug 2016 pervised neural retrieval models do not scale as they model a distribution over all retrievable entities; the approach is infeasible during training if the collection of retrievable entities is large.
Building on these insights, we introduce Latent Semantic Entities (LSE), a method that learns separate representations of words and retrievable objects jointly for the case where mostly unstructured documents are associated with the objects (i.e., descriptions and user reviews for products) and without relying on predefined relationships between objects (e.g., knowledge graphs). LSE learns to discriminate between entities for a given word sequence by mapping the sequence into the entity representation space. Contrary to heuristically constructed entity representations, LSE learns the relationship between words and entities directly using gradient descent. Unlike, we avoid computing the full probability distribution over entities; we do so by using noise-contrastive estimation.
Our research questions are as follows:(1) How do the parameters of LSE influence its efficacy? (2) How does LSE compare to latent vector models based on LDA, LSI and word2vec? (3) How does LSE compare to a smoothed language model that applies lexical term matching? (4) What is the benefit of incorporating LSE as a feature in a learning-to-rank setting?
We contribute:(1) A latent vector model, LSE, that jointly learns the representations of words, entities and the relationship between the former, together with an open-source implementation.1(2) A study of the influence of LSE's parameters and how these influence its ability to discriminate between entities. (3) An in-depth comparative analysis of the entity retrieval effectiveness of latent vector models. (4) Insights in how LSE can improve retrieval performance in entity-oriented search engines. (5) An analysis of the differences in performance between latent vector models by examining entity representations and mappings from queries to entity space.
RELATED WORK
Product retrieval
Product search engines are an important source of traffic in the e-commerce market. Specialized solutions are needed to maximize the utilization of these platforms. Nurmi et al. note a discrepancy between buyers' shopping lists and how retail stores maintain information. They introduce a grocery retrieval system that retrieves products using shopping lists written in natural language. Product resolution is an important task for e-commerce aggregation platforms, such as verticals of major web search engines and price comparison websites. Duan et al. propose a probabilistic mixture model for the attribute-level analysis of product search logs. They focus on structured aspects of product entities, while in this work we learn representations from unstructured documents. Duan et al. extend the language modeling approach to product databases by incorporating the ability to condition on specification (e.g., lightweight products only). They note that while languages such as SQL can be used effectively to query these databases, their use is difficult for non-experienced end users.
Duan and Zhai study the problem of learning query intent representation for structured product entities. They emphasize that existing methods focus only on the query space and overlook critical information from the entity space and the connection in between.
We agree that modeling the connection between query words and entities and propagating information from the entity representations back to words is essential. In contrast to their work, we consider the problem of learning representations for entities based on their associations with unstructured documents.
1https://github.com/cvangysel/SERT
Latent semantic information retrieval
The mismatch between queries and documents is a critical challenge in search. Latent Semantic Models (LSMs) enable retrieval based on conceptual content, instead of exact word matches.
LSMs have become popular through the introduction of Latent Semantic Indexing (LSI), followed by probabilistic LSI (pLSI). Salakhutdinov and Hinton use a deep auto-encoder for the unsupervised learning of latent semantic document bit patterns.
Deep Structured Semantic Models employ click data to predict a document's relevance to a query. Methods based on neural networks have also been used for machine-learned ranking. Van Gysel et al. introduce an LSM for entity retrieval, with an emphasis on expert finding; they remark that training the parameters of their model becomes infeasible when the number of entities increases. In this work we mitigate this problem by considering only a random sample of entities as negative examples during training. This allows us to efficiently estimate model parameters in large product retrieval collections, which is not possibly using the approach of due to its requirement to compute a normalization constant over all entities.
Representation learning
Recently, there has been a growing interest in neural probabilistic language models (LMs) for the modeling of word sequences. Distributed representations of words learned by neural LMs, also known as word embeddings, incorporate syntactic and semantic information as a side-effect of their ability to reduce the dimensionality. Feed-forward and recurrent neural networks perform well in various NLP tasks. Very recently, there has been an increased interest in multimodal neural language models, which are used for the task of automated image captioning, amongst others. Learning representations of entities is not new. Bordes et al. leverage structured relations captured in Knowledge Bases (KB) for entity representation learning and evaluate their representations on the link prediction task.
Our approach has a strong focus on modeling the language of all entities collaboratively, without the need for explicit entity relations during training. Zhao et al. employ matrix factorization methods to construct low-dimensional continuous representations of entities, categories and words for determining similarity of Wikipedia entities. They employ a word pair similarity evaluation set and only evaluate on pairs referring to Wikipedia entities; they learn a single semantic space for widely-differing concepts (entities, categories and words) of different cardinalities and make extensive use of an underlying Knowledge Graph (KG) to initialize their parameters.
In contrast, we model representations of words and entities jointly in separate spaces, in addition to a mapping from word to entity representations, in an unsupervised manner.
We tackle the task of learning latent continuous vector representations for e-commerce products for the purpose of product search.
The focus of this work lies in the language modeling and representation learning challenge. We learn distributed representations of words and entities and a mapping between the two. At retrieval time, we rank entities according to the similarity of their latent representations to the projected representation of a query. Our model
LSE is compared against existing entity-oriented latent vector representations that have been created using LSI, LDA and word2vec.
We provide an analysis of model parameters and give insight in the quality of the joint representation space. w ∈ V eV
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�������� v
�
�������� eE
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
��������
˜e
�
�������� eE
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�
�������� e
�
�������� x ∈ E
Look-up embedding in Wv
Transform with tanh (W · v + b)
Look-up embedding in We
Sc(˜e, e) f(w)
Figure 2: Schematic representation of the Latent Semantic Entities model for a single word w. Word embeddings Wv (eV -dim. for
|V | words), entity embeddings We (eE-dim. for |X| entities) and the mapping from words to entities (eE-by-eV matrix W, eE-dim. vector b) are learned using gradient descent.
Figure 1: Illustrative example of how entities are ranked in vector space models w.r.t. a projected query. Query q is projected into entity space E using mapping f (black arrow) and entities (black crosses) are ranked according to their similarity in decreasing order.
LATENT VECTOR SPACES FOR
ENTITY RETRIEVAL
We first introduce a generalized formalism and notation for entityoriented latent vector space models. After that, in §3.2, we introduce Latent Semantic Entities, a latent vector space model that jointly learns representations of words, entities and a mapping between the two directly, based on the idea that entities are characterized by the words they are associated with and vice versa. Product representations are constructed based on the n-grams the products are likely to generate based on their description and reviews, while word representations are based on the entities they are associated with and the context they appear in. We model the relation between word and product representations explicitly so that we can predict the product representation for a previously unseen word sequence.
Background
We focus on a product retrieval setting in which a user wants to retrieve the most relevant products on an e-commerce platform.
As in typical information retrieval scenarios, the user encodes their information need as a query q and submits it to a search engine.
Product search queries describe characteristics of the product the user is searching for, such as a set of terms that describe the product's category.
Below, X denotes the set of entities that we consider. For every xi ∈ X we assume to have a set of associated documents Dxi.
The exact relation between the entity and its documents depends on the problem setting. In this paper, entities are products and documents associated with these products are descriptions and product reviews.
Latent vector space models rely on a function f : V + → E that maps a sequence of words (e.g., a query q during retrieval) from a vocabulary V to a eE-dimensional continuous entity vector space
E ⊂ ReE. Every entity xi ∈ X has a corresponding vector representation ei ∈ E. Let Sc : E × E → R denote the cosine similarity between vectors in E. For a given query q, entities xi are ranked in decreasing order of the cosine similarity between ei and the query projected into the space of entities, f(q). Fig. 1 illustrates how entities are ranked according to a projected query. For
LSI, f is defined as the multiplication of the term-frequency vector representation of q with the rank-reduced term-concept matrix and the inverse of the rank-reduced singular value matrix. In the case of LDA, f becomes the distribution over topics conditioned on q. This distribution is computed as the sum of the topic distributions conditioned on the individual words of q. In this paper, the embedding f is learned; see §3.3 below.
Traditional vector space models operate on documents instead of entities. Demartini et al. extend document-oriented vector spaces to entities by representing an entity as a weighted sum of the representations of their associated documents: ei =
� dj∈Dxi ri,jf(dj)(1) where f(dj) is the vector representation of dj and ri,j denotes the relationship weight between document dj and entity xi. In this work we put ri,j = 1 whenever dj ∈ Dxi for a particular xi ∈
X and ri,j = 0 otherwise, as determining the relationship weight between entities and documents is a task in itself.
Latent semantic entities
While Eq. 1 adapts document-oriented vector space models to entities, in this work we define f by explicitly learning (§3.3) the mapping between word and entity representations and the representations themselves: f(s) = tanh
�
W · (Wv · 1
|s|
� wi∈s δi) + b
�(2) for a string s of constituent words w1,..., w|s| (an n-gram extracted from a document or a user-issued query), where Wv is the eV × |V | projection matrix that maps the averaged one-hot representations (i.e., a |V |-dimensional vector with element i turned on and zero elsewhere) of word wi, δi, to its eV -dimensional distributed representation. This is equivalent to taking the embeddings of the words in s and averaging them. In addition, b is a eE-dimensional bias vector, W is the eE ×eV matrix that maps averaged word embeddings to their corresponding position in entity space E and tanh is the element-wise smooth hyperbolic tangent with range (−1, 1). This transformation allows word embeddings and entity embeddings to be of a different dimensionality.
In other words, for a given string of words we take the representation of this string to be the average of the representations of the words it contains. This averaged word representation is then transformed using a linear map (W) and afterwards translated using b. We then apply the hyperbolic tangent as non-linearity such that every component lies between −1 and 1. First of all, this regularizes the domain of the space and avoids numerical instability issues that occur when the magnitude of the vector components becomes too large. Secondly, by making the function non-linear we are able to model non-linear class boundaries in the optimization objective that we introduce in the next section. We use We to denote the |X|×eE matrix that holds the entity representations. Row i of We corresponds to the vector representation, ei, of entity xi.
Fig. 2 depicts a schematic overview of the proposed model. The parameters Wv, W, b and We will be learned automatically using function approximation methods as explained below.
The model proposed in this section shares similarities with previous work on word embeddings and unsupervised neural retrieval models. However, its novelty lies in its ability to scale to large collections of entities and its underlying assumption that words and entities are embedded in spaces of different dimensionality:(1) The model of has no notion of entity retrieval as it estimates a language model for the whole corpus. (2) Similar to, Eq. 2 aggregates words wi ∈ s to create a single phrase representation of s. However, in, a distribution P(X | wi) is computed for every wi independently and aggregation occurs using the factor product. This is infeasible during model training when the collection of retrievable objects becomes too large, as is the case for product search. In the next section (§3.3) we solve this problem by sampling. (3) In both two sets of representations of the same dimensionality are learned for different types of objects with potentially different latent structures (e.g., words, word contexts and experts). As mentioned earlier, Eq. 2 alleviates this problem by transforming one latent space to the other.
Parameter estimation
For a particular document d ∈ Dxi associated with entity xi, we generate n-grams wj,1,..., wj,n where n (window size) remains fixed during training. For every n-gram wj,1,..., wj,n, we compute its projected representation f(wj,1,..., wj,n) in E using f(Eq. 2). The objective, then, is to directly maximize the similarity between the vector representation of the entity ei and the projected n-gram f(wj,1,..., wj,n) with respect to Sc (§3.1), while minimizing the similarity between f(wj,1,..., wj,n) and the representations of non-associated entities. This allows the model to learn relations between neighboring words in addition to the associated entity and every word.
However, considering the full set of entities for the purpose of discriminative training can be costly when the number of entities
|X| is large. Therefore, we apply a variant of Noise-Contrastive
Estimation (NCE) where we sample negative instances from a noise distribution with replacement. We use the uniform distribution over entities as noise distribution. Define
P(S | ei, f(wj,1,..., wj,n)) = σ(ei · f(wj,1,..., wj,n))(3) as the similarity of two representations in latent entity space, where σ(t) =
1 + e−t denotes the sigmoid function and S is an indicator binary random variable that says whether xi is similar to f(wj,1,..., wj,n).
We then approximate the probability of an entity xi given an ngram by randomly sampling z contrastive examples: log ˜P(xi | wj,1,..., wj,n)
= log P(S | ei, f(wj,1,..., wj,n))
+ z
� k=1, xk∼U(X) log (1 − P(S | ek, f(wj,1,..., wj,n))) where U(X) denotes the uniform distribution over entities X, the noise distribution used in NCE. Eq. 4 avoids iterating over all entities during parameter estimation as we stochastically sample z entities uniformly as negative training examples.2
During model construction we maximize the log-probability (4) using batched gradient descent. The loss function for a single batch of m instances ((wk,1,..., wk,n), xk) consisting of n-grams sampled from documents Dxk (see §4.2) and associated entity xk is as follows:
L(Wv, We, W, b)
=
− 1 m m
� k=1 log ˜P(xk | wk,1,..., wk,n)
+ λ
2m
�� i,j
Wv
2 i,j +
� i,j
We
2 i,j +
� i,j
W 2 i,j
�, (5) where λ is a weight regularization parameter. Instances are shuffled before batches are created. The update rule for a particular parameter θ (Wv, We, W or b) given a single batch of size m is: θ(t+1) = θ(t) − α(t) ⊙ ∂L
∂θ (Wv(t), We(t), W (t), b(t)), (6) where α(t) and θ(t) denote the per-parameter learning rate and parameter θ at time t, respectively. The learning rate α consists of the same number of elements as there are parameters; in the case of a global learning rate, all elements of α are equal to each other.
The derivatives of the loss function (5) are given in the Appendix.
EXPERIMENTAL SETUP
Research questions
In this paper we investigate the problem of constructing a latent vector model of words and entities by directly modeling the discriminative relation between entities and word context. We seek to answer the following research questions:
RQ1 How do the parameters of LSE influence its efficacy?
In §3 we introduced various hyper-parameters along with the definition of Latent Semantic Entities. We have the size of word representations eV and the dimensionality of the entity representations eE. During parameter estimation, the window size n influences the context width presented as evidence for a particular entity. What is the influence of these parameters on the effectiveness of LSE and can we identify relations among parameters?
RQ2 How does LSE compare to latent vector models based on
LDA, LSI and word2vec?
Is there a single method that always performs best or does effectiveness differ per domain? Does an increase in the vector space dimensionality impact the effectiveness of these methods?
RQ3 How does LSE compare to a smoothed language model that applies lexical term matching?
How does LSE compare to language models on a per-topic basis?
Are there particular topics that work especially well with either type of ranker?
RQ4 What is the benefit of incorporating LSE as a feature in a learning-to-rank setting?
2We exploit the special nature of our evaluation scenario where we know the unique association between documents and entities. The setup can easily be adapted to the more general case where a document is associated with multiple entities by extracting the same word sequences from the document for every associated entity.
What if we combine popularity-based, exact matching and latent vector space features in a linear learning-to-rank setting? Do we observe an increase in effectiveness if we combine these features?
Experimental design
To answer the research questions posed in §4.1, we evaluate LSE in an entity retrieval setting organized around Amazon products(see §4.3).
We choose to experiment with samples of Amazon product data for the following reasons: (1) The collection contains heterogeneous types of evidential documents associated with every entity: descriptions as well as reviews. (2) Every department (e.g., Home & Kitchen) constitutes a separate, self-contained domain. (3) Within each department there is a hierarchical taxonomy that partitions the space of entities in a rich structure. We can use the labels associated with these partitions and the partitions themselves as ground truth during evaluation. (4) Every department consists of a large number of products categorized over a large number of categories. Importantly, this allows us to construct benchmarks with an increasing number of entities. (5) Every product has a variety of attributes that can be used as popularity-based features in a learning-to-rank setting.
To answer RQ1 we investigate the relation between the dimensionality of the entity representations eE and window size n. The latter, the window size n, controls the context width the model can learn from, while the former, the dimensionality of the entity representations eE, influences the number of parameters and expressive power of the model. We sweep exponentially over n (2i for
0 ≤ i < 6) and eE (2i for 6 ≤ i < 11). RQ2 is answered by comparing LSE with latent vector space model baselines (§4.5) for an increasing entity space dimensionality eE (2i for 6 ≤ i < 11). For
RQ3, we compare the per-topic paired differences between LSE and a lexical language model. In addition, we investigate the correlation between lexical matches in relevant entity documents and ranker preference. We address RQ4 by evaluating LSE as a feature in a machine-learned ranking in addition to query-independent and lexical features.
The number of n-grams sampled per entity x ∈ X from associated documents Dx in every epoch (i.e., iteration of the training data) is equal to
�
|X|
� d∈D max (|d| − n + 1, 0)
�,where the | · | operator is used interchangeably for the size of set X and the number of tokens in documents d ∈ D. This implicitly imposes a uniform prior over entities (i.e., stratified sampling where every entity is of equal importance). The word vocabulary V is created for each benchmark by ignoring punctuation, stop words and case; numbers are replaced by a numerical placeholder token. We prune
V by only retaining the 216 most-frequent words so that each word can be encoded by a 16-bit unsigned integer. In terms of parameter initialization of the Latent Semantic Entities model, we sample the initial matrices Wv, W (Eq. 2) and We uniformly in the range
�
−
�
6.0 m+n, �
6.0 m+n
� for an m × n matrix, as this initialization scheme is known to improve model training convergence, and take the bias vector b to be null. The number of word features is set to eV = 300, similar to. We take the number of negative examples z = 10 to be fixed. Mikolov et al. note that a value of z between 10 and 20 is sufficient for large data sets.
We used Adam (α = 0.001, β1 = 0.9, β2 = 0.999) with batched gradient descent (m = 4096) and weight decay λ = 0.01 during training on NVidia Titan X GPUs. Adam has been designed specifically for non-stationary, stochastic cost functions like the one we defined in Eq. 4. For every model, we iterate over the training data 15 times and choose the best epoch based on the validation sets (Table 1).
Product search benchmarks
We evaluate on four samples from different product domains3(Amazon departments), each with of an increasing number of products: Home & Kitchen (8,192 products), Clothing, Shoes & Jewelry (16,384 products), Pet Supplies (32,768 products) and Sports
& Outdoors (65,536 products); see Table 1. The documents associated with every product consist of the product description plus reviews provided by Amazon customers.
Rowley [51, p. 24] describes directed product search as users searching for "a producer's name, a brand or a set of terms which describe the category of the product." Following this observation, the test topics ci are extracted from the categories each product belongs to. Category hierarchies of less than two levels are ignored, as the first level in the category hierarchy is often non-descriptive for the product (e.g., in Clothing, Shoes & Jewelry this is the gender for which the clothes are designated). Products belonging to a particular category hierarchy are considered as relevant for its extracted topic. Products can be relevant for multiple topics. Textual representations qci of the topics based on the categories are extracted as follows. For a single hierarchy of categories, we tokenize the titles of its sub-categories and remove stopwords and duplicate words.
For example, a digital camera lense found in the Electronics department under the categorical topic Camera & Photo → Digital
Camera Lenses will be relevant for the textual query "photo camera lenses digital." Thus, we only have two levels of relevance. We do not index the categories of the products as otherwise the query would match the category and retrieval would be trivial.
Evaluation measures and significance
To measure retrieval effectiveness, we report Normalized Discounted Cumulative Gain (NDCG). For RQ4, we additionally report Precision@k (k = 5, 10). Unless mentioned otherwise, significance of observed differences is determined using a two-tailed paired Student's t-test (∗∗∗ p < 0.01; ∗∗ p < 0.05; ∗ p < 0.1).
Methods used in comparisons
We compare Latent Semantic Entities to state-of-the-art latent vector space models for entity retrieval that are known to perform semantic matching. We also conduct a contrastive analysis between LSE and smoothed language models with exact matching capabilities.
Vector Space Models for entity finding. Demartini et al.
 propose a formal model for finding entities using document vector space models (§3.1). We compare the retrieval effectiveness of LSE with baseline latent vector space models created using (1) Latent Semantic Indexing (LSI) with TF-IDF term weighting, (2) Latent Dirichlet Allocation (LDA) with α = β = 0.1, where a document is represented by its topic distribution, and (3) word2vec with CBOW and negative sampling, where a query/document is represented by the average of its word embeddings (same for queries in LSE). Similar to LSE, we train word2vec for 15 iterations and select the best-performing model using the validation sets (Table 1).
Query-likelihood Language Model. For every entity a profile-based statistical language model is constructed using maximumlikelihood estimation, which is then smoothed by the language model of the entire corpus. The retrieval score of entity x for query q is defined as
˜P(q | x) =
� ti∈q
P(ti | θx), 3A list of product identifiers, topics and relevance assessments can be found at https://github.com/cvangysel/SERT.
Table 1: Overview of the Home & Kitchen, Clothing, Shoes & Jewelry, Pet Supplies and Sports & Outdoors product search benchmarks.
T and V denote the test and validation sets, respectively. Arithmetic mean and standard deviation are reported wherever applicable.
Home & Kitchen
Clothing, Shoes & Jewelry
Pet Supplies
Sports & Outdoors
Corpus (train)
Number of documents
Document length
70.02 ± 73.82
58.41 ± 61.90
77.48 ± 78.44
72.52 ± 81.47
Number of entities
Documents per entity
10.76 ± 52.01
5.74 ± 18.60
12.73 ± 55.98
7.66 ± 30.38
Topics (test)
Topics
657 (T)
72 (V)
750 (T)
83 (V)
385 (T)
42 (V)
1,879 (T)
208 (V)
Terms per topic
5.11 ± 1.79
4.10 ± 1.86
3.73 ± 1.62
4.64 ± 1.68
Relevant entities per topic
10.92 ± 32.41 (T)
10.29 ± 15.66 (V)
20.15 ± 57.78 (T)
12.13 ± 19.85 (V)
75.96 ± 194.44 (T)
57.40 ± 88.91 (V)
29.27 ± 61.71 (T)
38.25 ± 157.34 (V) where P(t | θx) is the probability of term t occurring in the smoothed language model of x (Jelinek-Mercer smoothing ). Given a query q, entities are ranked according to ˜P(q | x) in descending order.
Machine-learned ranking. RankSVM models in §5.2 and 6 are trained using stochastic gradient descent using the implementation of Sculley. We use default values for all parameters, unless stated otherwise. For the experiment investigating LSE as a feature in machine-learned ranking in §5.2, we construct training examples by using the relevant entities as positive examples. Negative instances are generated by sampling from the non-relevant entities with replacement until the class distribution is uniform.
RESULTS AND DISCUSSION
We start by giving a high-level overview of our experimental results (RQ1 and RQ2), followed by a comparison with lexical matching methods (RQ3) and the use of LSE as a ranking feature(RQ4) (see §4.2 for an overview of the experimental design).
Overview of experimental results
RQ1: Fig. 3 depicts a heat map for every combination of window size and entity space dimensionality evaluated on the validation sets (Table 1). Fig. 3 shows that neither extreme values for the dimensionality of the entity representations nor the context width alone achieve the highest performance on the validation sets.
Instead, a low-dimensional entity space (128- and 256-dimensional) combined with a medium-sized context window (4- and 8grams) achieve the highest NDCG. In the two largest benchmarks(Fig. 3c, 3d) we see that for 16-grams, NDCG actually lowers as the dimensionality of the entity space increases. This is due to the model fitting the optimization objective (Eq. 5), which we use as an unsupervised surrogate of relevance, too well. That is, as the model is given more learning capacity (i.e., higher dimensional representations), it starts to learn more regularities of natural language which counteract retrieval performance.
RQ2: Fig. 4 presents a comparison between LSE (window size n = 4) and vector space model baselines (§4.5) for increasing entity representation dimensionality (2i for 6 ≤ i < 11) on the test sets. LSE significantly outperforms (p < 0.01) all baseline methods in most cases (except for Fig. 4a where eE = 1024). For the smaller benchmarks (Fig. 4a, 4b), we see LSI as the main competitor of LSE. However, as the training corpora become larger (in
Fig. 4c, 4d), word2vec outperforms LSI and becomes the main conTable 2: Correlation coefficients between average IDF of lexically matched terms in documents associated with relevant entities and △NDCG. A negative correlation coefficient implies that queries consisting of more specific terms (i.e., low document freq.) that occur exactly in documents associated with relevant entities are more likely to benefit from QLM, whereas other queries (with less specific terms or less exact matches) gain more from LSE. Significance is achieved for all benchmarks (p < 0.01) using a permutation test.
Benchmark
Spearman R
Pearson R
Home & Kitchen
−0.30
−0.35
Clothing, Shoes & Jewelry
−0.40
−0.37
Pet Supplies
−0.17
−0.17
Sports & Outdoors
−0.34
−0.36 tester of LSE. On all benchmarks, LSE peaks when the entity representations are low-dimensional (128- or 256-dimensional) and afterwards (for a higher dimensionality) performance decreases. On the other hand, word2vec stagnates in terms of NDCG around representations of 512 dimensions and never achieves the same level as LSE did for one or two orders of magnitude (base 2) smaller representations. This is a beneficial trait of LSE, as high-dimensional vector spaces are undesirable due to their high computational cost during retrieval.
A feature for machine-learned ranking
We now investigate the use of LSE as a feature in a learning to rank setting. Latent vector space models are known to provide a means of semantic matching as opposed to a purely lexical matching. To determine to which degree this is indeed the case, we first perform a topic-wise comparison between LSE and a lexical language model, the Query-likelihood Language Model (QLM), as described in §4.5. We optimize the parameters of LSE and QLM on the validation sets for every benchmark (Table 1). In the case of LSE, we select the model that performs best in Fig. 3. For
QLM, we sweep over λ linearly from 0.0 to 1.0 (inclusive) with increments of 0.05.
RQ3: Fig. 5 shows the per-topic paired difference between LSE and QLM in terms of NDCG. Topics that benefit more from LSE have a positive value on the y-axis, while those that prefer QLM have a negative value. We can see that both methods perform similarly for many topics (where △ = 0.0). For certain topics one
Entity space dimensionality
Window size(a) Home & Kitchen
Entity space dimensionality
Window size(b) Clothing, Shoes & Jewelry
Entity space dimensionality
Window size(c) Pet Supplies
Entity space dimensionality
Window size(d) Sports & Outdoors
Figure 3: Sensitivity analysis of LSE in terms of NDCG for window size n and the size of entity representations eE during parameter estimation (Eq. 5) for models trained on Home & Kitchen, Clothing, Shoes & Jewelry, Pet Supplies and Sports & Outdoors product search benchmarks (§4.3) and evaluated on the validation sets.
64∗∗∗
128∗∗∗ 256∗∗∗ 512∗∗∗ 1024
Entity space dimensionality
NDCG
LSE
LDA
LSI word2vec(a) Home & Kitchen
64∗∗∗
128∗∗∗ 256∗∗∗ 512∗∗∗ 1024∗∗
Entity space dimensionality
NDCG
LSE
LDA
LSI word2vec(b) Clothing, Shoes & Jewelry
64∗∗∗ 128∗∗∗ 256∗∗∗ 512∗∗∗ 1024∗∗∗
Entity space dimensionality
NDCG
LSE
LDA
LSI word2vec(c) Pet Supplies
64∗∗∗ 128∗∗∗ 256∗∗∗ 512∗∗∗ 1024∗∗∗
Entity space dimensionality
NDCG
LSE
LDA
LSI word2vec(d) Sports & Outdoors
Figure 4: Comparison of LSE (with window size n = 4) with latent vector space baselines (LSI, LDA and word2vec; §4.5) on Home
& Kitchen, Clothing, Shoes & Jewelry, Pet Supplies and Sports & Outdoors product search benchmarks (§4.3) and evaluated on the test sets. Significance (§4.4) is computed between LSE and the baselines for each vector space size.
−1.0
−0.5
△NDCG(a) Home & Kitchen
−1.0
−0.5
△NDCG(b) Clothing, Shoes & Jewelry
−1.0
−0.5
△NDCG(c) Pet Supplies
−1.0
−0.5
△NDCG(d) Sports & Outdoors
Figure 5: Per-topic paired differences between LSE and Querylikelihood Language Model for models trained on Home &
Kitchen, Clothing, Shoes & Jewelry, Pet Supplies and Sports &
Outdoors product search benchmarks (§4.3) and evaluated on the test sets. For every plot, the y-axis indicates ∆NDCG between LSE and a Query-likelihood Language Model. The xaxis lists the topics in the referenced benchmark in decreasing order of ∆NDCG such that topics for which LSE performs better are on the left and vice versa for the Query-likelihood Language Model on the right. method performs substantially better than the other, suggesting that the two are complementary. To further quantify this, we investigate the relation between specific topic terms and their occurrence in documents relevant to these topics. That is, we measure the correlation between the per-topic △NDCG (as described above) and the average inverse document frequency (IDF) of exact/lexically matched terms in the profile-based language model. In Table 2 we observe that queries that contain specific tokens (i.e., with high inverse document frequency) and occur exactly in documents associated with relevant products, benefit more from QLM (lexical matches). Conversely, queries with less specific terms or without exact matches in the profiles of relevant products gain more from
LSE (semantic matches).
This observation motivates the use of LSE as a ranking feature in addition to traditional language models. Specifically, we now evaluate the use of LSE as a feature in a linear RankSVM(§4.5). Following Fang et al., we consider query-independent(QI) popularity-based features in addition to features provided by
LSE and QLM. This allows us to consider the effect of the querydependent features independent from their ability to model a popularity prior over entities. Table 3 lists the feature sets.
RQ4: Table 4 shows the results for different combinations of feature sets used in a machine-learned ranker, RankSVM. The experiment was performed using 10-fold cross validation on the test sets(Table 1). The combination using all features outperforms smaller subsets of features, on all metrics. We conclude that Latent Semantic Entities adds a signal that is complementary to traditional (lexical) language models, which makes it applicable in a wide range of entity-oriented search engines that use ranker fusion techniques.
Table 3: Overview of the feature sets used in the machinelearned ranking experiments.
Features
Description
QI
Query-independent features: (1) product price;(2) product description length; (3) reciprocal of the Amazon sales rank; and (4) product PageRank scores based on four related product graphs (also bought, also viewed, bought together, buy after viewing).
QLM
Query-likelihood Language Model using Jelinek-Mercer smoothing with λ optimized on the validation set(Table 1). Posterior P(q | x) is used as a feature for entity x and query q.
LSE
Latent Semantic Entities optimized on the validation set(Table 1, Fig. 3). Similarity Sc(f(q), e) is used as a feature for entity x, with vector representation e, and query q.
ANALYSIS OF REPRESENTATIONS
Next, we analyze the entity representations ei of the vector space models independent of the textual representations by providing empirical lower-bounds on their maximal retrieval performance, followed by a comparison with their actual performance so as to measure the effectiveness of word-to-entity mapping f.
Fig. 3 and 4 show which levels of performance may be achieved by using the latent models to generate a ranking from textual queries(Eq. 2). But this is only one perspective. As entities are ranked according to their similarity with the projected query vector f(qc), the performance for retrieving entities w.r.t. the textual representation of a topic c depends on the structure of the entity space E, the ideal retrieval vector e∗ c ∈ E (i.e., the vector that optimizes retrieval performance), and the similarity between f(qc) and e∗ c.
How can we determine the ideal vector e∗ c? First, we define it to be the vector for which the cosine similarity with each of the entity embeddings results in a ranking where relevant entities are ranked higher than non-relevant or unjudged entities. We approximate e∗ c by optimizing the pair-wise SVM objective. That is, for every topic c we construct a separate RankSVM model based on its ground-truth as follows. We only consider topics with at least two relevant entities, as topics with a single relevant entity have a trivial optimal retrieval vector (the entity representation of the single relevant entity). Using the notation of, the normalized entity representations are used as features, and hence the feature mapping φ is defined as φ(c, xi) = ei
∥ei∥2 for all xi ∈ X.
The target ranking r∗ c is given by the entities relevant to topic c.
Thus, the features for every entity become the entity's normalized representation and its label is positive if it is relevant for the topic and negative otherwise. The pair-wise objective then finds a weight vector such that the ranking generated by ordering according to the vector scalar product between the weight vector and the normalized entity representations correlates with the target ranking r∗ c. Thus, our approximation of the ideal vector, ˜e∗ c, is given by the weight vector wc for every c.4
What is the performance of this approximately ideal vector representation? And how far are our representations removed from it?
Fig. 6 shows the absolute performance of ˜e∗ c (dashed curves) and f(q) (solid curves) in terms of NDCG. Comparing the (absolute) difference between every pair of dashed and solid curves for a sin4Note that ˜e∗ c does not take into account the textual representations qc of topic c, but only the clustering of entities relevant to c and their relation to other entities.
Table 4: Ranking performance results for query independent(QI) features, the Query-likelihood Language Model (QLM) match feature, the Latent Semantic Entities (LSE) match feature and combinations thereof, weighted using RankSVM(§5.2), evaluated on the test sets using 10-fold cross validation, for Home & Kitchen, Clothing, Shoes & Jewelry, Pet Supplies and Sports & Outdoors product search benchmarks (§4.3). The hyperparameters of the individual query features (QLM and LSE) were optimized using the validation sets. Significance of the results (§4.4) is computed between QI + QLM + LSE and QI + QLM.
Home & Kitchen
NDCG
P@5
P@10
QI
QI + QLM
QI + LSE
QI + QLM + LSE
0.352∗∗∗
0.192∗∗
0.157∗∗∗
Clothing, Shoes & Jewelry
NDCG
P@5
P@10
QI
QI + QLM
QI + LSE
QI + QLM + LSE
0.198∗∗∗
0.094∗∗∗
0.080∗∗∗
Pet Supplies
NDCG
P@5
P@10
QI
QI + QLM
QI + LSE
QI + QLM + LSE
0.298∗∗∗
0.255∗∗∗
0.236∗∗∗
Sports & Outdoors
NDCG
P@5
P@10
QI
QI + QLM
QI + LSE
QI + QLM + LSE
0.264∗∗∗
0.192∗∗∗
0.172∗∗∗ gle latent model gives an intuition of how much performance in terms of NDCG there is to gain by improving the projection function f for that method. The approximately ideal vectors ˜e∗ c discovered for LSE outperform all baselines significantly. Interestingly, for representations created using LDA, the optimal performance goes up while the actual performance stagnates. This indicates that a higher vector space dimensionality renders better representations using LDA, however, the projection function f is unable to keep up in the sense that projected query vectors are not similar to the representations of their relevant entities. The latent models with the best representations (LSE and LSI) also have the biggest gap between f(q) and ˜e∗ c in terms of achieved NDCG.
We interpret the outcomes of our analysis as follows. The entity space E has more degrees of freedom to cluster entities more appropriately as the dimensionality of E increases. Consequently, the query projection function f is expected to learn a more complex function. In addition, as the dimensionality of E increases, so does the modeling capacity of the projection function f in the case of LSE and LSI (i.e., the transformation matrices become larger) and therefore more parameters have to be learned. We conclude that our method can more effectively represent entities in a lowerdimensional space than LSI by making better use of the vector space capacity. This is highly desirable, as the asymptotic runtime complexity of many algorithms operating on vector spaces increases at least linearly with the size of the vectors.
64∗∗∗
128∗∗∗ 256∗∗∗ 512∗∗∗ 1024∗∗∗
Entity space dimensionality
NDCG
LSE
LDA
LSI word2vec(a) Home & Kitchen
64∗∗∗
128∗∗∗ 256∗∗∗ 512∗∗∗ 1024∗∗∗
Entity space dimensionality
NDCG
LSE
LDA
LSI word2vec(b) Pet Supplies
Figure 6: Comparison of the approximately ideal retrieval vector ˜e∗ c with the projected query retrieval vector f(q) for latent entity models built using LSE, LSI, LDA and word2vec (§4.5) on Home & Kitchen and Pet Supplies product search benchmarks (§4.3) and evaluated on the test sets. The plots for Clothing, Shoes & Jewelry and Sports & Outdoors product search benchmarks are qualitatively similar to the ones shown. The figures show the absolute performance in terms of NDCG of ˜e∗ c (dashed curves) and f(q) (solid curves); significance (§4.4) for the results for the approximately ideal retrieval vectors ˜e∗ c is computed between LSE and the best-performing baseline for each vector space size and indicated along the x-axis.
CONCLUSIONS
We have introduced Latent Semantic Entities, an unsupervised latent vector space model for product search. It jointly learns a unidirectional mapping between, and latent vector representations of, words and products. We have also defined a formalism for latent vector space models where latent models are decomposed into a mapping from word sequences to the product vector space, representations of products in that space, and a similarity function. We have evaluated our model using Amazon product data, and compared it to state-of-the-art latent vector space models for product ranking (LSI, LDA and word2vec). LSE outperforms all baselines for lower-dimensional vector spaces.
In an analysis of the vector space models, we have compared the performance achieved with the ideal performance of the proposed product representations. We have shown that LSE constructs better product representations than any of the baselines. In addition, we have obtained important insights w.r.t. how much performance there is to gain by improving the individual components of latent vector space models. Future work can focus on improving the mapping from words to products by incorporating specialized features or increasing the mapping's complexity. In addition, semisupervised learning may help specialize the vector space and mapping function for particular retrieval settings.
A comparison of LSE with a smoothed lexical language model unveils that the two methods make very different errors. Some directed product search queries require lexical matching, others benefit from the semantic matching capabilities of latent models. We have evaluated LSE as a feature in a machine-learned ranking setting and found that adding LSE to language models and popularitybased features significantly improves retrieval performance.
As to future work, in this paper we focus on the unsupervised setting where we have a description and a set of reviews associated with every product. Fig. 6 shows that there is a lot of performance to gain by improving the query projection function f. In a semi-supervised setting, the difference between e∗ c and f(q) can be minimized according to pairs of queries and ideal rankings. As an additional step, query-relevance training data could be incorporated during estimation of the entity space E. Moreover, as mentioned in §6, the query projection function f is expected to learn a more complicated mapping. Hence, it may be beneficial to consider incorporating additional hierarchical depth using multiple non-linear transformations in the construction of f. More generally, the obtained product representations can be beneficial for various entityoriented prediction tasks such as entity disambiguation or related entity finding. While we have focused on product retrieval in this work, the proposed model, insights and ideas can be applied in broader settings, such as entity finding and ad-hoc retrieval.
Acknowledgments. The authors would like to thank Artem Grotov, Nikos
Voskarides, Zhaochun Ren, Tom Kenter, Manos Tsagkias, Hosein Azarbonyad and the anonymous reviewers for their valuable comments and suggestions. This research was supported by Ahold, Amsterdam Data Science, Blendle, the Bloomberg Research Grant program, the Dutch national program COMMIT, Elsevier, the European Community's Seventh Framework
Programme (FP7/2007-2013) under grant agreement nr 312827 (VOX-Pol), the ESF Research Network Program ELIAS, the Google Faculty Research
Award scheme, the Royal Dutch Academy of Sciences (KNAW) under the Elite Network Shifts project, the Microsoft Research Ph.D. program, the Netherlands eScience Center under project number 027.012.105, the Netherlands Institute for Sound and Vision, the Netherlands Organisation for Scientific Research (NWO) under project nrs 727.011.005, 612.001.116, HOR-11-10, 640.006.013, 612.066.930, CI-14-25, SH-322-15, 652.002.001, 612.001.551, 652.001.003, and Yandex. All content represents the opinion of the authors, which is not necessarily shared or endorsed by their respective employers and/or sponsors.
REFERENCES
 K. Balog. On the investigation of similarity measures for product resolution. In LHD workshop at IJCAI, 2011.
 K. Balog and M. de Rijke. Determining expert profiles (with an application to expert finding). IJCAI, 7:2657–2662, 2007.
 K. Balog and R. Neumayer. A test collection for entity search in dbpedia. In SIGIR, pages 737–740. ACM, 2013.
 K. Balog, L. Azzopardi, and M. de Rijke. Formal models for expert finding in enterprise corpora. In SIGIR, pages 43–50, 2006.
 K. Balog, P. Serdyukov, and A. P. de Vries. Overview of the TREC
2010 entity track. Techn. report, DTIC Document, 2011.
 K. Balog, Y. Fang, M. de Rijke, P. Serdyukov, and L. Si. Expertise retrieval. Found. & Tr. in Inform. Retr., 6(2-3):127–256, 2012.
 M. Baroni, G. Dinu, and G. Kruszewski. Don't count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors. In ACL, 2014.
 Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. A neural probabilistic language model. JMLR, 3:1137–1155, 2003.
 D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation.
JMLR, 3:993–1022, 2003.
 A. Bordes, J. Weston, R. Collobert, and Y. Bengio. Learning structured embeddings of knowledge bases. In AAAI, 2011.
 C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender. Learning to rank using gradient descent. In ICML, pages 89–96, 2005.
 R. Cai, H. Wang, and J. Zhang. Learning entity representation for named entity disambiguation. In Chin. Comp. Ling. and Nat. Lang.
Proc. Based on Nat. Ann. Big Data, pages 267–278. Springer, 2015.
 R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. Natural language processing (almost) from scratch. JMLR, 12(Aug):2493–2537, 2011.
 A. P. de Vries, A.-M. Vercoustre, J. A. Thom, N. Craswell, and M. Lalmas. Overview of the INEX 2007 entity ranking track. In
Focused Access to XML Documents, pages 245–251. Springer, 2007.
 S. C. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Landauer, and R. A. Harshman. Indexing by latent semantic analysis. JASIS, 41(6):
391–407, 1990.
 G. Demartini, J. Gaugaz, and W. Nejdl. A vector space model for ranking entities and its application to expert search. In Advances in Information Retrieval, pages 189–201. Springer, 2009.
 L. Deng, X. He, and J. Gao. Deep stacking networks for information retrieval. In ICASSP, pages 3153–3157, 2013.
 H. Duan and C. Zhai. Mining coordinated intent representation for entity search and recommendation. In CIKM, pages 333–342. ACM, H. Duan, C. Zhai, J. Cheng, and A. Gattani. A probabilistic mixture model for mining and analyzing product search log. In CIKM, pages
2179–2188. ACM, 2013.
 H. Duan, C. Zhai, J. Cheng, and A. Gattani. Supporting keyword search in product database: A probabilistic approach. Proceedings of the VLDB Endowment, 6(14):1786–1797, Sept. 2013.
 Y. Fang, L. Si, and A. P. Mathur. Discriminative models of integrating document evidence and document-candidate associations for expert search. In SIGIR, pages 683–690. ACM, 2010.
 I. Forrester Research. US online retail forecast, 2010 to 2015, February 2012.
 M. Gäde, M. Hall, H. Huurdeman, J. Kamps, M. Koolen, M. Skov, E. Toms, and D. Walsh. Overview of the SBS 2015 interactive track.
In CLEF 2015. Springer, 2015.
 X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks. In AISTATS, pages 249–256, D. Graus, M. Tsagkias, W. Weerkamp, E. Meij, and M. de Rijke.
Dynamic collective entity representations for entity ranking. In
WSDM. ACM, 2016.
 M. Gutmann and A. Hyvärinen. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In AISTATS, pages 297–304, 2010.
 G. E. Hinton. Learning distributed representations of concepts. In
8th Ann. Conf. of the Cogn. Sci. Soc., page 12, Amherst, MA, 1986.
 T. Hofmann. Probabilistic latent semantic indexing. In SIGIR, pages
50–57. ACM, 1999.
 P.-s. Huang, N. M. A. Urbana, X. He, J. Gao, L. Deng, A. Acero, and L. Heck. Learning deep structured semantic models for web search using clickthrough data. In CIKM, pages 2333–2338, 2013.
 B. J. Jansen and P. R. Molina. The effectiveness of web search engines for retrieving relevant ecommerce links. Information
Processing & Management, 42(4):1075–1098, 2006.
 T. Joachims. Optimizing search engines using clickthrough data. In
KDD, pages 133–142. ACM, 2002.
 D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014.
 R. Kiros, R. Salakhutdinov, and R. Zemel. Multimodal neural language models. In ICML, pages 595–603, 2014.
 H. Li and J. Xu. Semantic matching in search. Found. & Tr. in Information Retrieval, 7(5):343–469, June 2014.
 S. Liang and M. de Rijke. Formal language models for finding groups of experts. Information Processing & Management, 2016.
 T.-Y. Liu. Learning to Rank for Information Retrieval. Springer, X. Liu, W. B. Croft, and M. Koll. Finding experts in community-based question-answering services. In CIKM, pages
315–316. ACM, 2005.
 J. McAuley, R. Pandey, and J. Leskovec. Inferring networks of substitutable and complementary products. In KDD, pages 785–794.
ACM, 2015.
 J. McAuley, C. Targett, Q. Shi, and A. van den Hengel. Image-based recommendations on styles and substitutes. In SIGIR, pages 43–52.
ACM, 2015.
 S. McPartlin, L. F. Dugal, M. Jenson, and I. W. Kahn. Understanding how US online shoppers are reshaping the retail experience.
PricewaterhouseCoopers, 2012.
 T. Mikolov, K. Chen, G. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In
NIPS, pages 3111–3119, 2013.
 T. Mikolov, G. Corrado, K. Chen, and J. Dean. Efficient estimation of word representations in vector space. arXiv 1301.3781, 2013.
 A. Mnih and G. Hinton. Three new graphical models for statistical language modelling. In ICML, pages 641–648, 2007.
 A. Mnih and G. Hinton. A scalable hierarchical distributed language model. In NIPS, pages 1081–1088, 2008.
 A. Mnih and K. Kavukcuoglu. Learning word embeddings efficiently with noise-contrastive estimation. In NIPS, pages 2265–2273, 2013.
 A. Mnih and Y. W. Teh. A fast and simple algorithm for training neural probabilistic language models. In ICML, pages 1751–1758, A. Y. Ng and M. I. Jordan. On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes. In
NIPS, pages 841–848, 2002.
 P. Nurmi, E. Lagerspetz, W. Buntine, P. Floréen, and J. Kukkonen.
Product retrieval for grocery stores. In SIGIR, pages 781–782. ACM, J. Pennington, R. Socher, and C. D. Manning. GloVe: Global Vectors for Word Representation. In EMNLP, pages 1532–1543, 2014.
 L. Quoc and T. Mikolov. Distributed representations of sentences and documents. In ICML, pages 1188–1196, 2014.
 J. Rowley. Product search in e-shopping: a review and research propositions. Journal of Consumer Marketing, 17(1):20–35, 2000.
 R. Salakhutdinov and G. Hinton. Semantic hashing. Int. J.
Approximate Reasoning, 50(7):969–978, 2009.
 D. Sculley. Large scale learning to rank. In NIPS Workshop on
Advances in Ranking, 2009.
 Y. Shen, X. He, J. Gao, L. Deng, and G. Mesnil. A latent semantic model with convolutional-pooling structure for information retrieval.
In CIKM, pages 101–110, 2014.
 M. D. Smucker, J. Allan, and B. Carterette. A comparison of statistical significance tests for information retrieval evaluation. In
CIKM, pages 623–632. ACM, 2007.
 J. Turian, L. Ratinov, and Y. Bengio. Word representations: a simple and general method for semi-supervised learning. In ACL, pages
384–394. Association for Computational Linguistics, 2010.
 C. Van Gysel, M. de Rijke, and M. Worring. Unsupervised, efficient and semantic expertise retrieval. In WWW, 2016.
 V. Vapnik. Statistical Learning Theory, volume 1. Wiley New York, R. Weber, H.-J. Schek, and S. Blott. A quantitative analysis and performance study for similarity-search methods in high-dimensional spaces. In VLDB, volume 98, pages 194–205, 1998.
 C. Zhai and J. Lafferty. A study of smoothing methods for language models applied to information retrieval. TOIS, 22(2):179–214, 2004.
 Y. Zhao, L. Zhiyuan, and M. Sun. Representation learning for measuring entity relatedness with rich information. In IJCAI, 2015.
APPENDIX
Denote pk = ˜P(xk | wk,1,..., wk,n). The derivative of (5) w.r.t. bias term b equals
∂L
∂b (Wv, We, W, b) = − 1 m
� m
� k=1
1 pk
∂pk
∂b
� and w.r.t. an arbitrary matrix parameter θ (Wv, We or W):
∂L
∂θ (Wv, We, W, b) = − 1 m
� m
� k=1
1 pk
∂pk
∂θ
�
+ λ m
� i,j θi,j.
Ignoring the subscripts for batch instances and word positions for ease of notation, for a single instance we denote x+ as the target entity and X− as the sample of z contrastive negative examples. We have p
=
P(S | e+, f(wj,1,..., wj,n)) ·
� x−∈X−(1 − P(S | e−, f(wj,1,..., wj,n))) where application of the product rule in the computation of ∂p
∂θ is omitted due to space constraints.
For θ (W, b, We or Wv) we observe
∂P(S | e, f(wj,1,..., wj,n))
∂θ
= P(S | e, f(wj,1,..., wj,n))·(1 − P(S | e, f(wj,1,..., wj,n))) · ∂ e · f(w1,..., wn)
∂θ
For a single entity representation e (a row of matrix We), ∂ e · f(w1,..., wn)
∂e
= f(w1,..., wn) where we observe that the update to an entity representation is the projected representation of the input n-gram multiplied by a scalar.
The symbolic derivative of the dot product between the entity representation and the projected n-gram w.r.t. bias term b, linear map W and word representations Wv, respectively, are:
∂ e · f(w1,..., wn)
∂b
= e ⊙ sech2
�
�W · (Wv · 1
|s|
� wi∈s δi) + b
�
�
∂ e · f(w1,..., wn)
∂W
=
�
�e ⊙ sech2
�
�W · (Wv · 1
|s|
� wi∈s δi) + b
�
�
�
� ·
�
�Wv · 1
|s|
� wi∈s δi
�
�
⊺
∂ e · f(w1,..., wn)
∂Wv
= W ⊺ ·
�
�e ⊙ sech2
�
�W · (Wv · 1
|s|
� wi∈s δi) + b
�
�
�
� ·
�
� 1
|s|
� wi∈s δi
�
�
⊺Deep Visual-Semantic Alignments for Generating Image Descriptions
Andrej Karpathy
Li Fei-Fei
Department of Computer Science, Stanford University
{karpathy,feifeili}@cs.stanford.edu
Abstract
We present a model that generates natural language descriptions of images and their regions. Our approach leverages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between language and visual data. Our alignment model is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks over sentences, and a structured objective that aligns the two modalities through a multimodal embedding. We then describe a Multimodal Recurrent Neural Network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions. We demonstrate that our alignment model produces state of the art results in retrieval experiments on Flickr8K, Flickr30K and MSCOCO datasets. We then show that the generated descriptions significantly outperform retrieval baselines on both full images and on a new dataset of region-level annotations.
1. Introduction
A quick glance at an image is sufficient for a human to point out and describe an immense amount of details about the visual scene. However, this remarkable ability has proven to be an elusive task for our visual recognition models. The majority of previous work in visual recognition has focused on labeling images with a fixed set of visual categories and great progress has been achieved in these endeavors. However, while closed vocabularies of visual concepts constitute a convenient modeling assumption, they are vastly restrictive when compared to the enormous amount of rich descriptions that a human can compose.
Some pioneering approaches that address the challenge of generating image descriptions have been developed. However, these models often rely on hard-coded visual concepts and sentence templates, which imposes limits on their variety. Moreover, the focus of these works has been on reducing complex visual scenes into a single sentence, which we consider to be an unnecessary restriction.
In this work, we strive to take a step towards the goal of Figure 1. Motivation/Concept Figure: Our model treats language as a rich label space and generates descriptions of image regions. generating dense descriptions of images (Figure 1). The primary challenge towards this goal is in the design of a model that is rich enough to simultaneously reason about contents of images and their representation in the domain of natural language. Additionally, the model should be free of assumptions about specific hard-coded templates, rules or categories and instead rely on learning from the training data. The second, practical challenge is that datasets of image captions are available in large quantities on the internet, but these descriptions multiplex mentions of several entities whose locations in the images are unknown.
Our core insight is that we can leverage these large imagesentence datasets by treating the sentences as weak labels, in which contiguous segments of words correspond to some particular, but unknown location in the image.
Our approach is to infer these alignments and use them to learn a generative model of descriptions. Concretely, our contributions are twofold:
• We develop a deep neural network model that infers the latent alignment between segments of sentences and the region of the image that they describe.
Our model associates the two modalities through a common, multimodal embedding space and a structured objective. We validate the effectiveness of this approach on image-sentence retrieval experiments in which we surpass the state-of-the-art.
• We introduce a multimodal Recurrent Neural Network architecture that takes an input image and generates its description in text. Our experiments show that the generated sentences significantly outperform retrievalbased baselines, and produce sensible qualitative predictions. We then train the model on the inferred correspondences and evaluate its performance on a new dataset of region-level annotations.
We make code, data and annotations publicly available. 1
2. Related Work
Dense image annotations. Our work shares the high-level goal of densely annotating the contents of images with many works before us. Barnard et al. and Socher et al. studied the multimodal correspondence between words and images to annotate segments of images. Several works studied the problem of holistic scene understanding in which the scene type, objects and their spatial support in the image is inferred. However, the focus of these works is on correctly labeling scenes, objects and regions with a fixed set of categories, while our focus is on richer and higher-level descriptions of regions.
Generating descriptions. The task of describing images with sentences has also been explored. A number of approaches pose the task as a retrieval problem, where the most compatible annotation in the training set is transferred to a test image, or where training annotations are broken up and stitched together.
Several approaches generate image captions based on fixed templates that are filled based on the content of the image
 or generative grammars, but this approach limits the variety of possible outputs.
Most closely related to us, Kiros et al. developed a logbilinear model that can generate full sentence descriptions for images, but their model uses a fixed window context while our Recurrent Neural Network (RNN) model conditions the probability distribution over the next word in a sentence on all previously generated words. Multiple closely related preprints appeared on Arxiv during the submission of this work, some of which also use RNNs to generate image descriptions. Our RNN is simpler than most of these approaches but also suffers in performance. We quantify this comparison in our experiments.
Grounding natural language in images. A number of approaches have been developed for grounding text in the vi1cs.stanford.edu/people/karpathy/deepimagesent sual domain. Our approach is inspired by
Frome et al. who associate words and images through a semantic embedding. More closely related is the work of Karpathy et al., who decompose images and sentences into fragments and infer their inter-modal alignment using a ranking objective. In contrast to their model which is based on grounding dependency tree relations, our model aligns contiguous segments of sentences which are more meaningful, interpretable, and not fixed in length.
Neural networks in visual and language domains. Multiple approaches have been developed for representing images and words in higher-level representations. On the image side, Convolutional Neural Networks (CNNs) have recently emerged as a powerful class of models for image classification and object detection. On the sentence side, our work takes advantage of pretrained word vectors to obtain low-dimensional representations of words. Finally, Recurrent Neural Networks have been previously used in language modeling, but we additionally condition these models on images.
3. Our Model
Overview. The ultimate goal of our model is to generate descriptions of image regions. During training, the input to our model is a set of images and their corresponding sentence descriptions (Figure 2). We first present a model that aligns sentence snippets to the visual regions that they describe through a multimodal embedding. We then treat these correspondences as training data for a second, multimodal Recurrent Neural Network model that learns to generate the snippets.
3.1. Learning to align visual and language data
Our alignment model assumes an input dataset of images and their sentence descriptions. Our key insight is that sentences written by people make frequent references to some particular, but unknown location in the image. For example, in Figure 2, the words "Tabby cat is leaning" refer to the cat, the words "wooden table" refer to the table, etc.
We would like to infer these latent correspondences, with the eventual goal of later learning to generate these snippets from image regions. We build on the approach of Karpathy et al., who learn to ground dependency tree relations to image regions with a ranking objective. Our contribution is in the use of bidirectional recurrent neural network to compute word representations in the sentence, dispensing of the need to compute dependency trees and allowing unbounded interactions of words and their context in the sentence. We also substantially simplify their objective and show that both modifications improve ranking performance.
We first describe neural networks that map words and image regions into a common, multimodal embedding. Then we introduce our novel objective, which learns the embedding
Figure 2. Overview of our approach. A dataset of images and their sentence descriptions is the input to our model (left). Our model first infers the correspondences (middle, Section 3.1) and then learns to generate novel descriptions (right, Section 3.2). representations so that semantically similar concepts across the two modalities occupy nearby regions of the space.
Representing images
Following prior work, we observe that sentence descriptions make frequent references to objects and their attributes. Thus, we follow the method of Girshick et al. to detect objects in every image with a Region Convolutional Neural Network (RCNN). The CNN is pre-trained on
ImageNet and finetuned on the 200 classes of the ImageNet Detection Challenge. Following Karpathy et al., we use the top 19 detected locations in addition to the whole image and compute the representations based on the pixels Ib inside each bounding box as follows: v = Wm[CNNθc(Ib)] + bm, (1) where CNN(Ib) transforms the pixels inside bounding box
Ib into 4096-dimensional activations of the fully connected layer immediately before the classifier. The CNN parameters θc contain approximately 60 million parameters. The matrix Wm has dimensions h × 4096, where h is the size of the multimodal embedding space (h ranges from 10001600 in our experiments). Every image is thus represented as a set of h-dimensional vectors {vi | i = 1... 20}.
Representing sentences
To establish the inter-modal relationships, we would like to represent the words in the sentence in the same hdimensional embedding space that the image regions occupy. The simplest approach might be to project every individual word directly into this embedding. However, this approach does not consider any ordering and word context information in the sentence. An extension to this idea is to use word bigrams, or dependency tree relations as previously proposed. However, this still imposes an arbitrary maximum size of the context window and requires the use of Dependency Tree Parsers that might be trained on unrelated text corpora.
To address these concerns, we propose to use a Bidirectional Recurrent Neural Network (BRNN) to compute the word representations. The BRNN takes a sequence of N words (encoded in a 1-of-k representation) and transforms each one into an h-dimensional vector. However, the representation of each word is enriched by a variably-sized context around that word. Using the index t = 1... N to denote the position of a word in a sentence, the precise form of the BRNN is as follows: xt = WwIt(2) et = f(Wext + be)(3) hf t = f(et + Wfhf t−1 + bf)(4) hb t = f(et + Wbhb t+1 + bb)(5) st = f(Wd(hf t + hb t) + bd).
Here, It is an indicator column vector that has a single one at the index of the t-th word in a word vocabulary. The weights Ww specify a word embedding matrix that we initialize with 300-dimensional word2vec weights and keep fixed due to overfitting concerns. However, in practice we find little change in final performance when these vectors are trained, even from random initialization. Note that the BRNN consists of two independent streams of processing, one moving left to right (hf t ) and the other right to left (hb t) (see Figure 3 for diagram). The final h-dimensional representation st for the t-th word is a function of both the word at that location and also its surrounding context in the sentence. Technically, every st is a function of all words in the entire sentence, but our empirical finding is that the final word representations (st) align most strongly to the visual concept of the word at that location (It).
We learn the parameters We, Wf, Wb, Wd and the respective biases be, bf, bb, bd. A typical size of the hidden representation in our experiments ranges between 300-600 dimensions. We set the activation function f to the rectified linear unit (ReLU), which computes f : x �→ max(0, x).
Alignment objective
We have described the transformations that map every image and sentence into a set of vectors in a common hdimensional space. Since the supervision is at the level of entire images and sentences, our strategy is to formulate an image-sentence score as a function of the individual regionword scores. Intuitively, a sentence-image pair should have a high matching score if its words have a confident support in the image. The model of Karpathy et a. interprets the dot product vT i st between the i-th region and t-th word as a measure of similarity and use it to define the score between image k and sentence l as:
Skl =
� t∈gl
� i∈gk max(0, vT i st).
Here, gk is the set of image fragments in image k and gl is the set of sentence fragments in sentence l. The indices k, l range over the images and sentences in the training set.
Together with their additional Multiple Instance Learning objective, this score carries the interpretation that a sentence fragment aligns to a subset of the image regions whenever the dot product is positive. We found that the following reformulation simplifies the model and alleviates the need for additional objectives and their hyperparameters:
Skl =
� t∈gl maxi∈gkvT i st.
Here, every word st aligns to the single best image region.
As we show in the experiments, this simplified model also leads to improvements in the final ranking performance.
Assuming that k = l denotes a corresponding image and sentence pair, the final max-margin, structured loss remains:
C(θ) =
� k
� � l max(0, Skl − Skk + 1)
�
��
� rank images
� l max(0, Slk − Skk + 1)
�
��
� rank sentences
�
This objective encourages aligned image-sentences pairs to have a higher score than misaligned pairs, by a margin.
Decoding text segment alignments to images
Consider an image from the training set and its corresponding sentence. We can interpret the quantity vT i st as the unnormalized log probability of the t-th word describing any of the bounding boxes in the image. However, since we are ultimately interested in generating snippets of text instead of single words, we would like to align extended, contiguous sequences of words to a single bounding box. Note that the na¨ıve solution that assigns each word independently to the highest-scoring region is insufficient because it leads to words getting scattered inconsistently to different regions.
To address this issue, we treat the true alignments as latent variables in a Markov Random Field (MRF) where the binary interactions between neighboring words encourage an
Figure 3. Diagram for evaluating the image-sentence score Skl.
Object regions are embedded with a CNN (left). Words (enriched by their context) are embedded in the same multimodal space with a BRNN (right). Pairwise similarities are computed with inner products (magnitudes shown in grayscale) and finally reduced to image-sentence score with Equation 8. alignment to the same region. Concretely, given a sentence with N words and an image with M bounding boxes, we introduce the latent alignment variables aj ∈ {1... M} for j = 1... N and formulate an MRF in a chain structure along the sentence as follows:
E(a) =
� j=1...N ψU j (aj) +
� j=1...N−1 ψB j (aj, aj+1)(10) ψU j (aj = t) = vT i st(11) ψB j (aj, aj+1) = β1[aj = aj+1].
Here, β is a hyperparameter that controls the affinity towards longer word phrases. This parameter allows us to interpolate between single-word alignments (β = 0) and aligning the entire sentence to a single, maximally scoring region when β is large. We minimize the energy to find the best alignments a using dynamic programming. The output of this process is a set of image regions annotated with segments of text. We now describe an approach for generating novel phrases based on these correspondences.
3.2. Multimodal Recurrent Neural Network for generating descriptions
In this section we assume an input set of images and their textual descriptions. These could be full images and their sentence descriptions, or regions and text snippets, as inferred in the previous section. The key challenge is in the design of a model that can predict a variable-sized sequence of outputs given an image. In previously developed language models based on Recurrent Neural Networks (RNNs), this is achieved by defining a probability distribution of the next word in a sequence given the current word and context from previous time steps. We explore a simple but effective extension that additionally conditions the generative process on the content of an input image. More formally, during training our Multimodal RNN takes the image pixels I and a sequence of input vectors (x1,..., xT ). It then computes a sequence of hidden states (h1,..., ht) and a sequence of outputs (y1,..., yt) by iterating the following recurrence relation for t = 1 to T: bv = Whi[CNNθc(I)](13) ht = f(Whxxt + Whhht−1 + bh + 1(t = 1) ⊙ bv) (14) yt = softmax(Wohht + bo).
In the equations above, Whi, Whx, Whh, Woh, xi and bh, bo are learnable parameters, and CNNθc(I) is the last layer of a CNN. The output vector yt holds the (unnormalized) log probabilities of words in the dictionary and one additional dimension for a special END token. Note that we provide the image context vector bv to the RNN only at the first iteration, which we found to work better than at each time step. In practice we also found that it can help to also pass both bv, (Whxxt) through the activation function. A typical size of the hidden layer of the RNN is 512 neurons.
RNN training. The RNN is trained to combine a word (xt), the previous context (ht−1) to predict the next word (yt).
We condition the RNN's predictions on the image information (bv) via bias interactions on the first step. The training proceeds as follows (refer to Figure 4): We set h0 = ⃗0, x1 to a special START vector, and the desired label y1 as the first word in the sequence. Analogously, we set x2 to the word vector of the first word and expect the network to predict the second word, etc. Finally, on the last step when xT represents the last word, the target label is set to a special END token. The cost function is to maximize the log probability assigned to the target labels (i.e. Softmax classifier).
RNN at test time. To predict a sentence, we compute the image representation bv, set h0 = 0, x1 to the START vector and compute the distribution over the first word y1. We sample a word from the distribution (or pick the argmax), set its embedding vector as x2, and repeat this process until the END token is generated. In practice we found that beam search (e.g. beam size 7) can improve results.
3.3. Optimization
We use SGD with mini-batches of 100 image-sentence pairs and momentum of 0.9 to optimize the alignment model. We cross-validate the learning rate and the weight decay. We also use dropout regularization in all layers except in the recurrent layers and clip gradients elementwise at 5(important). The generative RNN is more difficult to optimize, party due to the word frequency disparity between rare words and common words (e.g. "a" or the END token).
We achieved the best results using RMSprop, which is an adaptive step size method that scales the update of each weight by a running average of its gradient norm.
Figure 4. Diagram of our multimodal Recurrent Neural Network generative model. The RNN takes a word, the context from previous time steps and defines a distribution over the next word in the sentence. The RNN is conditioned on the image information at the first time step. START and END are special tokens.
4. Experiments
Datasets. We use the Flickr8K, Flickr30K and MSCOCO datasets in our experiments. These datasets contain 8,000, 31,000 and 123,000 images respectively and each is annotated with 5 sentences using Amazon
Mechanical Turk.
For Flickr8K and Flickr30K, we use
1,000 images for validation, 1,000 for testing and the rest for training (consistent with ). For MSCOCO we use 5,000 images for both validation and testing.
Data Preprocessing. We convert all sentences to lowercase, discard non-alphanumeric characters. We filter words to those that occur at least 5 times in the training set, which results in 2538, 7414, and 8791 words for Flickr8k, Flickr30K, and MSCOCO datasets respectively.
4.1. Image-Sentence Alignment Evaluation
We first investigate the quality of the inferred text and image alignments with ranking experiments. We consider a withheld set of images and sentences and retrieve items in one modality given a query from the other by sorting based on the image-sentence score Skl (Section 3.1.3). We report the median rank of the closest ground truth result in the list and Recall@K, which measures the fraction of times a correct item was found among the top K results. The result of these experiments can be found in Table 1, and example retrievals in Figure 5. We now highlight some of the takeaways.
Our full model outperforms previous work. First, our full model ("Our model: BRNN") outperforms Socher et al. who trained with a similar loss but used a single image representation and a Recursive Neural Network over the sentence. A similar loss was adopted by Kiros et al., who use an LSTM to encode sentences. We list their performance with a CNN that is equivalent in power(AlexNet ) to the one used in this work, though similar to they outperform our model with a more powerful
CNN (VGGNet, GoogLeNet ). "DeFrag" are the results reported by Karpathy et al.. Since we use different word vectors, dropout for regularization and different cross-validation ranges and larger embedding sizes, we reimplemented their loss for a fair comparison ("Our impleImage Annotation
Image Search
Model
R@1
R@5
R@10
Med r
R@1
R@5
R@10
Med r
Flickr30K
SDT-RNN (Socher et al. )
Kiros et al. 
Mao et al. 
Donahue et al. DeFrag (Karpathy et al. )
Our implementation of DeFrag 
Our model: DepTree edges
Our model: BRNN
Vinyals et al. (more powerful CNN)MSCOCO
Our model: 1K test images
Our model: 5K test images
Table 1. Image-Sentence ranking experiment results. R@K is Recall@K (high is good). Med r is the median rank (low is good). In the results for our models, we take the top 5 validation set models, evaluate each independently on the test set and then report the average performance. The standard deviations on the recall values range from approximately 0.5 to 1.0.
Figure 5. Example alignments predicted by our model. For every test image above, we retrieve the most compatible test sentence and visualize the highest-scoring region for each word (before MRF smoothing described in Section 3.1.4) and the associated scores (vT i st).
We hide the alignments of low-scoring words to reduce clutter. We assign each region an arbitrary color. mentation of DeFrag"). Compared to other work that uses
AlexNets, our full model shows consistent improvement.
Our simpler cost function improves performance. We strive to better understand the source of our performance.
First, we removed the BRNN and used dependency tree relations exactly as described in Karpathy et al. ("Our model: DepTree edges"). The only difference between this model and "Our reimplementation of DeFrag" is the new, simpler cost function introduced in Section 3.1.3. We see that our formulation shows consistent improvements.
BRNN outperforms dependency tree relations. Furthermore, when we replace the dependency tree relations with the BRNN we observe additional performance improvements. Since the dependency relations were shown to work better than single words and bigrams, this suggests that the BRNN is taking advantage of contexts longer than two words. Furthermore, our method does not rely on extracting a Dependency Tree and instead uses the raw words directly.
MSCOCO results for future comparisons. We are not aware of other published ranking results on MSCOCO.
Therefore, we report results on a subset of 1,000 images and the full set of 5,000 test images for future comparisons.
Note that the 5000 images numbers are lower since Recall@K is a function of test set size.
Qualitative. As can be seen from example groundings in Figure 5, the model discovers interpretable visual-semantic correspondences, even for small or relatively rare objects such as an "accordion". These would be likely missed by models that only reason about full images.
Learned region and word vector magnitudes. An appealing feature of our model is that it learns to modulate the magnitude of the region and word embeddings. Due to their inner product interaction, we observe that representations of visually discriminative words such as "kayaking, pumpkins" have embedding vectors with higher magnitudes, which in turn translates to a higher influence on the image-sentence score. Conversely, stop words such as
"now, simply, actually, but" are mapped near the origin, which reduces their influence. See more analysis in supplementary material.
Flickr8K
Flickr30K
MSCOCO 2014
Model
B-1
B-2
B-3
B-4
B-1
B-2
B-3
B-4
B-1
B-2
B-3
B-4
METEOR
CIDEr
Nearest Neighbor
—
—
—
—
—
—
—
—
Mao et al. 
—
—
—
—
—
—
—
—
Google NIC 
—
—
—
LRCN 
—
—
—
—
—
—
—
MS Research 
—
—
—
—
—
—
—
—
—
—
—
—
Chen and Zitnick 
—
—
—
—
—
—
—
—
—
—
Our model
Table 2. Evaluation of full image predictions on 1,000 test images. B-n is BLEU score that uses up to n-grams. High is good in all columns.
For future comparisons, our METEOR/CIDEr Flickr8K scores are 16.7/31.8 and the Flickr30K scores are 15.3/24.7.
Figure 6. Example sentences generated by the multimodal RNN for test images. We provide many more examples on our project page.
4.2. Generated Descriptions: Fulframe evaluation
We now evaluate the ability of our RNN model to describe images and regions. We first trained our Multimodal RNN to generate sentences on full images with the goal of verifying that the model is rich enough to support the mapping from image data to sequences of words. For these full image experiments we use the more powerful VGGNet image features. We report the BLEU, METEOR and CIDEr scores computed with the coco-caption code 2. Each method evaluates a candidate sentence by measuring how well it matches a set of five reference sentences written by humans.
Qualitative. The model generates sensible descriptions of images (see Figure 6), although we consider the last two images failure cases. The first prediction "man in black shirt is playing a guitar" does not appear in the training set.
However, there are 20 occurrences of "man in black shirt" and 60 occurrences of "is paying guitar", which the model may have composed to describe the first image. In general, we find that a relatively large portion of generated sentences(60% with beam size 7) can be found in the training data.
This fraction decreases with lower beam size; For instance, with beam size 1 this falls to 25%, but the performance also deteriorates (e.g. from 0.66 to 0.61 CIDEr).
Multimodal RNN outperforms retrieval baseline. Our first comparison is to a nearest neighbor retrieval baseline.
2https://github.com/tylin/coco-caption
Here, we annotate each test image with a sentence of the most similar training set image as determined by L2 norm over VGGNet fc7 features. Table 2 shows that the Multimodal RNN confidently outperforms this retrieval method.
Hence, even with 113,000 train set images in MSCOCO the retrieval approach is inadequate. Additionally, the RNN takes only a fraction of a second to evaluate per image.
Comparison to other work. Several related models have been proposed in Arxiv preprints since the original submission of this work. We also include these in Table 2 for comparison. Most similar to our model is Vinyals et al..
Unlike this work where the image information is communicated through a bias term on the first step, they incorporate it as a first word, they use a more powerful but more complex sequence learner (LSTM ), a different CNN(GoogLeNet ), and report results of a model ensemble.
Donahue et al. use a 2-layer factored LSTM (similar in structure to the RNN in Mao et al. ). Both models appear to work worse than ours, but this is likely in large part due to their use of the less powerful AlexNet features. Compared to these approaches, our model prioritizes simplicity and speed at a slight cost in performance.
4.3. Generated Descriptions: Region evaluation
We now train the Multimodal RNN on the correspondences between image regions and snippets of text, as inferred by the alignment model. To support the evaluation, we used
Amazon Mechanical Turk (AMT) to collect a new dataset
Figure 7. Example region predictions. We use our region-level multimodal RNN to generate text (shown on the right of each image) for some of the bounding boxes in each image. The lines are grounded to centers of bounding boxes and the colors are chosen arbitrarily. of region-level annotations that we only use at test time. The labeling interface displayed a single image and asked annotators (we used nine per image) to draw five bounding boxes and annotate each with text. In total, we collected 9,000 text snippets for 200 images in our MSCOCO test split (i.e. 45 snippets per image). The snippets have an average length of 2.3 words. Example annotations include "sports car", "elderly couple sitting", "construction site", "three dogs on leashes", "chocolate cake". We noticed that asking annotators for grounded text snippets induces language statistics different from those in full image captions. Our region annotations are more comprehensive and feature elements of scenes that would rarely be considered salient enough to be included in a single sentence sentence about the full image, such as "heating vent", "belt buckle", and "chimney".
Qualitative. We show example region model predictions in Figure 7. To reiterate the difficulty of the task, consider for example the phrase "table with wine glasses" that is generated on the image on the right in Figure 7. This phrase only occurs in the training set 30 times. Each time it may have a different appearance and each time it may occupy a few (or none) of our object bounding boxes. To generate this string for the region, the model had to first correctly learn to ground the string and then also learn to generate it.
Region model outperforms full frame model and ranking baseline. Similar to the full image description task, we evaluate this data as a prediction task from a 2D array of pixels (one image region) to a sequence of words and record the BLEU score. The ranking baseline retrieves training sentence substrings most compatible with each region as judged by the BRNN model. Table 3 shows that the region
RNN model produces descriptions most consistent with our collected data. Note that the fullframe model was trained only on full images, so feeding it smaller image regions deteriorates its performance.
However, its sentences are also longer than the region model sentences, which likely negatively impacts the BLEU score. The sentence length is non-trivial to control for with an RNN, but we note that the region model also outperforms the fullframe model on all other metrics: CIDEr 61.6/20.3, METEOR 15.8/13.3, ROUGE 35.1/21.0 for region/fullframe respectively.
Model
B-1
B-2
B-3
B-4
Human agreement
Nearest Neighbor
RNN: Fullframe model
RNN: Region level model
Table 3. BLEU score evaluation of image region annotations.
4.4. Limitations
Although our results are encouraging, the Multimodal RNN model is subject to multiple limitations. First, the model can only generate a description of one input array of pixels at a fixed resolution. A more sensible approach might be to use multiple saccades around the image to identify all entities, their mutual interactions and wider context before generating a description. Additionally, the RNN receives the image information only through additive bias interactions, which are known to be less expressive than more complicated multiplicative interactions. Lastly, our approach consists of two separate models. Going directly from an imagesentence dataset to region-level annotations as part of a single model trained end-to-end remains an open problem.
5. Conclusions
We introduced a model that generates natural language descriptions of image regions based on weak labels in form of a dataset of images and sentences, and with very few hardcoded assumptions. Our approach features a novel ranking model that aligned parts of visual and language modalities through a common, multimodal embedding. We showed that this model provides state of the art performance on image-sentence ranking experiments. Second, we described a Multimodal Recurrent Neural Network architecture that generates descriptions of visual data. We evaluated its performance on both fullframe and region-level experiments and showed that in both cases the Multimodal RNN outperforms retrieval baselines.
Acknowledgements.
We thank Justin Johnson and Jon Krause for helpful comments and discussions. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the GPUs used for this research. This research is partially supported by an ONR MURI grant, and NSF ISS-1115313.
References
 A. Barbu, A. Bridge, Z. Burchill, D. Coroian, S. Dickinson, S. Fidler, A. Michaux, S. Mussman, S. Narayanaswamy, D. Salvi, et al.
Video in sentences out. arXiv preprint arXiv:1204.2742, 2012. 2
 K. Barnard, P. Duygulu, D. Forsyth, N. De Freitas, D. M.
Blei, and M. I. Jordan. Matching words and pictures. JMLR, Y. Bengio, H. Schwenk, J.-S. Sen´ecal, F. Morin, and J.-L.
Gauvain. Neural probabilistic language models. In Innovations in Machine Learning. Springer, 2006. 2
 X. Chen, H. Fang, T.-Y. Lin, R. Vedantam, S. Gupta, P. Dollar, and C. L. Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, X. Chen and C. L. Zitnick.
Learning a recurrent visual representation for image caption generation.
CoRR, abs/1411.5654, 2014. 2, 7
 J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. FeiFei. Imagenet: A large-scale hierarchical image database. In
CVPR, 2009. 3
 M. Denkowski and A. Lavie. Meteor universal: Language specific translation evaluation for any target language.
In
Proceedings of the EACL 2014 Workshop on Statistical Machine Translation, 2014. 7
 J. Donahue, L. A. Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan, K. Saenko, and T. Darrell. Long-term recurrent convolutional networks for visual recognition and description. arXiv preprint arXiv:1411.4389, 2014. 2, 6, 7
 D. Elliott and F. Keller. Image description using visual dependency representations.
In EMNLP, pages 1292–1302, J. L. Elman. Finding structure in time. Cognitive science, 14(2):179–211, 1990. 4
 M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The pascal visual object classes (voc) challenge. International Journal of Computer Vision, 88(2):303–
338, June 2010. 1
 H. Fang, S. Gupta, F. Iandola, R. Srivastava, L. Deng, P. Doll´ar, J. Gao, X. He, M. Mitchell, J. Platt, et al.
From captions to visual concepts and back. arXiv preprint arXiv:1411.4952, 2014. 2, 7
 A. Farhadi, M. Hejrati, M. A. Sadeghi, P. Young, C. Rashtchian, J. Hockenmaier, and D. Forsyth. Every picture tells a story: Generating sentences from images.
In
ECCV. 2010. 1, 2
 L. Fei-Fei, A. Iyer, C. Koch, and P. Perona. What do we perceive in a glance of a real-world scene? Journal of vision, S. Fidler, A. Sharma, and R. Urtasun. A sentence is worth a thousand pixels. In CVPR, 2013. 2
 A. Frome, G. S. Corrado, J. Shlens, S. Bengio, J. Dean, T. Mikolov, et al. Devise: A deep visual-semantic embedding model. In NIPS, 2013. 2
 R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014. 3
 S. Gould, R. Fulton, and D. Koller. Decomposing a scene into geometric and semantically consistent regions. In Computer Vision, 2009 IEEE 12th International Conference on, pages 1–8. IEEE, 2009. 2
 A. Gupta and P. Mannem. From image annotation to image description. In Neural information processing. Springer, S. Hochreiter and J. Schmidhuber. Long short-term memory.
Neural computation, 9(8):1735–1780, 1997. 5, 7, 8
 M. Hodosh, P. Young, and J. Hockenmaier. Framing image description as a ranking task: data, models and evaluation metrics. Journal of Artificial Intelligence Research, 2013. 1, R. JeffreyPennington and C. Manning. Glove: Global vectors for word representation. 2014. 2
 Y. Jia, M. Salzmann, and T. Darrell. Learning cross-modality similarity for multinomial data. In ICCV, 2011. 2
 A. Karpathy, A. Joulin, and L. Fei-Fei. Deep fragment embeddings for bidirectional image sentence mapping. arXiv preprint arXiv:1406.5679, 2014. 2, 3, 4, 5, 6
 R. Kiros, R. Salakhutdinov, and R. S. Zemel.
Unifying visual-semantic embeddings with multimodal neural language models. arXiv preprint arXiv:1411.2539, 2014. 2, R. Kiros, R. S. Zemel, and R. Salakhutdinov. Multimodal neural language models. ICML, 2014. 2
 C. Kong, D. Lin, M. Bansal, R. Urtasun, and S. Fidler. What are you talking about? text-to-image coreference. In CVPR, A. Krizhevsky, I. Sutskever, and G. E. Hinton.
Imagenet classification with deep convolutional neural networks. In
NIPS, 2012. 2, 5, 7
 G. Kulkarni, V. Premraj, S. Dhar, S. Li, Y. Choi, A. C. Berg, and T. L. Berg. Baby talk: Understanding and generating simple image descriptions. In CVPR, 2011. 1, 2, 3
 P. Kuznetsova, V. Ordonez, A. C. Berg, T. L. Berg, and Y. Choi. Collective generation of natural image descriptions.
In ACL, 2012. 2
 P. Kuznetsova, V. Ordonez, T. L. Berg, U. C. Hill, and Y. Choi. Treetalk: Composition and compression of trees for image descriptions. Transactions of the Association for
Computational Linguistics, 2(10):351–362, 2014. 2
 Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradientbased learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. 2
 L.-J. Li and L. Fei-Fei. What, where and who? classifying events by scene and object recognition. In ICCV, 2007. 2
 L.-J. Li, R. Socher, and L. Fei-Fei. Towards total scene understanding: Classification, annotation and segmentation in an automatic framework. In Computer Vision and Pattern
Recognition, 2009. CVPR 2009. IEEE Conference on, pages
2036–2043. IEEE, 2009. 2
 S. Li, G. Kulkarni, T. L. Berg, A. C. Berg, and Y. Choi. Composing simple image descriptions using web-scale n-grams.
In CoNLL, 2011. 2
 D. Lin, S. Fidler, C. Kong, and R. Urtasun. Visual semantic search: Retrieving videos via complex textual queries. 2014.
 T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll´ar, and C. L. Zitnick. Microsoft coco: Common objects in context. arXiv preprint arXiv:1405.0312, J. Mao, W. Xu, Y. Yang, J. Wang, and A. L. Yuille. Explain images with multimodal recurrent neural networks. arXiv preprint arXiv:1410.1090, 2014. 2, 6, 7
 C. Matuszek*, N. FitzGerald*, L. Zettlemoyer, L. Bo, and D. Fox.
A Joint Model of Language and Perception for
Grounded Attribute Learning. In Proc. of the 2012 International Conference on Machine Learning, Edinburgh, Scotland, June 2012. 2
 T. Mikolov, M. Karafi´at, L. Burget, J. Cernock`y, and S. Khudanpur. Recurrent neural network based language model. In
INTERSPEECH, 2010. 2, 4
 T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In NIPS, 2013. 2, 3
 M. Mitchell, X. Han, J. Dodge, A. Mensch, A. Goyal, A. Berg, K. Yamaguchi, T. Berg, K. Stratos, and H. Daum´e, III. Midge: Generating image descriptions from computer vision detections. In EACL, 2012. 2
 V. Ordonez, G. Kulkarni, and T. L. Berg. Im2text: Describing images using 1 million captioned photographs. In NIPS, K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. Bleu: a method for automatic evaluation of machine translation. In
Proceedings of the 40th annual meeting on association for computational linguistics, pages 311–318. Association for
Computational Linguistics, 2002. 7
 O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. Imagenet large scale visual recognition challenge, 2014. 1, 2, 3
 M. Schuster and K. K. Paliwal. Bidirectional recurrent neural networks. Signal Processing, IEEE Transactions on, 1997.
 K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. 5, 7
 R. Socher and L. Fei-Fei.
Connecting modalities: Semisupervised segmentation and annotation of images using unaligned text corpora. In CVPR, 2010. 2
 R. Socher, A. Karpathy, Q. V. Le, C. D. Manning, and A. Y.
Ng. Grounded compositional semantics for finding and describing images with sentences. TACL, 2014. 2, 5, 6
 I. Sutskever, J. Martens, and G. E. Hinton. Generating text with recurrent neural networks. In ICML, 2011. 2, 4, 8
 C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. arXiv preprint arXiv:1409.4842, 2014. 5, 7
 T. Tieleman and G. E. Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude., R. Vedantam, C. L. Zitnick, and D. Parikh.
Cider:
Consensus-based image description evaluation.
CoRR, abs/1411.5726, 2014. 7
 O. Vinyals, A. Toshev, S. Bengio, and D. Erhan.
Show and tell: A neural image caption generator. arXiv preprint arXiv:1411.4555, 2014. 2, 5, 6, 7
 Y. Yang, C. L. Teo, H. Daum´e III, and Y. Aloimonos.
Corpus-guided sentence generation of natural images.
In
EMNLP, 2011. 2
 B. Z. Yao, X. Yang, L. Lin, M. W. Lee, and S.-C. Zhu. I2t:
Image parsing to text description. Proceedings of the IEEE, 98(8):1485–1508, 2010. 2
 M. Yatskar, L. Vanderwende, and L. Zettlemoyer. See no evil, say no evil: Description generation from densely labeled images. Lexical and Computational Semantics, 2014.
 P. Young, A. Lai, M. Hodosh, and J. Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. TACL, W. Zaremba, I. Sutskever, and O. Vinyals. Recurrent neural network regularization. arXiv preprint arXiv:1409.2329, C. L. Zitnick, D. Parikh, and L. Vanderwende. Learning the visual interpretation of sentences. ICCV, 2013. 2Text Matching as Image Recognition
Liang Pang∗, Yanyan Lan†, Jiafeng Guo†, Jun Xu†, Shengxian Wan∗, and Xueqi Cheng†
CAS Key Laboratory of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences, Beijing 100190, China
∗{pangliang,wanshengxian}@software.ict.ac.cn, †{lanyanyan,guojiafeng,junxu,cxq}@ict.ac.cn
Abstract
Matching two texts is a fundamental problem in many natural language processing tasks. An effective way is to extract meaningful matching patterns from words, phrases, and sentences to produce the matching score.
Inspired by the success of convolutional neural network in image recognition, where neurons can capture many complicated patterns based on the extracted elementary visual patterns such as oriented edges and corners, we propose to model text matching as the problem of image recognition. Firstly, a matching matrix whose entries represent the similarities between words is constructed and viewed as an image. Then a convolutional neural network is utilized to capture rich matching patterns in a layer-by-layer way. We show that by resembling the compositional hierarchies of patterns in image recognition, our model can successfully identify salient signals such as n-gram and n-term matchings. Experimental results demonstrate its superiority against the baselines.
Introduction
Matching two texts is central to many natural language applications, such as machine translation (Brown et al.
1993), question and answering (Xue, Jeon, and Croft 2008), paraphrase identification (Socher et al. 2011) and document retrieval (Li and Xu 2014). Given two texts T1 =(w1, w2,..., wm) and T2 = (v1, v2,..., vn), the degree of matching is typically measured as a score produced by a scoring function on the representation of each text: match(T1, T2) = F
�
Φ(T1), Φ(T2)
�, (1) where wi and vj denotes the i-th and j-th word in T1 and T2, respectively. Φ is a function to map each text to a vector, and F is the scoring function for modeling the interactions between them.
A successful matching algorithm needs to capture the rich interaction structures in the matching process. Taking the task of paraphrase identification for example, given the following two texts:
T1 : Down the ages noodles and dumplings were famous
Chinese food.
Copyright c⃝ 2016, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.
T2 : Down the ages dumplings and noodles were popular in China.
We can see that the interaction structures are of different levels, from words, phrases to sentences. Firstly, there are many word level matching signals, including identical word matching between "down" in T1 and "down" in T2, and similar word matching between "famous" in T1 and "popular" in T2. These signals compose phrase level matching signals, including n-gram matching between "down the ages" in T1 and "down the ages" in T2, unordered n-term matching between "noodles and dumplings" in T1 and "dumplings and noodles" in T2, and semantic n-term matching between "were famous Chinese food" in T1 and "were popular in China" in T2. They further form sentence level matching signals, which are critical for determining the matching degree of T1 and T2. How to automatically find and utilize these hierarchical interaction patterns remains a challenging problem.
In image recognition, it has been widely observed that the convolutional neural network (CNN) (LeCun et al. 1998;
Simard, Steinkraus, and Platt 2003) can successfully abstract visual patterns from raw pixels with layer-by-layer composition (Girshick et al. 2014). Inspired by this observation, we propose to view text matching as image recognition and use
CNN to solve the above problem. Specifically, we first construct a word level similarity matrix, namely matching matrix, to capture the basic word level matching signals. The matching matrix can be viewed as: 1) a binary image if we define the similarity to be 0-1, indicating whether the two corresponding words are identical; 2) a gray image if we define the similarity to be real valued, which can be achieved by calculating the cosine or inner product based on the word embeddings. Then we apply a convolutional neural network on this matrix. Meaningful matching patterns such as n-gram and n-term can therefore be fully captured within this architecture. We can see that our model takes text matching as a multi-level abstraction of interaction patterns between words, phrases and sentences, with a layer-by-layer architecture, so we name it MatchPyramid.
The experiments on the task of paraphrase identification show that MatchPyramid (with 0-1 matching matrix) outperforms the baselines, by solely leveraging interactions between texts. While for other tasks such as paper citation matching, where semantic is somehow important, MatchPyramid (with real-valued matching matrix) performs the best by considering both interactions and semantic representations.
Contributions of this paper include: 1) a novel view of text matching as image recognition; 2) the proposal of a new deep architecture based on the matching matrix, which can capture the rich matching patterns at different levels, from words, phrases, to the whole sentences; 3) experimental analysis on different tasks to demonstrate the superior power of the proposed architecture against competitor matching algorithms.
Motivation
It has been widely recognized that making a good matching decision requires to take into account the rich interaction structures in the text matching process, starting from the interactions between words, to various matching patterns in the phrases and the whole sentences. Taking the aforementioned two sentences as an example, the interaction structures are of different levels, as illustrated in Figure 1.
Figure 1: An example of interaction structures in paraphrase identification.
Word Level Matching Signals refer to matchings between words in the two texts, including not only identical word matchings, such as "down–down", "the–the", "ages–ages", "noodles–noodles", "and–and","dumplings– dumplings" and "were–were", but also similar word matchings, such as "famous–popular" and "chinese–china".
Phrase Level Matching Signals refer to matchings between phrases, including n-gram and n-term. N-gram matching occurs with n exactly matched successive words, e.g.
"(down the ages)–(down the ages)". While n-term matching allows for order or semantic alternatives, e.g. "(noodles and dumplings)–(dumplings and noodles)", and "(were famous chinese food)–(were popular in china)".
Sentence Level Matching Signals refer to matchings between sentences, which are composed of multiple lower level matching signals, e.g. the three successive phrase level matchings mentioned above. When we consider matchings between paragraphs that contain multiple sentences, the whole paragraph will be viewed as a long sentence and the same composition strategy would generate paragraph level matching signals.
To sum up, the interaction structures are compositional hierarchies, in which higher level signals are obtained by composing lower level ones. This is similar to image recognition. In an image, raw pixels provide basic units of the image, and each patch may contain some elementary visual features such as oriented edges and corners. Local combinations of edges form motifs, motifs assemble into parts, and parts form objects. We give an example to show the relationships between text matching and image recognition (Jia et al.
2014), as illustrated in Figure 2. In the area of image recognition, CNN has been recognized as one the most successful way to capture different levels of patterns in image (Zeiler and Fergus 2014). Therefore, it inspires us to transform text matching to image recognition and employ CNN to solve it.
However, the representations of text and image are so different that it remains a challenging problem to perform such transformation.
MatchPyramid
In this section we introduce a new deep architecture for text matching, namely MatchPyramid. The main idea comes from modeling text matching as image recognition, by taking the matching matrix as an image, as illustrated in Figure 3.
Matching Matrix: Bridging the Gap between Text
Matching and Image Recognition
As discussed before, one challenging problem by modeling text matching as image recognition lies in the different representations of text and image: the former are two 1D (onedimensional) word sequences while the latter is typically a 2D pixel grid. To address this issue, we represent the input of text matching as a matching matrix M, with each element Mij standing for the basic interaction, i.e. similarity between word wi and vj (see Eq. 2). Here for convenience, wi and vj denotes the i-th and j-th word in two texts respectively, and ⊗ stands for a general operator to obtain the similarity.
Mij = wi ⊗ vj.
In this way, we can view the matching matrix M as an image, where each entry (i.e. the similarity between two words) stands for the corresponding pixel value. We can adopt different kinds of ⊗ to model the interactions between two words, leading to different kinds of raw images. In this paper, we give three examples as follows.
Indicator Function produces either 1 or 0 to indicate whether two words are identical.
Mij = I{wi=vj} =
� 1, if wi = vj
0, otherwise.
One limitation of the indicator function is that it cannot capture the semantic matching between similar words. To tackle this problem, we define ⊗ based on word embeddings, which will make the matrix more flexible to capture semantic interactions. Given the embedding of each word
⃗αi = Φ(wi) and ⃗βj = Φ(vj), which can be obtained by recent Word2Vec (Mikolov et al. 2013) technique, we introduce the other two operators: cosine and dot product.
Cosine views angles between word vectors as the similarity, and it acts as a soft indicator function.
Mij =
⃗αi
⊤ ⃗βj
∥ ⃗αi∥ · ∥ ⃗βj∥, (4) where ∥ · ∥ stands for the norm of a vector, and ℓ2 norm is used in this paper.
Dot Product further considers the norm of word vectors, as compared to cosine.
Mij = ⃗αi
⊤ ⃗βj.
Figure 2: Relationships between text matching and image recognition.
Figure 3: An overview of MatchPyramid on Text Matching.
Based on these three different operators, the matching matrices of the given example are shown in Fig 4. Obviously we can see that Fig 4(a) corresponds to a binary image, and Fig 4(b) correspond to gray images.
Hierarchical Convolution: A Way to Capture Rich
Matching Patterns
The body of MatchPyramid is a typical convolutional neural network, which can extract different levels of matching patterns. For the first layer of CNN, the k-th kernel w(1,k) scans over the whole matching matrix z(0) =M to generate a feature map z(1,k): z(1,k) i,j
= σ
�rk−1
� s=0 rk−1
� t=0 w(1,k) s,t
· z(0) i+s,j+t + b(1,k)
�(a) Indicator(b) Dot Product
Figure 4: Three different matching matrices, where solid circles elements are all valued 0. where rk denotes the size of the k-th kernel. In this paper we use square kernel, and ReLU (Dahl, Sainath, and Hinton
2013) is adopted as the active function σ.
Dynamic pooling strategy (Socher et al. 2011) is then used to deal with the text length variability. By applying dynamic pooling, we will get fixed-size feature maps: z(2,k) i,j
= max
0≤s<dk max
0≤t<d′ k z(1,k) i·dk+s,j·d′ k+t, (7) where dk and d′ k denote the width and length of the corresponding pooling kernel, which are determined by the text lengths n and m, and output feature map size n′ × m′, i.e. dk = ⌈n/n′⌉, d′ k = ⌈m/m′⌉.
After the first convolution and dynamic pooling, we continue to obtain higher level features z(l), l ≥ 2 by further convolution and max-pooling, with general formulations: z(l+1,k′) i,j
=σ
�cl−1
� k=0 rk−1
� s=0 rk−1
� t=0 w(l+1,k′) s,t
·z(l,k) i+s,j+t+b(l+1,k)
�, l = 2, 4, 6,..., (8) z(l+1,k) i,j
= max
0≤s<dk max
0≤t<dk z(l,k) i·dk+s,j·dk+t, l = 3, 5, 7,..., (9) where cl denote the number of feature maps in the l-th layer.
Analysis of Hierarchical Convolution
Similar to CNN in image recognition where it can make abstractions based on extracted elementary visual patterns
Figure 5: An illustration of Hierarchical Convolution. such as oriented edges and corners, the hierarchical convolution in MatchPyramid can also capture important phrase level interactions from word level matching and make further compositions. We revisit our example, and show how it works1, as illustrated in Figure 5.(1) With the given two kernels, we can see clearly that the first convolutional layer can capture both n-gram matching signals "(down the ages)–(down the ages)" and n-term matching signal "(noodles and dumplings)–(dumplings and noodles)". The extracted matching patterns are like edges in image recognition (refer Figure 2).(2) The following convolutional layers make compositions and form higher level of matching patterns. For example, from the second layer, we can see that a more complicated "T-type" pattern captured with the given 3D kernel. It looks like some motif (parts) obtained in image recognition(refer Figure 2).
From the above analysis we can see that MatchPyramid can abstract complicated matching patterns, from phrase to sentence level, by hierarchical convolution.
Matching Score and Training
We use a MLP (Multi-Layer Perception) to produce the final matching score. Take binary classification and two-layer perceptron for example, we will obtain a 2-dimensional matching score vector:(s0, s1)⊤ =W2σ
�
W1z + b1
�
+ b2, (10) where s0 and s1 are the matching scores of the corresponding class, z is the output of the hierarchical convolution, Wi is the weight of the i-th MLP layer and σ denotes the activation function.
Softmax function is utilized to output the probability of belonging to each class, and cross entropy is used as the objective function for training. Therefore the optimization
1Here we take the matching matrix with indicator function as example, similar observations can be obtained for other matching matrix with cosine similarity and dot product. becomes minimizing: loss = −
N
� i=1
� y(i) log(p(i)
1 )+(1 − y(i)) log(p(i)
�, pk = esk es0 + es1, k = 0, 1, (11) where y(i) is the label of the i-th training instance. The optimization is relatively straightforward with the standard back-propagation (Williams and Hinton 1986). We apply stochastic gradient descent method Adagrad (Duchi, Hazan, and Singer 2011) for the optimization of models. It performs better when we use the mini-batch strategy (32∼50 in size), which can be easily parallelized on single machine with multi-cores. For regularization, we find that some common strategies like early stopping (Giles 2001) and dropout (Hinton et al. 2012) are enough for our model.
Experiments
In this section, we conduct experiments on two tasks, i.e. paraphrase identification and paper citation matching, to demonstrate the superiority of MatchPyramid against baselines.
Competitor Methods and Experimental Settings
ALLPOSITIVE: All of the test data are predicted as positive.
TF-IDF: TF-IDF (Salton, Fox, and Wu 1983) is a widely used method in text mining. In this method, each text is represented as a |V |-dimensional vector with each element stands for the TF-IDF score of the corresponding word in the text, where |V | is the vocabulary size. In this paper, idf score is calculated in the whole dataset. The final matching score is produced by the inner product of the two vectors.
DSSM/CDSSM: Since DSSM (Huang et al. 2013) and CDSSM (Gao et al. 2014; Shen et al. 2014) need large data for training, we directly use the released models2 (trained on large click-through dataset) on our test data.
2http://research.microsoft.com/en-us/downloads/731572aa98e4-4c50-b99d-ae3f0c9562b9/
ARC-I/ARC-II: We implement ARC-I and ARC-II (Hu et al. 2014) due to there is no publicly available codes, using exactly the same setting as described in the original paper.
There are three versions of MatchPyramid, depending on different methods used for constructing the matching matrices, denoted as MP-IND, MP-COS, and MP-DOT, respectively. All these models use two convolutional layers, two max-pooling layers (one of which is a dynamic pooling layer for variable length) and two full connection layers. The number of feature maps is 8 and 16 for the first and second convolutional layer, respectively. While the kernel size is set to be 5 × 5 and 3 × 3, respectively. Unlike ARC-II which initiates with Word2Vec trained on Wikipedia, we initiate the word vectors in MP-COS and MP-DOT randomly from a unit ball. Thus our model do not require any external sources.
Experiment I: Paraphrase Identification
Paraphrase identification aims to determine whether two sentences have the same meaning, a problem considered as a touchstone of natural language understanding. Here we use the benchmark MSRP dataset (Dolan and Brockett 2005), which contains 4076 instances for training and 1725 for testing. The experimental results are listed in Table 1. We can
Table 1: Results on MSRP.
Model
Acc.(%)
F1(%)
ALLPOSITIVE
TF-IDF
DSSM
CDSSM
ARC-I
ARC-II
MP-IND
MP-COS
MP-DOT
83.01 see that traditional simple model such as TF-IDF has already achieved a high accuracy of about 70%, though it only uses the unigram matching signals. Our methods performs much better than TF-IDF, which indicates that the complicated matching patterns captured by hierarchical convolution are important to the text matching task. For the comparison with recent deep models, we can see that DSSM performs better than the others (though the improvement is quite limited), and our models (MP-IND, MP-COS and MP-DOT) outperform all of them. Though the best performance of our model(75.94%/83.01%) is still slightly worse than URAE (Socher et al. 2011) (76.8%/83.6%), URAE relies heavily on pretraining with an external large dataset annotated with parse tree information. In the future work, we will study how to utilize external data to further improve our models.
We also visualize what we have learned in MatchPyramid3, with expectation that we can gain some insights from
3Here we only demonstrate the case of MP-DOT due to space limitation. Similar results can be observed with MP-IND and MPCOS. the process. Specifically, we take a pair of texts as an example (selected from the MSRP dataset), and illustrate the feature maps and kernels in Figure 6. We can see that n-gram and n-term matching, which are emphasized in the blue and yellow color in original texts, are represented as a diagonal sub-matrix emphasized with the blue and yellow rectangles in the matching matrix, respectively. Kernel 1 and kernel 2 are the two kernels learned in the first convolutional layer, which well captures the important n-gram and n-term matching signals respectively. We can see that these patterns are quite similar to the edge extracted by CNN in image recognition (see Figure 2). We also give some more kernels and show some patterns learned in the second convolutional layer. We can see that the latter layer make compositions and keep the useful matching signals until passing it to the MLP classifier. This explains clearly why our model works well:
MatchPyramid captures useful matching patterns at different levels, from words, phrase, to sentences, with a similar process in image recognition.
Experiment II: Paper Citation Matching
We evaluate the effectiveness of MatchPyramid with another text matching task called paper citation matching, based on a large academic dataset4. Basically, we are given a set of papers along with their abstracts. A paper and its citations' abstracts then becomes a pair of texts, and defined as a type of matching. One representative example is given as follows:
T1 : this article describes pulsed thermal time of flight ttof flow sensor system as two subsystems pulsed wire system and heat flow system the entire flow sensor is regarded system theoretically as linear.
T2 : the authors report on novel linear time invariant lti modeling of flow sensor system based on thermal time of flight tof principle by using pulsed hot wire anemometry thermal he at pulses.
We can see that the matching here should take both lexical and semantic information into consideration. The dataset is collected from a commercial academic website. It contains 838 908 instances (text pairs) in total, where there are
279 636 positive (matched) instances and 559 272 negative(mismatch) instances. The negative instances are randomly sampled papers which have no citation relations. We split the whole dataset into three parts, 599 196 instances for training, 119 829 for validation and 119 883 for testing.
The results in Table 2 show that TF-IDF is also a strong baseline on this dataset, which is even better than some deep models such as DSSM and CDSSM. This may be caused by the large difference between the testing data (paper citation data) and training data (click-through data) used in DSSM and CDSSM. ARC-I and ARC-II gain a significant improvement over these models, which may benefit much from the large training data. As for our models, the best performance is still achieved by MP-DOT (88.73%/82.86%), which is better than ARC-II (86.84%/79.57%). MP-COS also gains a better result than ARC-II. The reason of the poor performance of MP-IND on this task may lie in that the indicator
4We only use the first 32 words in the abstract.
Figure 6: Analysis of the feature maps and kernels in the MatchPyramid Model. The brighter the pixel is, the larger value it has.
Better viewed in color.
Table 2: Results on the task of paper citation matching.
Model
Acc.(%)
F1(%)
ALLPOSITIVE
TF-IDF
DSSM
CDSSM
ARC-I
ARC-II
MP-IND
MP-COS
MP-DOT
82.86 function only captures the exact matching between words, but omits the semantic similarity.
Table 3: The norm of learned word embeddings on the task of paper citation matching.
Word the with for be are
Len
Word robotics java snakes musical rfid
Len
We further show the reason why MP-DOT performs better than MP-COS by analyzing the learned word embeddings.
Specifically, we pick some words with large and small norm, listed in Table 3. We can see that most words with small norm are indeed useless for matching, while most words with large norm (such as robotics and java) are domain terms which play an important role in paper citation matching. By further considering the importance of words, MPDOT can capture more semantic information than MP-COS and thus achieve better performance.
Related Work
Most previous work on text matching tries to find good representations for a single text, and usually use a simple scoring function to obtain the matching results. Examples include Partial Least Square (Wu, Li, and Xu 2013), Canonical Correlation Analysis (Hardoon and Shawe-Taylor 2003) and some deep models such as DSSM (Huang et al. 2013), CDSSM (Gao et al. 2014; Shen et al. 2014) and ARC-I (Hu et al. 2014).
Recently, a brand new approach focusing on modeling the interaction between two sentences has been proposed and gained much attention, examples include DEEPMATCH (Lu and Li 2013), URAE (Socher et al. 2011) and ARC-II (Hu et al. 2014). Our model falls into this category, thus we give some detailed discussions on the differences of our model against these methods.
DEEPMATCH uses topic model to construct the interactions between two texts, and then make different levels of abstractions by a hierarchical architecture based on the relationships between topics. Compared with our matching matrix defined at word level, DEEPMATCH uses topic information with more rough granularity. Moreover, it relies largely on the quality of learned topic model, and the hierarchies are usually ambiguous since the relationships between topics are not absolute. On the contrary, MatchPyramid clearly models the interactions at different levels, from words, phrases to sentences.
URAE constructs the interactions between two texts based on the syntactic trees, thus it relies on a predefined compact vectorial representation of text. Specifically, URAE first learns the representation of each node on the tree by a auto-encoder, then directly inserts different levels of interaction, such as word, prase and sentence, to a single matrix.
Different from that, our MatchPyramid is end-to-end, and captures different levels of interactions in a hierarchical way.
ARC-II and ARC-I are both proposed based on convolutional sentence model DCNN (Kalchbrenner, Grefenstette, and Blunsom 2014). Different from ARC-I which defers the interaction of two texts to the end of the process, ARCII lets them meet early by directly interleaving them to a single representation, and makes abstractions on this basis.
Therefore, ARC-II is capturing sentence level interactions directly. However, it is not clear what exactly the interactions are, since they used a sum operation. Our model is also based on a convolutional neural network, but the idea is quite different from that of ARC-II. It is clear that we start from word level matching patterns, and compose to phrase and sentence level matching pattern layer by layer.
Conclusion
In this paper, we view text matching as image recognition, and propose a new deep architecture, namely MatchPyramid. Our model can automatically capture important matching patterns such as unigram, n-gram and n-term at different levels. Experimental results show that our model can outperform baselines, including some recently proposed deep matching algorithms.
Acknowledgments
This work was funded by 973 Program of China under
Grants No. 2014CB340401 and 2012CB316303, 863 Program of China under Grants No. 2014AA015204, the National Natural Science Foundation of China (NSFC) under Grants No. 61472401, 61433014, 61425016, 61425016, and 61203298, Key Research Program of the Chinese
Academy of Sciences under Grant No. KGZD-EW-T03-2, and Youth Innovation Promotion Association CAS under
Grants No. 20144310.
References
Brown, P. F.; Pietra, V. J. D.; Pietra, S. A. D.; and Mercer, R. L.
The mathematics of statistical machine translation: Parameter estimation. Computational linguistics
19(2):263–311.
Dahl, G. E.; Sainath, T. N.; and Hinton, G. E. 2013. Improving deep neural networks for lvcsr using rectified linear units and dropout. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, 8609–8613. IEEE.
Dolan, W. B., and Brockett, C. 2005. Automatically constructing a corpus of sentential paraphrases. In Proc. of IWP.
Duchi, J.; Hazan, E.; and Singer, Y. 2011. Adaptive subgradient methods for online learning and stochastic optimization.
The Journal of Machine Learning Research 12:2121–2159.
Gao, J.; Pantel, P.; Gamon, M.; He, X.; Deng, L.; and Shen, Y.
2014. Modeling interestingness with deep neural networks. In
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing.
Giles, R. C. S. L. L. 2001. Overfitting in neural nets: Backpropagation, conjugate gradient, and early stopping. In Advances in Neural Information Processing Systems 13: Proceedings of the 2000 Conference, volume 13, 402. MIT Press.
Girshick, R.; Donahue, J.; Darrell, T.; and Malik, J. 2014.
Rich feature hierarchies for accurate object detection and semantic segmentation. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, 580–587. IEEE.
Hardoon, D. R., and Shawe-Taylor, J. 2003. Kcca for different level precision in content-based image retrieval. In Proceedings of Third International Workshop on Content-Based
Multimedia Indexing, IRISA, Rennes, France.
Hinton, G. E.; Srivastava, N.; Krizhevsky, A.; Sutskever, I.; and Salakhutdinov, R.
Improving neural networks by preventing co-adaptation of feature detectors.
CoRR abs/1207.0580.
Hu, B.; Lu, Z.; Li, H.; and Chen, Q. 2014. Convolutional neural network architectures for matching natural language sentences. In Advances in Neural Information Processing Systems, 2042–2050.
Huang, P.-S.; He, X.; Gao, J.; Deng, L.; Acero, A.; and Heck, L. 2013. Learning deep structured semantic models for web search using clickthrough data. In Proceedings of the 22nd
ACM international conference on Conference on Information and Knowledge Management, 2333–2338. ACM.
Jia, Y.; Shelhamer, E.; Donahue, J.; Karayev, S.; Long, J.;
Girshick, R.; Guadarrama, S.; and Darrell, T. 2014. Caffe:
Convolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093.
Kalchbrenner, N.; Grefenstette, E.; and Blunsom, P. 2014. A convolutional neural network for modelling sentences. CoRR abs/1404.2188.
LeCun, Y.; Bottou, L.; Bengio, Y.; and Haffner, P.
Gradient-based learning applied to document recognition.
Proceedings of the IEEE 86(11):2278–2324.
Li, H., and Xu, J. 2014. Semantic matching in search. Foundations and Trends in Information Retrieval 7(5):343–469.
Lu, Z., and Li, H. 2013. A deep architecture for matching short texts. In Advances in Neural Information Processing
Systems, 1367–1375.
Mikolov, T.; Chen, K.; Corrado, G.; and Dean, J. 2013. Efficient estimation of word representations in vector space.
CoRR abs/1301.3781.
Salton, G.; Fox, E. A.; and Wu, H.
Extended boolean information retrieval. Communications of the ACM
26(11):1022–1036.
Shen, Y.; He, X.; Gao, J.; Deng, L.; and Mesnil, G. 2014.
A latent semantic model with convolutional-pooling structure for information retrieval. In Proceedings of the 23rd ACM
International Conference on Conference on Information and Knowledge Management, 101–110. ACM.
Simard, P. Y.; Steinkraus, D.; and Platt, J. C. 2003. Best practices for convolutional neural networks applied to visual document analysis. In 2013 12th International Conference on Document Analysis and Recognition, volume 2, 958–958.
IEEE Computer Society.
Socher, R.; Huang, E. H.; Pennin, J.; Manning, C. D.; and Ng, A. Y. 2011. Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. In Advances in Neural
Information Processing Systems, 801–809.
Williams, D. R. G. H. R., and Hinton, G. 1986. Learning representations by back-propagating errors. Nature 323–533.
Wu, W.; Li, H.; and Xu, J. 2013. Learning query and document similarities from click-through bipartite graph with metadata. In Proceedings of the sixth ACM international conference on WSDM, 687–696. ACM.
Xue, X.; Jeon, J.; and Croft, W. B. 2008. Retrieval models for question and answer archives. In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, 475–482. ACM.
Zeiler, M. D., and Fergus, R. 2014. Visualizing and understanding convolutional networks. In Computer Vision–ECCV
2014. Springer. 818–833.Learning Transferable Architectures for Scalable Image Recognition
Barret Zoph
Google Brain barretzoph@google.com
Vijay Vasudevan
Google Brain vrv@google.com
Jonathon Shlens
Google Brain shlens@google.com
Quoc V. Le
Google Brain qvl@google.com
Abstract
Developing neural network image classification models often requires significant architecture engineering. In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. The key contribution of this work is the design of a new search space (which we call the "NASNet search space") which enables transferability. In our experiments, we search for the best convolutional layer (or "cell") on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, which we name a "NASNet architecture". We also introduce a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models. On
CIFAR-10 itself, a NASNet found by our method achieves
2.4% error rate, which is state-of-the-art. Although the cell is not searched for directly on ImageNet, a NASNet constructed from the best cell achieves, among the published works, state-of-the-art accuracy of 82.7% top-1 and 96.2% top-5 on ImageNet. Our model is 1.2% better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS – a reduction of 28% in computational demand from the previous state-of-the-art model.
When evaluated at different levels of computational cost, accuracies of NASNets exceed those of the state-of-the-art human-designed models. For instance, a small version of NASNet also achieves 74% top-1 accuracy, which is 3.1% better than equivalently-sized, state-of-the-art models for mobile platforms. Finally, the image features learned from image classification are generically useful and can be transferred to other computer vision problems. On the task of object detection, the learned features by NASNet used with the Faster-RCNN framework surpass state-of-the-art by 4.0% achieving 43.1% mAP on the COCO dataset.
1. Introduction
Developing neural network image classification models often requires significant architecture engineering. Starting from the seminal work of on using convolutional architectures for ImageNet classification, successive advancements through architecture engineering have achieved impressive results.
In this paper, we study a new paradigm of designing convolutional architectures and describe a scalable method to optimize convolutional architectures on a dataset of interest, for instance the ImageNet classification dataset. Our approach is inspired by the recently proposed Neural Architecture Search (NAS) framework, which uses a reinforcement learning search method to optimize architecture configurations.
Applying NAS, or any other search methods, directly to a large dataset, such as the ImageNet dataset, is however computationally expensive. We therefore propose to search for a good architecture on a proxy dataset, for example the smaller CIFAR-10 dataset, and then transfer the learned architecture to ImageNet. We achieve this transferrability by designing a search space (which we call "the NASNet search space") so that the complexity of the architecture is independent of the depth of the network and the size of input images. More concretely, all convolutional networks in our search space are composed of convolutional layers (or "cells") with identical structure but different weights. Searching for the best convolutional architectures is therefore reduced to searching for the best cell structure. Searching for the best cell structure has two main benefits: it is much faster than searching for an entire network architecture and the cell itself is more likely to generalize to other problems. In our experiments, this approach significantly accelerates the search for the best architectures using CIFAR-10 by a factor of 7× and learns architectures that successfully transfer to ImageNet.
Our main result is that the best architecture found on
CIFAR-10, called NASNet, achieves state-of-the-art accuracy when transferred to ImageNet classification without much modification. On ImageNet, NASNet achieves, among the published works, state-of-the-art accuracy of 82.7% top-1 and 96.2% top-5. This result amounts to a 1.2% improvement in top-1 accuracy than the best humaninvented architectures while having 9 billion fewer FLOPS.
On CIFAR-10 itself, NASNet achieves 2.4% error rate, which is also state-of-the-art.
Additionally, by simply varying the number of the convolutional cells and number of filters in the convolutional cells, we can create different versions of NASNets with different computational demands. Thanks to this property of the cells, we can generate a family of models that achieve accuracies superior to all human-invented models at equivalent or smaller computational budgets. Notably, the smallest version of NASNet achieves 74.0% top-1 accuracy on ImageNet, which is 3.1% better than previously engineered architectures targeted towards mobile and embedded vision tasks.
Finally, we show that the image features learned by
NASNets are generically useful and transfer to other computer vision problems.
In our experiments, the features learned by NASNets from ImageNet classification can be combined with the Faster-RCNN framework to achieve state-of-the-art on COCO object detection task for both the largest as well as mobile-optimized models. Our largest
NASNet model achieves 43.1% mAP, which is 4% better than previous state-of-the-art.
2. Related Work
The proposed method is related to previous work in hyperparameter optimization – especially recent approaches in designing architectures such as Neural Fabrics, DiffRNN, MetaQNN and DeepArchitect. A more flexible class of methods for designing architecture is evolutionary algorithms, yet they have not had as much success at large scale. Xie and Yuille also transferred learned architectures from CIFAR-10 to ImageNet but performance of these models (top-1 accuracy 72.1%) are notably below previous state-of-the-art (Table 2).
The concept of having one neural network interact with a second neural network to aid the learning process, or learning to learn or meta-learning has attracted much attention in recent years. Most of these approaches have not been scaled to large problems like ImageNet. An exception is the recent work focused on learning an optimizer for ImageNet classification that achieved notable improvements.
The design of our search space took much inspiration from LSTMs, and Neural Architecture Search
Cell. The modular structure of the convolutional cell is also related to previous methods on ImageNet such as VGG, Inception, ResNet/ResNext, and Xception/MobileNet.
3. Method
Our work makes use of search methods to find good convolutional architectures on a dataset of interest. The main search method we use in this work is the Neural Architecture Search (NAS) framework proposed by. In NAS, a controller recurrent neural network (RNN) samples child networks with different architectures. The child networks are trained to convergence to obtain some accuracy on a held-out validation set. The resulting accuracies are used to update the controller so that the controller will generate better architectures over time. The controller weights are updated with policy gradient (see Figure 1).
The controller (RNN)
Train a child network� with architecture A to � convergence to get � validation accuracy R
Sample architecture A� with probability p
Scale gradient of p by R� to update the controller
Figure 1. Overview of Neural Architecture Search. A controller RNN predicts architecture A from a search space with probability p. A child network with architecture A is trained to convergence achieving accuracy R. Scale the gradients of p by R to update the RNN controller.
The main contribution of this work is the design of a novel search space, such that the best architecture found on the CIFAR-10 dataset would scale to larger, higherresolution image datasets across a range of computational settings.
We name this search space the NASNet search space as it gives rise to NASNet, the best architecture found in our experiments. One inspiration for the NASNet search space is the realization that architecture engineering with
CNNs often identifies repeated motifs consisting of combinations of convolutional filter banks, nonlinearities and a prudent selection of connections to achieve state-of-the-art results (such as the repeated modules present in the Inception and ResNet models ). These observations suggest that it may be possible for the controller RNN to predict a generic convolutional cell expressed in terms of these motifs. This cell can then be stacked in series to handle inputs of arbitrary spatial dimensions and filter depth.
In our approach, the overall architectures of the convolutional nets are manually predetermined. They are composed of convolutional cells repeated many times where each convolutional cell has the same architecture, but different weights. To easily build scalable architectures for images of any size, we need two types of convolutional cells to serve two main functions when taking in a feature map
Figure 2. Scalable architectures for image classification consist of two repeated motifs termed Normal Cell and Reduction Cell. This diagram highlights the model architecture for CIFAR-10 and ImageNet. The choice for the number of times the Normal Cells that gets stacked between reduction cells, N, can vary in our experiments. as input: (1) convolutional cells that return a feature map of the same dimension, and (2) convolutional cells that return a feature map where the feature map height and width is reduced by a factor of two. We name the first type and second type of convolutional cells Normal Cell and Reduction Cell respectively. For the Reduction Cell, we make the initial operation applied to the cell's inputs have a stride of two to reduce the height and width. All of our operations that we consider for building our convolutional cells have an option of striding.
Figure 2 shows our placement of Normal and Reduction
Cells for CIFAR-10 and ImageNet. Note on ImageNet we have more Reduction Cells, since the incoming image size is 299x299 compared to 32x32 for CIFAR. The Reduction and Normal Cell could have the same architecture, but we empirically found it beneficial to learn two separate architectures. We use a common heuristic to double the number of filters in the output whenever the spatial activation size is reduced in order to maintain roughly constant hidden state dimension. Importantly, much like Inception and ResNet models, we consider the number of motif repetitions N and the number of initial convolutional filters as free parameters that we tailor to the scale of an image classification problem.
What varies in the convolutional nets is the structures of the Normal and Reduction Cells, which are searched by the controller RNN. The structures of the cells can be searched within a search space defined as follows (see Appendix, Figure 7 for schematic). In our search space, each cell receives as input two initial hidden states hi and hi−1 which are the outputs of two cells in previous two lower layers or the input image. The controller RNN recursively predicts the rest of the structure of the convolutional cell, given these two initial hidden states (Figure 3). The predictions of the controller for each cell are grouped into B blocks, where each block has 5 prediction steps made by 5 distinct softmax classifiers corresponding to discrete choices of the elements of a block:
Step 1. Select a hidden state from hi, hi−1 or from the set of hidden states created in previous blocks.
Step 2. Select a second hidden state from the same options as in Step 1.
Step 3. Select an operation to apply to the hidden state selected in Step 1.
Step 4. Select an operation to apply to the hidden state selected in Step 2.
Step 5. Select a method to combine the outputs of Step 3 and 4 to create a new hidden state.
The algorithm appends the newly-created hidden state to the set of existing hidden states as a potential input in subsequent blocks. The controller RNN repeats the above 5 prediction steps B times corresponding to the B blocks in a convolutional cell. In our experiments, selecting B = 5 provides good results, although we have not exhaustively searched this space due to computational limitations.
In steps 3 and 4, the controller RNN selects an operation to apply to the hidden states. We collected the following set of operations based on their prevalence in the CNN literature:
• identity
• 1x3 then 3x1 convolution
• 1x7 then 7x1 convolution
• 3x3 dilated convolution
• 3x3 average pooling
• 3x3 max pooling
• 5x5 max pooling
• 7x7 max pooling
• 1x1 convolution
• 3x3 convolution
• 3x3 depthwise-separable conv
• 5x5 depthwise-seperable conv
• 7x7 depthwise-separable conv
In step 5 the controller RNN selects a method to combine the two hidden states, either (1) element-wise addition between two hidden states or (2) concatenation between two hidden states along the filter dimension. Finally, all of the unused hidden states generated in the convolutional cell are concatenated together in depth to provide the final cell output.
To allow the controller RNN to predict both Normal Cell and Reduction Cell, we simply make the controller have
2 × 5B predictions in total, where the first 5B predictions are for the Normal Cell and the second 5B predictions are for the Reduction Cell.
8699 softmax� layer controller� hidden layer
Select one� hidden state
Select second� hidden state
Select operation for � first hidden state
Select operation for� second hidden state
Select method to� combine hidden state repeat B times new hidden layer add
3 x 3 conv
2 x 2 maxpool hidden layer B hidden layer A
Figure 3. Controller model architecture for recursively constructing one block of a convolutional cell. Each block requires selecting 5 discrete parameters, each of which corresponds to the output of a softmax layer. Example constructed block shown on right. A convolutional cell contains B blocks, hence the controller contains 5B softmax layers for predicting the architecture of a convolutional cell. In our experiments, the number of blocks B is 5.
Finally, our work makes use of the reinforcement learning proposal in NAS ; however, it is also possible to use random search to search for architectures in the NASNet search space. In random search, instead of sampling the decisions from the softmax classifiers in the controller
RNN, we can sample the decisions from the uniform distribution. In our experiments, we find that random search is slightly worse than reinforcement learning on the CIFAR10 dataset. Although there is value in using reinforcement learning, the gap is smaller than what is found in the original work of. This result suggests that 1) the NASNet search space is well-constructed such that random search can perform reasonably well and 2) random search is a difficult baseline to beat. We will compare reinforcement learning against random search in Section 4.4.
4. Experiments and Results
In this section, we describe our experiments with the method described above to learn convolutional cells.
In summary, all architecture searches are performed using the CIFAR-10 classification task. The controller RNN was trained using Proximal Policy Optimization (PPO) by employing a global workqueue system for generating a pool of child networks controlled by the RNN. In our experiments, the pool of workers in the workqueue consisted of 500 GPUs.
The result of this search process over 4 days yields several candidate convolutional cells. We note that this search procedure is almost 7× faster than previous approaches that took 28 days.1 Additionally, we demonstrate below that the resulting architecture is superior in accuracy.
Figure 4 shows a diagram of the top performing Normal
Cell and Reduction Cell. Note the prevalence of separable
1In particular, we note that previous architecture search used 800
GPUs for 28 days resulting in 22,400 GPU-hours. The method in this paper uses 500 GPUs across 4 days resulting in 2,000 GPU-hours. The former effort used Nvidia K40 GPUs, whereas the current efforts used faster
NVidia P100s. Discounting the fact that the we use faster hardware, we estimate that the current procedure is roughly about 7× more efficient. convolutions and the number of branches compared with competing architectures. Subsequent experiments focus on this convolutional cell architecture, although we examine the efficacy of other, top-ranked convolutional cells in ImageNet experiments (described in Appendix B) and report their results as well. We call the three networks constructed from the best three searches NASNetA, NASNet-B and NASNet-C.
We demonstrate the utility of the convolutional cells by employing this learned architecture on CIFAR-10 and a family of ImageNet classification tasks. The latter family of tasks is explored across a few orders of magnitude in computational budget. After having learned the convolutional cells, several hyper-parameters may be explored to build a final network for a given task: (1) the number of cell repeats
N and (2) the number of filters in the initial convolutional cell. After selecting the number of initial filters, we use a common heuristic to double the number of filters whenever the stride is 2. Finally, we define a simple notation, e.g., 4 @ 64, to indicate these two parameters in all networks, where 4 and 64 indicate the number of cell repeats and the number of filters in the penultimate layer of the network, respectively.
For complete details of of the architecture learning algorithm and the controller system, please refer to Appendix A.
Importantly, when training NASNets, we discovered ScheduledDropPath, a modified version of DropPath, to be an effective regularization method for NASNet. In DropPath, each path in the cell is stochastically dropped with some fixed probability during training. In our modified version, ScheduledDropPath, each path in the cell is dropped out with a probability that is linearly increased over the course of training. We find that DropPath does not work well for NASNets, while ScheduledDropPath significantly improves the final performance of NASNets in both
CIFAR and ImageNet experiments.
Normal Cell
Reduction Cell hi hi-1... hi+1 concat avg�
3x3 sep�
5x5 sep�
7x7 sep�
5x5 max�
3x3 sep�
7x7 add add add add add sep�
3x3 iden� tity avg�
3x3 max�
3x3 hi hi-1... hi+1 concat sep�
3x3 avg�
3x3 avg�
3x3 sep�
5x5 sep�
3x3 iden� tity iden� tity sep�
3x3 sep�
5x5 avg�
3x3 add add add add add
Figure 4. Architecture of the best convolutional cells (NASNet-A) with B = 5 blocks identified with CIFAR-10. The input (white) is the hidden state from previous activations (or input image). The output (pink) is the result of a concatenation operation across all resulting branches. Each convolutional cell is the result of B blocks. A single block is corresponds to two primitive operations (yellow) and a combination operation (green). Note that colors correspond to operations in Figure 3.
4.1. Results on CIFAR-10 Image Classification
For the task of image classification with CIFAR-10, we set N = 4 or 6 (Figure 2).
The test accuracies of the best architectures are reported in Table 1 along with other state-of-the-art models. As can be seen from the Table, a large NASNet-A model with cutout data augmentation achieves a state-of-the-art error rate of 2.40% (averaged across 5 runs), which is slightly better than the previous best record of 2.56% by. The best single run from our model achieves 2.19% error rate.
4.2. Results on ImageNet Image Classification
We performed several sets of experiments on ImageNet with the best convolutional cells learned from CIFAR-10.
We emphasize that we merely transfer the architectures from CIFAR-10 but train all ImageNet models weights from scratch.
Results are summarized in Table 2 and 3 and Figure 5.
In the first set of experiments, we train several image classification systems operating on 299x299 or 331x331 resolution images with different experiments scaled in computational demand to create models that are roughly on par in computational cost with Inception-v2, Inception-v3
 and PolyNet. We show that this family of models achieve state-of-the-art performance with fewer floating point operations and parameters than comparable architectures. Second, we demonstrate that by adjusting the scale of the model we can achieve state-of-the-art performance at smaller computational budgets, exceeding streamlined
CNNs hand-designed for this operating regime.
Note we do not have residual connections between convolutional cells as the models learn skip connections on their own. We empirically found manually inserting residual connections between cells to not help performance. Our training setup on ImageNet is similar to, but please see
Appendix A for details.
Table 2 shows that the convolutional cells discovered with CIFAR-10 generalize well to ImageNet problems.
In particular, each model based on the convolutional cells exceeds the predictive performance of the corresponding hand-designed model. Importantly, the largest model achieves a new state-of-the-art performance for ImageNet (82.7%) based on single, non-ensembled predictions, surpassing previous best published result by ∼1.2%.
Among the unpublished works, our model is on par with the best reported result of 82.7%, while having significantly fewer floating point operations. Figure 5 shows a complete summary of our results in comparison with other published results. Note the family of models based on convolutional cells provides an envelope over a broad class of human-invented architectures.
Finally, we test how well the best convolutional cells may perform in a resource-constrained setting, e.g., mobile devices (Table 3). In these settings, the number of floating point operations is severely constrained and predictive performance must be weighed against latency requirements on a device with limited computational resources.
MobileNet and ShuffleNet provide state-of-the-art results obtaining 70.6% and 70.9% accuracy, respectively on
8701 model depth
# params error rate (%)
DenseNet (L = 40, k = 12) 
1.0M
DenseNet(L = 100, k = 12) 
7.0M
DenseNet (L = 100, k = 24) 
27.2M
DenseNet-BC (L = 100, k = 40) 
25.6M
Shake-Shake 26 2x32d 
2.9M
Shake-Shake 26 2x96d 
26.2M
Shake-Shake 26 2x96d + cutout 
26.2M
NAS v3 
7.1M
NAS v3 
37.4M
NASNet-A (6 @ 768)3.3M
NASNet-A (6 @ 768) + cutout3.3M
NASNet-A (7 @ 2304)27.6M
NASNet-A (7 @ 2304) + cutout27.6M
NASNet-B (4 @ 1152)2.6M
NASNet-C (4 @ 640)3.1M
Table 1. Performance of Neural Architecture Search and other state-of-the-art models on CIFAR-10. All results for NASNet are the mean accuracy across 5 runs.
# Mult-Add operations (millions) accuracy (precision @1)
PolyNet
Inception-v1
VGG-16
MobileNet
Inception-v3
Inception-v2
ResNeXt-101
ResNet-152
Inception-v4
Inception-ResNet-v2
Xception
NASNet-A (6 @ 4032)
ShuffleNet
DPN-131
NASNet-A (7 @ 1920)
NASNet-A (5 @ 1538)
NASNet-A (4 @ 1056)
SENet
# parameters (millions) accuracy (precision @1)
NASNet-A (5 @ 1538)
NASNet-A (4 @ 1056)
VGG-16
PolyNet
MobileNet
Inception-v1
ResNeXt-101
Inception-v2
Inception-v4
Inception-ResNet-v2
ResNet-152
Xception
Inception-v3
ShuffleNet
DPN-131
NASNet-A (6 @ 4032)
NASNet-A (7 @ 1920)
SENet
Figure 5. Accuracy versus computational demand (left) and number of parameters (right) across top performing published CNN architectures on ImageNet 2012 ILSVRC challenge prediction task. Computational demand is measured in the number of floating-point multiplyadd operations to process a single image. Black circles indicate previously published results and red squares highlight our proposed models.
224x224 images using ∼550M multliply-add operations.
An architecture constructed from the best convolutional cells achieves superior predictive performance (74.0% accuracy) surpassing previous models but with comparable computational demand.
In summary, we find that the learned convolutional cells are flexible across model scales achieving state-of-the-art performance across almost 2 orders of magnitude in computational budget.
4.3. Improved features for object detection
Image classification networks provide generic image features that may be transferred to other computer vision problems. One of the most important problems is the spatial localization of objects within an image.
To further validate the performance of the family of NASNet-A networks, we test whether object detection systems derived from NASNet-A lead to improvements in object detection.
To address this question, we plug in the family of NASNet-A networks pretrained on ImageNet into the Faster-RCNN object detection pipeline using an opensource software platform. We retrain the resulting object detection pipeline on the combined COCO training plus validation dataset excluding 8,000 mini-validation images.
Model image size
# parameters
Mult-Adds
Top 1 Acc. (%)
Top 5 Acc. (%)
Inception V2 
224×224
11.2 M
1.94 B
NASNet-A (5 @ 1538)
299×299
10.9 M
2.35 B
Inception V3 
299×299
23.8 M
5.72 B
Xception 
299×299
22.8 M
8.38 B
Inception ResNet V2 
299×299
55.8 M
13.2 B
NASNet-A (7 @ 1920)
299×299
22.6 M
4.93 B
ResNeXt-101 (64 x 4d) 
320×320
83.6 M
31.5 B
PolyNet 
331×331
92 M
34.7 B
DPN-131 
320×320
79.5 M
32.0 B
SENet 
320×320
145.8 M
42.3 B
NASNet-A (6 @ 4032)
331×331
88.9 M
23.8 B
Table 2. Performance of architecture search and other published state-of-the-art models on ImageNet classification. Mult-Adds indicate the number of composite multiply-accumulate operations for a single image. Note that the composite multiple-accumulate operations are calculated for the image size reported in the table. Model size for calculated from open-source implementation.
Model
# parameters
Mult-Adds
Top 1 Acc. (%)
Top 5 Acc. (%)
Inception V1 
6.6M
1,448 M
69.8 †
MobileNet-224 
4.2 M
569 M
ShuffleNet (2x) 
∼ 5M
524 M
NASNet-A (4 @ 1056)
5.3 M
564 M
NASNet-B (4 @ 1536)
5.3M
488 M
NASNet-C (3 @ 960)
4.9M
558 M
Table 3. Performance on ImageNet classification on a subset of models operating in a constrained computational setting, i.e., < 1.5 B multiply-accumulate operations per image. All models use 224x224 images. † indicates top-1 accuracy not reported in but from open-source implementation.
Model resolution mAP (mini-val) mAP (test-dev)
MobileNet-224 
600 × 600ShuffleNet (2x) 
600 × 600
24.5%†NASNet-A (4 @ 1056)
600 × 600ResNet-101-FPN 
800 (short side)Inception-ResNet-v2 (G-RMI) 
600 × 600
Inception-ResNet-v2 (TDM) 
600 × 1000
NASNet-A (6 @ 4032)
800 × 800
NASNet-A (6 @ 4032)
1200 × 1200
ResNet-101-FPN (RetinaNet) 
800 (short side)Table 4. Object detection performance on COCO on mini-val and test-dev datasets across a variety of image featurizations. All results are with the Faster-RCNN object detection framework from a single crop of an image. Top rows highlight mobile-optimized image featurizations, while bottom rows indicate computationally heavy image featurizations geared towards achieving best results. All mini-val results employ the same 8K subset of validation images in.
We perform single model evaluation using 300-500 RPN proposals per image. In other words, we only pass a single image through a single network. We evaluate the model on the COCO mini-val and test-dev dataset and report the mean average precision (mAP) as computed with the standard COCO metric library. We perform a simple search over learning rate schedules to identify the best possible model. Finally, we examine the behavior of two object detection systems employing the best performing NASNetA image featurization (NASNet-A, 6 @ 4032) as well as the image featurization geared towards mobile platforms(NASNet-A, 4 @ 1056).
For the mobile-optimized network, our resulting system achieves a mAP of 29.6% – exceeding previous mobileoptimized networks that employ Faster-RCNN by over 5.0% (Table 4). For the best NASNet network, our resulting
8703 network operating on images of the same spatial resolution(800 × 800) achieves mAP = 40.7%, exceeding equivalent object detection systems based off lesser performing image featurization (i.e. Inception-ResNet-v2) by 4.0% (see Appendix for example detections on images and sideby-side comparisons). Finally, increasing the spatial resolution of the input image results in the best reported, single model result for object detection of 43.1%, surpassing the best previous best by over 4.0%.2 These results provide further evidence that NASNet provides superior, generic image features that may be transferred across other computer vision tasks. Figure 10 and Figure 11 in Appendix C show four examples of object detection results produced by
NASNet-A with the Faster-RCNN framework.
4.4. Efficiency of architecture search methods
Number of Models Sampled
Accuracy at 20 Epochs
RL Top 1 Unique Models
RL Top 5 Unique Models
RL Top 25 Unique Models
RS Top 1 Unique Models
RS Top 5 Unique Models
RS Top 25 Unique Models
Figure 6. Comparing the efficiency of random search (RS) to reinforcement learning (RL) for learning neural architectures. The x-axis measures the total number of model architectures sampled, and the y-axis is the validation performance on CIFAR-10 after 20 epochs of training.
Though what search method to use is not the focus of the paper, an open question is how effective is the reinforcement learning search method. In this section, we study the effectiveness of reinforcement learning for architecture search on the CIFAR-10 image classification problem and compare it to brute-force random search (considered to be a very strong baseline for black-box optimization ) given an equivalent amount of computational resources.
Figure 6 shows the performance of reinforcement learning (RL) and random search (RS) as more model architec2A primary advance in the best reported object detection system is the introduction of a novel loss. Pairing this loss with NASNet-A image featurization may lead to even further performance gains. Additionally, performance gains are achievable through ensembling multiple inferences across multiple model instances and image crops (e.g., ). tures are sampled. Note that the best model identified with
RL is significantly better than the best model found by RS by over 1% as measured by on CIFAR-10. Additionally, RL finds an entire range of models that are of superior quality to random search. We observe this in the mean performance of the top-5 and top-25 models identified in RL versus RS.
We take these results to indicate that although RS may provide a viable search strategy, RL finds better architectures in the NASNet search space.
5. Conclusion
In this work, we demonstrate how to learn scalable, convolutional cells from data that transfer to multiple image classification tasks. The learned architecture is quite flexible as it may be scaled in terms of computational cost and parameters to easily address a variety of problems. In all cases, the accuracy of the resulting model exceeds all human-designed models – ranging from models designed for mobile applications to computationally-heavy models designed to achieve the most accurate results.
The key insight in our approach is to design a search space that decouples the complexity of an architecture from the depth of a network. This resulting search space permits identifying good architectures on a small dataset (i.e., CIFAR-10) and transferring the learned architecture to image classifications across a range of data and computational scales.
The resulting architectures approach or exceed stateof-the-art performance in both CIFAR-10 and ImageNet datasets with less computational demand than humandesigned architectures.
The ImageNet results are particularly important because many state-of-theart computer vision problems (e.g., object detection, face detection, image localization ) derive image features or architectures from ImageNet classification models.
For instance, we find that image features obtained from ImageNet used in combination with the FasterRCNN framework achieves state-of-the-art object detection results. Finally, we demonstrate that we can use the resulting learned architecture to perform ImageNet classification with reduced computational budgets that outperform streamlined architectures targeted to mobile and embedded platforms.
References
 M. Andrychowicz, M. Denil, S. Gomez, M. W. Hoffman, D. Pfau, T. Schaul, and N. de Freitas. Learning to learn by gradient descent by gradient descent. In Advances in Neural
Information Processing Systems, pages 3981–3989, 2016. 2
 J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. 12
 B. Baker, O. Gupta, N. Naik, and R. Raskar. Designing neural network architectures using reinforcement learning. In In8704 ternational Conference on Learning Representations, 2016.
 J. Bergstra, R. Bardenet, Y. Bengio, and B. K´egl.
Algorithms for hyper-parameter optimization. In Neural Information Processing Systems, 2011. 2
 J. Bergstra and Y. Bengio.
Random search for hyperparameter optimization. Journal of Machine Learning Research, 2012. 2, 8
 J. Bergstra, D. Yamins, and D. D. Cox. Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures. International Conference on Machine Learning, 2013. 2
 J. Chen, R. Monga, S. Bengio, and R. Jozefowicz. Revisiting distributed synchronous sgd. In International Conference on
Learning Representations Workshop Track, 2016. 12
 Y. Chen, J. Li, H. Xiao, X. Jin, S. Yan, and J. Feng. Dual path networks. arXiv preprint arXiv:1707.01083, 2017. 5, 7
 F. Chollet. Xception: Deep learning with depthwise separable convolutions. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, 2017. 2, 7
 D.-A. Clevert, T. Unterthiner, and S. Hochreiter. Fast and accurate deep network learning by exponential linear units(elus). In International Conference on Learning Representations, 2016. 11
 J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. FeiFei. Imagenet: A large-scale hierarchical image database. In
IEEE Conference on Computer Vision and Pattern Recognition. IEEE, 2009. 1, 12
 T. DeVries and G. W. Taylor. Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017. 5, 6
 J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and T. Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. In International Conference on Machine Learning, volume 32, pages
647–655, 2014. 6
 Y. Duan, J. Schulman, X. Chen, P. L. Bartlett, I. Sutskever, and P. Abbeel. RL2: Fast reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, C. Finn, P. Abbeel, and S. Levine. Model-agnostic metalearning for fast adaptation of deep networks. In International Conference on Machine Learning, 2017. 2
 D. Floreano, P. D¨urr, and C. Mattiussi. Neuroevolution: from architectures to learning. Evolutionary Intelligence, 2008. 2
 K. Fukushima. A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. Biological Cybernetics, page 93202, 1980. 1
 X. Gastaldi. Shake-shake regularization of 3-branch residual networks. In International Conference on Learning Representations Workshop Track, 2017. 6, 12
 D. Ha, A. Dai, and Q. V. Le. Hypernetworks. In International Conference on Learning Representations, 2017. 2
 K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition.
In IEEE Conference on Computer
Vision and Pattern Recognition, 2016. 1, 2, 3, 4
 K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. In European Conference on Computer Vision, 2016. 11
 S. Hochreiter and J. Schmidhuber. Long short-term memory.
Neural Computation, 1997. 2, 11
 S. Hochreiter, A. Younger, and P. Conwell. Learning to learn using gradient descent.
Artificial Neural Networks, pages
87–94, 2001. 2
 A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and H. Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017. 2, 5, 7, 8, J. Hu, L. Shen, and G. Sun.
Squeeze-and-excitation networks. arXiv preprint arXiv:1709.01507, 2017. 5, 7
 G. Huang, Z. Liu, and K. Q. Weinberger. Densely connected convolutional networks. In IEEE Conference on Computer
Vision and Pattern Recognition, 2017. 6
 G. Huang, Y. Sun, Z. Liu, D. Sedra, and K. Weinberger. Deep networks with stochastic depth. In European Conference on
Computer Vision, 2016. 11
 J. Huang, V. Rathod, C. Sun, M. Zhu, A. Korattikara, A. Fathi, I. Fischer, Z. Wojna, Y. Song, S. Guadarrama, et al.
Speed/accuracy trade-offs for modern convolutional object detectors. In IEEE Conference on Computer Vision and Pattern Recognition, 2017. 6, 7, 8, 14
 S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift.
In International Conference on Learning Representations, R. Jozefowicz, W. Zaremba, and I. Sutskever. An empirical exploration of recurrent network architectures. In International Conference on Learning Representations, 2015. 2
 A. Krizhevsky.
Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009.
 A. Krizhevsky, I. Sutskever, and G. E. Hinton.
Imagenet classification with deep convolutional neural networks. In
Advances in Neural Information Processing System, 2012.
 G. Larsson, M. Maire, and G. Shakhnarovich. Fractalnet:
Ultra-deep neural networks without residuals. arXiv preprint arXiv:1605.07648, 2016. 4, 11
 Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradientbased learning applied to document recognition. Proceedings of the IEEE, 1998. 1
 K. Li and J. Malik. Learning to optimize neural nets. arXiv preprint arXiv:1703.00441, 2017. 2
 T.-Y. Lin, P. Doll´ar, R. Girshick, K. He, B. Hariharan, and S. Belongie. Feature pyramid networks for object detection.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017. 7
 T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Doll´ar.
Focal loss for dense object detection. arXiv preprint arXiv:1708.02002, 2017. 7, 8
 T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll´ar, and C. L. Zitnick. Microsoft coco: Com8705 mon objects in context. In European Conference on Computer Vision, pages 740–755. Springer, 2014. 7
 I. Loshchilov and F. Hutter. SGDR: Stochastic gradient descent with warm restarts.
In International Conference on
Learning Representations, 2017. 12
 H. Mendoza, A. Klein, M. Feurer, J. T. Springenberg, and F. Hutter. Towards automatically-tuned neural networks. In
Proceedings of the 2016 Workshop on Automatic Machine
Learning, pages 58–65, 2016. 2
 T. Miconi.
Neural networks with differentiable structure. arXiv preprint arXiv:1606.06216, 2016. 2
 R. Miikkulainen, J. Liang, E. Meyerson, A. Rawal, D. Fink, O. Francon, B. Raju, A. Navruzyan, N. Duffy, and B. Hodjat.
Evolving deep neural networks. arXiv preprint arXiv:1703.00548, 2017. 2
 R. Negrinho and G. Gordon. DeepArchitect: Automatically designing and training deep architectures. arXiv preprint arXiv:1704.08792, 2017. 2
 N. Pinto, D. Doukhan, J. J. DiCarlo, and D. D. Cox. A highthroughput screening approach to discovering good forms of biologically inspired visual representation. PLoS Computational Biology, 5(11):e1000579, 2009. 2
 S. Ravi and H. Larochelle. Optimization as a model for fewshot learning. In International Conference on Learning Representations, 2017. 2
 E. Real, S. Moore, A. Selle, S. Saxena, Y. L. Suematsu, Q. Le, and A. Kurakin. Large-scale evolution of image classifiers. In International Conference on Machine Learning, S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: Towards real-time object detection with region proposal networks. In Advances in Neural Information Processing Systems, pages 91–99, 2015. 2, 6, 7
 S. Saxena and J. Verbeek. Convolutional neural fabrics. In
Advances in Neural Information Processing Systems, 2016.
 T. Schaul and J. Schmidhuber. Metalearning. Scholarpedia, F. Schroff, D. Kalenichenko, and J. Philbin. Facenet: A unified embedding for face recognition and clustering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 815–823, 2015. 8
 J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 4, 11
 A. Shrivastava, R. Sukthankar, J. Malik, and A. Gupta. Beyond skip connections: Top-down modulation for object detection. arXiv preprint arXiv:1612.06851, 2016. 7, 8
 K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In International
Conference on Learning Representations, 2015. 1, 2, 3, 4
 J. Snoek, H. Larochelle, and R. P. Adams. Practical Bayesian optimization of machine learning algorithms. In Neural Information Processing Systems, 2012. 2
 J. Snoek, O. Rippel, K. Swersky, R. Kiros, N. Satish, N. Sundaram, M. Patwary, M. Ali, R. P. Adams, et al. Scalable
Bayesian optimization using deep neural networks. In International Conference on Machine Learning, 2015. 2
 N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning
Research, 15(1):1929–1958, 2014. 11
 K. O. Stanley, D. B. D'Ambrosio, and J. Gauci.
A hypercube-based encoding for evolving large-scale neural networks. Artificial Life, 2009. 2
 C. Szegedy, S. Ioffe, V. Vanhoucke, and A. Alemi. Inceptionv4, Inception-Resnet and the impact of residual connections on learning. In International Conference on Learning Representations Workshop Track, 2016. 1, 2, 3, 4, 7
 C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions.
In IEEE Conference on
Computer Vision and Pattern Recognition, 2015.
 C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna.
Rethinking the Inception architecture for computer vision. In
IEEE Conference on Computer Vision and Pattern Recognition, 2016. 1, 2, 3, 4, 5, 7, 8, 12
 D. Ulyanov, A. Vedaldi, and V. Lempitsky. Instance normalization: The missing ingredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016. 12
 J. X. Wang, Z. Kurth-Nelson, D. Tirumala, H. Soyer, J. Z. Leibo, R. Munos, C. Blundell, D. Kumaran, and M. Botvinick.
Learning to reinforcement learn. arXiv preprint arXiv:1611.05763, 2016. 2
 T. Weyand, I. Kostrikov, and J. Philbin. Planet-photo geolocation with convolutional neural networks. In European
Conference on Computer Vision, 2016. 8
 O. Wichrowska, N. Maheswaranathan, M. W. Hoffman, S. G.
Colmenarejo, M. Denil, N. de Freitas, and J. Sohl-Dickstein.
Learned optimizers that scale and generalize. arXiv preprint arXiv:1703.04813, 2017. 2
 D. Wierstra, F. J. Gomez, and J. Schmidhuber. Modeling systems with internal state using evolino. In The Genetic and Evolutionary Computation Conference, 2005. 2
 R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. In Machine
Learning, 1992. 11
 L. Xie and A. Yuille.
Genetic CNN. arXiv preprint arXiv:1703.01513, 2017. 2
 S. Xie, R. Girshick, P. Doll´ar, Z. Tu, and K. He. Aggregated residual transformations for deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017. 1, 2, 7
 X. Zhang, Z. Li, C. C. Loy, and D. Lin. Polynet: A pursuit of structural diversity in very deep networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 2017. 5, 7, 8, 11
 X. Zhang, X. Zhou, L. Mengxiao, and J. Sun. Shufflenet: An extremely efficient convolutional neural network for mobile devices. arXiv preprint arXiv:1707.01083, 2017. 2, 5, 7, 8
 B. Zoph and Q. V. Le. Neural architecture search with reinforcement learning. In International Conference on Learning
Representations, 2017. 1, 2, 4, 6, 11
Appendix
A. Experimental Details
A.1. Dataset for Architecture Search
The CIFAR-10 dataset consists of 60,000 32x32
RGB images across 10 classes (50,000 train and 10,000 test images). We partition a random subset of 5,000 images from the training set to use as a validation set for the controller RNN. All images are whitened and then undergone several data augmentation steps: we randomly crop 32x32 patches from upsampled images of size 40x40 and apply random horizontal flips. This data augmentation procedure is common among related work.
A.2. Controller architecture
The controller RNN is a one-layer LSTM with 100 hidden units at each layer and 2 × 5B softmax predictions for the two convolutional cells (where B is typically 5) associated with each architecture decision. Each of the 10B predictions of the controller RNN is associated with a probability. The joint probability of a child network is the product of all probabilities at these 10B softmaxes. This joint probability is used to compute the gradient for the controller
RNN. The gradient is scaled by the validation accuracy of the child network to update the controller RNN such that the controller assigns low probabilities for bad child networks and high probabilities for good child networks.
Unlike, who used the REINFORCE rule to update the controller, we employ Proximal Policy Optimization (PPO) with learning rate 0.00035 because training with PPO is faster and more stable. To encourage exploration we also use an entropy penalty with a weight of 0.00001. In our implementation, the baseline function is an exponential moving average of previous rewards with a weight of 0.95. The weights of the controller are initialized uniformly between -0.1 and 0.1.
A.3. Training of the Controller
For distributed training, we use a workqueue system where all the samples generated from the controller RNN are added to a global workqueue. A free "child" worker in a distributed worker pool asks the controller for new work from the global workqueue. Once the training of the child network is complete, the accuracy on a held-out validation set is computed and reported to the controller RNN.
In our experiments we use a child worker pool size of 450, which means there are 450 networks being trained on 450
GPUs concurrently at any time. Upon receiving enough child model training results, the controller RNN will perform a gradient update on its weights using PPO and then sample another batch of architectures that go into the global workqueue. This process continues until a predetermined number of architectures have been sampled. In our experiments, this predetermined number of architectures is 20,000 which means the search process is terminated after 20,000 child models have been trained. Additionally, we update the controller RNN with minibatches of 20 architectures. Once the search is over, the top 250 architectures are then chosen to train until convergence on CIFAR-10 to determine the very best architecture.
A.4. Details of architecture search space
We performed preliminary experiments to identify a flexible, expressive search space for neural architectures that learn effectively. Generally, our strategy for preliminary experiments involved small-scale explorations to identify how to run large-scale architecture search.
• All convolutions employ ReLU nonlinearity. Experiments with ELU nonlinearity showed minimal benefit.
• To ensure that the shapes always match in convolutional cells, 1x1 convolutions are inserted as necessary.
• Unlike, all depthwise separable convolution do not employ Batch Normalization and/or a ReLU between the depthwise and pointwise operations.
• All convolutions followed an ordering of ReLU, convolution operation and Batch Normalization following.
• Whenever a separable convolution is selected as an operation by the model architecture, the separable convolution is applied twice to the hidden state. We found this empirically to improve overall performance.
A.5. Training with ScheduledDropPath
We performed several experiments with various stochastic regularization methods. Naively applying dropout across convolutional filters degraded performance. However, we discovered a new technique called ScheduledDropPath, a modified version of DropPath, that works well in regularizing NASNets. In DropPath, we stochastically drop out each path (i.e., edge with a yellow box in Figure
4) in the cell with some fixed probability. This is similar to and where they dropout full parts of their model during training and then at test time scale the path by the probability of keeping that path during training. Interestingly we also found that DropPath alone does not help
NASNet training much, but DropPath with linearly increasing the probability of dropping out a path over the course of training significantly improves the final performance for both CIFAR and ImageNet experiments.
We name this method ScheduledDropPath.
Figure 7. Schematic diagram of the NASNet search space. Network motifs are constructed recursively in stages termed blocks. Each block consists of the controller selecting a pair of hidden states (dark gray), operations to perform on those hidden states (yellow) and a combination operation (green). The resulting hidden state is retained in the set of potential hidden states to be selected on subsequent blocks.
A.6. Training of CIFAR models
All of our CIFAR models use a single period cosine decay as in. All models use the momentum optimizer with momentum rate set to 0.9. All models also use L2 weight decay. Each architecture is trained for a fixed 20 epochs on CIFAR-10 during the architecture search process.
Additionally, we found it beneficial to use the cosine learning rate decay during the 20 epochs the CIFAR models were trained as this helped to further differentiate good architectures. We also found that having the CIFAR models use a small N = 2 during the architecture search process allowed for models to train quite quickly, while still finding cells that work well once more were stacked.
A.7. Training of ImageNet models
We use ImageNet 2012 ILSVRC challenge data for large scale image classification. The dataset consists of ∼ 1.2M images labeled across 1,000 classes. Overall our training and testing procedures are almost identical to. ImageNet models are trained and evaluated on 299x299 or 331x331 images using the same data augmentation procedures as described previously. We use distributed synchronous SGD to train the ImageNet model with 50 workers (and 3 backup workers) each with a Tesla K40 GPU.
We use RMSProp with a decay of 0.9 and epsilon of 1.0.
Evaluations are calculated using with a running average of parameters over time with a decay rate of 0.9999. We use label smoothing with a value of 0.1 for all ImageNet models as done in. Additionally, all models use an auxiliary classifier located at 2/3 of the way up the network. The loss of the auxiliary classifier is weighted by 0.4 as done in.
We empirically found our network to be insensitive to the number of parameters associated with this auxiliary classifier along with the weight associated with the loss. All models also use L2 regularization. The learning rate decay scheme is the exponential decay scheme used in.
Dropout is applied to the final softmax matrix with probability 0.5.
B. Additional Experiments
We now present two additional cells that performed well on CIFAR and ImageNet. The search spaces used for these cells are slightly different than what was used for NASNetA. For the NASNet-B model in Figure 8 we do not concatenate all of the unused hidden states generated in the convolutional cell. Instead all of the hiddenstates created within the convolutional cell, even if they are currently used, are fed into the next layer. Note that B = 4 and there are 4 hiddenstates as input to the cell as these numbers must match for this cell to be valid. We also allow addition followed by layer normalization or instance normalization to be predicted as two of the combination operations within the cell, along with addition or concatenation.
For NASNet-C (Figure 9), we concatenate all of the unused hidden states generated in the convolutional cell like in NASNet-A, but now we allow the prediction of addition followed by layer normalization or instance normalization like in NASNet-B.
Figure 8. Architecture of NASNet-B convolutional cell with B =
4 blocks identified with CIFAR-10. The input (white) is the hidden state from previous activations (or input image). Each convolutional cell is the result of B blocks. A single block is corresponds to two primitive operations (yellow) and a combination operation(green). As do we not concatenate the output hidden states, each output hidden state is used as a hidden state in the future layers.
Each cell takes in 4 hidden states and thus needs to also create 4 output hidden states. Each output hidden state is therefore labeled with 0, 1, 2, 3 to represent the next four layers in that order.
C. Example object detection results
Finally, we will present examples of object detection results on the COCO dataset in Figure 10 and Figure 11.
As can be seen from the figures, NASNet-A featurization works well with Faster-RCNN and gives accurate localization of objects.
Figure 9. Architecture of NASNet-C convolutional cell with B =
4 blocks identified with CIFAR-10. The input (white) is the hidden state from previous activations (or input image). The output(pink) is the result of a concatenation operation across all resulting branches. Each convolutional cell is the result of B blocks. A single block corresponds to two primitive operations (yellow) and a combination operation (green).
Figure 10. Example detections showing improvements of object detection over previous state-of-the-art model for Faster-RCNN with Inception-ResNet-v2 featurization (top) and NASNet-A featurization (bottom).
Figure 11. Example detections of best performing NASNet-A featurization with Faster-RCNN trained on COCO dataset. Top and middle images courtesy of http://wikipedia.org. Bottom image courtesy of Jonathan HuangDeViSE: A Deep Visual-Semantic Embedding Model
Andrea Frome*, Greg S. Corrado*, Jonathon Shlens*, Samy Bengio
Jeffrey Dean, Marc'Aurelio Ranzato, Tomas Mikolov
* These authors contributed equally.
{afrome, gcorrado, shlens, bengio, jeff, ranzato†, tmikolov}@google.com
Google, Inc.
Mountain View, CA, USA
Abstract
Modern visual recognition systems are often limited in their ability to scale to large numbers of object categories. This limitation is in part due to the increasing difficulty of acquiring sufficient training data in the form of labeled images as the number of object categories grows. One remedy is to leverage data from other sources – such as text data – both to train visual models and to constrain their predictions. In this paper we present a new deep visual-semantic embedding model trained to identify visual objects using both labeled image data as well as semantic information gleaned from unannotated text. We demonstrate that this model matches state-of-the-art performance on the 1000-class ImageNet object recognition challenge while making more semantically reasonable errors, and also show that the semantic information can be exploited to make predictions about tens of thousands of image labels not observed during training. Semantic knowledge improves such zero-shot predictions achieving hit rates of up to 18% across thousands of novel labels never seen by the visual model.
Introduction
The visual world is populated with a vast number of objects, the most appropriate labeling of which is often ambiguous, task specific, or admits multiple equally correct answers. Yet state-of-theart vision systems attempt to solve recognition tasks by artificially assigning images to a small number of rigidly defined classes. This has led to building labeled image data sets according to these artificial categories and in turn to building visual recognition systems based on N-way discrete classifiers. While growing the number of labels and labeled images has improved the utility of visual recognition systems, scaling such systems beyond a limited number of discrete categories remains an unsolved problem. This problem is exacerbated by the fact that N-way discrete classifiers treat all labels as disconnected and unrelated, resulting in visual recognition systems that cannot transfer semantic information about learned labels to unseen words or phrases. One way of dealing with this issue is to respect the natural continuity of visual space instead of artificially partitioning it into disjoint categories.
We propose an approach that addresses these shortcomings by training a visual recognition model with both labeled images and a comparatively large and independent dataset – semantic information from unannotated text data. This deep visual-semantic embedding model (DeViSE) leverages textual data to learn semantic relationships between labels, and explicitly maps images into a rich semantic embedding space. We show that this model performs comparably to state-of-the-art visual object classifiers when trained and evaluated on flat 1-of-N metrics, while simultaneously making fewer semantically unreasonable mistakes along the way. Furthermore, we show that the model leverages
†Current affiliation: Facebook, Inc.
1 visual and semantic similarity to correctly predict object category labels for unseen categories, i.e.
"zero-shot" classification, even when the number of unseen visual categories is 20,000 for a model trained on just 1,000 categories.
Previous Work
The current state-of-the-art approach to image classification is a deep convolutional neural network trained with a softmax output layer (i.e. multinomial logistic regression) that has as many units as the number of classes (see, for instance ). However, as the number of classes grows, the distinction between classes blurs, and it becomes increasingly difficult to obtain sufficient numbers of training images for rare concepts.
One solution to this problem, termed WSABIE, is to train a joint embedding model of both images and labels, by employing an online learning-to-rank algorithm. The proposed model contained two sets of parameters: (1) a linear mapping from image features to the joint embedding space, and(2) an embedding vector for each possible label. Compared to the proposed approach, WSABIE only explored linear mappings from image features to the embedding space, and the available labels were only those provided in the image training set. It could thus not generalize to new classes.
More recently, Socher et al presented a model for zero-shot learning where a deep neural network was first trained in an unsupervised manner from many images in order to obtain a rich image representation ; in parallel, a neural network language model was trained in order to obtain embedding representations for thousands of common terms. The authors trained a linear mapping between the image representations and the word embeddings representing 8 classes for which they had labeled images, thus linking the image representation space to the embedding space.
This last step was performed using a mean-squared error criterion. They also trained a simple model to determine if a given image was from any of the 8 original classes or not (i.e., an outlier detector).
When the model determined an image to be in the set of 8 classes, a separately trained softmax model was used to perform the 8-way classification; otherwise the model predicted the nearest class in the embedding space (in their setting, only 2 outlier classes were considered). Their model differs from our proposed approach in several ways: first and foremost, the scale, as our model considers
1,000 known classes for the image model and up to 20,000 unknown classes, instead of respectively
8 and 2; second, in there is an inherent trade-off between the quality of predictions for trained and outlier classes; third, by using a different visual model, different language model, and different training objective, we were able to train a single unified model that uses only embeddings.
There has been other recent work showing impressive zero-shot performance on visual recognition tasks, however all of these rely on a curated source of semantic information for the labels: the WordNet hierarchy is used in and, and uses a knowledge base containing descriptive properties for each class. By contrast, our approach learns its semantic representation directly from unannotated data.
Proposed Approach
Our objective is to leverage semantic knowledge learned in the text domain, and transfer it to a model trained for visual object recognition. We begin by pre-training a simple neural language model wellsuited for learning semantically-meaningful, dense vector representations of words. In parallel, we pre-train a state-of-the-art deep neural network for visual object recognition, complete with a traditional softmax output layer. We then construct a deep visual-semantic model by taking the lower layers of the pre-trained visual object recognition network and re-training them to predict the vector representation of the image label text as learned by the language model. These three training phases are detailed below.
Language Model Pre-training
The skip-gram text modeling architecture introduced by Mikolov et al has been shown to efficiently learn semantically-meaningful floating point representations of terms from unannotated text. The model learns to represent each term as a fixed length embedding vector by predicting adjacent terms in the document (Figure 1a, right). We call these vector representations embedding
2 transportation dogs birds musical instruments aquatic life insects animals clothing food reptiles embedding vector lookup table embedding vector lookup table similarity metric label image core visual model transformation
Deep Visual Semantic
Embedding Model image label softmax layer core visual model
Traditional
Visual Model source word nearby word softmax layer
Skip-gram
Language Model parameter initialization parameter initialization
A
B
Figure 1: (a) Left: a visual object categorization network with a softmax output layer; Right: a skip-gram language model; Center: our joint model, which is initialized with parameters pre-trained at the lower layers of the other two models. (b) t-SNE visualization of a subset of the ILSVRC 2012 1K label embeddings learned using skip-gram. vectors. Because synonyms tend to appear in similar contexts, this simple objective function drives the model to learn similar embedding vectors for semantically related words.
We trained a skip-gram text model on a corpus of 5.7 million documents (5.4 billion words) extracted from wikipedia.org. The text of the web pages was tokenized into a lexicon of roughly 155,000 single- and multi-word terms consisting of common English words and phrases as well as terms from commonly used visual object recognition datasets. Our skip-gram model used a hierarchical softmax layer for predicting adjacent terms and was trained using a 20-word window with a single pass through the corpus. For more details and a pointer to open-source code, see.
We trained skip-gram models of varying hidden dimensions, ranging from 100-D to 2,000-D, and found 500- and 1,000-D embeddings to be a good compromise between training speed, semantic quality, and the ultimate performance of the DeViSE model described below. The semantic quality of the embedding representations learned by these models is impressive.1 A visualization of the language embedding space over a subset of ImageNet labels indicates that the language model learned a rich semantic structure that could be exploited in vision tasks (Figure 1b).
Visual Model Pre-training
The visual model architecture we employ is based on the winning model for the 1,000-class ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012. The deep neural network model consists of several convolutional filtering, local contrast normalization, and max-pooling layers, followed by several fully connected neural network layers trained using the dropout regularization technique. We trained this model with a softmax output layer, as described in, to predict one of 1,000 object categories from the ILSVRC 2012 1K dataset, and were able to reproduce their results. This trained model serves both as our benchmark for performance comparisons, as well as the initialization for our joint model.
Deep Visual-Semantic Embedding Model
Our deep visual-semantic embedding model (DeViSE) is initialized from these two pre-trained neural network models (Figure 1a). The embedding vectors learned by the language model are unit normed and used to map label terms into target vector representations2.
The core visual model, with its softmax prediction layer now removed, is trained to predict these vectors for each image, by means of a projection layer and a similarity metric. The projection layer is a linear transformation that maps the 4,096-D representation at the top of our core visual model into the 500- or 1,000-D representation native to our language model.
1For example, the 9 nearest terms to tiger shark using cosine distance are bull shark, blacktip shark, shark, oceanic whitetip shark, sandbar shark, dusky shark, blue shark, requiem shark, and great white shark. The 9 nearest terms to car are cars, muscle car, sports car, compact car, automobile, racing car, pickup truck, dealership, and sedans.
2In, which introduced the skip-gram model for text, cosine similarity between vectors is used for measuring semantic similarity. Unit-norming the vectors and using dot product similarity is an equivalent similarity measurement.
The choice of loss function proved to be important. We used a combination of dot-product similarity and hinge rank loss (similar to ) such that the model was trained to produce a higher dot-product similarity between the visual model output and the vector representation of the correct label than between the visual output and other randomly chosen text terms. We defined the per training example hinge rank loss: loss(image, label) =
� j̸=label max[0, margin − ⃗tlabelM⃗v(image) + ⃗tjM⃗v(image)](1) where ⃗v(image) is a column vector denoting the output of the top layer of our core visual network for the given image, M is the matrix of trainable parameters in the linear transformation layer, ⃗tlabel is a row vector denoting learned embedding vector for the provided text label, and ⃗tj are the embeddings of other text terms. In practice, we found that it was expedient to randomize the algorithm both by (1) restricting the set of false text terms to possible image labels, and (2) truncating the sum after the first margin-violating false term was encountered. The ⃗t vectors were constrained to be unit norm, and a fixed margin of 0.1 was used in all experiments3. We also experimented with an L2 loss between visual and label embeddings, as suggested by Socher et al., but that consistently yielded about half the accuracy of the rank loss model. We believe this is because the nearest neighbor evaluation is fundamentally a ranking problem and is best solved with a ranking loss, whereas the L2 loss only aims to make the vectors close to one another but remains agnostic to incorrect labels that are closer to the target image.
The DeViSE model was trained by asynchronous stochastic gradient descent on a distributed computing platform described in. As above, the model was presented only with images drawn from the ILSVRC 2012 1K training set, but now trained to predict the term strings as text4. The parameters of the projection layer M were first trained while holding both the core visual model and the text representation fixed. In the later stages of training the derivative of the loss function was backpropagated into the core visual model to fine-tune its output5, which typically improved accuracy by 1-3% (absolute). Adagrad per-parameter dynamic learning rates were utilized to keep gradients well scaled at the different layers of the network.
At test time, when a new image arrives, one first computes its vector representation using the visual model and the transformation layer; then one needs to look for the nearest labels in the embedding space. This last step can be done efficiently using either a tree or a hashing technique, in order to be faster than the naive linear search approach (see for instance ). The nearest labels are then mapped back to ImageNet synsets for scoring (see Section A.2 for details).
Results
The goals of this work are to develop a vision model that makes semantically relevant predictions even when it makes errors and that generalizes to classes outside of its labeled training set, i.e. zeroshot learning. We compare DeViSE to two models that employ the same high-quality core vision model, but lack the semantic structure imparted by our language model: (1) a softmax baseline model – a state-of-the-art vision model which employs a 1000-way softmax classifier; (2) a random embedding model – a version of our model that uses random unit-norm embedding vectors in place of those learned by the language model. Both use the trained visual model described in Section 3.2.
In order to demonstrate parity with the softmax baseline on the most commonly-reported metric, we compute "flat" hit@k metrics – the percentage of test images for which the model returns the one true label in its top k predictions. To measure the semantic quality of predictions beyond the true label, we employ a hierarchical precision@k metric based on the label hierarchy provided with the 3The margin was chosen to be a fraction of the norm of the vectors, which is 1.0. A wide range of values would likely work well.
4ImageNet image labels are synsets, a set of synonymous terms, where each term is a word or phrase. We found training the model to predict the first term in each synset to be sufficient, but sampling from the synset terms might work equally well.
5In principle the gradients can also be back-propagated into the vector representations of the text labels. In this case, the language model should continue to train simultaneously in order to maintain the global semantic structure over all terms in the vocabulary.
Flat hit@k (%)
Hierarchical precision@k
Model type dim
Softmax baseline
N/A
DeViSE
Random embeddings
Chance
N/A
Table 1: Comparison of model performance on our test set, taken from the ImageNet ILSVRC 2012 1K validation set. Note that hierarchical precision@1 is equivalent to flat hit@1. See text for details.
ImageNet image repository. In particular, for each true label and value of k, we generate a ground truth list from the semantic hierarchy, and compute a per-example precision equal to the fraction of the model's k predictions that overlap with the ground truth list. We report mean precision across the test set. Detailed descriptions of the generation of the ground truth lists, the hierarchical scoring metric, and train/validation/test dataset splits are provided in Sections A.1 and A.3.
ImageNet (ILSVRC) 2012 1K Results
This section presents flat and hierarchical results on the ILSVRC 2012 1K dataset, where the classes of the examples presented at test time are the same as those used for training. Table 1 shows results for the DeViSE model for 500- and 1000-dimensional skip-gram models compared to the random embedding and softmax baseline models, on both the flat and hierarchical metrics.6
On the flat metric, the softmax baseline shows higher accuracy for k = 1, 2. At k = 5, 10, the 1000-D DeViSE model has reached parity, and at k = 20 (not shown) it performs slightly better.
We expected the softmax model to be the best performing model on the flat metric, given that its cross-entropy training objective is most well matched to the evaluation metric, and are surprised that the performance of DeViSE is so close to softmax performance.
On the hierarchical metric, the DeViSE models show better semantic generalization than the softmax baseline, especially for larger k. At k = 5, the 500-D DeViSE model shows a 3% relative improvement over the softmax baseline, and at k = 20 almost a 7% relative improvement. This is a surprisingly large gain, considering that the softmax baseline is a reproduction of the best published model on these data. The gap that exists between the DeViSE model and softmax baseline on the hierarchical metric reflects the benefit of semantic information above and beyond visual similarity. The gap between the DeViSE model and the random embeddings model establishes that the source of the gain is the well-structured embeddings learned by the language model not some other property of our architecture.
Generalization and Zero-Shot Learning
A distinct advantage of our model is its ability to make reasonable inferences about candidate labels it has never visually observed. For example, a DeViSE model trained on images labeled tiger shark, bull shark, and blue shark, but never with images labeled shark, would likely have the ability to generalize to this more coarse-grained descriptor because the language model has learned a representation of the general concept of shark which is similar to all of the specific sharks. Similarly, if tested on images of highly specific classes which the model has never seen before, for example a photo of an oceanic whitecap shark, and asked whether the correct label is more likely oceanic whitecap shark or some other unfamiliar label (say, nuclear submarine), our model stands a fighting chance of guessing correctly because the language model ensures that representation of oceanic whitecap shark is closer to the representation of sharks the model has seen, while the representation of nuclear submarine is closer to those of other sea vessels.
6Note that our softmax baseline results differ from the results in due to a simplification in the evaluation procedure: creates several distorted versions of each test image and aggregates the results for a final label, whereas in our experiments, we evaluate using only the original test image. Our softmax baseline is able to reproduce the performance of the model in when evaluated with the same procedure.
5 barbet patas, hussar monkey,... babbler, cackler titmouse, tit bowerbird, catbird patas, hussar monkey,... proboscis monkey, Nasalis... macaque titi, titi monkey guenon, guenon monkey oboe, hautboy, hautbois bassoon
English horn, cor anglais hook and eye hand reel punching bag, punch bag,... whistle bassoon letter opener, paper knife,... eyepiece, ocular
Polaroid compound lens telephoto lens, zoom lens rangefinder, range finder typewriter keyboard tape player reflex camera
CD player space bar
Our model
Softmax over ImageNet 1K
A
B
C dune buggy, beach buggy searcher beetle,... seeker, searcher, quester
Tragelaphus eurycerus,... bongo, bongo drum warplane, military plane missile projectile, missile sports car, sport car submarine, pigboat, sub,... pot, flowerpot cauliflower guacamole cucumber, cuke broccoli comestible, edible,... dressing, salad dressing
Sicilian pizza vegetable, veggie, veg fruit fruit pineapple pineapple plant, Ananas... sweet orange sweet orange tree,... pineapple, ananas coral fungus artichoke, globe artichoke sea anemone, anemone cardoon
D
E
F
Our model
Softmax over ImageNet 1K
Figure 2: For each image, the top 5 zero-shot predictions of DeViSE+1K from the 2011 21K label set and the softmax baseline model, both trained on ILSVRC 2012 1K. Predictions ordered by decreasing score, with correct predictions in bold. Ground truth: (a) telephoto lens, zoom lens; (b) English horn, cor anglais; (c) babbler, cackler; (d) pineapple, pineapple plant, Ananas comosus; (e) salad bar; (f) spacecraft, ballistic capsule, space vehicle.
Flat hit@k (%)
Data Set
Model
# Candidate
Labels
2-hop
DeViSE-0
DeViSE+1K
3-hop
DeViSE-0
DeViSE+1K
ImageNet 2011 21K
DeViSE-0
DeViSE+1K
Table 2: Flat hit@k performance of DeViSE on ImageNet-based zero-shot datasets of increasing difficulty from top to bottom. DeViSE-0 and DeViSE+1K are the same trained model, but DeViSE-0 is restricted to only predict zero-shot classes, whereas DeViSE+1K predicts both the zero-shot and the 1K training labels. For all, zero-shot classes did not occur in the image training set.
To test this hypothesis, we extracted images from the ImageNet 2011 21K dataset with labels that were not included in the ILSVRC 2012 1K dataset on which DeViSE was trained. These are "zeroshot" data sets in the sense that our model has no visual knowledge of these labels, though embeddings for the labels were learned by the language model. The softmax baseline is only able to predict labels from ILSVRC 2012 1K. The zero-shot experiments were performed with the same trained
500-D DeViSE model used for results in Section 4.1, but it is evaluated in two ways: DeViSE-0 only predicts the zero-shot labels, and DeViSE+1K predicts zero-shot labels and the ILSVRC 2012
1K training labels.
Figure 2 shows label predictions for a handful of selected examples from this dataset to qualitatively illustrate model behavior. Note that DeViSE successfully predicts a wide range of labels outside its training set, and furthermore, the incorrect predictions are generally semantically "close" to the desired label. Figure 2 (a), (b), (c), and (d) show cases where our model makes significantly better top-5 predictions than the softmax-based model. For example, in Figure 2 (a), the DeViSE model is able to predict a number of lens-related labels even though it was not trained on images in any of the predicted categories. Figure 2 (d) illustrates a case where the top softmax prediction is quite good, but where it is unable to generalize to new labels and its remaining predictions are off the mark, while our model's predictions are more plausible. Figure 2 (e) highlights a case where neither model gets the exact true label, but both models are giving plausible labels. Figure 2 (f) shows a case where the softmax model emits more nearly correct labels than the DeViSE model.
To quantify the performance of the model on zero-shot data, we constructed from our ImageNet
2011 21K zero-shot data three test data sets of increasing difficulty based on the image labels' tree distance from the training ILSVRC 2012 1K labels in the ImageNet label hierarchy. The easiest dataset, "2-hop", is comprised of the 1,589 labels that are within two tree hops of the training labels, making them visually and semantically similar to the training set. A more difficult "3-hop" dataset was constructed in the same manner. Finally, we built a third, particularly challenging dataset consisting of all the labels in ImageNet 2011 21K that are not in ILSVRC 2012 1K.
Hierarchical precision@k
Data Set
Model
2-hop
DeViSE-0
DeViSE+1K
Softmax baseline
3-hop
DeViSE-0
DeViSE+1K
Softmax baseline
ImageNet 2011 21K
DeViSE-0
DeViSE+1K
Softmax baseline
Table 3: Hierarchical precision@k results on zero-shot classification. Performance of DeViSE compared to the softmax baseline model across the same datasets as in Table 2. Note that the softmax model can never directly predict the correct label so its precision@1 is 0.
Model
200 labels
1000 labels
DeViSE
Mensink et al. 2012 
Rohrbach et al. 2011 Table 4: Flat hit@5 accuracy on the zero-shot task from. DeViSE experiments were performed with a 500-D model. The model uses a curated hierarchy over labels for zero-shot classification, but without using this information, our model is close in performance on the 200 zero-shot class label task. When the models can predict any of the 1000 labels, we achieve better accuracy, indicating DeViSE has less of a bias toward training classes than. As in, we include a result on a similar task from, though their work used a different set of 200 zero-shot classes.
We again calculated the flat hit@k measure to determine how frequently DeViSE-0 and DeViSE+1K predicted the correct label for each of these data sets (Table 2). DeViSE-0's top prediction was the correct label 6.0% of the time across 1,589 novel labels, and the rate increases with k to 36.4% within the top 20 predictions. As the zero-shot data sets become more difficult, the accuracy decreases in absolute terms, though it is better relative to chance (not shown). Since a traditional softmax visual model can never produce the correct label on zero-shot data, its performance would be 0% for all k. The DeViSE+1K model performed uniformly worse than the plain DeViSE-0 model by a margin that indicates it has a bias toward training classes.
To provide a stronger baseline for comparison, we compared the performance of our model and the softmax model on the hierarchical metric we employed above. Although the softmax baseline model can never predict exactly the correct label, the hierarchical metric will give the model credit for predicting labels that are in the neighborhood of the correct label in the ImageNet hierarchy(for k > 1). Visual similarity is strongly correlated with semantic similarity for nearby object categories, and the softmax model does leverage visual similarity between zero-shot and training images to make predictions that will be scored favorably (e.g. Figure 2d).
The easiest dataset, "2-hop", contains object categories that are as visually and semantically similar to the training set as possible. For this dataset the softmax model outperforms the DeViSE model for hierarchical precision@2, demonstrating just how large a role visual similarity plays in predicting semantically "nearby" labels (Table 3). However, for k = 5, 10, 20, our model produces superior predictions relative to the ImageNet hierarchy, even on this easiest dataset. For the two more difficult datasets, where there are more novel categories and the novel categories are less closely related to those in the training data set, DeViSE outperforms the softmax model at all measured hierarchical precisions. The quantitative gains can be quite large, as much as 82% relative improvement over softmax performance, and qualitatively, the softmax model's predictions can be surprisingly unreasonable in some cases (e.g. Figure 2c). The random embeddings model we described above performed substantially worse than either of the real models. These results indicate that our architecture succeeds in leveraging the semantic knowledge captured by the language model to make reasonable predictions, even as test images become increasingly dissimilar from those used in the training set.
To provide a comparison with other work in zero-shot learning, we also directly compare to the zero-shot results from. These were performed on a particular 800/200 split of the 1000 classes
7 from ImageNet 2010: training and model tuning is performed using the 800 classes, and test images are drawn from the remaining 200 classes. Results are shown in Table 4.
Taken together, these zero-shot experiments indicate that the DeViSE model can exploit both visual and semantic information to predict novel classes never before observed. Furthermore, the presence of semantic information in the model substantially improves the quality of its predictions.
Conclusion
In contrast to previous attempts in this area, we have shown that our joint visual-semantic embedding model can be trained to give performance comparable to a state-of-the-art softmax based model on a flat object classification metric, while simultaneously making more semantically reasonable errors, as indicated by its improved performance on a hierarchical label metric. We have also shown that this model is able to make correct predictions across thousands of previously unseen classes by leveraging semantic knowledge elicited only from unannotated text.
The advantages of this architecture, however, extend beyond the experiments presented here.
First, we believe that our model's unusual compatibility with larger, less manicured data sets will prove to be a major strength moving forward. In particular, the skip-gram language model we constructed included only a modestly sized vocabulary, and was exposed only to the text of a single online encyclopedia; we believe that the gains available to models with larger vocabularies and trained on vastly larger text corpora will be significant, and easily outstrip methods which rely on manually constructed semantic hierarchies (e.g. ). Perhaps more importantly, though here we trained on a curated academic image dataset, our model's architecture naturally lends itself to being trained on all available images that can be annotated with any text term contained in the (larger) vocabulary. We believe that training massive "open" image datasets of this form will dramatically improve the quality of visual object categorization systems.
Second, we believe that the 1-of-N (and nearly balanced) visual object classification problem is soon to be outmoded by practical visual object categorization systems that can handle very large numbers of labels and the re-definition of valid label sets at test time. For example, our model can be trained once on all available data, and simultaneously used in one application requiring only coarse object categorization (e.g. house, car, pedestrian) and another application requiring fine categorization in a very specialized subset (e.g. Honda Civic, Ferrari F355, Tesla Model-S).
Moreover, because test time computation can be sub-linear in the number of labels contained in the training set, our model can be used in exactly such systems with much larger numbers of labels, including overlapping or never-observed categories.
Moving forward, we are experimenting with techniques which more directly leverage the structure inherent in the learned language embedding, greatly reducing training costs of the joint model and allowing even greater scaling.
Acknowledgments
Special thanks to those who lent their insight and technical support for this work, including Matthieu
Devin, Alex Krizhevsky, Quoc Le, Rajat Monga, Ilya Sutskever, and Wojciech Zaremba.
References
 S. Bengio, J. Weston, and D. Grangier. Label embedding trees for large multi-class tasks. In Advances in Neural Information Processing Systems, NIPS, 2010.
 Y. Bengio, R. Ducharme, and P. Vincent. A neural probabilistic language model. Journal of Machine
Learning Research, 3:1137–1155, 2003.
 A. Coates and A. Ng. The importance of encoding versus training with sparse coding and vector quantization. In International Conference on Machine Learning (ICML), 2011.
 Jeffrey Dean, Greg S. Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc V. Le, Mark Z. Mao, MarcAurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang, and Andrew Y. Ng. Large scale distributed deep networks. In Advances in Neural Information Processing Systems, NIPS, 2012.
 Thomas Dean, Mark Ruzon, Mark Segal, Jonathon Shlens, Sudheendra Vijayanarasimhan, and Jay Yagnik. Fast, accurate detection of 100,000 object classes on a single machine. In IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2013.
 Jia Deng, Alex Berg, Sanjeev Satheesh, Hao Su, Aditya Khosla, and Fei-Fei Li. Imagenet large scale visual recognition challenge 2012.
 Jia Deng, Wei Dong, Richard Socher, Li jia Li, Kai Li, and Li Fei-fei. Imagenet: A large-scale hierarchical image database. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2009.
 Thomas Deselaers and Vittorio Ferrari. Visual and semantic similarity in imagenet. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2011.
 J. C. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12:2121–2159, 2011.
 Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R. Salakhutdinov.
Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580, 2012.
 Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems, NIPS, 2012.
 Thomas Mensink, Jakob Verbeek, Florent Perronnin, and Gabriela Csurka. Metric learning for large scale image classification: Generalizing to new classes at near-zero cost. In European Conference on Computer
Vision (ECCV), 2012.
 Tomas Mikolov, Kai Chen, Greg S. Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. In International Conference on Learning Representations (ICLR), Scottsdale, Arizona, USA, 2013.
 Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, NIPS, 2013.
 Mohammad Norouzi, Tomas Mikolov, Samy Bengio, Jonathon Shlens, Andrea Frome, Greg S. Corrado, and Jeffrey Dean. Zero-shot learning by convex combination of semantic embeddings. arXiv (to be submitted), 2013.
 Mark Palatucci, Dean Pomerleau, Geoffrey E. Hinton, and Tom M. Mitchell. Zero-shot learning with semantic output codes. In Advances in Neural Information Processing Systems, NIPS, 2009.
 Marcus Rohrbach, Michael Stark, and Bernt Schiele. Evaluating knowledge transfer and zero-shot learning in a large-scale setting. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), R. Socher, M. Ganjoo, H. Sridhar, O. Bastani, C. D. Manning, and A. Y. Ng. Zero-shot learning through cross-modal transfer. In International Conference on Learning Representations (ICLR), Scottsdale, Arizona, USA, 2013.
 L.J.P. van der Maaten and G.E. Hinton. Visualizing high-dimensional data using t-sne. Journal of Machine
Learning Research, 9:2579–2605, 2008.
 Jason Weston, Samy Bengio, and Nicolas Usunier. Large scale image annotation: learning to rank with joint word-image embeddings. Machine Learning, 81(1):21–35, 2010.
A
Appendix
A.1
Validation/test Data Methodology
For all experiments except the comparison to Mensink et al. we train our visual model and DeViSE on the ILSVRC 2012 1K data set. Experiments in Section 4.1 use images and labels only from the 2012 1K set for testing, and the zero-shot experiments in Section 4.2 use image and labels from the ImageNet 2011 21K set and, for DeViSE+1K, also labels from ILSVRC 2012 1K. The same subset of the ILSVRC 2012 1K data used to train the visual model (Section 3.2) is also used to train DeViSE and the random embedding-based models, and when training all models, we randomly flip, crop and translate images to artificially expand the training set several-fold. The 50K images from the ILSVRC 2012 1K validation set are split randomly 10/90 into our experimental validation and held-out test sets of 5K and 45K images, respectively. Our validation set is used to choose hyperparameters, and results are reported on the held-out set. The 1,000 classes are roughly balanced in the validation and held-out sets. The 500-D DeViSE model trained for the experiments in Section
4.1 is also used for the corresponding zero-shot experiments in Section 4.2 with no additional tuning.
The zero-shot experiments performed to compare to Mensink et al. are trained with images and labels from the ILSVRC 2010 1K data set, using the same 800/200 training/test class split used in. We use the ILSVRC 2010 training/validation/test data split; training and validation images from the 800 classes are used to train and tune our visual model and DeViSE, and test images from the 200 zero-shot classes are used to generate our experimental results.
At test time, images are center-cropped to 225 × 225 for input to the visual model, and no other distortions are applied.
A.2
Mapping Text Terms to ImageNet Synset
The language model represents terms and phrases gathered from unannotated text as embedding vectors, whereas the ImageNet data set represents each class as a synset, a set of synonymous terms, where each term is a word or phrase. When training DeViSE, a method is needed for mapping from an ImageNet synset to the target embedding vector, and at prediction time, label predictions from the embedding space must be translated back into ImageNet synsets from the test set for scoring. There are two complications: (1) the same term can occur in multiple ImageNet synsets, often representing different concepts, for example the two synsets consisting only of "crane" in ILSVRC 2012 1K; (2) the language model as we have trained it only has one embedding for each word or phrase, so there is only one embedding vector representing both senses of "crane".
When training, we choose the target embedding vector by mapping the first term of the synset to its embedding vector through the text of the synset term. We found this to work well in practice; other possible approaches are to choose randomly from among the synset terms or take an average of the embedding vectors for the synset terms.
When making a prediction, the mapping from embedded text vectors back to ImageNet synsets is more difficult: each embedding vector can correspond to several ImageNet synsets due to the repetition of terms between synsets, up to 9 different synsets in the case of "head". Additionally, multiple of our predicted embedding vectors can correspond to terms from the same synset, e.g.
"ballroom", "dance hall", "dance palace". In practice, this happens frequently since synonymous terms are embedded close to one another by the skip-gram language model.
For a given visual output vector, our model first finds the N nearest embedding label vectors using cosine similarity, sorted by their similarity, where N > k. In these experiments, we chose N = 4∗k as this is close to the average number of text labels per synset in the ILSVRC 2012 data set. The first step in converting these to ImageNet synsets is to expand every embedded term to all of its corresponding ImageNet synsets. For example, "crane" would be expanded to the two synsets which contain the term "crane" (the order of the two "crane" synsets in the final list is arbitrary). After expansion, if there are duplicate entries for a given synset, then all but the first are removed from their places in the list, leaving a list where each synset occurs at most once in the prediction list.
Finally, the list is truncated to the top k predictions. We experimented with choosing randomly from among all the possible synsets instead of expanding to all of them and found this to slightly reduce performance in the ILSVRC 2012 1K experiments.
A.3
Hierarchical Precision-at-k Metric
We defined the following hierarchical precision-at-k metric, hp@k, to assess the accuracy of model predictions with respect to the ImageNet object category hierarchy. For each image in the test set, the model in question emits its top k predicted ImageNet object category labels (synsets). We calculate hp@k as the fraction of these k predictions which are in hCorrectSet, averaged across the test examples: hp@k = 1
N
N
� i=1 number of model's top k predictions in hCorrectSet for image i k
The hCorrectSet for a true label is constructed by iteratively adding nodes from the ImageNet hierarchy in a widening radius around the true label until hCorrectSet has a size ≥ k: hCorrectSet = {}
R = 0
10 k
Label set
# kCorrectSet Labels
ImageNet 2012 1K
Zero-shot 2-hop
Zero-shot 3-hop
Zero-shot ImageNet 2011 21K
Table 5: Mean sizes of hCorrectSet lists used for hierarchical evaluation, averaged across the test examples, shown for various label sets and values of k. At k = 1, hCorrectSet always contains only the true label.
Note that for the zero-shot data sets, kCorrectSet includes the test set labels as well as the ImageNet 2012
1K labels. List sizes vary among test examples depending upon the local topology of the graph around the true label as well as how many labels from the graph are in the ground truth set. while NumberElements(hCorrectSet < k): radiusSet = all nodes in the ImageNet hierarchy which are
R hops from the true label validRadiusSet = ValidLabelNodes(radiusSet) hCorrectSet = Union(hCorrectSet, validRadiusSet)
R = R + 1 return hCorrectSet
The size of hCorrectSet for a given test example depends on the combination of the structure of the hierarchy around a given label and which classes are included in the test set. It is exactly 1 when k = 1 and is rarely equal to k when k > 1. The ValidLabelNodes()function allows us to restrict hCorrectSet to any subset of labels in the ImageNet (or larger WordNet) hierarchy. For example, in generating the results in Table 2 we restrict the nodes in the hCorrectSet to be drawn from only those nodes which are both members the ImageNet 2011 21K label set and are three-hops or less from at least one of the ImageNet 2012 1K labels.
Note that this hierarchical metric differs from the hierarchical metric used in some of the earlier
ImageNet Challenge competitions. That metric was generally considered to be rather insensitive, and was withdrawn from more recent years of the competition. Our DeViSE model does perform better than the baseline softmax model on that metric as well, but effect sizes are generally much smaller.A Survey on Deep Learning in Medical Image Analysis
Geert Litjens, Thijs Kooi, Babak Ehteshami Bejnordi, Arnaud Arindra Adiyoso Setio, Francesco Ciompi, Mohsen Ghafoorian, Jeroen A.W.M. van der Laak, Bram van Ginneken, Clara I. S´anchez
Diagnostic Image Analysis Group
Radboud University Medical Center
Nijmegen, The Netherlands
Abstract
Deep learning algorithms, in particular convolutional networks, have rapidly become a methodology of choice for analyzing medical images. This paper reviews the major deep learning concepts pertinent to medical image analysis and summarizes over 300 contributions to the field, most of which appeared in the last year. We survey the use of deep learning for image classification, object detection, segmentation, registration, and other tasks. Concise overviews are provided of studies per application area: neuro, retinal, pulmonary, digital pathology, breast, cardiac, abdominal, musculoskeletal. We end with a summary of the current state-of-the-art, a critical discussion of open challenges and directions for future research.
Keywords: deep learning, convolutional neural networks, medical imaging, survey
1. Introduction
As soon as it was possible to scan and load medical images into a computer, researchers have built systems for automated analysis. Initially, from the 1970s to the 1990s, medical image analysis was done with sequential application of low-level pixel processing (edge and line detector filters, region growing) and mathematical modeling (fitting lines, circles and ellipses) to construct compound rule-based systems that solved particular tasks. There is an analogy with expert systems with many if-then-else statements that were popular in artificial intelligence in the same period.
These expert systems have been described as GOFAI (good oldfashioned artificial intelligence) (Haugeland, 1985) and were often brittle; similar to rule-based image processing systems.
At the end of the 1990s, supervised techniques, where training data is used to develop a system, were becoming increasingly popular in medical image analysis. Examples include active shape models (for segmentation), atlas methods (where the atlases that are fit to new data form the training data), and the concept of feature extraction and use of statistical classifiers (for computeraided detection and diagnosis). This pattern recognition or machine learning approach is still very popular and forms the basis of many successful commercially available medical image analysis systems.
Thus, we have seen a shift from systems that are completely designed by humans to systems that are trained by computers using example data from which feature vectors are extracted. Computer algorithms determine the optimal decision boundary in the high-dimensional feature space. A crucial step in the design of such systems is the extraction of discriminant features from the images.
This process is still done by human researchers and, as such, one speaks of systems with handcrafted features.
A logical next step is to let computers learn the features that optimally represent the data for the problem at hand. This concept lies at the basis of many deep learning algorithms: models (networks) composed of many layers that transform input data (e.g. images) to outputs(e.g. disease present/absent) while learning increasingly higher level features. The most successful type of models for image analysis to date are convolutional neural networks (CNNs). CNNs contain many layers that transform their input with convolution filters of a small extent. Work on CNNs has been done since the late seventies (Fukushima, 1980) and they were already applied to medical image analysis in 1995 by Lo et al. (1995).
They saw their first successful real-world application in LeNet (LeCun et al., 1998) for hand-written digit recog1 arXiv:1702.05747v2 [cs.CV] 4 Jun 2017 nition. Despite these initial successes, the use of CNNs did not gather momentum until various new techniques were developed for efficiently training deep networks, and advances were made in core computing systems.
The watershed was the contribution of Krizhevsky et al.(2012) to the ImageNet challenge in December 2012.
The proposed CNN, called AlexNet, won that competition by a large margin. In subsequent years, further progress has been made using related but deeper architectures (Russakovsky et al., 2014). In computer vision, deep convolutional networks have now become the technique of choice.
The medical image analysis community has taken notice of these pivotal developments. However, the transition from systems that use handcrafted features to systems that learn features from the data has been gradual.
Before the breakthrough of AlexNet, many different techniques to learn features were popular. Bengio et al. (2013) provide a thorough review of these techniques. They include principal component analysis, clustering of image patches, dictionary approaches, and many more. Bengio et al. (2013) introduce CNNs that are trained end-to-end only at the end of their review in a section entitled Global training of deep models. In this survey, we focus particularly on such deep models, and do not include the more traditional feature learning approaches that have been applied to medical images. For a broader review on the application of deep learning in health informatics we refer to Ravi et al. (2017), where medical image analysis is briefly touched upon.
Applications of deep learning to medical image analysis first started to appear at workshops and conferences, and then in journals. The number of papers grew rapidly in 2015 and 2016. This is illustrated in Figure
1. The topic is now dominant at major conferences and a first special issue appeared of IEEE Transaction on
Medical Imaging in May 2016 (Greenspan et al., 2016).
One dedicated review on application of deep learning to medical image analysis was published by Shen et al.(2017). Although they cover a substantial amount of work, we feel that important areas of the field were not represented. To give an example, no work on retinal image analysis was covered. The motivation for our review was to offer a comprehensive overview of (almost) all fields in medical imaging, both from an application and a methodology-drive perspective.
This also includes overview tables of all publications which readers can use to quickly assess the field. Last, we leveraged our own experience with the application of deep learning methods to medical image analysis to provide readers with a dedicated discussion section covering the stateof-the-art, open challenges and overview of research directions and technologies that will become important in the future.
This survey includes over 300 papers, most of them recent, on a wide variety of applications of deep learning in medical image analysis.
To identify relevant contributions PubMed was queried for papers containing ("convolutional" OR "deep learning") in title or abstract. ArXiv was searched for papers mentioning one of a set of terms related to medical imaging.
Additionally, conference proceedings for MICCAI (including workshops), SPIE, ISBI and EMBC were searched based on titles of papers. We checked references in all selected papers and consulted colleagues. We excluded papers that did not report results on medical image data or only used standard feed-forward neural networks with handcrafted features. When overlapping work had been reported in multiple publications, only the publication(s) deemed most important were included. We expect the search terms used to cover most, if not all, of the work incorporating deep learning methods. The last update to the included papers was on February 1, 2017.
The appendix describes the search process in more detail.
Summarizing, with this survey we aim to:
• show that deep learning techniques have permeated the entire field of medical image analysis;
• identify the challenges for successful application of deep learning to medical imaging tasks;
• highlight specific contributions which solve or circumvent these challenges.
The rest of this survey as structured as followed. In
Section 2 we introduce the main deep learning techniques that have been used for medical image analysis and that are referred to throughout the survey. Section 3 describes the contributions of deep learning to canonical tasks in medical image analysis: classification, detection, segmentation, registration, retrieval, image generation and enhancement. Section 4 discusses obtained results and open challenges in different application areas: neuro, ophthalmic, pulmonary, digital pathology and cell imaging, breast, cardiac, abdominal, musculoskeletal, and remaining miscellaneous applications. We end with a summary, a critical discussion and an outlook for future research.
2. Overview of deep learning methods
The goal of this section is to provide a formal introduction and definition of the deep learning concepts, Number of papers
All
CNN
RBM
RNN
AE
Other
Mul�ple
Registra�on
Segmenta�on (Object)
Detec�on (Organ, region, landmark)
Other
Classifica�on (Object)
Classifica�on (Exam)
Detec�on (Object)
Segmenta�on (Organ, substructure)
Color fundus photos
Mul�ple
Other
Mammography
X-ray
Ultrasound
CT
Microscopy
MRI
Number of papers
Mul�ple
Re�na
Bone
Breast
Cardiac
Abdomen
Lung
Other
Brain
Pathology
Number of papers
Figure 1: Breakdown of the papers included in this survey in year of publication, task addressed (Section 3), imaging modality, and application area (Section 4). The number of papers for 2017 has been extrapolated from the papers published in January. techniques and architectures that we found in the medical image analysis papers surveyed in this work.
2.1. Learning algorithms
Machine learning methods are generally divided into supervised and unsupervised learning algorithms, although there are many nuances. In supervised learning, a model is presented with a dataset D = {x, y}N n=1 of input features x and label y pairs, where y typically represents an instance of a fixed set of classes. In the case of regression tasks y can also be a vector with continuous values. Supervised training typically amounts to finding model parameters Θ that best predict the data based on a loss function L(y, ˆy). Here ˆy denotes the output of the model obtained by feeding a data point x to the function f(x; Θ) that represents the model.
Unsupervised learning algorithms process data without labels and are trained to find patterns, such as latent subspaces. Examples of traditional unsupervised learning algorithms are principal component analysis and clustering methods. Unsupervised training can be performed under many different loss functions. One example is reconstruction loss L(x, ˆx) where the model has to learn to reconstruct its input, often through a lowerdimensional or noisy representation.
2.2. Neural Networks
Neural networks are a type of learning algorithm which forms the basis of most deep learning methods. A neural network comprises of neurons or units with some activation a and parameters Θ = {W, B}, where W is a set of weights and B a set of biases. The activation represents a linear combination of the input x to the neuron and the parameters, followed by an element-wise nonlinearity σ(·), referred to as a transfer function: a = σ(wTx + b).
Typical transfer functions for traditional neural networks are the sigmoid and hyperbolic tangent function.
The multi-layered perceptrons (MLP), the most wellknown of the traditional neural networks, have several layers of these transformations: f(x; Θ) = σ(WTσ(WT... σ(WTx + b)) + b).
Here, W is a matrix comprising of columns wk, associated with activation k in the output. Layers in between the input and output are often referred to as 'hidden' layers. When a neural network contains multiple hidden layers it is typically considered a 'deep' neural network, hence the term 'deep learning'.
At the final layer of the network the activations are mapped to a distribution over classes P(y|x; Θ) through
3 a softmax function:
P(y|x; Θ) = softmax(x; Θ) = ewT i x+bi
�K k=1 ewT k x+bk, (3) where wi indicates the weight vector leading to the output node associated with class i. A schematic representation of three-layer MLP is shown in Figure 2.
Maximum likelihood with stochastic gradient descent is currently the most popular method to fit parameters Θ to a dataset D. In stochastic gradient descent a small subset of the data, a mini-batch, is used for each gradient update instead of the full data set. Optimizing maximum likelihood in practice amounts to minimizing the negative log-likelihood: arg min
Θ −
N
� n=1 log �P(yn|xn; Θ)�.
This results in the binary cross-entropy loss for twoclass problems and the categorical cross-entropy for multi-class tasks. A downside of this approach is that it typically does not optimize the quantity we are interested in directly, such as area under the receiveroperating characteristic (ROC) curve or common evaluation measures for segmentation, such as the Dice coefficient.
For a long time, deep neural networks (DNN) were considered hard to train efficiently. They only gained popularity in 2006 (Bengio et al., 2007; Hinton and Salakhutdinov, 2006; Hinton et al., 2006) when it was shown that training DNNs layer-by-layer in an unsupervised manner (pre-training), followed by supervised fine-tuning of the stacked network, could result in good performance. Two popular architectures trained in such a way are stacked auto-encoders (SAEs) and deep belief networks (DBNs). However, these techniques are rather complex and require a significant amount of engineering to generate satisfactory results.
Currently, the most popular models are trained endto-end in a supervised fashion, greatly simplifying the training process.
The most popular architectures are convolutional neural networks (CNNs) and recurrent neural networks (RNNs).
CNNs are currently most widely used in (medical) image analysis, although
RNNs are gaining popularity. The following sections will give a brief overview of each of these methods, starting with the most popular ones, and discussing their differences and potential challenges when applied to medical problems.
2.3. Convolutional Neural Networks (CNNs)
There are two key differences between MLPs and CNNs. First, in CNNs weights in the network are shared in such a way that it the network performs convolution operations on images. This way, the model does not need to learn separate detectors for the same object occurring at different positions in an image, making the network equivariant with respect to translations of the input. It also drastically reduces the amount of parameters (i.e. the number of weights no longer depends on the size of the input image) that need to be learned. An example of a 1D CNN is shown in Figure 2.
At each layer, the input image is convolved with a set of K kernels W = {W1, W2,..., WK} and added biases B = {b1,..., bK}, each generating a new feature map Xk. These features are subjected to an elementwise non-linear transform σ(·) and the same process is repeated for every convolutional layer l:
Xl k = σ�Wl−1 k
∗ Xl−1 + bl−1 k
�.
The second key difference between CNNs and MLPs, is the typical incorporation of pooling layers in CNNs, where pixel values of neighborhoods are aggregated using a permutation invariant function, typically the max or mean operation. This induces a certain amount of translation invariance and again reduces the amount of parameters in the network. At the end of the convolutional stream of the network, fully-connected layers(i.e. regular neural network layers) are usually added, where weights are no longer shared. Similar to MLPs, a distribution over classes is generated by feeding the activations in the final layer through a softmax function and the network is trained using maximum likelihood.
2.4. Deep CNN Architectures
Given the prevalence of CNNs in medical image analysis, we elaborate on the most common architectures and architectural differences among the widely used models.
2.4.1. General classification architectures
LeNet (LeCun et al., 1998) and AlexNet (Krizhevsky et al., 2012), introduced over a decade later, were in essence very similar models. Both networks were relatively shallow, consisting of two and five convolutional layers, respectively, and employed kernels with large receptive fields in layers close to the input and smaller kernels closer to the output. AlexNet did incorporate rectified linear units instead of the hyperbolic tangent as activation function.
After 2012 the exploration of novel architectures took off, and in the last three years there is a preference for far deeper models. By stacking smaller kernels, instead of using a single layer of kernels with a large receptive field, a similar function can be represented with less parameters. These deeper architectures generally have a lower memory footprint during inference, which enable their deployment on mobile computing devices such as smartphones. Simonyan and Zisserman (2014) were the first to explore much deeper networks, and employed small, fixed size kernels in each layer. A 19-layer model often referred to as VGG19 or OxfordNet won the ImageNet challenge of 2014.
On top of the deeper networks, more complex building blocks have been introduced that improve the efficiency of the training procedure and again reduce the amount of parameters. Szegedy et al. (2014) introduced a 22-layer network named GoogLeNet, also referred to as Inception, which made use of so-called inception blocks (Lin et al., 2013), a module that replaces the mapping defined in Eq. (5) with a set of convolutions of different sizes. Similar to the stacking of small kernels, this allows a similar function to be represented with less parameters. The ResNet architecture (He et al., 2015) won the ImageNet challenge in 2015 and consisted of so-called ResNet-blocks. Rather than learning a function, the residual block only learns the residual and is thereby pre-conditioned towards learning mappings in each layer that are close to the identity function. This way, even deeper models can be trained effectively.
Since 2014, the performance on the ImageNet benchmark has saturated and it is difficult to assess whether the small increases in performance can really be attributed to 'better' and more sophisticated architectures.
The advantage of the lower memory footprint these models provide is typically not as important for medical applications. Consequently, AlexNet or other simple models such as VGG are still popular for medical data, though recent landmark studies all use a version of GoogleNet called Inception v3 (Gulshan et al., 2016;
Esteva et al., 2017; Liu et al., 2017). Whether this is due to a superior architecture or simply because the model is a default choice in popular software packages is again difficult to assess.
2.4.2. Multi-stream architectures
The default CNN architecture can easily accommodate multiple sources of information or representations of the input, in the form of channels presented to the input layer. This idea can be taken further and channels can be merged at any point in the network. Under the intuition that different tasks require different ways of fusion, multi-stream architectures are being explored.
These models, also referred to as dual pathway architectures (Kamnitsas et al., 2017), have two main applications at the time of writing: (1) multi-scale image analysis and (2) 2.5D classification; both relevant for medical image processing tasks.
For the detection of abnormalities, context is often an important cue. The most straightforward way to increase context is to feed larger patches to the network, but this can significantly increase the amount of parameters and memory requirements of a network. Consequently, architectures have been investigated where context is added in a down-scaled representation in addition to high resolution local information. To the best of our knowledge, the multi-stream multi-scale architecture was first explored by Farabet et al. (2013), who used it for segmentation in natural images. Several medical applications have also successfully used this concept (Kamnitsas et al., 2017; Moeskops et al., 2016a;
Song et al., 2015; Yang et al., 2016c).
As so much methodology is still developed on natural images, the challenge of applying deep learning techniques to the medical domain often lies in adapting existing architectures to, for instance, different input formats such as three-dimensional data. In early applications of CNNs to such volumetric data, full 3D convolutions and the resulting large amount of parameters were circumvented by dividing the Volume of Interest(VOI) into slices which are fed as different streams to a network. Prasoon et al. (2013) were the first to use this approach for knee cartilage segmentation. Similarly, the network can be fed with multiple angled patches from the 3D-space in a multi-stream fashion, which has been applied by various authors in the context of medical imaging (Roth et al., 2016b; Setio et al., 2016). These approaches are also referred to as 2.5D classification.
2.4.3. Segmentation Architectures
Segmentation is a common task in both natural and medical image analysis and to tackle this, CNNs can simply be used to classify each pixel in the image individually, by presenting it with patches extracted around the particular pixel. A drawback of this naive 'slidingwindow' approach is that input patches from neighboring pixels have huge overlap and the same convolutions are computed many times. Fortunately, the convolution and dot product are both linear operators and thus inner products can be written as convolutions and vice versa.
By rewriting the fully connected layers as convolutions, the CNN can take input images larger than it was trained on and produce a likelihood map, rather than an output for a single pixel. The resulting 'fully convolutional
5 network' (fCNN) can then be applied to an entire input image or volume in an efficient fashion.
However, because of pooling layers, this may result in output with a far lower resolution than the input.
'Shift-and-stitch' (Long et al., 2015) is one of several methods proposed to prevent this decrease in resolution.
The fCNN is applied to shifted versions of the input image. By stitching the result together, one obtains a full resolution version of the final output, minus the pixels lost due to the 'valid' convolutions.
Ronneberger et al. (2015) took the idea of the fCNN one step further and proposed the U-net architecture, comprising a 'regular' fCNN followed by an upsampling part where 'up'-convolutions are used to increase the image size, coined contractive and expansive paths. Although this is not the first paper to introduce learned upsampling paths in convolutional neural networks (e.g. Long et al. (2015)), the authors combined it with so called skip-connections to directly connect opposing contracting and expanding convolutional layers.
A similar approach was used by C¸ ic¸ek et al. (2016) for
3D data. Milletari et al. (2016b) proposed an extension to the U-Net layout that incorporates ResNet-like residual blocks and a Dice loss layer, rather than the conventional cross-entropy, that directly minimizes this commonly used segmentation error measure.
2.5. Recurrent Neural Networks (RNNs)
Traditionally, RNNs were developed for discrete sequence analysis. They can be seen as a generalization of MLPs because both the input and output can be of varying length, making them suitable for tasks such as machine translation where a sentence of the source and target language are the input and output. In a classification setting, the model learns a distribution over classes
P(y|x1, x2,..., xT; Θ) given a sequence x1, x2,..., xT, rather than a single input vector x.
The plain RNN maintains a latent or hidden state h at time t that is the output of a non-linear mapping from its input xt and the previous state ht−1: ht = σ(Wxt + Rht−1 + b), (6) where weight matrices W and R are shared over time.
For classification, one or more fully connected layers are typically added followed by a softmax to map the sequence to a posterior over the classes.
P(y|x1, x2,..., xT; Θ) = softmax(hT; Wout, bout).
Since the gradient needs to be backpropagated from the output through time, RNNs are inherently deep(in time) and consequently suffer from the same problems with training as regular deep neural networks(Bengio et al., 1994). To this end, several specialized memory units have been developed, the earliest and most popular being the Long Short Term Memory(LSTM) cell (Hochreiter and Schmidhuber, 1997). The Gated Recurrent Unit (Cho et al., 2014) is a recent simplification of the LSTM and is also commonly used.
Although initially proposed for one-dimensional input, RNNs are increasingly applied to images. In natural images 'pixelRNNs' are used as autoregressive models, generative models that can eventually produce new images similar to samples in the training set. For medical applications, they have been used for segmentation problems, with promising results (Stollenga et al., 2015) in the MRBrainS challenge.
2.6. Unsupervised models
2.6.1. Auto-encoders (AEs) and Stacked Auto-encoders(SAEs)
AEs are simple networks that are trained to reconstruct the input x on the output layer x′ through one hidden layer h. They are governed by a weight matrix Wx,h and bias bx,h from input to hidden state and Wh,x′ with corresponding bias bh,x′ from the hidden layer to the reconstruction. A non-linear function is used to compute the hidden activation: h = σ(Wx,hx + bx,h).
Additionally, the dimension of the hidden layer |h| is taken to be smaller than |x|. This way, the data is projected onto a lower dimensional subspace representing a dominant latent structure in the input. Regularization or sparsity constraints can be employed to enhance the discovery process. If the hidden layer had the same size as the input and no further non-linearities were added, the model would simply learn the identity function.
The denoising auto-encoder (Vincent et al., 2010) is another solution to prevent the model from learning a trivial solution.
Here the model is trained to reconstruct the input from a noise corrupted version (typically salt-and-pepper-noise). SAEs (or deep AEs) are formed by placing auto-encoder layers on top of each other.
In medical applications surveyed in this work, autoencoder layer were often trained individually ('greedily') after which the full network was fine-tuned using supervised training to make a prediction.
2.6.2. Restricted Boltzmann Machines (RBMs) and Deep Belief Networks (DBNs)
RBMs (Hinton, 2010) are a type of Markov Random Field (MRF), constituting an input layer or visible layer x = (x1, x2,..., xN) and a hidden layer h =(h1, h2,..., hM) that carries the latent feature representation.
The connections between the nodes are bidirectional, so given an input vector x one can obtain the latent feature representation h and also vice versa.
As such, the RBM is a generative model, and we can sample from it and generate new data points. In analogy to physical systems, an energy function is defined for a particular state (x, h) of input and hidden units:
E(x, h) = hTWx − cTx − bTh, (9) with c and b bias terms. The probability of the 'state' of the system is defined by passing the energy to an exponential and normalizing: p(x, h) = 1
Z exp{−E(x, h)}.
Computing the partition function Z is generally intractable. However, conditional inference in the form of computing h conditioned on v or vice versa is tractable and results in a simple formula:
P(hj|x) =
1 + exp{−bj − W jx}.
Since the network is symmetric, a similar expression holds for P(xi|h).
DBNs (Bengio et al., 2007; Hinton et al., 2006) are essentially SAEs where the AE layers are replaced by
RBMs. Training of the individual layers is, again, done in an unsupervised manner.
Final fine-tuning is performed by adding a linear classifier to the top layer of the DBN and performing a supervised optimization.
2.6.3. Variational Auto-Encoders and Generative Adverserial Networks
Recently, two novel unsupervised architectures were introduced: the variational auto-encoder (VAE)(Kingma and Welling, 2013) and the generative adversarial network (GAN) (Goodfellow et al., 2014). There are no peer-reviewed papers applying these methods to medical images yet, but applications in natural images are promising. We will elaborate on their potential in the discussion.
2.7. Hardware and Software
One of the main contributors to steep rise of deep learning has been the widespread availability of GPU and GPU-computing libraries (CUDA, OpenCL). GPUs are highly parallel computing engines, which have an order of magnitude more execution threads than central processing units (CPUs). With current hardware, deep learning on GPUs is typically 10 to 30 times faster than on CPUs.
Next to hardware, the other driving force behind the popularity of deep learning methods is the wide availability of open source software packages.
These libraries provide efficient GPU implementations of important operations in neural networks, such as convolutions; allowing the user to implement ideas at a high level rather than worrying about low-level efficient implementations. At the time of writing, the most popular packages were (in alphabetical order):
• Caffe (Jia et al., 2014). Provides C++ and Python interfaces, developed by graduate students at UC
Berkeley.
• Tensorflow (Abadi et al., 2016). Provides C++ and Python and interfaces, developed by Google and is used by Google research.
• Theano (Bastien et al., 2012). Provides a Python interface, developed by MILA lab in Montreal.
• Torch (Collobert et al., 2011). Provides a Lua interface and is used by, among others, Facebook AI research.
There are third-party packages written on top of one or more of these frameworks, such as Lasagne (https:// github.com/Lasagne/Lasagne) or Keras (https:
//keras.io/). It goes beyond the scope of this paper to discuss all these packages in detail.
3. Deep Learning Uses in Medical Imaging
3.1. Classification
3.1.1. Image/exam classification
Image or exam classification was one of the first areas in which deep learning made a major contribution to medical image analysis. In exam classification one typically has one or multiple images (an exam) as input with a single diagnostic variable as output (e.g., disease present or not). In such a setting, every diagnostic exam is a sample and dataset sizes are typically
Concatenate
Up-convolu�on
Up-sample
Down-sample(b)(c)(d)(e)(f)
Input node
Weighted connec�on
Weighted connec�on(similar colors indicate shared weights)
Hidden node
Output node
Probabilis�c node
Pooling connec�on(a)
Figure 2: Node graphs of 1D representations of architectures commonly used in medical imaging. a) Auto-encoder, b) restricted Boltzmann machine, c) recurrent neural network, d) convolutional neural network, e) multi-stream convolutional neural network, f) U-net (with a single downsampling stage).. small compared to those in computer vision (e.g., hundreds/thousands vs. millions of samples). The popularity of transfer learning for such applications is therefore not surprising.
Transfer learning is essentially the use of pre-trained networks (typically on natural images) to try to work around the (perceived) requirement of large data sets for deep network training. Two transfer learning strategies were identified: (1) using a pre-trained network as a feature extractor and (2) fine-tuning a pre-trained network on medical data. The former strategy has the extra benefit of not requiring one to train a deep network at all, allowing the extracted features to be easily plugged in to existing image analysis pipelines. Both strategies are popular and have been widely applied. However, few authors perform a thorough investigation in which strategy gives the best result. The two papers that do, Antony et al. (2016) and Kim et al. (2016a), offer conflicting results. In the case of Antony et al. (2016), finetuning clearly outperformed feature extraction, achieving 57.6% accuracy in multi-class grade assessment of knee osteoarthritis versus 53.4%. Kim et al. (2016a), however, showed that using CNN as a feature extractor outperformed fine-tuning in cytopathology image classification accuracy (70.5% versus 69.1%). If any guidance can be given to which strategy might be most successful, we would refer the reader to two recent papers, published in high-ranking journals, which fine-tuned a pre-trained version of Google's Inception v3 architecture on medical data and achieved (near) human expert
8 performance (Esteva et al., 2017; Gulshan et al., 2016).
As far as the authors are aware, such results have not yet been achieved by simply using pre-trained networks as feature extractors.
With respect to the type of deep networks that are commonly used in exam classification, a timeline similar to computer vision is apparent.
The medical imaging community initially focused on unsupervised pre-training and network architectures like SAEs and RBMs. The first papers applying these techniques for exam classification appeared in 2013 and focused on neuroimaging.
Brosch and Tam (2013), Plis et al.(2014), Suk and Shen (2013), and Suk et al. (2014) applied DBNs and SAEs to classify patients as having Alzheimer's disease based on brain Magnetic Resonance Imaging (MRI). Recently, a clear shift towards
CNNs can be observed.
Out of the 47 papers published on exam classification in 2015, 2016, and 2017, 36 are using CNNs, 5 are based on AEs and 6 on RBMs.
The application areas of these methods are very diverse, ranging from brain MRI to retinal imaging and digital pathology to lung computed tomography (CT).
In the more recent papers using CNNs authors also often train their own network architectures from scratch instead of using pre-trained networks. Menegola et al.(2016) performed some experiments comparing training from scratch to fine-tuning of pre-trained networks and showed that fine-tuning worked better given a small data set of around a 1000 images of skin lesions. However, these experiments are too small scale to be able to draw any general conclusions from.
Three papers used an architecture leveraging the unique attributes of medical data: two use 3D convolutions (Hosseini-Asl et al., 2016; Payan and Montana, 2015) instead of 2D to classify patients as having
Alzheimer; Kawahara et al. (2016b) applied a CNNlike architecture to a brain connectivity graph derived from MRI diffusion-tensor imaging (DTI). In order to do this, they developed several new layers which formed the basis of their network, so-called edge-to-edge, edgeto-node, and node-to-graph layers. They used their network to predict brain development and showed that they outperformed existing methods in assessing cognitive and motor scores.
Summarizing, in exam classification CNNs are the current standard techniques.
Especially CNNs pretrained on natural images have shown surprisingly strong results, challenging the accuracy of human experts in some tasks.
Last, authors have shown that
CNNs can be adapted to leverage intrinsic structure of medical images.
3.1.2. Object or lesion classification
Object classification usually focuses on the classification of a small (previously identified) part of the medical image into two or more classes (e.g. nodule classification in chest CT). For many of these tasks both local information on lesion appearance and global contextual information on lesion location are required for accurate classification.
This combination is typically not possible in generic deep learning architectures. Several authors have used multi-stream architectures to resolve this in a multi-scale fashion (Section 2.4.2). Shen et al. (2015b) used three CNNs, each of which takes a nodule patch at a different scale as input. The resulting feature outputs of the three CNNs are then concatenated to form the final feature vector. A somewhat similar approach was followed by Kawahara and Hamarneh(2016) who used a multi-stream CNN to classify skin lesions, where each stream works on a different resolution of the image. Gao et al. (2015) proposed to use a combination of CNNs and RNNs for grading nuclear cataracts in slit-lamp images, where CNN filters were pre-trained. This combination allows the processing of all contextual information regardless of image size. Incorporating 3D information is also often a necessity for good performance in object classification tasks in medical imaging. As images in computer vision tend to be
2D natural images, networks developed in those scenarios do not directly leverage 3D information. Authors have used different approaches to integrate 3D in an effective manner with custom architectures. Setio et al.(2016) used a multi-stream CNN to classify points of interest in chest CT as a nodule or non-nodule. Up to nine differently oriented patches extracted from the candidate were used in separate streams and merged in the fully-connected layers to obtain the final classification output. In contrast, Nie et al. (2016c) exploited the 3D nature of MRI by training a 3D CNN to assess survival in patients suffering from high-grade gliomas.
Almost all recent papers prefer the use of end-to-end trained CNNs. In some cases other architectures and approaches are used, such as RBMs (van Tulder and de Bruijne, 2016; Zhang et al., 2016c), SAEs (Cheng et al., 2016a) and convolutional sparse auto-encoders(CSAE) (Kallenberg et al., 2016). The major difference between CSAE and a classic CNN is the usage of unsupervised pre-training with sparse auto-encoders.
An interesting approach, especially in cases where object annotation to generate training data is expensive, is the integration of multiple instance learning (MIL) and deep learning. Xu et al. (2014) investigated the use of a MIL-framework with both supervised and unsu9 pervised feature learning approaches as well as handcrafted features. The results demonstrated that the performance of the MIL-framework was superior to handcrafted features, which in turn closely approaches the performance of a fully supervised method. We expect such approaches to be popular in the future as well, as obtaining high-quality annotated medical data is challenging.
Overall, object classification sees less use of pretrained networks compared to exam classifications, mostly due to the need for incorporation of contextual or three-dimensional information. Several authors have found innovative solutions to add this information to deep networks with good results, and as such we expect deep learning to become even more prominent for this task in the near future.
3.2. Detection
3.2.1. Organ, region and landmark localization
Anatomical object localization (in space or time), such as organs or landmarks, has been an important preprocessing step in segmentation tasks or in the clinical workflow for therapy planning and intervention. Localization in medical imaging often requires parsing of 3D volumes. To solve 3D data parsing with deep learning algorithms, several approaches have been proposed that treat the 3D space as a composition of 2D orthogonal planes. Yang et al. (2015) identified landmarks on the distal femur surface by processing three independent sets of 2D MRI slices (one for each plane) with regular CNNs. The 3D position of the landmark was defined as the intersection of the three 2D slices with the highest classification output. de Vos et al. (2016b) went one step further and localized regions of interest(ROIs) around anatomical regions (heart, aortic arch, and descending aorta) by identifying a rectangular 3D bounding box after 2D parsing the 3D CT volume. Pretrained CNN architectures, as well as RBM, have been used for the same purpose (Cai et al., 2016b; Chen et al., 2015b; Kumar et al., 2016), overcoming the lack of data to learn better feature representations. All these studies cast the localization task as a classification task and as such generic deep learning architectures and learning processes can be leveraged.
Other authors try to modify the network learning process to directly predict locations. For example, Payer et al. (2016) proposed to directly regress landmark locations with CNNs. They used landmark maps, where each landmark is represented by a Gaussian, as ground truth input data and the network is directly trained to predict this landmark map.
Another interesting approach was published by Ghesu et al. (2016a), in which reinforcement learning is applied to the identification of landmarks. The authors showed promising results in several tasks: 2D cardiac MRI and ultrasound (US) and 3D head/neck CT.
Due to its increased complexity, only a few methods addressed the direct localization of landmarks and regions in the 3D image space. Zheng et al. (2015) reduced this complexity by decomposing 3D convolution as three one-dimensional convolutions for carotid artery bifurcation detection in CT data. Ghesu et al. (2016b) proposed a sparse adaptive deep neural network powered by marginal space learning in order to deal with data complexity in the detection of the aortic valve in 3D transesophageal echocardiogram.
CNNs have also been used for the localization of scan planes or key frames in temporal data. Baumgartner et al. (2016) trained CNNs on video frame data to detect up to 12 standardized scan planes in mid-pregnancy fetal US. Furthermore, they used saliency maps to obtain a rough localization of the object of interest in the scan plan (e.g. brain, spine). RNNs, particularly LSTMRNNs, have also been used to exploit the temporal information contained in medical videos, another type of high dimensional data. Chen et al. (2015a), for example, employed LSTM models to incorporate temporal information of consecutive sequence in US videos for fetal standard plane detection. Kong et al. (2016) combined an LSTM-RNN with a CNN to detect the end-diastole and end-systole frames in cine-MRI of the heart.
Concluding, localization through 2D image classification with CNNs seems to be the most popular strategy overall to identify organs, regions and landmarks, with good results. However, several recent papers expand on this concept by modifying the learning process such that accurate localization is directly emphasized, with promising results. We expect such strategies to be explored further as they show that deep learning techniques can be adapted to a wide range of localization tasks (e.g. multiple landmarks). RNNs have shown promise in localization in the temporal domain, and multi-dimensional RNNs could play a role in spatial localization as well.
3.2.2. Object or lesion detection
The detection of objects of interest or lesions in images is a key part of diagnosis and is one of the most labor-intensive for clinicians. Typically, the tasks consist of the localization and identification of small lesions in the full image space. There has been a long research tradition in computer-aided detection systems that are designed to automatically detect lesions, improving the 10 detection accuracy or decreasing the reading time of human experts. Interestingly, the first object detection system using CNNs was already proposed in 1995, using a CNN with four layers to detect nodules in x-ray images(Lo et al., 1995).
Most of the published deep learning object detection systems still uses CNNs to perform pixel (or voxel) classification, after which some form of post processing is applied to obtain object candidates. As the classification task performed at each pixel is essentially object classification, CNN architecture and methodology are very similar to those in section 3.1.2. The incorporation of contextual or 3D information is also handled using multi-stream CNNs (Section 2.4.2, for example by
Barbu et al. (2016) and Roth et al. (2016b). Teramoto et al. (2016) used a multi-stream CNN to integrate CT and Positron Emission Tomography (PET) data. Dou et al. (2016c) used a 3D CNN to find micro-bleeds in brain MRI. Last, as the annotation burden to generate training data can be similarly significant compared to object classification, weakly-supervised deep learning has been explored by Hwang and Kim (2016), who adopted such a strategy for the detection of nodules in chest radiographs and lesions in mammography.
There are some aspects which are significantly different between object detection and object classification.
One key point is that because every pixel is classified, typically the class balance is skewed severely towards the non-object class in a training setting. To add insult to injury, usually the majority of the non-object samples are easy to discriminate, preventing the deep learning method to focus on the challenging samples. van
Grinsven et al. (2016) proposed a selective data sampling in which wrongly classified samples were fed back to the network more often to focus on challenging areas in retinal images. Last, as classifying each pixel in a sliding window fashion results in orders of magnitude of redundant calculation, fCNNs, as used in Wolterink et al. (2016), are important aspect of an object detection pipeline as well.
Challenges in meaningful application of deep learning algorithms in object detection are thus mostly similar to those in object classification.
Only few papers directly address issues specific to object detection like class imbalance/hard-negative mining or efficient pixel/voxel-wise processing of images. We expect that more emphasis will be given to those areas in the near future, for example in the application of multi-stream networks in a fully convolutional fashion.
3.3. Segmentation
3.3.1. Organ and substructure segmentation
The segmentation of organs and other substructures in medical images allows quantitative analysis of clinical parameters related to volume and shape, as, for example, in cardiac or brain analysis. Furthermore, it is often an important first step in computer-aided detection pipelines. The task of segmentation is typically defined as identifying the set of voxels which make up either the contour or the interior of the object(s) of interest.
Segmentation is the most common subject of papers applying deep learning to medical imaging (Figure 1), and as such has also seen the widest variety in methodology, including the development of unique CNN-based segmentation architectures and the wider application of RNNs.
The most well-known, in medical image analysis, of these novel CNN architectures is U-net, published by
Ronneberger et al. (2015) (section 2.4.3). The two main architectural novelties in U-net are the combination of an equal amount of upsampling and downsampling layers. Although learned upsampling layers have been proposed before, U-net combines them with so-called skip connections between opposing convolution and deconvolution layers. This which concatenate features from the contracting and expanding paths. From a training perspective this means that entire images/scans can be processed by U-net in one forward pass, resulting in a segmentation map directly. This allows U-net to take into account the full context of the image, which can be an advantage in contrast to patch-based CNNs. Furthermore, in an extended paper by C¸ ic¸ek et al. (2016), it is shown that a full 3D segmentation can be achieved by feeding U-net with a few 2D annotated slices from the same volume. Other authors have also built derivatives of the U-net architecture; Milletari et al. (2016b), for example, proposed a 3D-variant of U-net architecture, called V-net, performing 3D image segmentation using
3D convolutional layers with an objective function directly based on the Dice coefficient.
Drozdzal et al.(2016) investigated the use of short ResNet-like skip connections in addition to the long skip-connections in a regular U-net.
RNNs have recently become more popular for segmentation tasks. For example, Xie et al. (2016b) used a spatial clockwork RNN to segment the perimysium in H&E-histopathology images.
This network takes into account prior information from both the row and column predecessors of the current patch.
To incorporate bidirectional information from both left/top and right/bottom neighbors, the RNN is applied four times
11 in different orientations and the end-result is concatenated and fed to a fully-connected layer. This produces the final output for a single patch. Stollenga et al. (2015) where the first to use a 3D LSTM-RNN with convolutional layers in six directions. Andermatt et al. (2016) used a 3D RNN with gated recurrent units to segment gray and white matter in a brain MRI data set. Chen et al. (2016d) combined bi-directional LSTM-RNNs with 2D U-net-like-architectures to segment structures in anisotropic 3D electron microscopy images. Last, Poudel et al. (2016) combined a 2D U-net architecture with a gated recurrent unit to perform 3D segmentation.
Although these specific segmentation architectures offered compelling advantages, many authors have also obtained excellent segmentation results with patchtrained neural networks. One of the earliest papers covering medical image segmentation with deep learning algorithms used such a strategy and was published by
Ciresan et al. (2012). They applied pixel-wise segmentation of membranes in electron microscopy imagery in a sliding window fashion. Most recent papers now use fCNNs (subsection 2.4.3) in preference over slidingwindow-based classification to reduce redundant computation. fCNNs have also been extended to 3D and have been applied to multiple targets at once: Korez et al.(2016), used 3D fCNNs to generate vertebral body likelihood maps which drove deformable models for vertebral body segmentation in MR images, Zhou et al.(2016) segmented nineteen targets in the human torso, and Moeskops et al. (2016b) trained a single fCNN to segment brain MRI, the pectoral muscle in breast MRI, and the coronary arteries in cardiac CT angiography(CTA).
One challenge with voxel classification approaches is that they sometimes lead to spurious responses. To combat this, groups have tried to combine fCNNs with graphical models like MRFs (Shakeri et al., 2016; Song et al., 2015) and Conditional Random Fields (CRFs)(Alansary et al., 2016; Cai et al., 2016a; Christ et al., 2016; Dou et al., 2016c; Fu et al., 2016a; Gao et al., 2016c) to refine the segmentation output. In most of the cases, graphical models are applied on top of the likelihood map produced by CNNs or fCNNs and act as label regularizers.
Summarizing, segmentation in medical imaging has seen a huge influx of deep learning related methods.
Custom architectures have been created to directly target the segmentation task. These have obtained promising results, rivaling and often improving over results obtained with fCNNs.
3.3.2. Lesion segmentation
Segmentation of lesions combines the challenges of object detection and organ and substructure segmentation in the application of deep learning algorithms.
Global and local context are typically needed to perform accurate segmentation, such that multi-stream networks with different scales or non-uniformly sampled patches are used as in for example Kamnitsas et al.(2017) and Ghafoorian et al. (2016b). In lesion segmentation we have also seen the application of U-net and similar architectures to leverage both this global and local context. The architecture used by Wang et al.(2015), similar to the U-net, consists of the same downsampling and upsampling paths, but does not use skip connections. Another U-net-like architecture was used by Brosch et al. (2016) to segment white matter lesions in brain MRI. However, they used 3D convolutions and a single skip connection between the first convolutional and last deconvolutional layers.
One other challenge that lesion segmentation shares with object detection is class imbalance, as most voxels/pixels in an image are from the non-diseased class.
Some papers combat this by adapting the loss function:
Brosch et al. (2016) defined it to be a weighted combination of the sensitivity and the specificity, with a larger weight for the specificity to make it less sensitive to the data imbalance. Others balance the data set by performing data augmentation on positive samples (Kamnitsas et al., 2017; Litjens et al., 2016; Pereira et al., 2016).
Thus lesion segmentation sees a mixture of approaches used in object detection and organ segmentation. Developments in these two areas will most likely naturally propagate to lesion segmentation as the existing challenges are also mostly similar.
3.4. Registration
Registration (i.e. spatial alignment) of medical images is a common image analysis task in which a coordinate transform is calculated from one medical image to another. Often this is performed in an iterative framework where a specific type of (non-)parametric transformation is assumed and a pre-determined metric (e.g.
L2-norm) is optimized. Although segmentation and lesion detection are more popular topics for deep learning, researchers have found that deep networks can be beneficial in getting the best possible registration performance. Broadly speaking, two strategies are prevalent in current literature: (1) using deep-learning networks to estimate a similarity measure for two images to drive an iterative optimization strategy, and (2) to directly predict transformation parameters using deep regression networks.
Wu et al. (2013), Simonovsky et al. (2016), and Cheng et al. (2015) used the first strategy to try to optimize registration algorithms. Cheng et al. (2015) used two types of stacked auto-encoders to assess the local similarity between CT and MRI images of the head.
Both auto-encoders take vectorized image patches of CT and MRI and reconstruct them through four layers.
After the networks are pre-trained using unsupervised patch reconstruction they are fine-tuned using two prediction layers stacked on top of the third layer of the SAE. These prediction layers determine whether two patches are similar (class 1) or dissimilar (class 2).
Simonovsky et al. (2016) used a similar strategy, albeit with CNNs, to estimate a similarity cost between two patches from differing modalities. However, they also presented a way to use the derivative of this metric to directly optimize the transformation parameters, which are decoupled from the network itself. Last, Wu et al. (2013) combined independent subspace analysis and convolutional layers to extract features from input patches in an unsupervised manner. The resultant feature vectors are used to drive the HAMMER registration algorithm instead of handcrafted features.
Miao et al. (2016) and Yang et al. (2016d) used deep learning algorithms to directly predict the registration transform parameters given input images. Miao et al.(2016) leveraged CNNs to perform 3D model to 2D xray registration to assess the pose and location of an implanted object during surgery. In total the transformation has 6 parameters, two translational, 1 scaling and 3 angular parameters. They parameterize the feature space in steps of 20 degrees for two angular parameters and train a separate CNN to predict the update to the transformation parameters given an digitally reconstructed x-ray of the 3D model and the actual interoperative x-ray. The CNNs are trained with artificial examples generated by manually adapting the transformation parameters for the input training data.
They showed that their approach has significantly higher registration success rates than using traditional - purely intensity based - registration methods. Yang et al. (2016d) tackled the problem of prior/current registration in brain
MRI using the OASIS data set. They used the large deformation diffeomorphic metric mapping (LDDMM) registration methodology as a basis. This method takes as input an initial momentum value for each pixel which is then evolved over time to obtain the final transformation. However, the calculation of the initial momentum map is often an expensive procure. The authors circumvent this by training a U-net like architecture to predict the x- and y-momentum map given the input images. They obtain visually similar results but with significantly improved execution time: 1500x speed-up for
2D and 66x speed-up for 3D.
In contrast to classification and segmentation, the research community seems not have yet settled on the best way to integrate deep learning techniques in registration methods. Not many papers have yet appeared on the subject and existing ones each have a distinctly different approach. Thus, giving recommendations on what method is most promising seems inappropriate. However, we expect to see many more contributions of deep learning to medical image registration in the near future.
3.5. Other tasks in medical imaging
3.5.1. Content-based image retrieval
Content-based image retrieval (CBIR) is a technique for knowledge discovery in massive databases and offers the possibility to identify similar case histories, understand rare disorders, and, ultimately, improve patient care. The major challenge in the development of CBIR methods is extracting effective feature representations from the pixel-level information and associating them with meaningful concepts. The ability of deep CNN models to learn rich features at multiple levels of abstraction has elicited interest from the CBIR community.
All current approaches use (pre-trained) CNNs to extract feature descriptors from medical images. Anavi et al. (2016) and Liu et al. (2016b) applied their methods to databases of X-ray images. Both used a five-layer
CNN and extracted features from the fully-connected layers. Anavi et al. (2016) used the last layer and a pre-trained network. Their best results were obtained by feeding these features to a one-vs-all support vector machine (SVM) classifier to obtain the distance metric. They showed that incorporating gender information resulted in better performance than just CNN features.
Liu et al. (2016b) used the penultimate fully-connected layer and a custom CNN trained to classify X-rays in 193 classes to obtain the descriptive feature vector. After descriptor binarization and data retrieval using Hamming separation values, the performance was inferior to the state of the art, which the authors attributed to small patch sizes of 96 pixels. The method proposed by Shah et al. (2016) combines CNN feature descriptors with hashing-forests. 1000 features were extracted for overlapping patches in prostate MRI volumes, after which a large feature matrix was constructed over all volumes. Hashing forests were then used to compress this into descriptors for each volume.
Content-based image retrieval as a whole has thus not seen many successful applications of deep learning
Figure 3: Collage of some medical imaging applications in which deep learning has achieved state-of-the-art results. From top-left to bottom-right: mammographic mass classification (Kooi et al., 2016), segmentation of lesions in the brain (top ranking in BRATS, ISLES and MRBrains challenges, image from Ghafoorian et al. (2016b), leak detection in airway tree segmentation (Charbonnier et al., 2017), diabetic retinopathy classification (Kaggle Diabetic Retinopathy challenge 2015, image from van Grinsven et al. (2016), prostate segmentation (top rank in PROMISE12 challenge), nodule classification (top ranking in LUNA16 challenge), breast cancer metastases detection in lymph nodes (top ranking and human expert performance in CAMELYON16), human expert performance in skin lesion classification (Esteva et al., 2017), and state-of-the-art bone suppression in x-rays, image from Yang et al. (2016c). methods yet, but given the results in other areas it seems only a matter of time. An interesting avenue of research could be the direct training of deep networks for the retrieval task itself.
3.5.2. Image Generation and Enhancement
A variety of image generation and enhancement methods using deep architectures have been proposed, ranging from removing obstructing elements in images, normalizing images, improving image quality, data completion, and pattern discovery.
In image generation, 2D or 3D CNNs are used to convert one input image into another. Typically these architectures lack the pooling layers present in classification networks. These systems are then trained with a data set in which both the input and the desired output are present, defining the differences between the generated and desired output as the loss function. Examples are regular and bone-suppressed X-ray in Yang et al.(2016c), 3T and 7T brain MRI in Bahrami et al. (2016), PET from MRI in Li et al. (2014), and CT from MRI in Nie et al. (2016a). Li et al. (2014) even showed that one can use these generated images in computer-aided diagnosis systems for Alzheimer's disease when the original data is missing or not acquired.
With multi-stream CNNs super-resolution images can be generated from multiple low-resolution inputs(section 2.4.2). In Oktay et al. (2016), multi-stream networks reconstructed high-resolution cardiac MRI from one or more low-resolution input MRI volumes. Not only can this strategy be used to infer missing spatial information, but can also be leveraged in other domains; for example, inferring advanced MRI diffusion parameters from limited data (Golkov et al., 2016). Other image enhancement applications like intensity normalization and denoising have seen only limited application of deep learning algorithms. Janowczyk et al. (2016a) used
SAEs to normalize H&E-stained histopathology images whereas Benou et al. (2016) used CNNs to perform denoising in DCE-MRI time-series.
Image generation has seen impressive results with very creative applications of deep networks in significantly differing tasks. One can only expect the number of tasks to increase further in the future.
3.5.3. Combining Image Data With Reports
The combination of text reports and medical image data has led to two avenues of research: (1) leveraging reports to improve image classification accuracy(Schlegl et al., 2015), and (2) generating text reports from images (Kisilev et al., 2016; Shin et al., 2015, 2016a; Wang et al., 2016e); the latter inspired by recent caption generation papers from natural images (Karpathy and Fei-Fei, 2015). To the best of our knowledge, the first step towards leveraging reports was taken by
Schlegl et al. (2015), who argued that large amounts of annotated data may be difficult to acquire and proposed to add semantic descriptions from reports as labels. The system was trained on sets of images along with their textual descriptions and was taught to predict semantic class labels during test time. They showed that semantic information increases classification accuracy for a variety of pathologies in Optical Coherence Tomography(OCT) images.
Shin et al. (2015) and Wang et al. (2016e) mined semantic interactions between radiology reports and images from a large data set extracted from a PACS system. They employed latent Dirichlet allocation (LDA), a type of stochastic model that generates a distribution over a vocabulary of topics based on words in a document. In a later work, Shin et al. (2016a) proposed a sysTable 1: Overview of papers using deep learning techniques for brain image analysis. All works use MRI unless otherwise mentioned.
Reference
Method
Application; remarks
Disorder classification (AD, MCI, Schizophrenia)
Brosch and Tam (2013)
DBN
AD/HC classification; Deep belief networks with convolutional RBMs for manifold learning
Plis et al. (2014)
DBN
Deep belief networks evaluated on brain network estimation, Schizophrenia and Huntington's disease classification
Suk and Shen (2013)
SAE
AD/MCI classification; Stacked auto encoders with supervised fine tuning
Suk et al. (2014)
RBM
AD/MCI/HC classification; Deep Boltzmann Machines on MRI and PET modalities
Payan and Montana (2015)
CNN
AD/MCI/HC classification; 3D CNN pre-trained with sparse auto-encoders
Suk et al. (2015)
SAE
AD/MCI/HC classification; SAE for latent feature extraction on a large set of hand-crafted features from MRI and PET
Hosseini-Asl et al. (2016)
CNN
AD/MCI/HC classification; 3D CNN pre-trained with a 3D convolutional auto-encoder on fMRI data
Kim et al. (2016b)
ANN
Schizophrenia/NH classification on fMRI; Neural network showing advantage of pre-training with SAEs, and L1 sparsification
Ortiz et al. (2016)
DBN
AD/MCI/HC classification; An ensemble of Deep belief networks, with their votes fused using an SVM classifier
Pinaya et al. (2016)
DBN
Schizophrenia/NH classification; DBN pre-training followed by supervised fine-tuning
Sarraf and Tofighi (2016)
CNN
AD/HC classification; Adapted Lenet-5 architecture on fMRI data
Suk et al. (2016)
SAE
MCI/HC classification of fMRI data; Stacked auto-encoders for feature extraction, HMM as a generative model on top
Suk and Shen (2016)
CNN
AD/MCI/HC classification; CNN on sparse representations created by regression models
Shi et al. (2017)
ANN
AD/MCI/HC classification; Multi-modal stacked deep polynomial networks with an SVM classifier on top using MRI and PET
Tissue/anatomy/lesion/tumor segmentation
Guo et al. (2014)
SAE
Hippocampus segmentation; SAE for representation learning used for target/atlas patch similarity measurement de Brebisson and Montana (2015)
CNN
Anatomical segmentation; fusing multi-scale 2D patches with a 3D patch using a CNN
Choi and Jin (2016)
CNN
Striatum segmentation; Two-stage (global/local) approximations with 3D CNNs
Stollenga et al. (2015)
RNN
Tissue segmentation; PyraMiD-LSTM, best brain segmentation results on MRBrainS13 (and competitive results on EM-ISBI12)
Zhang et al. (2015)
CNN
Tissue segmentation; multi-modal 2D CNN
Andermatt et al. (2016)
RNN
Tissue segmentation; two convolutional gated recurrent units in different directions for each dimension
Bao and Chung (2016)
CNN
Anatomical segmentation; Multi-scale late fusion CNN with random walker as a novel label consistency method
Birenbaum and Greenspan (2016)
CNN
Lesion segmentation; Multi-view (2.5D) CNN concatenating features from previous time step for a longitudinal analysis
Brosch et al. (2016)
CNN
Lesion segmentation; Convolutional encoder-decoder network with shortcut connections and convolutional RBM pretraining
Chen et al. (2016a)
CNN
Tissue segmentation; 3D res-net combining features from different layers
Ghafoorian et al. (2016b)
CNN
Lesion segmentation; CNN trained on non-uniformly sampled patch to integrate a larger context with a foviation effect
Ghafoorian et al. (2016a)
CNN
Lesion segmentation; multi-scale CNN with late fusion that integrates anatomical location information into network
Havaei et al. (2016b)
CNN
Tumor segmentation; CNN handling missing modalities with abstraction layer that transforms feature maps to their statistics
Havaei et al. (2016a)
CNN
Tumor segmentation; two-path way CNN with different receptive fields
Kamnitsas et al. (2017)
CNN
Tumor segmentation; 3D multi-scale fully convolutional network with CRF for label consistency
Kleesiek et al. (2016)
CNN
Brain extraction; 3D fully convolutional CNN on multi-modal input
Mansoor et al. (2016)
SAE
Visual pathway segmentation; Learning appearance features from SAE for steering the shape model for segmentation
Milletari et al. (2016a)
CNN
Anatomical segmentation on MRI and US; Hough-voting to acquire mapping from CNN features to full patch segmentations
Moeskops et al. (2016a)
CNN
Tissue segmentation; CNN trained on multiple patch sizes
Nie et al. (2016b)
CNN
Infant tissue segmentation; FCN with a late fusion method on different modalities
Pereira et al. (2016)
CNN
Tumor segmentation; CNN on multiple modality input
Shakeri et al. (2016)
CNN
Anatomical segmentation; FCN followed by Markov random fields
Zhao and Jia (2016)
CNN
Tumor segmentation; Multi-scale CNN with a late fusion architecture
Lesion/tumor detection and classification
Pan et al. (2015)
CNN
Tumor grading; 2D tumor patch classification using a CNN
Dou et al. (2015)
ISA
Microbleed detection; 3D stacked Independent Subspace Analysis for candidate feature extraction, SVM classification
Dou et al. (2016c)
CNN
Microbleed detection; 3D FCN for candidate segmentation followed by a 3D CNN as false positive reduction
Ghafoorian et al. (2017)
CNN
Lacune detection; FCN for candidate segmentation then a multi-scale 3D CNN with anatomical features as false positive reduction
Survival/disease activity/development prediction
Kawahara et al. (2016b)
CNN
Neurodevelopment prediction; CNN with specially-designed edge-to-edge, edge-to-node and node-to-graph conv. layers for brain nets
Nie et al. (2016c)
CNN
Survival prediction; features from a Multi-modal 3D CNN is fused with hand-crafted features to train an SVM
Yoo et al. (2016)
CNN
Disease activity prediction; Training a CNN on the Euclidean distance transform of the lesion masks as the input van der Burgh et al. (2017)
CNN
Survival prediction; DBN on MRI and fusing it with clinical characteristics and structural connectivity data
Image construction/enhancement
Li et al. (2014)
CNN
Image construction; 3D CNN for constructing PET from MR images
Bahrami et al. (2016)
CNN
Image construction; 3D CNN for constructing 7T-like images from 3T MRI
Benou et al. (2016)
SAE
Denoising DCE-MRI; using an ensemble of denoising SAE (pretrained with RBMs)
Golkov et al. (2016)
CNN
Image construction; Per-pixel neural network to predict complex diffusion parameters based on fewer measurements
Hoffmann et al. (2016)
ANN
Image construction; Deep neural nets with SRelu nonlinearity for thermal image construction
Nie et al. (2016a)
CNN
Image construction; 3D fully convolutional network for constructing CT from MR images
Sevetlidis et al. (2016)
ANN
Image construction; Encoder-decoder network for synthesizing one MR modality from another
Other
Brosch et al. (2014)
DBN
Manifold Learning; DBN with conv. RBM layers for modeling the variability in brain morphology and lesion distribution in MS
Cheng et al. (2015)
ANN
Similarity measurement; neural network fusing the moving and reference image patches, pretrained with SAE
Huang et al. (2016)
RBM fMRI blind source separation; RBM for both internal and functional interaction-induced latent sources detection
Simonovsky et al. (2016)
CNN
Similarity measurement; 3D CNN estimating similarity between reference and moving images stacked in the input
Wu et al. (2013)
ISA
Correspondence detection in deformable registration; stacked convolutional ISA for unsupervised feature learning
Yang et al. (2016d)
CNN
Image registration; Conv. encoder-decoder net. predicting momentum in x and y directions, given the moving and fixed image patches tem to generate descriptions from chest X-rays. A CNN was employed to generate a representation of an image one label at a time, which was then used to train an
RNN to generate sequence of MeSH keywords. Kisilev et al. (2016) used a completely different approach and predicted categorical BI-RADS descriptors for breast lesions. In their work they focused on three descriptors used in mammography: shape, margin, and density, Table 2: Overview of papers using deep learning techniques for retinal image analysis. All works use CNNs.
Color fundus images: segmentation of anatomical structures and quality assessment
Fu et al. (2016b)
Blood vessel segmentation; CNN combined with CRF to model long-range pixel interactions
Fu et al. (2016a)
Blood vessel segmentation; extending the approach by Fu et al. (2016b) by reformulating CRF as RNN
Mahapatra et al. (2016)
Image quality assessment; classification output using CNN-based features combined with the output using saliency maps
Maninis et al. (2016)
Segmentation of blood vessels and optic disk; VGG-19 network extended with specialized layers for each segmentation task
Wu et al. (2016)
Blood vessel segmentation; patch-based CNN followed by mapping PCA solution of last layer feature maps to full segmentation
Zilly et al. (2017)
Segmentation of the optic disk and the optic cup; simple CNN with filters sequentially learned using boosting
Color fundus images: detection of abnormalities and diseases
Chen et al. (2015d)
Glaucoma detection; end-to-end CNN, the input is a patch centered at the optic disk
Abr`amoff et al. (2016)
Diabetic retinopathy detection; end-to-end CNN, outperforms traditional method, evaluated on a public dataset
Burlina et al. (2016)
Age-related macular degeneration detection; uses overfeat pretrained network for feature extraction van Grinsven et al. (2016)
Hemorrhage detection; CNN dynamically trained using selective data sampling to perform hard negative mining
Gulshan et al. (2016)
Diabetic retinopathy detection; Inception network, performance comparable to a panel of seven certified ophthalmologists
Prentasic and Loncaric (2016)
Hard exudate detection; end-to-end CNN combined with the outputs of traditional classifiers for detection of landmarks
Worrall et al. (2016)
Retinopathy of prematurity detection; fine-tuned ImageNet trained GoogLeNet, feature map visualization to highlight disease
Work in other imaging modalities
Gao et al. (2015)
Cataract classification in slit lamp images; CNN followed by a set of recursive neural networks to extract higher order features
Schlegl et al. (2015)
Fluid segmentation in OCT; weakly supervised CNN improved with semantic descriptors from clinical reports
Prentasic et al. (2016)
Blood vessel segmentation in OCT angiography; simple CNN, segmentation of several capillary networks where each have their own class label. The system was fed with the image data and region proposals and predicts the correct label for each descriptor (e.g. for shape either oval, round, or irregular).
Given the wealth of data that is available in PACS systems in terms of images and corresponding diagnostic reports, it seems like an ideal avenue for future deep learning research. One could expect that advances in captioning natural images will in time be applied to these data sets as well.
4. Anatomical application areas
This section presents an overview of deep learning contributions to the various application areas in medical imaging. We highlight some key contributions and discuss performance of systems on large data sets and on public challenge data sets. All these challenges are listed on http:\\www.grand-challenge.org.
4.1. Brain
DNNs have been extensively used for brain image analysis in several different application domains (Table 1). A large number of studies address classification of Alzheimer's disease and segmentation of brain tissue and anatomical structures (e.g. the hippocampus).
Other important areas are detection and segmentation of lesions (e.g. tumors, white matter lesions, lacunes, micro-bleeds).
Apart from the methods that aim for a scan-level classification (e.g. Alzheimer diagnosis), most methods learn mappings from local patches to representations and subsequently from representations to labels.
However, the local patches might lack the contextual information required for tasks where anatomical information is paramount (e.g. white matter lesion segmentation). To tackle this, Ghafoorian et al. (2016b) used non-uniformly sampled patches by gradually lowering sampling rate in patch sides to span a larger context.
An alternative strategy used by many groups is multiscale analysis and a fusion of representations in a fullyconnected layer.
Even though brain images are 3D volumes in all surveyed studies, most methods work in 2D, analyzing the 3D volumes slice-by-slice. This is often motivated by either the reduced computational requirements or the thick slices relative to in-plane resolution in some data sets. More recent publications had also employed 3D networks.
DNNs have completely taken over many brain image analysis challenges. In the 2014 and 2015 brain tumor segmentation challenges (BRATS), the 2015 longitudinal multiple sclerosis lesion segmentation challenge, the 2015 ischemic stroke lesion segmentation challenge(ISLES), and the 2013 MR brain image segmentation challenge (MRBrains), the top ranking teams to date have all used CNNs. Almost all of the aforementioned methods are concentrating on brain MR images. We expect that other brain imaging modalities such as CT and US can also benefit from deep learning based analysis.
4.2. Eye
Ophthalmic imaging has developed rapidly over the past years, but only recently are deep learning algorithms being applied to eye image understanding. As summarized in Table 2, most works employ simple
Table 3: Overview of papers using deep learning techniques for chest x-ray image analysis.
Reference
Application
Remarks
Lo et al. (1995)
Nodule detection
Classifies candidates from small patches with two-layer CNN, each with 12 5 × 5 filters
Anavi et al. (2015)
Image retrieval
Combines classical features with those from pre-trained CNN for image retrieval using SVM
Bar et al. (2015)
Pathology detection
Features from a pre-trained CNN and low level features are used to detect various diseases
Anavi et al. (2016)
Image retrieval
Continuation of Anavi et al. (2015), adding age and gender as features
Bar et al. (2016)
Pathology detection
Continuation of Bar et al. (2015), more experiments and adding feature selection
Cicero et al. (2016)
Pathology detection
GoogLeNet CNN detects five common abnormalities, trained and validated on a large data set
Hwang et al. (2016)
Tuberculosis detection
Processes entire radiographs with a pre-trained fine-tuned network with 6 convolution layers
Kim and Hwang (2016)
Tuberculosis detection
MIL framework produces heat map of suspicious regions via deconvolution
Shin et al. (2016a)
Pathology detection
CNN detects 17 diseases, large data set (7k images), recurrent networks produce short captions
Rajkomar et al. (2017)
Frontal/lateral classification
Pre-trained CNN performs frontal/lateral classification task
Yang et al. (2016c)
Bone suppression
Cascade of CNNs at increasing resolution learns bone images from gradients of radiographs
Wang et al. (2016a)
Nodule classification
Combines classical features with CNN features from pre-trained ImageNet CNN
Table 4: Overview of papers using deep learning techniques for chest CT image analysis.
Reference
Application; remarks
Segmentation
Charbonnier et al. (2017)
Airway segmentation where multi-view CNN classifies candidate branches as true airways or leaks
Nodule detection and analysis
Ciompi et al. (2015)
Used a standard feature extractor and a pre-trained CNN to classify detected lesions as benign peri-fissural nodules van Ginneken et al. (2015)
Detects nodules with pre-trained CNN features from orthogonal patches around candidate, classified with SVM
Shen et al. (2015b)
Three CNNs at different scales estimate nodule malignancy scores of radiologists (LIDC-IDRI data set)
Chen et al. (2016e)
Combines features from CNN, SDAE and classical features to characterize nodules from LIDC-IDRI data set
Ciompi et al. (2016)
Multi-stream CNN to classify nodules into subtypes: solid, part-solid, non-solid, calcified, spiculated, perifissural
Dou et al. (2016b)
Uses 3D CNN around nodule candidates; ranks #1 in LUNA16 nodule detection challenge
Li et al. (2016a)
Detects nodules with 2D CNN that processes small patches around a nodule
Setio et al. (2016)
Detects nodules with end-to-end trained multi-stream CNN with 9 patches per candidate
Shen et al. (2016)
3D CNN classifies volume centered on nodule as benign/malignant, results are combined to patient level prediction
Sun et al. (2016b)
Same dataset as Shen et al. (2015b), compares CNN, DBN, SDAE and classical computer-aided diagnosis schemes
Teramoto et al. (2016)
Combines features extracted from 2 orthogonal CT patches and a PET patch
Interstitial lung disease
Anthimopoulos et al. (2016)
Classification of 2D patches into interstitial lung texture classes using a standard CNN
Christodoulidis et al. (2017)
2D interstitial pattern classification with CNNs pre-trained with a variety of texture data sets
Gao et al. (2016c)
Propagates manually drawn segmentations using CNN and CRF for more accurate interstitial lung disease reference
Gao et al. (2016a)
AlexNet applied to large parts of 2D CT slices to detect presence of interstitial patterns
Gao et al. (2016b)
Uses regression to predict area covered in 2D slice with a particular interstitial pattern
Tarando et al. (2016)
Combines existing computer-aided diagnosis system and CNN to classify lung texture patterns. van Tulder and de Bruijne (2016)
Classification of lung texture and airways using an optimal set of filters derived from DBNs and RBMs
Other applications
Tajbakhsh et al. (2015a)
Multi-stream CNN to detect pulmonary embolism from candidates obtained from a tobogganing algorithm
Carneiro et al. (2016)
Predicts 5-year mortality from thick slice CT scans and segmentation masks de Vos et al. (2016a)
Identifies the slice of interest and determine the distance between CT slices
CNNs for the analysis of color fundus imaging (CFI).
A wide variety of applications are addressed: segmentation of anatomical structures, segmentation and detection of retinal abnormalities, diagnosis of eye diseases, and image quality assessment.
In 2015, Kaggle organized a diabetic retinopathy detection competition: Over 35,000 color fundus images were provided to train algorithms to predict the severity of disease in 53,000 test images. The majority of the 661 teams that entered the competition applied deep learning and four teams achieved performance above that of humans, all using end-to-end CNNs. Recently
Gulshan et al. (2016) performed a thorough analysis of the performance of a Google Inception v3 network for diabetic retinopathy detection, showing performance comparable to a panel of seven certified ophthalmologists.
4.3. Chest
In thoracic image analysis of both radiography and computed tomography, the detection, characterization, and classification of nodules is the most commonly addressed application. Many works add features derived from deep networks to existing feature sets or compare
Table 5: Overview of papers using deep learning for digital pathology images. The staining and imaging modality abbreviations used in the table are as follows: H&E: hematoxylin and eosin staining, TIL: Tumor-infiltrating lymphocytes, BCC: Basal cell carcinoma, IHC: immunohistochemistry, RM: Romanowsky, EM: Electron microscopy, PC: Phase contrast, FL: Fluorescent, IFL: Immunofluorescent, TPM: Two-photon microscopy, CM:
Confocal microscopy, Pap: Papanicolaou.
Reference
Topic
Staining\Modality
Method
Nucleus detection, segmentation, and classification
Cires¸an et al. (2013)
Mitosis detection
H&E
CNN-based pixel classifier
Cruz-Roa et al. (2013)
Detection of basal cell carcinoma
H&E
Convolutional auto-encoder neural network
Malon and Cosatto (2013)
Mitosis detection
H&E
Combines shapebased features with CNN
Wang et al. (2014)
Mitosis detection
H&E
Cascaded ensemble of CNN and handcrafted features
Ferrari et al. (2015)
Bacterial colony counting
Culture plate
CNN-based patch classifier
Ronneberger et al. (2015)
Cell segmentation
EM
U-Net with deformation augmentation
Shkolyar et al. (2015)
Mitosis detection
Live-imaging
CNN-based patch classifier
Song et al. (2015)
Segmentation of cytoplasm and nuclei
H&E
Multi-scale CNN and graph-partitioning-based method
Xie et al. (2015a)
Nucleus detection
Ki-67
CNN model that learns the voting offset vectors and voting confidence
Xie et al. (2015b)
Nucleus detection
H&E, Ki-67
CNN-based structured regression model for cell detection
Akram et al. (2016)
Cell segmentation
FL, PC, H&E fCNN for cell bounding box proposal and CNN for segmentation
Albarqouni et al. (2016)
Mitosis detection
H&E
Incorporated 'crowd sourcing' layer into the CNN framework
Bauer et al. (2016)
Nucleus classification
IHC
CNN-based patch classifier
Chen et al. (2016b)
Mitosis detection
H&E
Deep regression network (DRN)
Gao et al. (2016e)
Nucleus classification
IFL
Classification of Hep2-cells with CNN
Han et al. (2016)
Nucleus classification
IFL
Classification of Hep2-cells with CNN
Janowczyk et al. (2016b)
Nucleus segmentation
H&E
Resolution adaptive deep hierarchical learning scheme
Kashif et al. (2016)
Nucleus detection
H&E
Combination of CNN and hand-crafted features
Mao and Yin (2016)
Mitosis detection
PC
Hierarchical CNNs for patch sequence classification
Mishra et al. (2016)
Classification of mitochondria
EM
CNN-based patch classifier
Phan et al. (2016)
Nucleus classification
FL
Classification of Hep2-cells using transfer learning (pre-trained CNN)
Romo-Bucheli et al. (2016)
Tubule nuclei detection
H&E
CNN-based classification of pre-selected candidate nuclei
Sirinukunwattana et al. (2016)
Nucleus detection and classification
H&E
CNN with spatially constrained regression
Song et al. (2017)
Cell segmentation
H&E
Multi-scale CNN
Turkki et al. (2016)
TIL detection
H&E
CNN-based classification of superpixels
Veta et al. (2016)
Nuclear area measurement
H&E
A CNN directly measures nucleus area without requiring segmentation
Wang et al. (2016d)
Subtype cell detection
H&E
Combination of two CNNs for joint cell detection and classification
Xie et al. (2016a)
Nucleus detection and cell counting
FL and H&E
Microscopy cell counting with fully convolutional regression networks
Xing et al. (2016)
Nucleus segmentation
H&E, IHC
CNN and selection-based sparse shape model
Xu et al. (2016b)
Nucleus detection
H&E
Stacked sparse auto-encoders (SSAE)
Xu and Huang (2016)
Nucleus detection
Various
General deep learning framework to detect cells in whole-slide images
Yang et al. (2016b)
Glial cell segmentation
TPM fCNN with an iterative k-terminal cut algorithm
Yao et al. (2016)
Nucleus classification
H&E
Classifies cellular tissue into tumor, lymphocyte, and stromal
Zhao et al. (2016)
Classification of leukocytes
RM
CNN-based patch classifier
Large organ segmentation
Ciresan et al. (2012)
Segmentation of neuronal membranes
EM
Ensemble of several CNNs with different architectures
Kainz et al. (2015)
Segmentation of colon glands
H&E
Used two CNNs to segment glands and their separating structures
Apou et al. (2016)
Detection of lobular structures in breast
IHC
Combined the outputs of a CNN and a texture classification system
BenTaieb and Hamarneh (2016)
Segmentation of colon glands
H&E fCNN with a loss accounting for smoothness and object interactions
BenTaieb et al. (2016)
Segmentation of colon glands
H&E
A multi-loss fCNN to perform both segmentation and classification
Chen et al. (2016d)
Neuronal membrane and fungus segmentation
EM
Combination of bi-directional LSTM-RNNs and kU-Nets
Chen et al. (2017)
Segmentation of colon glands
H&E
Deep contour-aware CNN
C¸ ic¸ek et al. (2016)
Segmentation of xenopus kidney
CM
3D U-Net
Drozdzal et al. (2016)
Segmentation of neuronal structures
EM fCNN with skip connections
Li et al. (2016b)
Segmentation of colon glands
H&E
Compares CNN with an SVM using hand-crafted features
Teikari et al. (2016)
Volumetric vascular segmentation
FL
Hybrid 2D-3D CNN architecture
Wang et al. (2016c)
Segmentation of messy and muscle regions
H&E
Conditional random field jointly trained with an fCNN
Xie et al. (2016b)
Perimysium segmentation
H&E
2D spatial clockwork RNN
Xu et al. (2016d)
Segmentation of colon glands
H&E
Used three CNNs to predict gland and contour pixels
Xu et al. (2016a)
Segmenting epithelium & stroma
H&E, IHC
CNNs applied to over-segmented image regions (superpixels)
Detection and classification of disease
Cruz-Roa et al. (2014)
Detection of invasive ductal carcinoma
H&E
CNN-based patch classifier
Xu et al. (2014)
Patch-level classification of colon cancer
H&E
Multiple instance learning framework with CNN features
Bychkov et al. (2016)
Outcome prediction of colorectal cancer
H&E
Extracted CNN features from epithelial tissue for prediction
Chang et al. (2017)
Multiple cancer tissue classification
Various
Transfer learning using multi-Scale convolutional sparse coding
G¨unhan Ertosun and Rubin (2015)
Grading glioma
H&E
Ensemble of CNNs
K¨all´en et al. (2016)
Predicting Gleason score
H&E
OverFeat pre-trained network as feature extractor
Kim et al. (2016a)
Thyroid cytopathology classification
H&E, RM & Pap
Fine-tuning pre-trained AlexNet
Litjens et al. (2016)
Detection of prostate and breast cancer
H&E fCNN-based pixel classifier
Quinn et al. (2016)
Malaria, tuberculosis and parasites detection
Light microscopy
CNN-based patch classifier
Rezaeilouyeh et al. (2016)
Gleason grading and breast cancer detection
H&E
The system incorporates shearlet features inside a CNN
Schaumberg et al. (2016)
SPOP mutation prediction of prostate cancer
H&E
Ensemble of ResNets
Wang et al. (2016b)
Metastases detection in lymph node
H&E
Ensemble of CNNs with hard negative mining
Other pathology applications
Janowczyk et al. (2016a)
Stain normalization
H&E
Used SAE for classifying tissue and subsequent histogram matching
Janowczyk and Madabhushi (2016)
Deep learning tutorial
Various
Covers different detecting, segmentation, and classification tasks
Sethi et al. (2016)
Comparison of normalization algorithms
H&E
Presents effectiveness of stain normalization for application of CNNs
CNNs with classical machine learning approaches using handcrafted features. In chest X-ray, several groups detect multiple diseases with a single system. In CT the detection of textural patterns indicative of interstitial lung diseases is also a popular research topic.
Chest radiography is the most common radiological exam; several works use a large set of images with text reports to train systems that combine CNNs for image analysis and RNNs for text analysis. This is a branch of research we expect to see more of in the near future.
In a recent challenge for nodule detection in CT, LUNA16, CNN architectures were used by all top performing systems.
This is in contrast with a previous lung nodule detection challenge, ANODE09, where handcrafted features were used to classify nodule candidates. The best systems in LUNA16 still rely on nodule candidates computed by rule-based image processing, but systems that use deep networks for candidate detection also performed very well (e.g. U-net). Estimating the probability that an individual has lung cancer from a CT scan is an important topic: It is the objective of the Kaggle Data Science Bowl 2017, with $1 million in prizes and more than one thousand participating teams.
4.4. Digital pathology and microscopy
The growing availability of large scale gigapixel whole-slide images (WSI) of tissue specimen has made digital pathology and microscopy a very popular application area for deep learning techniques. The developed techniques applied to this domain focus on three broad challenges: (1) Detecting, segmenting, or classifying nuclei, (2) segmentation of large organs, and (3) detecting and classifying the disease of interest at the lesionor WSI-level. Table 5 presents an overview for each of these categories.
Deep learning techniques have also been applied for normalization of histopathology images. Color normalization is an important research area in histopathology image analysis. In Janowczyk et al. (2016a), a method for stain normalization of hematoxylin and eosin (H&E) stained histopathology images was presented based on deep sparse auto-encoders. Recently, the importance of color normalization was demonstrated by Sethi et al.(2016) for CNN based tissue classification in H&E stained images.
The introduction of grand challenges in digital pathology has fostered the development of computerized digital pathology techniques.
The challenges that evaluated existing and new approaches for analysis of digital pathology images are: EM segmentation challenge 2012 for the 2D segmentation of neuronal processes, mitosis detection challenges in ICPR 2012 and AMIDA 2013, GLAS for gland segmentation and, CAMELYON16 and TUPAC for processing breast cancer tissue samples.
In both ICPR 2012 and the AMIDA13 challenges on mitosis detection the IDSIA team outperformed other algorithms with a CNN based approach (Cires¸an et al., 2013). The same team had the highest performing system in EM 2012 (Ciresan et al., 2012) for 2D segmentation of neuronal processes. In their approach, the task of segmenting membranes of neurons was performed by mild smoothing and thresholding of the output of a CNN, which computes pixel probabilities.
GLAS addressed the problem of gland instance segmentation in colorectal cancer tissue samples. Xu et al.(2016d) achieved the highest rank using three CNN models. The first CNN classifies pixels as gland versus non-gland.
From each feature map of the first
CNN, edge information is extracted using the holistically nested edge technique, which uses side convolutions to produce an edge map. Finally, a third CNN merges gland and edge maps to produce the final segmentation.
CAMELYON16 was the first challenge to provide participants with WSIs.
Contrary to other medical imaging applications, the availability of large amount of annotated data in this challenge allowed for training very deep models such as 22-layer GoogLeNet(Szegedy et al., 2014), 16-layer VGG-Net (Simonyan and Zisserman, 2014), and 101-layer ResNet (He et al., 2015). The top-five performing systems used one of these architectures. The best performing solution in the Camelyon16 challenge was presented in Wang et al.(2016b).
This method is based on an ensemble of two GoogLeNet architectures, one trained with and one without hard-negative mining to tackle the challenge. The latest submission of this team using the WSI standardization algorithm by Ehteshami Bejnordi et al.(2016) achieved an AUC of 0.9935, for task 2, which outperformed the AUC of a pathologist (AUC = 0.966) who independently scored the complete test set.
The recently held TUPAC challenge addressed detection of mitosis in breast cancer tissue, and prediction of tumor grading at the WSI level. The top performing system by Paeng et al. (2016) achieved the highest performance in all tasks. The method has three main components: (1) Finding high cell density regions, (2) using a CNN to detect mitoses in the regions of interest, (3) converting the results of mitosis detection to a feature vector for each WSI and using an SVM classifier to compute the tumor proliferation and molecular data scores.
Table 6: Overview of papers using deep learning techniques for breast image analysis. MG = mammography; TS = tomosynthesis; US = ultrasound;
ADN = Adaptive Deconvolution Network.
Reference
Modality
Method
Application; remarks
Sahiner et al. (1996)
MG
CNN
First application of a CNN to mammography
Jamieson et al. (2012)
MG, US
ADN
Four layer ADN, an early form of CNN for mass classification
Fonseca et al. (2015)
MG
CNN
Pre-trained network extracted features classified with SVM for breast density estimation
Akselrod-Ballin et al. (2016)
MG
CNN
Use a modified region proposal CNN (R-CNN) for the localization and classification of masses
Arevalo et al. (2016)
MG
CNN
Lesion classification, combination with hand-crafted features gave the best performance
Dalmis et al. (2017)
MRI
CNN
Breast and fibroglandular tissue segmentation
Dubrovina et al. (2016)
MG
CNN
Tissue classification using regular CNNs
Dhungel et al. (2016)
MG
CNN
Combination of different CNNs combined with hand-crafted features
Fotin et al. (2016)
TS
CNN
Improved state-of-the art for mass detection in tomosynthesis
Hwang and Kim (2016)
MG
CNN
Weakly supervised CNN for localization of masses
Huynh et al. (2016)
MG
CNN
Pre-trained CNN on natural image patches applied to mass classification
Kallenberg et al. (2016)
MG
SAE
Unsupervised CNN feature learning with SAE for breast density classification
Kisilev et al. (2016)
MG
CNN
R-CNN combined with multi-class loss trained on semantic descriptions of potential masses
Kooi et al. (2016)
MG
CNN
Improved the state-of-the art for mass detection and show human performance on a patch level
Qiu et al. (2016)
MG
CNN
CNN for direct classification of future risk of developing cancer based on negative mammograms
Samala et al. (2016a)
TS
CNN
Microcalcification detection
Samala et al. (2016b)
TS
CNN
Pre-trained CNN on mammographic masses transfered to tomosynthesis
Sun et al. (2016a)
MG
CNN
Semi-supervised CNN for classification of masses
Zhang et al. (2016c)
US
RBM
Classification benign vs. malignant with shear wave elastography
Kooi et al. (2017)
MG
CNN
Pre-trained CNN on mass/normal patches to discriminate malignant masses from (benign) cysts
Wang et al. (2017)
MG
CNN
Detection of cardiovascular disease based on vessel calcification
Table 7: Overview of papers using deep learning techniques for cardiac image analysis.
Reference
Modality
Method
Application; remarks
Emad et al. (2015)
MRI
CNN
Left ventricle slice detection; simple CNN indicates if structure is present
Avendi et al. (2016)
MRI
CNN
Left ventricle segmentation; AE used to initialize filters because training data set was small
Kong et al. (2016)
MRI
RNN
Identification of end-diastole and end-systole frames from cardiac sequences
Oktay et al. (2016)
MRI
CNN
Super-resolution; U-net/ResNet hybrid, compares favorably with standard superresolution methods
Poudel et al. (2016)
MRI
RNN
Left ventricle segmentation; RNN processes stack of slices, evaluated on several public datasets
Rupprecht et al. (2016)
MRI
CNN
Cardiac structure segmentation; patch-based CNNs integrated in active contour framework
Tran (2016)
MRI
CNN
Left and right ventricle segmentation; 2D fCNN architecture, evaluated on several public data sets
Yang et al. (2016a)
MRI
CNN
Left ventricle segmentation; CNN combined with multi-atlas segmentation
Zhang et al. (2016b)
MRI
CNN
Identifying presence of apex and base slices in cardiac exam for quality assessment
Ngo et al. (2017)
MRI
DBN
Left ventricle segmentation; DBN is used to initialize a level set framework
Carneiro et al. (2012)
US
DBN
Left ventricle segmentation; DBN embedded in system using landmarks and non-rigid registration
Carneiro and Nascimento (2013)
US
DBN
Left ventricle tracking; extension of Carneiro et al. (2012) for tracking
Chen et al. (2016c)
US
CNN
Structure segmentation in 5 different 2D views; uses transfer learning
Ghesu et al. (2016b)
US
CNN
3D aortic valve detection and segmentation; uses shallow and deeper sparse networks
Nascimento and Carneiro (2016)
US
DBN
Left ventricle segmentation; DBN applied to patches steers multi-atlas segmentation process
Moradi et al. (2016a)
US
CNN
Automatic generation of text descriptions for Doppler US images of cardiac valves using doc2vec
G¨uls¨un et al. (2016)
CT
CNN
Coronary centerline extraction; CNN classifies paths as correct or leakages
Lessmann et al. (2016)
CT
CNN
Coronary calcium detection in low dose ungated CT using multi-stream CNN (3 views)
Moradi et al. (2016b)
CT
CNN
Labeling of 2D slices from cardiac CT exams; comparison with handcrafted features de Vos et al. (2016b)
CT
CNN
Detect bounding boxes by slice classification and combining 3 orthogonal 2D CNNs
Wolterink et al. (2016)
CT
CNN
Coronary calcium detection in gated CTA; compares 3D CNN with multi-stream 2D CNNs
Zreik et al. (2016)
CT
CNN
Left ventricle segmentation; multi-stream CNN (3 views) voxel classification
4.5. Breast
One of the earliest DNN applications from Sahiner et al. (1996) was on breast imaging. Recently, interest has returned which resulted in significant advances over the state of the art, achieving the performance of human readers on ROIs (Kooi et al., 2016). Since most breast imaging techniques are two dimensional, methods successful in natural images can easily be transferred. With one exception, the only task addressed is the detection of breast cancer; this consisted of three subtasks: (1) detection and classification of mass-like lesions, (2) detection and classification of micro-calcifications, and (3) breast cancer risk scoring of images. Mammography is by far the most common modality and has consequently enjoyed the most attention.
Work on tomosynthesis, US, and shear wave elastography is still scarce, and we have only one paper that analyzed breast MRI with deep learning; these other modalities will likely receive more
20 attention in the next few years. Table 6 summarizes the literature and main messages.
Since many countries have screening initiatives for breast cancer, there should be massive amounts of data available, especially for mammography, and therefore enough opportunities for deep models to flourish. Unfortunately, large public digital databases are unavailable and consequently older scanned screen-film data sets are still in use. Challenges such as the recently launched DREAM challenge have not yet had the desired success.
As a result, many papers used small data sets resulting in mixed performance. Several projects have addressed this issue by exploring semi-supervised learning(Sun et al., 2016a), weakly supervised learning (Hwang and Kim, 2016), and transfer learning (Kooi et al., 2017;
Samala et al., 2016b)). Another method combines deep models with handcrafted features (Dhungel et al., 2016), which have been shown to be complementary still, even for very big data sets (Kooi et al., 2016). State of the art techniques for mass-like lesion detection and classification tend to follow a two-stage pipeline with a candidate detector; this design reduces the image to a set of potentially malignant lesions, which are fed to a deep
CNN (Fotin et al., 2016; Kooi et al., 2016). Alternatives use a region proposal network (R-CNN) that bypasses the cascaded approach (Akselrod-Ballin et al., 2016; Kisilev et al., 2016).
When large data sets are available, good results can be obtained.
At the SPIE Medical Imaging conference of 2016, a researcher from a leading company in the mammography CAD field told a packed conference room how a few weeks of experiments with a standard architecture (AlexNet) - trained on the company's proprietary database - yielded a performance that was superior to what years of engineering handcrafted feature systems had achieved (Fotin et al., 2016).
4.6. Cardiac
Deep learning has been applied to many aspects of cardiac image analysis; the literature is summarized in Table 7. MRI is the most researched modality and left ventricle segmentation the most common task, but the number of applications is highly diverse: segmentation, tracking, slice classification, image quality assessment, automated calcium scoring and coronary centerline tracking, and super-resolution.
Most papers used simple 2D CNNs and analyzed the 3D and often 4D data slice by slice; the exception is Wolterink et al. (2016) where 3D CNNs were used.
DBNs are used in four papers, but these all originated from the same author group. The DBNs are only used for feature extraction and are integrated in compound segmentation frameworks. Two papers are exceptional because they combined CNNs with RNNs: Poudel et al.(2016) introduced a recurrent connection within the Unet architecture to segment the left ventricle slice by slice and learn what information to remember from the previous slices when segmenting the next one. Kong et al. (2016) used an architecture with a standard 2D
CNN and an LSTM to perform temporal regression to identify specific frames and a cardiac sequence. Many papers use publicly available data.
The largest challenge in this field was the 2015 Kaggle Data Science
Bowl where the goal was to automatically measure endsystolic and end-diastolic volumes in cardiac MRI. 192 teams competed for $200,000 in prize money and the top ranking teams all used deep learning, in particular fCNN or U-net segmentation schemes.
4.7. Abdomen
Most papers on the abdomen aimed to localize and segment organs, mainly the liver, kidneys, bladder, and pancreas (Table 8). Two papers address liver tumor segmentation. The main modality is MRI for prostate analysis and CT for all other organs. The colon is the only area where various applications were addressed, but always in a straightforward manner: A CNN was used as a feature extractor and these features were used for classification.
It is interesting to note that in two segmentation challenges - SLIVER07 for liver and PROMISE12 for prostate - more traditional image analysis methods were dominant up until 2016. In PROMISE12, the current second and third in rank among the automatic methods used active appearance models.
The algorithm from
IMorphics was ranked first for almost five years (now ranked second). However, a 3D fCNN similar to Unet (Yu et al., 2017) has recently taken the top position.
This paper has an interesting approach where a sumoperation was used instead of the concatenation operation used in U-net, making it a hybrid between a ResNet and U-net architecture. Also in SLIVER07 - a 10-yearold liver segmentation challenge - CNNs have started to appear in 2016 at the top of the leaderboard, replacing previously dominant methods focused on shape and appearance modeling.
4.8. Musculoskeletal
Musculoskeletal images have also been analyzed by deep learning algorithms for segmentation and identification of bone, joint, and associated soft tissue abnormalities in diverse imaging modalities. The works are summarized in Table 9.
Table 8: Overview of papers using deep learning for abdominal image analysis.
Reference
Topic
Modality
Method
Remarks
Multiple
Hu et al. (2016a)
Segmentation
CT
CNN
3D CNN with time-implicit level sets for segmentation of liver, spleen and kidneys
Segmentation tasks in liver imaging
Li et al. (2015)
Lesion
CT
CNN
2D 17×17 patch-based classification, Ben-Cohen et al. (2016) repeats this approach
Vivanti et al. (2015)
Lesion
CT
CNN
2D CNN for liver tumor segmentation in follow-up CT taking baseline CT as input
Ben-Cohen et al. (2016)
Liver
CT
CNN
2D CNN similar to U-net, but without cross-connections; good results on SLIVER07
Christ et al. (2016)
Liver & tumor
CT
CNN
U-net, cascaded fCNN and dense 3D CRF
Dou et al. (2016a)
Liver
CT
CNN
3D CNN with conditional random field; good results on SLIVER07
Hoogi et al. (2016)
Lesion
CT/MRI
CNN
2D CNN obtained probabilities are used to drive active contour model
Hu et al. (2016b)
Liver
CT
CNN
3D CNN with surface evolution of a shape prior; good results on SLIVER07
Lu et al. (2017)
Liver
CT
CNN
3D CNN, competitive results on SLIVER07
Kidneys
Lu et al. (2016)
Localization
CT
CNN
Combines local patch and slice based CNN
Ravishankar et al. (2016b)
Localization
US
CNN
Combines CNN with classical features to detect regions around kidneys
Thong et al. (2016)
Segmentation
CT
CNN
2D CCN with 43×43 patches, tested on 20 scans
Pancreas segmentation in CT
Farag et al. (2015)
Segmentation
CT
CNN
Approach with elements similar to Roth et al. (2015b)
Roth et al. (2015b)
Segmentation
CT
CNN
Orthogonal patches from superpixel regions are fed into CNNs in three different ways
Cai et al. (2016a)
Segmentation
CT
CNN
2 CNNs detect inside and boundary of organ, initializes conditional random field
Roth et al. (2016a)
Segmentation
CT
CNN
2 CNNs detect inside and boundary of pancreas, combined with random forests
Colon
Tajbakhsh et al. (2015b)
Polyp detection
Colonoscopy
CNN
CNN computes additional features, improving existing scheme
Liu et al. (2016a)
Colitis detection
CT
CNN
Pre-trained ImageNet CNN generates features for linear SVM
Nappi et al. (2016)
Polyp detection
CT
CNN
Substantial reduction of false positives using pre-trained and fine-tuned CNN
Tachibana et al. (2016)
Electronic cleansing
CT
CNN
Voxel classification in dual energy CT, material other than soft tissue is removed
Zhang et al. (2017)
Polyp detection
Colonoscopy
CNN
Pre-trained ImageNet CNN for feature extraction, two SVMs for cascaded classification
Prostate segmentation in MRI
Liao et al. (2013)
Application of stacked independent subspace analysis networks
Cheng et al. (2016b)
CNN produces energy map for 2D slice based active appearance segmentation
Guo et al. (2016)
Stacked sparse auto-encoders extract features from patches, input to atlas matching and a deformable model
Milletari et al. (2016b)
3D U-net based CNN architecture with objective function that directly optimizes Dice coefficient, ranks #5 in PROMISE12
Yu et al. (2017)
3D fully convolutional network, hybrid between a ResNet and U-net architecture, ranks #1 on PROMISE12
Prostate
Azizi et al. (2016))
Lesion classification
US
DBN
DBN learns features from temporal US to classify prostate lesions benign/malignant
Shah et al. (2016)
CBIR
MRI
CNN
Features from pre-trained CNN combined with features from hashing forest
Zhu et al. (2017)
Lesion classification
MRI
SAE
Learns features from multiple modalities, hierarchical random forest for classification
Bladder
Cha et al. (2016)
Segmentation
CT
CNN
CNN patch classification used as initialization for level set
Table 9: Overview of papers using deep learning for musculoskeletal image analysis.
Reference
Modality
Application; remarks
Prasoon et al. (2013)
MRI
Knee cartilage segmentation using multi-stream CNNs
Chen et al. (2015c)
CT
Vertebrae localization; joint learning of vertebrae appearance and dependency on neighbors using CNN
Roth et al. (2015c)
CT
Sclerotic metastases detection; random 2D views are analyzed by CNN and aggregated
Shen et al. (2015a)
CT
Vertebrae localization and segmentation; CNN for segmenting vertebrae and for center detection
Suzani et al. (2015)
MRI
Vertebrae localization, identification and segmentation of vertebrae; CNN used for initial localization
Yang et al. (2015)
MRI
Anatomical landmark detection; uses CNN for slice classification for presence of landmark
Antony et al. (2016)
X-ray
Osteoarthritis grading; pre-trained ImageNet CNN fine-tuned on knee X-rays
Cai et al. (2016b)
CT, MRI
Vertebrae localization; RBM determines position, orientation and label of vertebrae
Golan et al. (2016)
US
Hip dysplasia detection; CNN with adversarial component detects structures and performs measurements
Korez et al. (2016)
MRI
Vertebral bodies segmentation; voxel probabilities obtained with a 3D CNN are input to deformable model
Jamaludin et al. (2016)
MRI
Automatic spine scoring; VGG-19 CNN analyzes vertebral discs and finds lesion hotspots
Miao et al. (2016)
X-ray
Total Knee Arthroplasty kinematics by real-time 2D/3D registration using CNN
Roth et al. (2016c)
CT
Posterior-element fractures detection; CNN for 2.5D patch-based analysis
ˇStern et al. (2016)
MRI
Hand age estimation; 2D regression CNN analyzes 13 bones
Forsberg et al. (2017)
MRI
Vertebrae detection and labeling; outputs of two CNNs are input to graphical model
Spampinato et al. (2017)
X-ray
Skeletal bone age assessment; comparison among several deep learning approaches for the task at hand
A surprising number of complete applications with promising results are available; one that stands out is Jamaludin et al. (2016) who trained their system with
12K discs and claimed near-human performances across four different radiological scoring tasks.
4.9. Other
This final section lists papers that address multiple applications (Table 10) and a variety of other applications (Table 11).
It is remarkable that one single architecture or approach based on deep learning can be applied without modifications to different tasks; this illustrates the versatility of deep learning and its general applicability. In some works, pre-trained architectures are used, sometimes trained with images from a completely different domain.
Several authors analyze the effect of fine-tuning a network by training it with a small data set of images from the intended application domain. Combining features extracted by a CNN with 'traditional' features is also commonly seen.
From Table 11, the large number of papers that address obstetric applications stand out. Most papers address the groundwork, such as selecting an appropriate frame from an US stream. More work on automated measurements with deep learning in these US sequences is likely to follow.
The second area where CNNs are rapidly improving the state of the art is dermoscopic image analysis. For a long time, diagnosing skin cancer from photographs was considered very difficult and out of reach for computers. Many studies focused only on images obtained with specialized cameras, and recent systems based on deep networks produced promising results. A recent work by Esteva et al. (2017) demonstrated excellent results with training a recent standard architecture(Google's Inception v3) on a data set of both dermoscopic and standard photographic images. This data set was two orders of magnitude larger than what was used in literature before. In a thorough evaluation, the proposed system performed on par with 30 board certified dermatologists.
5. Discussion
Overview
From the 308 papers reviewed in this survey, it is evident that deep learning has pervaded every aspect of medical image analysis. This has happened extremely quickly: the vast majority of contributions, 242 papers, were published in 2016 or the first month of 2017. A large diversity of deep architectures are covered. The earliest studies used pre-trained CNNs as feature extractors. The fact that these pre-trained networks could simply be downloaded and directly applied to any medical image facilitated their use. Moreover, in this approach already existing systems based on handcrafted features could simply be extended. In the last two years, however, we have seen that end-to-end trained CNNs have become the preferred approach for medical imaging interpretation (see Figure 1). Such CNNs are often integrated into existing image analysis pipelines and replace traditional handcrafted machine learning methods. This is the approach followed by the largest group of papers in this survey and we can confidently state that this is the current standard practice.
Key aspects of successful deep learning methods
After reviewing so many papers one would expect to be able to distill the perfect deep learning method and architecture for each individual task and application area. Although convolutional neural networks (and derivatives) are now clearly the top performers in most medical image analysis competitions, one striking conclusion we can draw is that the exact architecture is not the most important determinant in getting a good solution. We have seen, for example in challenges like the Kaggle Diabetic Retinopathy Challenge, that many researchers use the exact same architectures, the same type of networks, but have widely varying results. A key aspect that is often overlooked is that expert knowledge about the task to be solved can provide advantages that go beyond adding more layers to a CNN.
Groups and researchers that obtain good performance when applying deep learning algorithms often differentiate themselves in aspects outside of the deep network, like novel data preprocessing or augmentation techniques.
An example is that the best performing method in the CAMELYON16-challenge improved significantly (AUC from 0.92 to 0.99) by adding a stain normalization pre-processing step to improve generalization without changing the CNN. Other papers focus on data augmentation strategies to make networks more robust, and they report that these strategies are essential to obtain good performance. An example is the elastic deformations that were applied in the original U-Net paper (Ronneberger et al., 2015).
Augmentation and pre-processing are, of course, not the only key contributors to good solutions. Several researchers have shown that designing architectures incorporating unique task-specific properties can obtain better results than straightforward CNNs. Two examples which we encountered several times are multi-view
Table 10: Overview of papers using a single deep learning approach for different tasks. DQN = Deep Q-Network
Reference
Task
Modality
Method
Remarks
Shin et al. (2013)
Heart, kidney, liver segmentation
MRI
SAE
SAE to learn temporal/spatial features on 2D + time DCE-MRI
Roth et al. (2015a)
2D slice classification
CT
CNN
Automatically classifying slices in 5 anatomical regions
Shin et al. (2015)
2D key image labeling
CT, MRI
CNN
Text and 2D image analysis on a diverse set of 780 thousand images
Cheng et al. (2016a)
Various detection tasks
US, CT
AE, CNN
Detection of breast lesions in US and pulmonary nodules in CT
Ghesu et al. (2016a)
Landmark detection
US, CT, MRI
CNN, DQN
Reinforcement learning with CNN features, cardiac MR/US, head&neck CT
Liu et al. (2016b)
Image retrieval
X-ray
CNN
Combines CNN feature with Radon transform, evaluated on IRMA database
Merkow et al. (2016)
Vascular network segmentation
CT, MRI
CNN
Framework to find various vascular networks
Moeskops et al. (2016b)
Various segmentation tasks
MRI, CT
CNN
Single architecture to segment 6 brain tissues, pectoral muscle & coronaries
Roth et al. (2016b)
Various detection tasks
CT
CNN
Multi-stream CNN to detect sclerotic lesions, lymph nodes and polyps
Shin et al. (2016b)
Abnormality detection
CT
CNN
Compares architectures for detecting interstitial disease and lymph nodes
Tajbakhsh et al. (2016)
Abnormality detection
CT, US
CNN
Compares pre-trained with fully trained networks for three detection tasks
Wang et al. (2016e)
2D key image labeling
CT, MRI
CNN
Text concept clustering, related to Shin et al. (2015)
Yan et al. (2016)
2D slice classification
CT
CNN
Automatically classifying CT slices in 12 anatomical regions
Zhou et al. (2016)
Thorax-abdomen segmentation
CT
CNN
21 structures are segmented with 3 orthogonal 2D fCNNs and majority voting
Table 11: Overview of papers using deep learning for various image analysis tasks.
Reference
Task
Modality
Method
Remarks
Fetal imaging
Chen et al. (2015b)
Frame labeling
US
CNN
Locates abdominal plane from fetal ultrasound videos
Chen et al. (2015a)
Frame labeling
US
RNN
Same task as Chen et al. (2015b), now using RNNs
Baumgartner et al. (2016)
Frame labeling
US
CNN
Labeling 12 standard frames in 1003 mid pregnancy fetal US videos
Gao et al. (2016d)
Frame labeling
US
CNN
4 class frame classification using transfer learning with pre-trained networks
Kumar et al. (2016)
Frame labeling
US
CNN
12 standard anatomical planes, CNN extracts features for support vector machine
Rajchl et al. (2016b)
Segmentation with non expert labels
MRI
CNN
Crowd-sourcing annotation efforts to segment brain structures
Rajchl et al. (2016a)
Segmentation given bounding box
MRI
CNN
CNN and CRF for segmentation of structures
Ravishankar et al. (2016a)
Quantification
US
CNN
Hybrid system using CNN and texture features to find abdominal circumference
Yu et al. (2016b)
Left ventricle segmentation
US
CNN
Frame-by-frame segmentation by dynamically fine-tuning CNN to the latest frame
Dermatology
Codella et al. (2015)
Melanoma detection in dermoscopic images
CNN
Features from pre-trained CNN combined with other features
Demyanov et al. (2016)
Pattern identification in dermoscopic images
CNN
Comparison to simpler networks and simple machine learning
Kawahara et al. (2016a)
5 and 10-class classification photographic images
CNN
Pre-trained CNN for feature extraction at two image resolutions
Kawahara and Hamarneh (2016)
10-class classification photographic images
CNN
Extending Kawahara et al. (2016a) now training multi-resolution CNN end-to-end
Yu et al. (2016a)
Melanoma detection in dermoscopic images
CNN
Deep residual networks for lesion segmentation and classification, winner ISIC16
Menegola et al. (2016)
Classification of dermoscopic images
CNN
Various pre-training and fine-tuning strategies are compared
Esteva et al. (2017)
Classification of photographic and dermoscopic images
CNN
Inception CNN trained on 129k images; compares favorably to 29 dermatologists
Lymph nodes
Roth et al. (2014)
Lymph node detection
CT
CNN
Introduces multi-stream framework of 2D CNNs with orthogonal patches
Barbu et al. (2016)
Lymph node detection
CT
CNN
Compares effect of different loss functions
Nogues et al. (2016)
Lymph node detection
CT
CNN
2 fCNNs, for inside and for contour of lymph nodes, are combined in a CRF
Other
Wang et al. (2015)
Wound segmentation photographs
CNN
Additional detection of infection risk and healing progress
Ypsilantis et al. (2015)
Chemotherapy response prediction
PET
CNN
CNN outperforms classical radiomics features in patients with esophageal cancer
Zheng et al. (2015)
Carotid artery bifurcation detection
CT
CNN
Two stage detection process, CNNs combined with Haar features
Alansary et al. (2016)
Placenta segmentation
MRI
CNN
3D multi-stream CNN with extension for motion correction
Fritscher et al. (2016)
Head&Neck tumor segmentation
CT
CNN
3 orthogonal patches in 2D CNNs, combined with other features
Jaumard-Hakoun et al. (2016)
Tongue contour extraction
US
RBM
Analysis of tongue motion during speech, combines auto-encoders with RBMs
Payer et al. (2016)
Hand landmark detection
X-ray
CNN
Various architectures are compared
Quinn et al. (2016)
Disease detection microscopy
CNN
Smartphone mounted on microscope detects malaria, tuberculosis & parasite eggs
Smistad and Løvstakken (2016)
Vessel detection and segmentation
US
CNN
Femoral and carotid vessels analyzed with standard fCNN
Twinanda et al. (2017)
Task recognition in laparoscopy
Videos
CNN
Fine-tuned AlexNet applied to video frames
Xu et al. (2016c)
Cervical dysplasia cervigrams
CNN
Fine-tuned pre-trained network with added non-imaging features
Xue et al. (2016)
Esophageal microvessel classification
Microscopy
CNN
Simple CNN used for feature extraction
Zhang et al. (2016a)
Image reconstruction
CT
CNN
Reconstructing from limited angle measurements, reducing reconstruction artefacts
Lekadir et al. (2017)
Carotid plaque classification
US
CNN
Simple CNN for characterization of carotid plaque composition in ultrasound
Ma et al. (2017)
Thyroid nodule detection
US
CNN
CNN and standard features combines for 2D US analysis and multi-scale networks. Other, often underestimated, parts of network design are the network input size and receptive field (i.e. the area in input space that contributes to a single output unit). Input sizes should be selected considering for example the required resolution and context to solve a problem. One might increase the size of the patch to obtain more context, but without changing the receptive field of the network this might not be beneficial. As a standard sanity check researchers could perform the same task themselves via visual assessment of the network input. If they, or domain experts, cannot achieve good performance, the chance that you need to modify your network input or architecture is high.
The last aspect we want to touch on is model hyperparameter optimization (e.g. learning rate, dropout rate), which can help squeeze out extra performance from a network. We believe this is of secondary im24 portance with respect to performance to the previously discussed topics and training data quality. Disappointingly, no clear recipe can be given to obtain the best set of hyper-parameters as it is a highly empirical exercise.
Most researchers fall back to an intuition-based random search (Bergstra and Bengio, 2012), which often seems to work well enough. Some basic tips have been covered before by Bengio (2012). Researchers have also looked at Bayesian methods for hyper-parameter optimization (Snoek et al., 2012), but this has not been applied in medical image analysis as far as we are aware of.
Unique challenges in medical image analysis
It is clear that applying deep learning algorithms to medical image analysis presents several unique challenges. The lack of large training data sets is often mentioned as an obstacle. However, this notion is only partially correct. The use of PACS systems in radiology has been routine in most western hospitals for at least a decade and these are filled with millions of images.
There are few other domains where this magnitude of imaging data, acquired for specific purposes, are digitally available in well-structured archives. PACS-like systems are not as broadly used for other specialties in medicine, like ophthalmology and pathology, but this is changing as imaging becomes more prevalent across disciplines. We are also seeing that increasingly large public data sets are made available: Esteva et al. (2017) used 18 public data sets and more than 105 training images; in the Kaggle diabetic retinopathy competition a similar number of retinal images were released; and several chest x-ray studies used more than 104 images.
The main challenge is thus not the availability of image data itself, but the acquisition of relevant annotations/labeling for these images. Traditionally PACS systems store free-text reports by radiologists describing their findings. Turning these reports into accurate annotations or structured labels in an automated manner requires sophisticated text-mining methods, which is an important field of study in itself where deep learning is also widely used nowadays. With the introduction of structured reporting into several areas of medicine, extracting labels to data is expected to become easier in the future. For example, there are already papers appearing which directly leverage BI-RADS categorizations by radiologist to train deep networks (Kisilev et al., 2016) or semantic descriptions in analyzing optical coherence tomography images (Schlegl et al., 2015). We expect the amount of research in optimally leveraging free-text and structured reports for network training to increase in the near future.
Given the complexity of leveraging free-text reports from PACS or similar systems to train algorithms, generally researchers request domain experts (e.g. radiologist, pathologists) to make task-specific annotations for the image data. Labeling a sufficiently large dataset can take a significant amount of time, and this is problematic. For example, to train deep learning systems for segmentation in radiology often 3D, slice-by-slice annotations need to be made and this is very time consuming. Thus, learning efficiently from limited data is an important area of research in medical image analysis. A recent paper focused on training a deep learning segmentation system for 3D segmentation using only sparse 2D segmentations (C¸ ic¸ek et al., 2016). Multipleinstance or active learning approaches might also offer benefit in some cases, and have recently been pursued in the context of deep learning (Yan et al., 2016).
One can also consider leveraging non-expert labels via crowd-sourcing (Rajchl et al., 2016b).
Other potential solutions can be found within the medical field itself; in histopathology one can sometimes use specific immunohistochemical stains to highlight regions of interest, reducing the need for expert experience (Turkki et al., 2016).
Even when data is annotated by domain expert, label noise can be a significant limiting factor in developing algorithms, whereas in computer vision the noise in the labeling of images is typically relatively low. To give an example, a widely used dataset for evaluating image analysis algorithms to detect nodules in lung CT is the LIDC-IDRI dataset (Armato et al., 2011). In this dataset pulmonary nodules were annotated by four radiologists independently. Subsequently the readers reviewed each others annotations but no consensus was forced. It turned out that the number of nodules they did not unanimously agreed on to be a nodule, was three times larger than the number they did fully agree on.
Training a deep learning system on such data requires careful consideration of how to deal with noise and uncertainty in the reference standard.
One could think of solutions like incorporating labeling uncertainty directly in the loss function, but this is still an open challenge.
In medical imaging often classification or segmentation is presented as a binary task: normal versus abnormal, object versus background. However, this is often a gross simplification as both classes can be highly heterogeneous. For example, the normal category often consists of completely normal tissue but also several categories of benign findings, which can be rare, and may occasionally include a wide variety of imaging artifacts. This often leads to systems that are ex25 tremely good at excluding the most common normal subclasses, but fail miserably on several rare ones. A straightforward solution would be to turn the deep learning system in a multi-class system by providing it with detailed annotations of all possible subclasses. Obviously this again compounds the issue of limited availability of expert time for annotating and is therefore often simply not feasible. Some researchers have specifically looked into tackling this imbalance by incorporating intelligence in the training process itself, by applying selective sampling (van Grinsven et al., 2016) or hard negative mining (Wang et al., 2016b). However, such strategies typically fail when there is substantial noise in the reference standard. Additional methods for dealing with within-class heterogeneity would be highly welcome.
Another data-related challenge is class imbalance. In medical imaging, images for the abnormal class might be challenging to find, depending on the task at hand.
As an example, the implementation of breast cancer screening programs has resulted in vast databases of mammograms that have been established at many locations world-wide. However, the majority of these images are normal and do not contain any suspicious lesions. When a mammogram does contain a suspicious lesion this is often not cancerous, and even most cancerous lesions will not lead to the death of a patient.
Designing deep learning systems that are adept at handling this class imbalance is another important area of research. A typical strategy we encountered in current literature is the application of specific data augmentation algorithms to just the underrepresented class, for example scaling and rotation transforms to generate new lesions. Pereira et al. (2016) performed a thorough evaluation of data augmentation strategies for brain lesion segmentation to combat class imbalance.
In medical image analysis useful information is not just contained within the images themselves. Physicians often leverage a wealth of data on patient history, age, demographics and others to arrive at better decisions.
Some authors have already investigated combining this information into deep learning networks in a straightforward manner (Kooi et al., 2017). However, as these authors note, the improvements that were obtained were not as large as expected. One of the challenges is to balance the number of imaging features in the deep learning network (typically thousands) with the number of clinical features (typically only a handful) to prevent the clinical features from being drowned out. Physicians often also need to use anatomical information to come to an accurate diagnosis. However, many deep learning systems in medical imaging are still based on patch classification, where the anatomical location of the patch is often unknown to network. One solution would be to feed the entire image to the deep network and use a different type of evaluation to drive learning, as was done by, for example, Milletari et al. (2016b), who designed a loss function based on the Dice coefficient. This also takes advantage of the fact that medical images are often acquired using a relatively static protocol, where the anatomy is always roughly in the same position and at the same scale. However, as mentioned above, if the receptive field of the network is small feeding in the entire image offers no benefit. Furthermore, feeding full images to the network is not always feasible due to, for example, memory constraints. In some cases this might be solved in the near future due to advances in GPU technology, but in others, for example digital pathology with its gigapixel-sized images, other strategies have to be invented.
Outlook
Although most of the challenges mentioned above have not been adequately tackled yet, several highprofile successes of deep learning in medical imaging have been reported, such as the work by Esteva et al.(2017) and Gulshan et al. (2016) in the fields of dermatology and ophthalmology. Both papers show that it is possible to outperform medical experts in certain tasks using deep learning for image classification. However, we feel it is important to put these papers into context relative to medical image analysis in general, as most tasks can by no means be considered 'solved'. One aspect to consider is that both Esteva et al. (2017) and Gulshan et al. (2016) focus on small 2D color image classification, which is relatively similar to the tasks that have been tackled in computer vision (e.g. ImageNet). This allows them to take advantage of well-explored network architectures like ResNet and VGG-Net which have shown to have excellent results in these tasks. However, there is no guarantee that these architectures are optimal in for example regressions/detection tasks. It also allowed the authors to use networks that were pre-trained on a very well-labeled dataset of millions of natural images, which helps combat the lack of similarly large, labeled medical datasets. In contrast, in most medical imaging tasks 3D gray-scale or multi-channel images are used for which pre-trained networks or architectures dont exist. In addition this data typically has very specific challenges, like anisotropic voxel sizes, small registration errors between varying channels (e.g. in multiparametric MRI) or varying intensity ranges. Although many tasks in medical image analysis can be postulated as a classification problem, this might not always be the 26 optimal strategy as it typically requires some form of post-processing with non-deep learning methods (e.g. counting, segmentation or regression tasks). An interesting example is the paper by Sirinukunwattana et al.(2016), which details a method directly predicting the center locations of nuclei and shows that this outperforms classification-based center localization. Nonetheless, the papers by Esteva et al. (2017) and Gulshan et al.(2016) do show what ideally is possible with deep learning methods that are well-engineered for specific medical image analysis tasks.
Looking at current trends in the machine learning community with respect to deep learning, we identify a key area which can be highly relevant for medical imaging and is receiving (renewed) interest: unsupervised learning. The renaissance of neural networks started around 2006 with the popularization of greedy layerwise pre-training of neural networks in an unsupervised manner. This was quickly superseded by fully supervised methods which became the standard after the success of AlexNet during the ImageNet competition of 2012, and most papers in this survey follow a supervised approach. However, interest in unsupervised training strategies has remained and recently has regained traction.
Unsupervised methods are attractive as they allow(initial) network training with the wealth of unlabeled data available in the world.
Another reason to assume that unsupervised methods will still have a significant role to play is the analogue to human learning, which seems to be much more data efficient and also happens to some extent in an unsupervised manner; we can learn to recognize objects and structures without knowing the specific label. We only need very limited supervision to categorize these recognized objects into classes. Two novel unsupervised strategies which we expect to have an impact in medical imaging are variational auto-encoders (VAEs), introduced by
Kingma and Welling (2013) and generative adversarial networks (GANs), introduced by Goodfellow et al.(2014). The former merges variational Bayesian graphical models with neural networks as encoders/decoders.
The latter uses two competing convolutional neural networks where one is generating artificial data samples and the other is discriminating artificial from real samples. Both have stochastic components and are generative networks. Most importantly, they can be trained end-to-end and learn representative features in a completely unsupervised manner. As we discussed in previous paragraphs, obtaining large amounts of unlabeled medical data is generally much easier than labeled data and unsupervised methods like VAEs and GANs could optimally leverage this wealth of information.
Finally, deep learning methods have often been described as 'black boxes'. Especially in medicine, where accountability is important and can have serious legal consequences, it is often not enough to have a good prediction system. This system also has to be able to articulate itself in a certain way. Several strategies have been developed to understand what intermediate layers of convolutional networks are responding to, for example deconvolution networks (Zeiler and Fergus, 2014), guided back-propagation (Springenberg et al., 2014) or deep Taylor composition (Montavon et al., 2017). Other researchers have tied prediction to textual representations of the image (i.e. captioning) (Karpathy and FeiFei, 2015), which is another useful avenue to understand what a network is perceiving. Last, some groups have tried to combine Bayesian statistics with deep networks to obtain true network uncertainty estimates Kendall and Gal (2017).
This would allow physicians to assess when the network is giving unreliable predictions.
Leveraging these techniques in the application of deep learning methods to medical image analysis could accelerate acceptance of deep learning applications among clinicians, and among patients. We also foresee deep learning approaches will be used for related tasks in medical imaging, mostly unexplored, such as image reconstruction (Wang, 2016). Deep learning will thus not only have a great impact in medical image analysis, but in medical imaging as a whole.
Acknowledgments
The authors would like to thank members of the Diagnostic Image Analysis Group for discussions and suggestions.
This research was funded by grants KUN
2012-5577, KUN 2014-7032, and KUN 2015-7970 of the Dutch Cancer Society.
Appendix A: Literature selection
PubMed was searched for papers containing "convolutional" OR "deep learning" in any field. We specifically did not include the term neural network here as this would result in an enormous amount of 'false positive' papers covering brain research. This search initially gave over 700 hits. ArXiv was searched for papers mentioning one of a set of terms related to medical imaging. The exact search string was: 'abs:((medical
OR mri OR "magnetic resonance" OR CT OR "computed tomography" OR ultrasound OR pathology OR xray OR x-ray OR radiograph OR mammography OR
27 fundus OR OCT) AND ("deep learning" OR convolutional OR cnn OR "neural network"))'. Conference proceedings for MICCAI (including workshops), SPIE, ISBI and EMBC were searched based on titles of papers. Again we looked for mentions of 'deep learning' or 'convolutional' or 'neural network'. We went over all these papers and excluded the ones that did not discuss medical imaging (e.g. applications to genetics, chemistry), only used handcrafted features in combination with neural networks, or only referenced deep learning as future work. When in doubt whether a paper should be included we read the abstract and when the exact methodology was still unclear we read the paper itself. We checked references in all selected papers iteratively and consulted colleagues to identify any papers which were missed by our initial search. When largely overlapping work had been reported in multiple publications, only the publication deemed most important was included. A typical example here was arXiv preprints that were subsequently published or conference contributions which were expanded and published in journals.
References
Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G. S., Davis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I., Harp, A., Irving, G., Isard, M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M., Levenberg, J., Mane, D., Monga, R., Moore, S., Murray, D., Olah, C., Schuster, M., Shlens, J., Steiner, B., Sutskever, I., Talwar, K., Tucker, P., Vanhoucke, V., Vasudevan, V., Viegas, F., Vinyals, O., Warden, P., Wattenberg, M., Wicke, M., Yu, Y., Zheng, X., 2016. Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv:1603.04467.
Abr`amoff, M. D., Lou, Y., Erginay, A., Clarida, W., Amelon, R., Folk, J. C., Niemeijer, M., 2016. Improved automated detection of diabetic retinopathy on a publicly available dataset through integration of deep learning. Invest Ophthalmol Vis Sci 57 (13), 5200–
Akram, S. U., Kannala, J., Eklund, L., Heikkil¨a, J., 2016. Cell segmentation proposal network for microscopy image analysis. In:
DLMIA. Vol. 10008 of Lect Notes Comput Sci. pp. 21–29.
Akselrod-Ballin, A., Karlinsky, L., Alpert, S., Hasoul, S., Ben-Ari, R., Barkan, E., 2016. A region based convolutional network for tumor detection and classification in breast mammography. In: DLMIA.
Vol. 10008 of Lect Notes Comput Sci. pp. 197–205.
Alansary, A., Kamnitsas, K., Davidson, A., Khlebnikov, R., Rajchl, M., Malamateniou, C., Rutherford, M., Hajnal, J. V., Glocker, B., Rueckert, D., Kainz, B., 2016. Fast fully automatic segmentation of the human placenta from motion corrupted MRI. In: Med Image
Comput Comput Assist Interv. Vol. 9901 of Lect Notes Comput
Sci. pp. 589–597.
Albarqouni, S., Baur, C., Achilles, F., Belagiannis, V., Demirci, S., Navab, N., 2016. AggNet: Deep learning from crowds for mitosis detection in breast cancer histology images. IEEE Trans Med
Imaging 35, 1313–1321.
Anavi, Y., Kogan, I., Gelbart, E., Geva, O., Greenspan, H., 2015. A comparative study for chest radiograph image retrieval using binary texture and deep learning classification. In: Conf Proc IEEE
Eng Med Biol Soc. pp. 2940–2943.
Anavi, Y., Kogan, I., Gelbart, E., Geva, O., Greenspan, H., 2016. Visualizing and enhancing a deep learning framework using patients age and gender for chest X-ray image retrieval. In: Medical Imaging. Vol. 9785 of Proceedings of the SPIE. p. 978510.
Andermatt, S., Pezold, S., Cattin, P., 2016. Multi-dimensional gated recurrent units for the segmentation of biomedical 3D-data. In:
DLMIA. Vol. 10008 of Lect Notes Comput Sci. pp. 142–151.
Anthimopoulos, M., Christodoulidis, S., Ebner, L., Christe, A., Mougiakakou, S., 2016. Lung pattern classification for interstitial lung diseases using a deep convolutional neural network. IEEE
Trans Med Imaging 35 (5), 1207–1216.
Antony, J., McGuinness, K., Connor, N. E. O., Moran, K., 2016.
Quantifying radiographic knee osteoarthritis severity using deep convolutional neural networks. arXiv:1609.02469.
Apou, G., Schaadt, N. S., Naegel, B., Forestier, G., Sch¨onmeyer, R., Feuerhake, F., Wemmert, C., Grote, A., 2016. Detection of lobular structures in normal breast tissue. Comput Biol Med 74, 91–102.
Arevalo, J., Gonz´alez, F. A., Ramos-Poll´an, R., Oliveira, J. L., Guevara Lopez, M. A., 2016. Representation learning for mammography mass lesion classification with convolutional neural networks.
Comput Methods Programs Biomed 127, 248–257.
Armato, S. G., McLennan, G., Bidaut, L., McNitt-Gray, M. F., Meyer, C. R., Reeves, A. P., Zhao, B., Aberle, D. R., Henschke, C. I., Hoffman, E. A., Kazerooni, E. A., MacMahon, H., Beek, E. J. R. V., Yankelevitz, D., Biancardi, A. M., Bland, P. H., Brown, M. S., Engelmann, R. M., Laderach, G. E., Max, D., Pais, R. C., Qing, D.
P. Y., Roberts, R. Y., Smith, A. R., Starkey, A., Batrah, P., Caligiuri, P., Farooqi, A., Gladish, G. W., Jude, C. M., Munden, R. F., Petkovska, I., Quint, L. E., Schwartz, L. H., Sundaram, B., Dodd, L. E., Fenimore, C., Gur, D., Petrick, N., Freymann, J., Kirby, J., Hughes, B., Casteele, A. V., Gupte, S., Sallamm, M., Heath, M. D., Kuhn, M. H., Dharaiya, E., Burns, R., Fryd, D. S., Salganicoff, M., Anand, V., Shreter, U., Vastagh, S., Croft, B. Y., 2011. The lung image database consortium (LIDC) and image database resource initiative (IDRI): a completed reference database of lung nodules on CT scans. Med Phys 38, 915–931.
Avendi, M., Kheradvar, A., Jafarkhani, H., 2016. A combined deeplearning and deformable-model approach to fully automatic segmentation of the left ventricle in cardiac MRI. Med Image Anal
30, 108–119.
Azizi, S., Imani, F., Ghavidel, S., Tahmasebi, A., Kwak, J. T., Xu, S., Turkbey, B., Choyke, P., Pinto, P., Wood, B., Mousavi, P., Abolmaesumi, P., 2016. Detection of prostate cancer using temporal sequences of ultrasound data: a large clinical feasibility study. Int
J Comput Assist Radiol Surg 11 (6), 947–956.
Bahrami, K., Shi, F., Rekik, I., Shen, D., 2016. Convolutional neural network for reconstruction of 7T-like images from 3T MRI using appearance and anatomical features. In: DLMIA. Vol. 10008 of Lect Notes Comput Sci. pp. 39–47.
Bao, S., Chung, A. C., 2016. Multi-scale structured CNN with label consistency for brain MR image segmentation. Computer Methods in Biomechanics and Biomedical Engineering: Imaging & Visualization, 1–5.
Bar, Y., Diamant, I., Wolf, L., Greenspan, H., 2015. Deep learning with non-medical training used for chest pathology identification.
In: Medical Imaging. Vol. 9414 of Proceedings of the SPIE. p.
94140V.
Bar, Y., Diamant, I., Wolf, L., Lieberman, S., Konen, E., Greenspan, H., 2016. Chest pathology identification using deep feature selection with non-medical training. Computer Methods in Biomechanics and Biomedical Engineering: Imaging & Visualization, 1–5.
Barbu, A., Lu, L., Roth, H., Seff, A., Summers, R. M., 2016. An analysis of robust cost functions for CNN in computer-aided diagnosis.
Computer Methods in Biomechanics and Biomedical Engineering:
Imaging & Visualization 2016, 1–6.
Bastien, F., Lamblin, P., Pascanu, R., Bergstra, J., Goodfellow, I., Bergeron, A., Bouchard, N., Warde-Farley, D., Bengio, Y., 2012.
Theano: new features and speed improvements. In: Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop.
Bauer, S., Carion, N., Sch¨affler, P., Fuchs, T., Wild, P., Buhmann, J. M., 2016. Multi-organ cancer classification and survival analysis. arXiv:1606.00897.
Baumgartner, C. F., Kamnitsas, K., Matthew, J., Smith, S., Kainz, B., Rueckert, D., 2016. Real-time standard scan plane detection and localisation in fetal ultrasound using fully convolutional neural networks. In: Med Image Comput Comput Assist Interv. Vol. 9901 of Lect Notes Comput Sci. pp. 203–211.
Ben-Cohen, A., Diamant, I., Klang, E., Amitai, M., Greenspan, H., 2016. Dlmia. In: International Workshop on Large-Scale Annotation of Biomedical Data and Expert Label Synthesis. Vol. 10008 of Lect Notes Comput Sci. pp. 77–85.
Bengio, Y., 2012. Practical recommendations for gradient-based training of deep architectures. In: Neural Networks: Tricks of the Trade. Springer Berlin Heidelberg, pp. 437–478.
Bengio, Y., Courville, A., Vincent, P., 2013. Representation learning:
A review and new perspectives. IEEE Trans Pattern Anal Mach
Intell 35 (8), 1798–1828.
Bengio, Y., Lamblin, P., Popovici, D., Larochelle, H., 2007. Greedy layer-wise training of deep networks. In: Advances in Neural Information Processing Systems. pp. 153–160.
Bengio, Y., Simard, P., Frasconi, P., 1994. Learning long-term dependencies with gradient descent is difficult. IEEE Trans Neural Netw
5, 157–166.
Benou, A., Veksler, R., Friedman, A., Raviv, T. R., 2016. De-noising of contrast-enhanced mri sequences by an ensemble of expert deep neural networks. In: DLMIA. Vol. 10008 of Lect Notes Comput
Sci. pp. 95–110.
BenTaieb, A., Hamarneh, G., 2016. Topology aware fully convolutional networks for histology gland segmentation. In: Med Image
Comput Comput Assist Interv. Vol. 9901 of Lect Notes Comput
Sci. pp. 460–468.
BenTaieb, A., Kawahara, J., Hamarneh, G., 2016. Multi-loss convolutional networks for gland analysis in microscopy. In: IEEE Int
Symp Biomedical Imaging. pp. 642–645.
Bergstra, J., Bengio, Y., 2012. Random search for hyper-parameter optimization. J Mach Learn Res 13 (1), 281–305.
Birenbaum, A., Greenspan, H., 2016. Longitudinal multiple sclerosis lesion segmentation using multi-view convolutional neural networks. In: DLMIA. Vol. 10008 of Lect Notes Comput Sci. pp.
58–67.
Brosch, T., Tam, R., 2013. Manifold learning of brain MRIs by deep learning. In: Med Image Comput Comput Assist Interv. Vol. 8150 of Lect Notes Comput Sci. pp. 633–640.
Brosch, T., Tang, L. Y., Yoo, Y., Li, D. K., Traboulsee, A., Tam, R., 2016. Deep 3D convolutional encoder networks with shortcuts for multiscale feature integration applied to Multiple Sclerosis lesion segmentation. IEEE Trans Med Imaging 35 (5), 1229–1239.
Brosch, T., Yoo, Y., Li, D. K. B., Traboulsee, A., Tam, R., 2014.
Modeling the variability in brain morphology and lesion distribution in multiple sclerosis by deep learning. In: Med Image Comput
Comput Assist Interv. Vol. 8674 of Lect Notes Comput Sci. pp.
462–469.
Burlina, P., Freund, D. E., Joshi, N., Wolfson, Y., Bressler, N. M., 2016. Detection of age-related macular degeneration via deep learning. In: IEEE Int Symp Biomedical Imaging. pp. 184–188.
Bychkov, D., Turkki, R., Haglund, C., Linder, N., Lundin, J., 2016.
Deep learning for tissue microarray image-based outcome prediction in patients with colorectal cancer. In: Medical Imaging. Vol.
9791 of Proceedings of the SPIE. p. 979115.
Cai, J., Lu, L., Zhang, Z., Xing, F., Yang, L., Yin, Q., 2016a. Pancreas segmentation in mri using graph-based decision fusion on convolutional neural networks. In: Med Image Comput Comput Assist
Interv. Vol. 9901 of Lect Notes Comput Sci. pp. 442–450.
Cai, Y., Landis, M., Laidley, D. T., Kornecki, A., Lum, A., Li, S., 2016b. Multi-modal vertebrae recognition using transformed deep convolution network. Comput Med Imaging Graph 51, 11–19.
Carneiro, G., Nascimento, J. C., 2013. Combining multiple dynamic models and deep learning architectures for tracking the left ventricle endocardium in ultrasound data. IEEE Trans Pattern Anal Mach
Intell 35, 2592–2607.
Carneiro, G., Nascimento, J. C., Freitas, A., 2012. The segmentation of the left ventricle of the heart from ultrasound data using deep learning architectures and derivative-based search methods. IEEE
Trans Image Process, 968–982.
Carneiro, G., Oakden-Rayner, L., Bradley, A. P., Nascimento, J., Palmer, L., 2016. Automated 5-year mortality prediction using deep learning and radiomics features from chest computed tomography. arXiv:1607.00267.
Cha, K. H., Hadjiiski, L. M., Samala, R. K., Chan, H.-P., Cohan, R. H., Caoili, E. M., Paramagul, C., Alva, A., Weizer, A. Z., Dec. 2016.
Bladder cancer segmentation in CT for treatment response assessment: Application of deep-learning convolution neural network-a pilot study. Tomography 2, 421–429.
Chang, H., Han, J., Zhong, C., Snijders, A., Mao, J.-H., Jan. 2017. Unsupervised transfer learning via multi-scale convolutional sparse coding for biomedical applications. IEEE transactions on pattern analysis and machine intelligence.
Charbonnier, J., van Rikxoort, E., Setio, A., Schaefer-Prokop, C., van
Ginneken, B., Ciompi, F., 2017. Improving airway segmentation in computed tomography using leak detection with convolutional networks. Med Image Anal 36, 52–60.
Chen, H., Dou, Q., Ni, D., Cheng, J.-Z., Qin, J., Li, S., Heng, P.-A., 2015a. Automatic fetal ultrasound standard plane detection using knowledge transferred recurrent neural networks. In: Med Image
Comput Comput Assist Interv. Vol. 9349 of Lect Notes Comput
Sci. Cham, pp. 507–514.
Chen, H., Dou, Q., Yu, L., Heng, P.-A., 2016a. VoxResNet: Deep voxelwise residual networks for volumetric brain segmentation. arXiv:1608.05895.
Chen, H., Ni, D., Qin, J., Li, S., Yang, X., Wang, T., Heng, P. A., 2015b. Standard plane localization in fetal ultrasound via domain transferred deep neural networks. IEEE J Biomed Health Inform
19 (5), 1627–1636.
Chen, H., Qi, X., Yu, L., Heng, P.-A., 2017. DCAN: Deep contouraware networks for accurate gland segmentation. Med Image Anal
36, 135–146.
Chen, H., Shen, C., Qin, J., Ni, D., Shi, L., Cheng, J. C. Y., Heng, P.A., 2015c. Automatic localization and identification of vertebrae in spine CT via a joint learning model with deep neural networks.
In: Med Image Comput Comput Assist Interv. Vol. 9349 of Lect
Notes Comput Sci. pp. 515–522.
Chen, H., Wang, X., Heng, P. A., 2016b. Automated mitosis detection with deep regression networks. In: IEEE Int Symp Biomedical
Imaging. pp. 1204–1207.
Chen, H., Zheng, Y., Park, J.-H., Heng, P.-A., Zhou, S. K., 2016c.
Iterative multi-domain regularized deep learning for anatomical structure detection and segmentation from ultrasound images. In:
Med Image Comput Comput Assist Interv. Vol. 9901 of Lect Notes
Comput Sci. pp. 487–495.
Chen, J., Yang, L., Zhang, Y., Alber, M., Chen, D. Z., 2016d. Combining fully convolutional and recurrent neural networks for 3D biomedical image segmentation. In: Advances in Neural Information Processing Systems. pp. 3036–3044.
Chen, S., Qin, J., Ji, X., Lei, B., Wang, T., Ni, D., Cheng, J.-Z., 2016e.
Automatic scoring of multiple semantic attributes with multi-task feature leverage: A study on pulmonary nodules in CT images.
IEEE Trans Med Imaging, in press.
Chen, X., Xu, Y., Wong, D. W. K., Wong, T. Y., Liu, J., 2015d. Glaucoma detection based on deep convolutional neural network. In:
Conf Proc IEEE Eng Med Biol Soc. pp. 715–718.
Cheng, J.-Z., Ni, D., Chou, Y.-H., Qin, J., Tiu, C.-M., Chang, Y.C., Huang, C.-S., Shen, D., Chen, C.-M., 2016a. Computer-Aided
Diagnosis with deep learning architecture: Applications to breast lesions in US images and pulmonary nodules in CT scans. Nat Sci
Rep 6, 24454.
Cheng, R., Roth, H. R., Lu, L., Wang, S., Turkbey, B., Gandler, W., McCreedy, E. S., Agarwal, H. K., Choyke, P., Summers, R. M., McAuliffe, M. J., 2016b. Active appearance model and deep learning for more accurate prostate segmentation on MRI. In: Medical
Imaging. Vol. 9784 of Proceedings of the SPIE. p. 97842I.
Cheng, X., Zhang, L., Zheng, Y., 2015. Deep similarity learning for multimodal medical images. Computer Methods in Biomechanics and Biomedical Engineering, 1–5.
Cho, K., Van Merri¨enboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., Bengio, Y., 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv:1406.1078.
Choi, H., Jin, K. H., 2016. Fast and robust segmentation of the striatum using deep convolutional neural networks. Journal of Neuroscience Methods 274, 146–153.
Christ, P. F., Elshaer, M. E. A., Ettlinger, F., Tatavarty, S., Bickel, M., Bilic, P., Rempfler, M., Armbruster, M., Hofmann, F., D'Anastasi, M., et al., 2016. Automatic liver and lesion segmentation in CT using cascaded fully convolutional neural networks and 3D conditional random fields. In: Med Image Comput Comput Assist Interv.
Vol. 9901 of Lect Notes Comput Sci. pp. 415–423.
Christodoulidis, S., Anthimopoulos, M., Ebner, L., Christe, A., Mougiakakou, S., 2017. Multi-source transfer learning with convolutional neural networks for lung pattern analysis. IEEE J Biomed
Health Inform 21, 76–84.
C¸ ic¸ek, ¨O., Abdulkadir, A., Lienkamp, S. S., Brox, T., Ronneberger, O., 2016. 3D U-Net: Learning dense volumetric segmentation from sparse annotation. In: Med Image Comput Comput Assist
Interv. Vol. 9901 of Lect Notes Comput Sci. Springer, pp. 424–
Cicero, M., Bilbily, A., Colak, E., Dowdell, T., Gray, B., Perampaladas, K., Barfett, J., 2016. Training and validating a deep convolutional neural network for computer-aided detection and classification of abnormalities on frontal chest radiographs. Invest Radiol, in press.
Ciompi, F., Chung, K., van Riel, S. J., Setio, A. A. A., Gerke, P. K., Jacobs, C., Scholten, E. T., Schaefer-Prokop, C. M., Wille, M. M. W., Marchiano, A., Pastorino, U., Prokop, M., van Ginneken, B., 2016.
Towards automatic pulmonary nodule management in lung cancer screening with deep learning. arXiv:1610.09157.
Ciompi, F., de Hoop, B., van Riel, S. J., Chung, K., Scholten, E. T., Oudkerk, M., de Jong, P. A., Prokop, M., van Ginneken, B., 2015. Automatic classification of pulmonary peri-fissural nodules in computed tomography using an ensemble of 2D views and a convolutional neural network out-of-the-box. Med Image Anal 26, 195–202.
Cires¸an, D. C., Giusti, A., Gambardella, L. M., Schmidhuber, J., 2013.
Mitosis detection in breast cancer histology images with deep neural networks. In: Med Image Comput Comput Assist Interv. Vol.
8150 of Lect Notes Comput Sci. pp. 411–418.
Ciresan, D., Giusti, A., Gambardella, L. M., Schmidhuber, J., 2012.
Deep neural networks segment neuronal membranes in electron microscopy images. In: Advances in Neural Information Processing Systems. pp. 2843–2851.
Codella, N., Cai, J., Abedini, M., Garnavi, R., Halpern, A., Smith, J. R., 2015. Deep learning, sparse coding, and svm for melanoma recognition in dermoscopy images. In: International Workshop on
Machine Learning in Medical Imaging. pp. 118–126.
Collobert, R., Kavukcuoglu, K., Farabet, C., 2011. Torch7: A matlablike environment for machine learning. In: Advances in Neural
Information Processing Systems.
Cruz-Roa, A., Basavanhally, A., Gonz´alez, F., Gilmore, H., Feldman, M., Ganesan, S., Shih, N., Tomaszewski, J., Madabhushi, A., 2014.
Automatic detection of invasive ductal carcinoma in whole slide images with convolutional neural networks. In: Medical Imaging.
Vol. 9041 of Proceedings of the SPIE. p. 904103.
Cruz-Roa, A. A., Ovalle, J. E. A., Madabhushi, A., Osorio, F. A. G., 2013. A deep learning architecture for image representation, visual interpretability and automated basal-cell carcinoma cancer detection. In: Med Image Comput Comput Assist Interv. Vol. 8150 of Lect Notes Comput Sci. pp. 403–410.
Dalmis, M., Litjens, G., Holland, K., Setio, A., Mann, R., Karssemeijer, N., Gubern-M´erida, A., Feb. 2017. Using deep learning to segment breast and fibroglandular tissue in mri volumes. Medical physics 44, 533–546. de Brebisson, A., Montana, G., 2015. Deep neural networks for anatomical brain segmentation. In: Comput Vis Pattern Recognit. pp. 20–28. de Vos, B. D., Viergever, M. A., de Jong, P. A., Iˇsgum, I., 2016a. Automatic slice identification in 3D medical images with a ConvNet regressor. In: DLMIA. Vol. 10008 of Lect Notes Comput Sci. pp.
161–169. de Vos, B. D., Wolterink, J. M., de Jong, P. A., Viergever, M. A., Iˇsgum, I., 2016b. 2D image classification for 3D anatomy localization: employing deep convolutional neural networks. In: Medical
Imaging. Vol. 9784 of Proceedings of the SPIE. p. 97841Y.
Demyanov, S., Chakravorty, R., Abedini, M., Halpern, A., Garnavi, R., 2016. Classification of dermoscopy patterns using deep convolutional neural networks. In: IEEE Int Symp Biomedical Imaging. pp. 364–368.
Dhungel, N., Carneiro, G., Bradley, A. P., 2016. The automated learning of deep features for breast mass classification from mammograms. In: Med Image Comput Comput Assist Interv. Vol. 9901 of Lect Notes Comput Sci. Springer, pp. 106–114.
Dou, Q., Chen, H., Jin, Y., Yu, L., Qin, J., Heng, P.-A., 2016a. 3D deeply supervised network for automatic liver segmentation from
CT volumes. arXiv:1607.00582.
Dou, Q., Chen, H., Yu, L., Qin, J., Heng, P. A., 2016b. Multi-level contextual 3D CNNs for false positive reduction in pulmonary nodule detection, in press.
Dou, Q., Chen, H., Yu, L., Shi, L., Wang, D., Mok, V. C., Heng, P. A., 2015. Automatic cerebral microbleeds detection from MR images via independent subspace analysis based hierarchical features. Conf Proc IEEE Eng Med Biol Soc, 7933–7936.
Dou, Q., Chen, H., Yu, L., Zhao, L., Qin, J., Wang, D., Mok, V. C., Shi, L., Heng, P.-A., 2016c. Automatic detection of cerebral microbleeds from MR images via 3D convolutional neural networks.
IEEE Trans Med Imaging 35, 1182–1195.
Drozdzal, M., Vorontsov, E., Chartrand, G., Kadoury, S., Pal, C., 2016. The importance of skip connections in biomedical image segmentation. In: DLMIA. Vol. 10008 of Lect Notes Comput Sci. pp. 179–187.
Dubrovina, A., Kisilev, P., Ginsburg, B., Hashoul, S., Kimmel, R., 2016. Computational mammography using deep neural networks.
Computer Methods in Biomechanics and Biomedical Engineering:
Imaging & Visualization, 1–5.
Ehteshami Bejnordi, B., Litjens, G., Timofeeva, N., Otte-Holler, I., Homeyer, A., Karssemeijer, N., van der Laak, J., Sep 2016. Stain
30 specific standardization of whole-slide histopathological images.
IEEE Trans Med Imaging 35 (2), 404–415.
URL http://dx.doi.org/10.1109/TMI.2015.2476509
Emad, O., Yassine, I. A., Fahmy, A. S., 2015. Automatic localization of the left ventricle in cardiac MRI images using deep learning. In:
Conf Proc IEEE Eng Med Biol Soc. pp. 683–686.
Esteva, A., Kuprel, B., Novoa, R. A., Ko, J., Swetter, S. M., Blau, H. M., Thrun, S., 2017. Dermatologist-level classification of skin cancer with deep neural networks. Nature 542, 115–118.
Farabet, C., Couprie, C., Najman, L., LeCun, Y., 2013. Learning hierarchical features for scene labeling. IEEE Trans Pattern Anal Mach
Intell 35 (8), 1915–1929.
Farag, A., Lu, L., Roth, H. R., Liu, J., Turkbey, E., Summers, R. M., 2015. A bottom-up approach for pancreas segmentation using cascaded superpixels and (deep) image patch labeling. arXiv:1505.06236.
Ferrari, A., Lombardi, S., Signoroni, A., 2015. Bacterial colony counting by convolutional neural networks. Conf Proc IEEE Eng
Med Biol Soc, 7458–7461.
Fonseca, P., Mendoza, J., Wainer, J., Ferrer, J., Pinto, J., Guerrero, J.and Castaneda, B., 2015. Automatic breast density classification using a convolutional neural network architecture search procedure. In: Medical Imaging. Vol. 9413 of Proceedings of the SPIE. p. 941428.
Forsberg, D., Sj¨oblom, E., Sunshine, J. L., 2017. Detection and labeling of vertebrae in MR images using deep learning with clinical annotations as training data. J Digit Imaging, in press.
Fotin, S. V., Yin, Y., Haldankar, H., Hoffmeister, J. W., Periaswamy, S., 2016. Detection of soft tissue densities from digital breast tomosynthesis: comparison of conventional and deep learning approaches. In: Medical Imaging. Vol. 9785 of Proceedings of the SPIE. p. 97850X.
Fritscher, K., Raudaschl, P., Zaffino, P., Spadea, M. F., Sharp, G. C., Schubert, R., 2016. Deep neural networks for fast segmentation of 3D medical images. In: Med Image Comput Comput Assist Interv.
Vol. 9901 of Lect Notes Comput Sci. pp. 158–165.
Fu, H., Xu, Y., Lin, S., Kee Wong, D. W., Liu, J., 2016a. Deepvessel: Retinal vessel segmentation via?deep learning and conditional random?field. In: Med Image Comput Comput Assist Interv. Vol.
9901 of Lect Notes Comput Sci. pp. 132–139.
Fu, H., Xu, Y., Wong, D. W. K., Liu, J., 2016b. Retinal vessel segmentation via deep learning network and fully-connected conditional random fields. In: IEEE Int Symp Biomedical Imaging. pp.
698–701.
Fukushima, K., 1980. Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. Biol Cybern 36 (4), 193–202.
Gao, M., Bagci, U., Lu, L., Wu, A., Buty, M., Shin, H.-C., Roth, H., Papadakis, G. Z., Depeursinge, A., Summers, R. M., Xu, Z., Mollura, D. J., 2016a. Holistic classification of CT attenuation patterns for interstitial lung diseases via deep convolutional neural networks. Computer Methods in Biomechanics and Biomedical
Engineering: Imaging & Visualization, 1–6.
Gao, M., Xu, Z., Lu, L., Harrison, A. P., Summers, R. M., Mollura, D. J., 2016b. Multi-label deep regression and unordered pooling for holistic interstitial lung disease pattern detection. In: Machine
Learning in Medical Imaging. Vol. 10019 of Lect Notes Comput
Sci. pp. 147–155.
Gao, M., Xu, Z., Lu, L., Nogues, I., Summers, R., Mollura, D., 2016c.
Segmentation label propagation using deep convolutional neural networks and dense conditional random field. In: IEEE Int Symp
Biomedical Imaging. pp. 1265–1268.
Gao, X., Lin, S., Wong, T. Y., 2015. Automatic feature learning to grade nuclear cataracts based on deep learning. IEEE Trans
Biomed Eng 62 (11), 2693–2701.
Gao, Y., Maraci, M. A., Noble, J. A., 2016d. Describing ultrasound video content using deep convolutional neural networks. In: IEEE
Int Symp Biomedical Imaging. pp. 787–790.
Gao, Z., Wang, L., Zhou, L., Zhang, J., 2016e. Hep-2 cell image classification with deep convolutional neural networks. Journal of Biomedical and Health Informatics.
Ghafoorian, M., Karssemeijer, N., Heskes, T., Bergkamp, M., Wissink, J., Obels, J., Keizer, K., de Leeuw, F.-E., van Ginneken, B., Marchiori, E., Platel, B., 2017. Deep multi-scale locationaware 3d convolutional neural networks for automated detection of lacunes of presumed vascular origin. NeuroImage: Clinical, in press.
Ghafoorian, M., Karssemeijer, N., Heskes, T., van Uden, I., Sanchez, C., Litjens, G., de Leeuw, F.-E., van Ginneken, B., Marchiori, E., Platel, B., 2016a. Location sensitive deep convolutional neural networks for segmentation of white matter hyperintensities. arXiv:1610.04834.
Ghafoorian, M., Karssemeijer, N., Heskes, T., van Uden, I. W. M., de Leeuw, F.-E., Marchiori, E., van Ginneken, B., Platel, B., 2016b. Non-uniform patch sampling with deep convolutional neural networks for white matter hyperintensity segmentation. In:
IEEE Int Symp Biomedical Imaging. pp. 1414–1417.
Ghesu, F. C., Georgescu, B., Mansi, T., Neumann, D., Hornegger, J., Comaniciu, D., 2016a. An artificial agent for anatomical landmark detection in medical images. In: Med Image Comput Comput Assist Interv. Vol. 9901 of Lect Notes Comput Sci.
Ghesu, F. C., Krubasik, E., Georgescu, B., Singh, V., Zheng, Y., Hornegger, J., Comaniciu, D., 2016b. Marginal space deep learning: Efficient architecture for volumetric image parsing. IEEE
Trans Med Imaging 35, 1217–1228.
Golan, D., Donner, Y., Mansi, C., Jaremko, J., Ramachandran, M., 2016. Fully automating Graf's method for DDH diagnosis using deep convolutional neural networks. In: DLMIA. Vol. 10008 of Lect Notes Comput Sci. pp. 130–141.
Golkov, V., Dosovitskiy, A., Sperl, J., Menzel, M., Czisch, M., Samann, P., Brox, T., Cremers, D., 2016. q-Space deep learning:
Twelve-fold shorter and model-free diffusion MRI scans. IEEE
Trans Med Imaging 35, 1344 – 1351.
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y., 2014. Generative adversarial nets. arXiv:1406.2661.
Greenspan, H., Summers, R. M., van Ginneken, B., 2016. Deep learning in medical imaging: Overview and future promise of an exciting new technique. IEEE Trans Med Imaging 35 (5), 1153–1159.
Gulshan, V., Peng, L., Coram, M., Stumpe, M. C., Wu, D., Narayanaswamy, A., Venugopalan, S., Widner, K., Madams, T., Cuadros, J., Kim, R., Raman, R., Nelson, P. C., Mega, J. L., Webster, D. R., Dec. 2016. Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs. JAMA 316, 2402–2410.
G¨uls¨un, M. A., Funka-Lea, G., Sharma, P., Rapaka, S., Zheng, Y., 2016. Coronary centerline extraction via optimal flow paths and CNN path pruning. In: Med Image Comput Comput Assist Interv.
Vol. 9902 of Lect Notes Comput Sci. Springer, pp. 317–325.
G¨unhan Ertosun, M., Rubin, D. L., 2015. Automated grading of gliomas using deep learning in digital pathology images: a modular approach with ensemble of convolutional neural networks. In:
AMIA Annual Symposium. pp. 1899–1908.
Guo, Y., Gao, Y., Shen, D., 2016. Deformable MR prostate segmentation via deep feature learning and sparse patch matching. IEEE
Trans Med Imaging 35 (4), 1077–1089.
Guo, Y., Wu, G., Commander, L. A., Szary, S., Jewells, V., Lin, W., Shen, D., 2014. Segmenting hippocampus from infant brains by sparse patch matching with deep-learned features. In: Med Image
Comput Comput Assist Interv. Vol. 8674 of Lect Notes Comput
Sci. pp. 308–315.
Han, X.-H., Lei, J., Chen, Y.-W., 2016. HEp-2 cell classification using
K-support spatial pooling in deep CNNs. In: DLMIA. Vol. 10008 of Lect Notes Comput Sci. pp. 3–11.
Haugeland, J., 1985. Artificial intelligence: the very idea. The MIT
Press, Cambridge, Mass.
Havaei, M., Davy, A., Warde-Farley, D., Biard, A., Courville, A., Bengio, Y., Pal, C., Jodoin, P.-M., Larochelle, H., 2016a. Brain tumor segmentation with Deep Neural Networks. Med Image Anal 35, 18–31.
Havaei, M., Guizard, N., Chapados, N., Bengio, Y., 2016b. HeMIS:
Hetero-modal image segmentation. In: Med Image Comput Comput Assist Interv. Vol. 9901 of Lect Notes Comput Sci. pp. 469–
He, K., Zhang, X., Ren, S., Sun, J., 2015. Deep residual learning for image recognition. arXiv:1512.03385.
Hinton, G., 2010. A practical guide to training restricted Boltzmann machines. Momentum 9 (1), 926.
Hinton, G. E., Osindero, S., Teh, Y.-W., 2006. A fast learning algorithm for deep belief nets. Neural Comput 18, 1527–1554.
Hinton, G. E., Salakhutdinov, R. R., 2006. Reducing the dimensionality of data with neural networks. Science 313, 504–507.
Hochreiter, S., Schmidhuber, J., 1997. Long short-term memory. Neural Computation 9 (8), 1735–1780.
Hoffmann, N., Koch, E., Steiner, G., Petersohn, U., Kirsch, M., 2016.
Learning thermal process representations for intraoperative analysis of cortical perfusion during ischemic strokes. In: DLMIA. Vol.
10008 of Lect Notes Comput Sci. pp. 152–160.
Hoogi, A., Subramaniam, A., Veerapaneni, R., Rubin, D., 2016.
Adaptive estimation of active contour parameters using convolutional neural networks and texture analysis. IEEE Trans Med Imaging.
Hosseini-Asl, E., Gimel'farb, G., El-Baz, A., 2016. Alzheimer's disease diagnostics by a deeply supervised adaptable 3D convolutional network. arXiv:1607.00556.
Hu, P., Wu, F., Peng, J., Bao, Y., Chen, F., Kong, D., Nov. 2016a.
Automatic abdominal multi-organ segmentation using deep convolutional neural network and time-implicit level sets. Int J Comput
Assist Radiol Surg.
Hu, P., Wu, F., Peng, J., Liang, P., Kong, D., Dec. 2016b. Automatic
3D liver segmentation based on deep learning and globally optimized surface evolution. Phys Med Biol 61, 8676–8698.
Huang, H., Hu, X., Han, J., Lv, J., Liu, N., Guo, L., Liu, T., 2016.
Latent source mining in FMRI data via deep neural network. In:
IEEE Int Symp Biomedical Imaging. pp. 638–641.
Huynh, B. Q., Li, H., Giger, M. L., Jul 2016. Digital mammographic tumor classification using transfer learning from deep convolutional neural networks. J Med Imaging 3, 034501.
Hwang, S., Kim, H., 2016. Self-transfer learning for fully weakly supervised object localization. arXiv:1602.01625.
Hwang, S., Kim, H.-E., Jeong, J., Kim, H.-J., 2016. A novel approach for tuberculosis screening based on deep convolutional neural networks. In: Medical Imaging. Vol. 9785 of Proceedings of the SPIE. pp. 97852W–1.
Jamaludin, A., Kadir, T., Zisserman, A., 2016. SpineNet: Automatically pinpointing classification evidence in spinal MRIs. In: Med
Image Comput Comput Assist Interv. Vol. 9901 of Lect Notes
Comput Sci. pp. 166–175.
Jamieson, A. R., Drukker, K., Giger, M. L., 2012. Breast image feature learning with adaptive deconvolutional networks. In: Medical
Imaging. Vol. 8315 of Proceedings of the SPIE. p. 831506.
Janowczyk, A., Basavanhally, A., Madabhushi, A., 2016a. Stain normalization using sparse autoencoders (StaNoSA): Application to digital pathology. Comput Med Imaging Graph, in press.
Janowczyk, A., Doyle, S., Gilmore, H., Madabhushi, A., 2016b.
A resolution adaptive deep hierarchical (RADHicaL) learning scheme applied to nuclear segmentation of digital pathology images. Computer Methods in Biomechanics and Biomedical Engineering: Imaging & Visualization, 1–7.
Janowczyk, A., Madabhushi, A., 2016. Deep learning for digital pathology image analysis: A comprehensive tutorial with selected use cases. Journal of pathology informatics 7, 29.
Jaumard-Hakoun, A., Xu, K., Roussel-Ragot, P., Dreyfus, G., Denby, B., 2016. Tongue contour extraction from ultrasound images based on deep neural network. arXiv:1605.05912.
Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadarrama, S., Darrell, T., 2014. Caffe: Convolutional architecture for fast feature embedding. In: Proceedings of the 22nd
ACM International Conference on Multimedia. pp. 675–678.
Kainz, P., Pfeiffer, M., Urschler, M., 2015. Semantic segmentation of colon glands with deep convolutional neural networks and total variation segmentation. arXiv:1511.06919.
K¨all´en, H., Molin, J., Heyden, A., Lundstr, C., Astr¨om, K., 2016. Towards grading gleason score using generically trained deep convolutional neural networks. In: IEEE Int Symp Biomedical Imaging. pp. 1163–1167.
Kallenberg, M., Petersen, K., Nielsen, M., Ng, A., Diao, P., Igel, C., Vachon, C., Holland, K., Karssemeijer, N., Lillholm, M., 2016.
Unsupervised deep learning applied to breast density segmentation and mammographic risk scoring. IEEE Trans Med Imaging
35, 1322–1331.
Kamnitsas, K., Ledig, C., Newcombe, V. F., Simpson, J. P., Kane, A. D., Menon, D. K., Rueckert, D., Glocker, B., 2017. Efficient multi-scale 3D CNN with fully connected CRF for accurate brain lesion segmentation. Med Image Anal 36, 61–78.
Karpathy, A., Fei-Fei, L., June 2015. Deep visual-semantic alignments for generating image descriptions. In: Comput Vis Pattern
Recognit. ArXiv:1412.2306.
Kashif, M. N., Raza, S. E. A., Sirinukunwattana, K., Arif, M., Rajpoot, N., 2016. Handcrafted features with convolutional neural networks for detection of tumor cells in histology images. In: IEEE
Int Symp Biomedical Imaging. pp. 1029–1032.
Kawahara, J., BenTaieb, A., Hamarneh, G., 2016a. Deep features to classify skin lesions. In: IEEE Int Symp Biomedical Imaging. pp.
1397–1400.
Kawahara, J., Brown, C. J., Miller, S. P., Booth, B. G., Chau, V., Grunau, R. E., Zwicker, J. G., Hamarneh, G., 2016b. BrainNetCNN: Convolutional neural networks for brain networks; towards predicting neurodevelopment. NeuroImage.
Kawahara, J., Hamarneh, G., 2016. Multi-resolution-tract CNN with hybrid pretrained and skin-lesion trained layers. In:
Machine
Learning in Medical Imaging. Vol. 10019 of Lect Notes Comput
Sci. pp. 164–171.
Kendall, A., Gal, Y., 2017. What uncertainties do we need in bayesian deep learning for computer vision? arXiv:1703.04977.
Kim, E., Cortre-Real, M., Baloch, Z., 2016a. A deep semantic mobile application for thyroid cytopathology. In: Medical Imaging. Vol.
9789 of Proceedings of the SPIE. p. 97890A.
Kim, H., Hwang, S., 2016. Scale-invariant feature learning using deconvolutional neural networks for weakly-supervised semantic segmentation. arXiv:1602.04984.
Kim, J., Calhoun, V. D., Shim, E., Lee, J.-H., 2016b. Deep neural network with weight sparsity control and pre-training extracts hierarchical features and enhances classification performance: Evidence from whole-brain resting-state functional connectivity patterns of schizophrenia. NeuroImage 124, 127–146.
Kingma, D. P., Welling, M., 2013. Auto-encoding variational bayes. arXiv:1312.6114.
Kisilev, P., Sason, E., Barkan, E., Hashoul, S., 2016. Medical image description using multi-task-loss CNN. In: International Workshop
32 on Large-Scale Annotation of Biomedical Data and Expert Label
Synthesis. Springer, pp. 121–129.
Kleesiek, J., Urban, G., Hubert, A., Schwarz, D., Maier-Hein, K., Bendszus, M., Biller, A., 2016. Deep MRI brain extraction: A 3D convolutional neural network for skull stripping. NeuroImage 129, 460–469.
Kong, B., Zhan, Y., Shin, M., Denny, T., Zhang, S., 2016. Recognizing end-diastole and end-systole frames via deep temporal regression network. In: Med Image Comput Comput Assist Interv. Vol. 9901 of Lect Notes Comput Sci. pp. 264–272.
Kooi, T., Litjens, G., van Ginneken, B., Gubern-M´erida, A., S´anchez, C. I., Mann, R., den Heeten, A., Karssemeijer, N., 2016. Large scale deep learning for computer aided detection of mammographic lesions. Med Image Anal 35, 303–312.
Kooi, T., van Ginneken, B., Karssemeijer, N., den Heeten, A., 2017.
Discriminating solitary cysts from soft tissue lesions in mammography using a pretrained deep convolutional neural network. Medical Physics.
Korez, R., Likar, B., Pernuˇs, F., Vrtovec, T., 2016. Model-based segmentation of vertebral bodies from MR images with 3D CNNs. In:
Med Image Comput Comput Assist Interv. Vol. 9901 of Lect Notes
Comput Sci. Springer, pp. 433–441.
Krizhevsky, A., Sutskever, I., Hinton, G., 2012. Imagenet classification with deep convolutional neural networks. In: Advances in Neural Information Processing Systems. pp. 1097–1105.
Kumar, A., Sridar, P., Quinton, A., Kumar, R. K., Feng, D., Nanan, R., Kim, J., 2016. Plane identification in fetal ultrasound images using saliency maps and convolutional neural networks. In: IEEE
Int Symp Biomedical Imaging. pp. 791–794.
LeCun, Y., Bottou, L., Bengio, Y., Haffner, P., 1998. Gradient-based learning applied to document recognition. Proceedings of the IEEE
86, 2278–2324.
Lekadir, K., Galimzianova, A., Betriu, A., Del Mar Vila, M., Igual, L., Rubin, D. L., Fernandez, E., Radeva, P., Napel, S., Jan. 2017.
A convolutional neural network for automatic characterization of plaque composition in carotid ultrasound. IEEE J Biomed Health
Inform 21, 48–55.
Lessmann, N., Isgum, I., Setio, A. A., de Vos, B. D., Ciompi, F., de Jong, P. A., Oudkerk, M., Mali, W. P. T. M., Viergever, M. A., van Ginneken, B., 2016. Deep convolutional neural networks for automatic coronary calcium scoring in a screening study with lowdose chest CT. In: Medical Imaging. Vol. 9785 of Proceedings of the SPIE. pp. 978511–1 – 978511–6.
Li, R., Zhang, W., Suk, H.-I., Wang, L., Li, J., Shen, D., Ji, S., 2014.
Deep learning based imaging data completion for improved brain disease diagnosis. In: Med Image Comput Comput Assist Interv.
Vol. 8675 of Lect Notes Comput Sci. pp. 305–312.
Li, W., Cao, P., Zhao, D., Wang, J., 2016a. Pulmonary nodule classification with deep convolutional neural networks on computed tomography images. Computational and Mathematical Methods in Medicine, 6215085.
Li, W., Jia, F., Hu, Q., 2015. Automatic segmentation of liver tumor in CT images with deep convolutional neural networks. Journal of Computer and Communications 3 (11), 146–151.
Li, W., Manivannan, S., Akbar, S., Zhang, J., Trucco, E., McKenna, S. J., 2016b. Gland segmentation in colon histology images using hand-crafted features and convolutional neural networks. In: IEEE
Int Symp Biomedical Imaging. pp. 1405–1408.
Liao, S., Gao, Y., Oto, A., Shen, D., 2013. Representation learning: A unified deep learning framework for automatic prostate mr segmentation. In: Med Image Comput Comput Assist Interv. Vol.
8150 of Lect Notes Comput Sci. pp. 254–261.
Lin, M., Chen, Q., Yan, S., Network in network. arXiv:1312.4400.
Litjens, G., S´anchez, C. I., Timofeeva, N., Hermsen, M., Nagtegaal, I., Kovacs, I., Hulsbergen-van de Kaa, C., Bult, P., van Ginneken, B., van der Laak, J., 2016. Deep learning as a tool for increased accuracy and efficiency of histopathological diagnosis. Nat Sci Rep
Liu, J., Wang, D., Wei, Z., Lu, L., Kim, L., Turkbey, E., Summers, R. M., 2016a. Colitis detection on computed tomography using regional convolutional neural networks. In: IEEE Int Symp Biomedical Imaging. pp. 863–866.
Liu, X., Tizhoosh, H. R., Kofman, J., 2016b. Generating binary tags for fast medical image retrieval based on convolutional nets and Radon transform. In: International Joint Conference on Neural
Networks. ArXiv:1604.04676.
Liu, Y., Gadepalli, K., Norouzi, M., Dahl, G. E., Kohlberger, T., Boyko, A., Venugopalan, S., Timofeev, A., Nelson, P. Q., Corrado, G. S., Hipp, J. D., Peng, L., Stumpe, M. C., 2017. Detecting cancer metastases on gigapixel pathology images. arXiv:1703.02442.
Lo, S.-C., Lou, S.-L., Lin, J.-S., Freedman, M. T., Chien, M. V., Mun, S. K., 1995. Artificial convolution neural network techniques and applications for lung nodule detection. IEEE Trans Med Imaging
14, 711–718.
Long, J., Shelhamer, E., Darrell, T., 2015. Fully convolutional networks for semantic segmentation. arXiv:1411.4038.
Lu, F., Wu, F., Hu, P., Peng, Z., Kong, D., Feb. 2017. Automatic 3D liver location and segmentation via convolutional neural network and graph cut. Int J Comput Assist Radiol Surg 12, 171–182.
Lu, X., Xu, D., Liu, D., 2016. Robust 3d organ localization with dual learning architectures and fusion. In: DLMIA. Vol. 10008 of Lect
Notes Comput Sci. pp. 12–20.
Ma, J., Wu, F., Zhu, J., Xu, D., Kong, D., Jan 2017. A pre-trained convolutional neural network based method for thyroid nodule diagnosis. Ultrasonics 73, 221–230.
Mahapatra, D., Roy, P. K., Sedai, S., Garnavi, R., 2016. Retinal image quality classification using saliency maps and CNNs. In: Machine
Learning in Medical Imaging. Vol. 10019 of Lect Notes Comput
Sci. pp. 172–179.
Malon, C. D., Cosatto, E., 2013. Classification of mitotic figures with convolutional neural networks and seeded blob features. Journal of pathology informatics.
Maninis, K.-K., Pont-Tuset, J., Arbel´aez, P., Gool, L., 2016. Deep retinal image understanding. In: Med Image Comput Comput Assist
Interv. Vol. 9901 of Lect Notes Comput Sci. pp. 140–148.
Mansoor, A., Cerrolaza, J., Idrees, R., Biggs, E., Alsharid, M., Avery, R., Linguraru, M. G., 2016. Deep learning guided partitioned shape model for anterior visual pathway segmentation. IEEE Trans Med
Imaging 35 (8), 1856–1865.
Mao, Y., Yin, Z., 2016. A hierarchical convolutional neural network for mitosis detection in phase-contrast microscopy images.
In: Med Image Comput Comput Assist Interv. Vol. 9901 of Lect
Notes Comput Sci. pp. 685–692.
Menegola, A., Fornaciali, M., Pires, R., Avila, S., Valle, E., 2016. Towards automated melanoma screening: Exploring transfer learning schemes. arXiv:1609.01228.
Merkow, J., Kriegman, D., Marsden, A., Tu, Z., 2016. Dense volumeto-volume vascular boundary detection. arXiv:1605.08401.
Miao, S., Wang, Z. J., Liao, R., 2016. A CNN regression approach for real-time 2D/3D registration. IEEE Trans Med Imaging 35 (5), 1352–1363.
Milletari, F., Ahmadi, S.-A., Kroll, C., Plate, A., Rozanski, V., Maiostre, J., Levin, J., Dietrich, O., Ertl-Wagner, B., B¨otzel, K., Navab, N., 2016a. Hough-CNN: Deep learning for segmentation of deep brain regions in MRI and ultrasound. arXiv:1601.07014.
Milletari, F., Navab, N., Ahmadi, S.-A., 2016b. V-Net: Fully convolutional neural networks for volumetric medical image segmentation. arXiv:1606.04797.
Mishra, M., Schmitt, S., Wang, L., Strasser, M. K., Marr, C., Navab, N., Zischka, H., Peng, T., 2016. Structure-based assessment of cancerous mitochondria using deep networks. In: IEEE Int Symp
Biomedical Imaging. pp. 545–548.
Moeskops, P., Viergever, M. A., Mendrik, A. M., de Vries, L. S., Benders, M. J. N. L., Isgum, I., 2016a. Automatic segmentation of MR brain images with a convolutional neural network. IEEE Trans
Med Imaging 35 (5), 1252–1262.
Moeskops, P., Wolterink, J. M., Velden, B. H. M., Gilhuijs, K. G. A., Leiner, T., Viergever, M. A., Isgum, I., 2016b. Deep learning for multi-task medical image segmentation in multiple modalities. In:
Med Image Comput Comput Assist Interv. Vol. 9901 of Lect Notes
Comput Sci. pp. 478–486.
Montavon, G., Lapuschkin, S., Binder, A., Samek, W., M¨uller, K.R., 2017. Explaining nonlinear classification decisions with deep taylor decomposition. Pattern Recognition 65, 211–222.
Moradi, M., Guo, Y., Gur, Y., Negahdar, M., Syeda-Mahmood, T., 2016a. A cross-modality neural network transform for semiautomatic medical image annotation. In:
Med Image Comput
Comput Assist Interv. Vol. 9901 of Lect Notes Comput Sci. pp.
300–307.
Moradi, M., Gur, Y., Wang, H., Prasanna, P., Syeda-Mahmood, T., 2016b. A hybrid learning approach for semantic labeling of cardiac
CT slices and recognition of body position. In: IEEE Int Symp
Biomedical Imaging.
Nappi, J. J., Hironaka, T., Regge, D., Yoshida, H., 2016. Deep transfer learning of virtual endoluminal views for the detection of polyps in CT colonography. In: Medical Imaging. Proceedings of the SPIE. p. 97852B.
Nascimento, J. C., Carneiro, G., 2016. Multi-atlas segmentation using manifold learning with deep belief networks. In: IEEE Int Symp
Biomedical Imaging. pp. 867–871.
Ngo, T. A., Lu, Z., Carneiro, G., 2017. Combining deep learning and level set for the automated segmentation of the left ventricle of the heart from cardiac cine magnetic resonance. Med Image Anal 35, 159–171.
Nie, D., Cao, X., Gao, Y., Wang, L., Shen, D., 2016a. Estimating CT image from MRI data using 3D fully convolutional networks. In:
DLMIA. Vol. 10008 of Lect Notes Comput Sci. pp. 170–178.
Nie, D., Wang, L., Gao, Y., Shen, D., 2016b. Fully convolutional networks for multi-modality isointense infant brain image segmentation. In: IEEE Int Symp Biomedical Imaging. pp. 1342–1345.
Nie, D., Zhang, H., Adeli, E., Liu, L., Shen, D., 2016c. 3D deep learning for multi-modal imaging-guided survival time prediction of brain tumor patients. In: Med Image Comput Comput Assist
Interv. Vol. 9901 of Lect Notes Comput Sci. pp. 212–220.
Nogues, I., Lu, L., Wang, X., Roth, H., Bertasius, G., Lay, N., Shi, J., Tsehay, Y., Summers, R. M., 2016. Automatic lymph node cluster segmentation using holistically-nested neural networks and structured optimization in CT images. In: Med Image Comput Comput
Assist Interv. Vol. 9901 of Lect Notes Comput Sci. pp. 388–397.
Oktay, O., Bai, W., Lee, M., Guerrero, R., Kamnitsas, K., Caballero, J., Marvao, A., Cook, S., O'Regan, D., Rueckert, D., 2016. Multiinput cardiac image super-resolution using convolutional neural networks. In: Med Image Comput Comput Assist Interv. Vol. 9902 of Lect Notes Comput Sci. pp. 246–254.
Ortiz, A., Munilla, J., G´orriz, J. M., Ram´ırez, J., 2016. Ensembles of deep learning architectures for the early diagnosis of the Alzheimer's disease. International Journal of Neural Systems 26, Paeng, K., Hwang, S., Park, S., Kim, M., Kim, S., 2016. A unified framework for tumor proliferation score prediction in breast histopathology. arXiv:1612.07180.
Pan, Y., Huang, W., Lin, Z., Zhu, W., Zhou, J., Wong, J., Ding, Z., 2015. Brain tumor grading based on neural networks and convolutional neural networks. In: Conf Proc IEEE Eng Med Biol Soc. pp.
699–702.
Payan, A., Montana, G., 2015. Predicting Alzheimer's disease: a neuroimaging study with 3D convolutional neural networks. arXiv:1502.02506.
Payer, C., Stern, D., Bischof, H., Urschler, M., 2016. Regressing heatmaps for multiple landmark localization using CNNs. In: Med
Image Comput Comput Assist Interv. Vol. 9901 of Lect Notes
Comput Sci. pp. 230–238.
Pereira, S., Pinto, A., Alves, V., Silva, C. A., 2016. Brain tumor segmentation using convolutional neural networks in MRI images.
IEEE Trans Med Imaging.
Phan, H. T. H., Kumar, A., Kim, J., Feng, D., 2016. Transfer learning of a convolutional neural network for HEp-2 cell image classification. In: IEEE Int Symp Biomedical Imaging. pp. 1208–1211.
Pinaya, W. H. L., Gadelha, A., Doyle, O. M., Noto, C., Zugman, A., Cordeiro, Q., Jackowski, A. P., Bressan, R. A., Sato, J. R., Dec.
2016. Using deep belief network modelling to characterize differences in brain morphometry in schizophrenia. Nat Sci Rep 6, Plis, S. M., Hjelm, D. R., Salakhutdinov, R., Allen, E. A., Bockholt, H. J., Long, J. D., Johnson, H. J., Paulsen, J. S., Turner, J. A., Calhoun, V. D., 2014. Deep learning for neuroimaging: a validation study. Frontiers in Neuroscience.
Poudel, R. P. K., Lamata, P., Montana, G., 2016. Recurrent fully convolutional neural networks for multi-slice MRI cardiac segmentation. arXiv:1608.03974.
Prasoon, A., Petersen, K., Igel, C., Lauze, F., Dam, E., Nielsen, M., 2013. Deep feature learning for knee cartilage segmentation using a triplanar convolutional neural network. In: Med Image Comput
Comput Assist Interv. Vol. 8150 of Lect Notes Comput Sci. pp.
246–253.
Prentasic, P., Heisler, M., Mammo, Z., Lee, S., Merkur, A., Navajas, E., Beg, M. F., Sarunic, M., Loncaric, S., 2016. Segmentation of the foveal microvasculature using deep learning networks. Journal of Biomedical Optics 21, 75008.
Prentasic, P., Loncaric, S., 2016. Detection of exudates in fundus photographs using deep neural networks and anatomical landmark detection fusion. Comput Methods Programs Biomed 137, 281–292.
Qiu, Y., Wang, Y., Yan, S., Tan, M., Cheng, S., Liu, H., Zheng, B., 2016. An initial investigation on developing a new method to predict short-term breast cancer risk based on deep learning technology. In: Medical Imaging. Vol. 9785 of Proceedings of the SPIE. p. 978521.
Quinn, J. A., Nakasi, R., Mugagga, P. K. B., Byanyima, P., Lubega, W., Andama, A., 2016. Deep convolutional neural networks for microscopy-based point of care diagnostics. arXiv:1608.02989.
Rajchl, M., Lee, M. C., Oktay, O., Kamnitsas, K., Passerat-Palmbach, J., Bai, W., Kainz, B., Rueckert, D., 2016a. DeepCut: Object segmentation from bounding box annotations using convolutional neural networks. IEEE Trans Med Imaging, in press.
Rajchl, M., Lee, M. C., Schrans, F., Davidson, A., Passerat-Palmbach, J., Tarroni, G., Alansary, A., Oktay, O., Kainz, B., Rueckert, D., 2016b. Learning under distributed weak supervision. arXiv:1606.01100.
Rajkomar, A., Lingam, S., Taylor, A. G., Blum, M., Mongan, J., 2017.
High-throughput classification of radiographs using deep convolutional neural networks. J Digit Imaging 30, 95–101.
Ravi, D., Wong, C., Deligianni, F., Berthelot, M., Andreu-Perez, J., Lo, B., Yang, G.-Z., Jan. 2017. Deep learning for health informatics. IEEE J Biomed Health Inform 21, 4–21.
Ravishankar, H., Prabhu, S. M., Vaidya, V., Singhal, N., 2016a.
Hybrid approach for automatic segmentation of fetal abdomen from ultrasound images using deep learning. In: IEEE Int Symp
Biomedical Imaging. pp. 779–782.
Ravishankar, H., Sudhakar, P., Venkataramani, R., Thiruvenkadam, S., Annangi, P., Babu, N., Vaidya, V., 2016b. Understanding the mechanisms of deep transfer learning for medical images. In:
DLMIA. Vol. 10008 of Lect Notes Comput Sci. pp. 188–196.
Rezaeilouyeh, H., Mollahosseini, A., Mahoor, M. H., 2016. Microscopic medical image classification framework via deep learning and shearlet transform. Journal of Medical Imaging 3 (4), 044501.
Romo-Bucheli, D., Janowczyk, A., Gilmore, H., Romero, E., Madabhushi, A., Sep 2016. Automated tubule nuclei quantification and correlation with Oncotype DX risk categories in ER+ breast cancer whole slide images. Nat Sci Rep 6, 32706.
Ronneberger, O., Fischer, P., Brox, T., 2015. U-net: Convolutional networks for biomedical image segmentation. In:
Med Image
Comput Comput Assist Interv. Vol. 9351 of Lect Notes Comput
Sci. pp. 234–241.
Roth, H. R., Lee, C. T., Shin, H.-C., Seff, A., Kim, L., Yao, J., Lu, L., Summers, R. M., 2015a. Anatomy-specific classification of medical images using deep convolutional nets. In: IEEE Int Symp
Biomedical Imaging. pp. 101–104.
Roth, H. R., Lu, L., Farag, A., Shin, H.-C., Liu, J., Turkbey, E. B., Summers, R. M., 2015b. DeepOrgan: Multi-level deep convolutional networks for automated pancreas segmentation. In: Med Image Comput Comput Assist Interv. Vol. 9349 of Lect Notes Comput Sci. pp. 556–564.
Roth, H. R., Lu, L., Farag, A., Sohn, A., Summers, R. M., 2016a.
Spatial aggregation of holistically-nested networks for automated pancreas segmentation. In: Med Image Comput Comput Assist Interv. Vol. 9901 of Lect Notes Comput Sci. pp. 451–459.
Roth, H. R., Lu, L., Liu, J., Yao, J., Seff, A., Cherry, K., Kim, L., Summers, R. M., 2016b. Improving computer-aided detection using convolutional neural networks and random view aggregation.
IEEE Trans Med Imaging 35 (5), 1170–1181.
Roth, H. R., Lu, L., Seff, A., Cherry, K. M., Hoffman, J., Wang, S., Liu, J., Turkbey, E., Summers, R. M., 2014. A new 2.5D representation for lymph node detection using random sets of deep convolutional neural network observations. In: Med Image Comput
Comput Assist Interv. Vol. 8673 of Lect Notes Comput Sci. pp.
520–527.
Roth, H. R., Wang, Y., Yao, J., Lu, L., Burns, J. E., Summers, R. M., 2016c. Deep convolutional networks for automated detection of posterior-element fractures on spine CT. In: Medical Imaging. Vol.
9785 of Proceedings of the SPIE. p. 97850P.
Roth, H. R., Yao, J., Lu, L., Stieger, J., Burns, J. E., Summers, R. M., 2015c. Detection of sclerotic spine metastases via random aggregation of deep convolutional?neural network classifications. In: Recent Advances in Computational Methods and Clinical Applications for Spine Imaging. Vol. 20 of Lecture Notes in Computational
Vision and Biomechanics. pp. 3–12.
Rupprecht, C., Huaroc, E., Baust, M., Navab, N., 2016. Deep active contours. arXiv:1607.05074.
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., Fei-Fei, L., 2014. ImageNet large scale visual recognition challenge. Int J Comput Vis 115 (3), 1–42.
Sahiner, B., Chan, H.-P., Petrick, N., Wei, D., Helvie, M. A., Adler, D. D., Goodsitt, M. M., 1996. Classification of mass and normal breast tissue: a convolution neural network classifier with spatial domain and texture images. IEEE Trans Med Imaging 15, 598–
Samala, R. K., Chan, H.-P., Hadjiiski, L., Cha, K., Helvie, M. A., 2016a. Deep-learning convolution neural network for computeraided detection of microcalcifications in digital breast tomosynthesis. In: Medical Imaging. Vol. 9785 of Proceedings of the SPIE. p. 97850Y.
Samala, R. K., Chan, H.-P., Hadjiiski, L., Helvie, M. A., Wei, J., Cha, K., 2016b. Mass detection in digital breast tomosynthesis: Deep convolutional neural network with transfer learning from mammography. Medical Physics 43 (12), 6654–6666.
Sarraf, S., Tofighi, G., 2016. Classification of Alzheimer's disease using fMRI data and deep learning convolutional neural networks. arXiv:1603.08631.
Schaumberg, A. J., Rubin, M. A., Fuchs, T. J., 2016. H&E-stained whole slide deep learning predicts SPOP mutation state in prostate cancer. bioRxiv:064279.
Schlegl, T., Waldstein, S. M., Vogl, W.-D., Schmidt-Erfurth, U., Langs, G., 2015. Predicting semantic descriptions from medical images with convolutional neural networks. In: Inf Process Med
Imaging. Vol. 9123 of Lect Notes Comput Sci. pp. 437–448.
Sethi, A., Sha, L., Vahadane, A. R., Deaton, R. J., Kumar, N., Macias, V., Gann, P. H., 2016. Empirical comparison of color normalization methods for epithelial-stromal classification in H and E images. J
Pathol Inform 7, 17.
Setio, A. A. A., Ciompi, F., Litjens, G., Gerke, P., Jacobs, C., van Riel, S., Wille, M. W., Naqibullah, M., Sanchez, C., van Ginneken, B., 2016. Pulmonary nodule detection in CT images: false positive reduction using multi-view convolutional networks. IEEE Trans Med
Imaging 35 (5), 1160–1169.
Sevetlidis, V., Giuffrida, M. V., Tsaftaris, S. A., Jan. 2016. Whole image synthesis using a deep encoder-decoder network. In: Simulation and Synthesis in Medical Imaging. Vol. 9968 of Lect Notes
Comput Sci. pp. 127–137.
Shah, A., Conjeti, S., Navab, N., Katouzian, A., 2016. Deeply learnt hashing forests for content based image retrieval in prostate MR images. In: Medical Imaging. Vol. 9784 of Proceedings of the SPIE. p. 978414.
Shakeri, M., Tsogkas, S., Ferrante, E., Lippe, S., Kadoury, S., Paragios, N., Kokkinos, I., 2016. Sub-cortical brain structure segmentation using F-CNNs. In: IEEE Int Symp Biomedical Imaging. pp.
269–272.
Shen, D., Wu, G., Suk, H.-I., Mar. 2017. Deep learning in medical image analysis. Annu Rev Biomed Eng.
Shen, W., Yang, F., Mu, W., Yang, C., Yang, X., Tian, J., 2015a.
Automatic localization of vertebrae based on convolutional neural networks. In: Medical Imaging. Vol. 9413 of Proceedings of the SPIE. p. 94132E.
Shen, W., Zhou, M., Yang, F., Dong, D., Yang, C., Zang, Y., Tian, J., 2016. Learning from experts: Developing transferable deep features for patient-level lung cancer prediction. In: Med Image Comput Comput Assist Interv. Vol. 9901 of Lect Notes Comput Sci. pp.
124–131.
Shen, W., Zhou, M., Yang, F., Yang, C., Tian, J., 2015b. Multi-scale convolutional neural networks for lung nodule classification. In:
Inf Process Med Imaging. Vol. 9123 of Lect Notes Comput Sci. pp. 588–599.
Shi, J., Zheng, X., Li, Y., Zhang, Q., Ying, S., Jan. 2017. Multimodal neuroimaging feature learning with multimodal stacked deep polynomial networks for diagnosis of Alzheimer's disease.
IEEE J Biomed Health Inform, in press.
Shin, H.-C., Lu, L., Kim, L., Seff, A., Yao, J., Summers, R. M., 2015.
Interleaved text/image deep mining on a very large-scale radiology database. In: Comput Vis Pattern Recognit. pp. 1090–1099.
Shin, H.-C., Orton, M. R., Collins, D. J., Doran, S. J., Leach, M. O., 2013. Stacked autoencoders for unsupervised feature learning and multiple organ detection in a pilot study using 4D patient data.
IEEE Trans Pattern Anal Mach Intell 35, 1930–1943.
Shin, H.-C., Roberts, K., Lu, L., Demner-Fushman, D., Yao, J., Summers, R. M., 2016a. Learning to read chest x-rays:
Recurrent neural cascade model for automated image annotation. arXiv:1603.08486.
Shin, H.-C., Roth, H. R., Gao, M., Lu, L., Xu, Z., Nogues, I., Yao, J., Mollura, D., Summers, R. M., 2016b. Deep convolu35 tional neural networks for computer-aided detection: CNN architectures, dataset characteristics and transfer learning. IEEE Trans
Med Imaging 35 (5), 1285–1298.
Shkolyar, A., Gefen, A., Benayahu, D., Greenspan, H., 2015. Automatic detection of cell divisions (mitosis) in live-imaging microscopy images using convolutional neural networks. In: Conf
Proc IEEE Eng Med Biol Soc. pp. 743–746.
Simonovsky, M., Guti´errez-Becker, B., Mateus, D., Navab, N., Komodakis, N., 2016. A deep metric for multimodal registration. In:
Med Image Comput Comput Assist Interv. Vol. 9902 of Lect Notes
Comput Sci. pp. 10–18.
Simonyan, K., Zisserman, A., 2014. Very deep convolutional networks for large-scale image recognition. arXiv:1409.1556.
Sirinukunwattana, K., Raza, S. E. A., Tsang, Y.-W., Snead, D. R., Cree, I. A., Rajpoot, N. M., 2016. Locality sensitive deep learning for detection and classification of nuclei in routine colon cancer histology images. IEEE Trans Med Imaging 35 (5), 1196–1206.
Smistad, E., Løvstakken, L., 2016. Vessel detection in ultrasound images using deep convolutional neural networks. In: DLMIA. Vol.
10008 of Lect Notes Comput Sci. pp. 30–38.
Snoek, J., Larochelle, H., Adams, R. P., 2012. Practical bayesian optimization of machine learning algorithms. In: Advances in Neural
Information Processing Systems. pp. 2951–2959.
Song, Y., Tan, E.-L., Jiang, X., Cheng, J.-Z., Ni, D., Chen, S., Lei, B., Wang, T., Sep 2017. Accurate cervical cell segmentation from overlapping clumps in pap smear images. IEEE Trans Med Imaging 36, 288–300.
Song, Y., Zhang, L., Chen, S., Ni, D., Lei, B., Wang, T., 2015. Accurate segmentation of cervical cytoplasm and nuclei based on multiscale convolutional network and graph partitioning. IEEE Trans
Biomed Eng 62 (10), 2421–2433.
Spampinato, C., Palazzo, S., Giordano, D., Aldinucci, M., Leonardi, R., Feb. 2017. Deep learning for automated skeletal bone age assessment in X-ray images. Med Image Anal 36, 41–51.
Springenberg, J. T., Dosovitskiy, A., Brox, T., Riedmiller, M., 2014.
Striving for simplicity: The all convolutional net. arXiv preprint arXiv:1412.6806.
ˇStern, D., Payer, C., Lepetit, V., Urschler, M., 2016. Automated age estimation from hand MRI volumes using deep learning. In: Med
Image Comput Comput Assist Interv. Vol. 9901 of Lect Notes
Comput Sci. pp. 194–202.
Stollenga, M. F., Byeon, W., Liwicki, M., Schmidhuber, J., 2015. Parallel multi-dimensional LSTM, with application to fast biomedical volumetric image segmentation. In: Advances in Neural Information Processing Systems. pp. 2998–3006.
Suk, H.-I., Lee, S.-W., Shen, D., 2014. Hierarchical feature representation and multimodal fusion with deep learning for AD/MCI diagnosis. NeuroImage 101, 569–582.
Suk, H.-I., Lee, S.-W., Shen, D., 2015. Latent feature representation with stacked auto-encoder for AD/MCI diagnosis. Brain Struct
Funct 220, 841–859.
Suk, H.-I., Shen, D., 2013. Deep learning-based feature representation for AD/MCI classification. In: Med Image Comput Comput Assist
Interv. Vol. 8150 of Lect Notes Comput Sci. pp. 583–590.
Suk, H.-I., Shen, D., 2016. Deep ensemble sparse regression network for Alzheimer's disease diagnosis. In: Med Image Comput Comput
Assist Interv. Vol. 10019 of Lect Notes Comput Sci. pp. 113–121.
Suk, H.-I., Wee, C.-Y., Lee, S.-W., Shen, D., 2016. State-space model with deep learning for functional dynamics estimation in restingstate fMRI. NeuroImage 129, 292–307.
Sun, W., Tseng, T.-L. B., Zhang, J., Qian, W., 2016a. Enhancing deep convolutional neural network scheme for breast cancer diagnosis with unlabeled data. Comput Med Imaging Graph.
Sun, W., Zheng, B., Qian, W., 2016b. Computer aided lung cancer diagnosis with deep learning algorithms. In: Medical Imaging. Vol.
9785 of Proceedings of the SPIE. p. 97850Z.
Suzani, A., Rasoulian, A., Seitel, A., Fels, S., Rohling, R., Abolmaesumi, P., 2015. Deep learning for automatic localization, identification, and segmentation of vertebral bodies in volumetric mr images. In: Medical Imaging. Vol. 9415 of Proceedings of the SPIE. p. 941514.
Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., Rabinovich, A., 2014. Going deeper with convolutions. arXiv:1409.4842.
Tachibana, R., N¨appi, J. J., Hironaka, T., Kim, S. H., Yoshida, H., 2016. Deep learning for electronic cleansing in dual-energy ct colonography. In: Medical Imaging. Vol. 9785 of Proceedings of the SPIE. p. 97851M.
Tajbakhsh, N., Gotway, M. B., Liang, J., 2015a. Computer-aided pulmonary embolism detection using a novel vessel-aligned multiplanar image representation and convolutional neural networks. In:
Med Image Comput Comput Assist Interv. Vol. 9350 of Lect Notes
Comput Sci. pp. 62–69.
Tajbakhsh, N., Gurudu, S. R., Liang, J., 2015b. A comprehensive computer-aided polyp detection system for colonoscopy videos.
In: Inf Process Med Imaging. Vol. 9123 of Lect Notes Comput
Sci. pp. 327–338.
Tajbakhsh, N., Shin, J. Y., Gurudu, S. R., Hurst, R. T., Kendall, C. B., Gotway, M. B., Liang, J., 2016. Convolutional neural networks for medical image analysis: Fine tuning or full training? IEEE Trans
Med Imaging 35 (5), 1299–1312.
Tarando, S. R., Fetita, C., Faccinetto, A., Yves, P., 2016. Increasing
CAD system efficacy for lung texture analysis using a convolutional network. In: Medical Imaging. Vol. 9785 of Proceedings of the SPIE. pp. 97850Q–97850Q.
Teikari, P., Santos, M., Poon, C., Hynynen, K., 2016. Deep learning convolutional networks for multiphoton microscopy vasculature segmentation. arXiv:1606.02382.
Teramoto, A., Fujita, H., Yamamuro, O., Tamaki, T., 2016. Automated detection of pulmonary nodules in PET/CT images: Ensemble false-positive reduction using a convolutional neural network technique. Med Phys 43, 2821–2827.
Thong, W., Kadoury, S., Pich´e, N., Pal, C. J., 2016. Convolutional networks for kidney segmentation in contrast-enhanced CT scans.
Computer Methods in Biomechanics and Biomedical Engineering:
Imaging & Visualization, 1–6.
Tran, P. V., 2016. A fully convolutional neural network for cardiac segmentation in short-axis MRI. arXiv:1604.00494.
Turkki, R., Linder, N., Kovanen, P. E., Pellinen, T., Lundin, J., 2016.
Antibody-supervised deep learning for quantification of tumorinfiltrating immune cells in hematoxylin and eosin stained breast cancer samples. Journal of pathology informatics 7, 38.
Twinanda, A. P., Shehata, S., Mutter, D., Marescaux, J., de Mathelin, M., Padoy, N., 2017. Endonet: A deep architecture for recognition tasks on laparoscopic videos. IEEE Trans Med Imaging 36, 86–97. van der Burgh, H. K., Schmidt, R., Westeneng, H.-J., de Reus, M. A., van den Berg, L. H., van den Heuvel, M. P., 2017. Deep learning predictions of survival based on MRI in amyotrophic lateral sclerosis. NeuroImage. Clinical 13, 361–369. van Ginneken, B., Setio, A. A., Jacobs, C., Ciompi, F., 2015. Off-theshelf convolutional neural network features for pulmonary nodule detection in computed tomography scans. In: IEEE Int Symp
Biomedical Imaging. pp. 286–289. van Grinsven, M. J. J. P., van Ginneken, B., Hoyng, C. B., Theelen, T., S´anchez, C. I., 2016. Fast convolutional neural network training using selective data sampling: Application to hemorrhage detection in color fundus images. IEEE Trans Med Imaging 35 (5), 1273–1284. van Tulder, G., de Bruijne, M., 2016. Combining generative and discriminative representation learning for lung CT analysis with
36 convolutional Restricted Boltzmann Machines. IEEE Trans Med
Imaging 35 (5), 1262–1272.
Veta, M., van Diest, P. J., Pluim, J. P. W., 2016. Cutting out the middleman: measuring nuclear area in histopathology slides without segmentation. In: Med Image Comput Comput Assist Interv. Vol.
9901 of Lect Notes Comput Sci. pp. 632–639.
Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., Manzagol, P.-A., 2010. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. J Mach
Learn Res 11, 3371–3408.
Vivanti, R., Ephrat, A., Joskowicz, L., Karaaslan, O., Lev-Cohain, N., Sosna, J., 2015. Automatic liver tumor segmentation in followup ct studies using convolutional neural networks. In:
Proc.
Patch-Based Methods in Medical Image Processing Workshop, MICCAI.–2015. pp. 54–61.
Wang, C., Elazab, A., Wu, J., Hu, Q., Nov. 2016a. Lung nodule classification using deep feature fusion in chest radiography. Comput
Med Imaging Graph.
Wang, C., Yan, X., Smith, M., Kochhar, K., Rubin, M., Warren, S. M., Wrobel, J., Lee, H., 2015. A unified framework for automatic wound segmentation and analysis with deep convolutional neural networks. In: Conf Proc IEEE Eng Med Biol Soc. pp. 2415–2418.
Wang, D., Khosla, A., Gargeya, R., Irshad, H., Beck, A. H., 2016b. Deep learning for identifying metastatic breast cancer. arXiv:1606.05718.
Wang, G., 2016. A perspective on deep imaging. IEEE Access 4, 8914–8924.
Wang, H., Cruz-Roa, A., Basavanhally, A., Gilmore, H., Shih, N., Feldman, M., Tomaszewski, J., Gonzalez, F., Madabhushi, A., 2014. Mitosis detection in breast cancer pathology images by combining handcrafted and convolutional neural network features. J
Med Imaging 1, 034003.
Wang, J., Ding, H., Azamian, F., Zhou, B., Iribarren, C., Molloi, S., Baldi, P., 2017. Detecting cardiovascular disease from mammograms with deep learning. IEEE Trans Med Imaging.
Wang, J., MacKenzie, J. D., Ramachandran, R., Chen, D. Z., 2016c.
A deep learning approach for semantic segmentation in histology tissue images. In: Med Image Comput Comput Assist Interv. Vol.
9901 of Lect Notes Comput Sci. Springer, pp. 176–184.
Wang, S., Yao, J., Xu, Z., Huang, J., 2016d. Subtype cell detection with an accelerated deep convolution neural network. In: Med Image Comput Comput Assist Interv. Vol. 9901 of Lect Notes Comput Sci. pp. 640–648.
Wang, X., Lu, L., Shin, H.-c., Kim, L., Nogues, I., Yao, J., Summers, R., 2016e. Unsupervised category discovery via looped deep pseudo-task optimization using a large scale radiology image database. arXiv:1603.07965.
Wolterink, J. M., Leiner, T., de Vos, B. D., van Hamersvelt, R. W., Viergever, M. A., Isgum, I., 2016. Automatic coronary artery calcium scoring in cardiac CT angiography using paired convolutional neural networks. Med Image Anal 34, 123–136.
Worrall, D. E., Wilson, C. M., Brostow, G. J., 2016. Automated retinopathy of prematurity case detection with convolutional neural networks. In: DLMIA. Vol. 10008 of Lect Notes Comput Sci. pp. 68–76.
Wu, A., Xu, Z., Gao, M., Buty, M., Mollura, D. J., 2016. Deep vessel tracking: A generalized probabilistic approach via deep learning.
In: IEEE Int Symp Biomedical Imaging. pp. 1363–1367.
Wu, G., Kim, M., Wang, Q., Gao, Y., Liao, S., Shen, D., 2013. Unsupervised deep feature learning for deformable registration of MR brain images. In: Med Image Comput Comput Assist Interv. Vol.
8150 of Lect Notes Comput Sci. pp. 649–656.
Xie, W., Noble, J. A., Zisserman, A., 2016a. Microscopy cell counting and detection with fully convolutional regression networks.
Computer Methods in Biomechanics and Biomedical Engineering:
Imaging & Visualization, 1–10.
Xie, Y., Kong, X., Xing, F., Liu, F., Su, H., Yang, L., 2015a. Deep voting: A robust approach toward nucleus localization in microscopy images. In: Med Image Comput Comput Assist Interv. Vol. 9351 of Lect Notes Comput Sci. pp. 374–382.
Xie, Y., Xing, F., Kong, X., Su, H., Yang, L., 2015b. Beyond classification: Structured regression for robust cell detection using convolutional neural network. In: Med Image Comput Comput Assist
Interv. Vol. 9351 of Lect Notes Comput Sci. pp. 358–365.
Xie, Y., Zhang, Z., Sapkota, M., Yang, L., 2016b. Spatial clockwork recurrent neural network for muscle perimysium segmentation. In: International Conference on Medical Image Computing and Computer-Assisted Intervention. Vol. 9901 of Lect Notes
Comput Sci. Springer, pp. 185–193.
Xing, F., Xie, Y., Yang, L., 2016. An automatic learning-based framework for robust nucleus segmentation. IEEE Trans Med Imaging
35 (2), 550–566.
Xu, J., Luo, X., Wang, G., Gilmore, H., Madabhushi, A., 2016a. A deep convolutional neural network for segmenting and classifying epithelial and stromal regions in histopathological images. Neurocomputing 191, 214–223.
Xu, J., Xiang, L., Liu, Q., Gilmore, H., Wu, J., Tang, J., Madabhushi, A., 2016b. Stacked sparse autoencoder (ssae) for nuclei detection on breast cancer histopathology images. IEEE Trans Med Imaging
35, 119–130.
Xu, T., Zhang, H., Huang, X., Zhang, S., Metaxas, D. N., 2016c. Multimodal deep learning for cervical dysplasia diagnosis. In: Med Image Comput Comput Assist Interv. Vol. 9901 of Lect Notes Comput Sci. pp. 115–123.
Xu, Y., Li, Y., Liu, M., Wang, Y., Lai, M., Chang, E. I.-C., 2016d.
Gland instance segmentation by deep multichannel side supervision. arXiv:1607.03222.
Xu, Y., Mo, T., Feng, Q., Zhong, P., Lai, M., Chang, E. I. C., 2014.
Deep learning of feature representation with multiple instance learning for medical image analysis. In: IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). pp.
1626–1630.
Xu, Z., Huang, J., 2016. Detecting 10,000 Cells in one second. In:
Med Image Comput Comput Assist Interv. Vol. 9901 of Lect Notes
Comput Sci. pp. 676–684.
Xue, D.-X., Zhang, R., Feng, H., Wang, Y.-L., 2016. CNN-SVM for microvascular morphological type recognition with data augmentation. J Med Biol Eng 36, 755–764.
Yan, Z., Zhan, Y., Peng, Z., Liao, S., Shinagawa, Y., Zhang, S., Metaxas, D. N., Zhou, X. S., 2016. Multi-instance deep learning:
Discover discriminative local anatomies for bodypart recognition.
IEEE Trans Med Imaging 35 (5), 1332–1343.
Yang, D., Zhang, S., Yan, Z., Tan, C., Li, K., Metaxas, D., 2015.
Automated anatomical landmark detection on distal femur surface using convolutional neural network. In: IEEE Int Symp Biomedical Imaging. pp. 17–21.
Yang, H., Sun, J., Li, H., Wang, L., Xu, Z., 2016a. Deep fusion net for multi-atlas segmentation: Application to cardiac mr images. In:
Med Image Comput Comput Assist Interv. Vol. 9901 of Lect Notes
Comput Sci. pp. 521–528.
Yang, L., Zhang, Y., Guldner, I. H., Zhang, S., Chen, D. Z., 2016b. 3d segmentation of glial cells using fully convolutional networks and k-terminal cut. In: Med Image Comput Comput Assist Interv. Vol.
9901 of Lect Notes Comput Sci. Springer, pp. 658–666.
Yang, W., Chen, Y., Liu, Y., Zhong, L., Qin, G., Lu, Z., Feng, Q., Chen, W., 2016c. Cascade of multi-scale convolutional neural networks for bone suppression of chest radiographs in gradient domain. Med Image Anal 35, 421–433.
Yang, X., Kwitt, R., Niethammer, M., 2016d. Fast predictive image registration. In: DLMIA. Vol. 10008 of Lect Notes Comput Sci.
37 pp. 48–57.
Yao, J., Wang, S., Zhu, X., Huang, J., 2016. Imaging biomarker discovery for lung cancer survival prediction. In: Med Image Comput
Comput Assist Interv. Vol. 9901 of Lect Notes Comput Sci. pp.
649–657.
Yoo, Y., Tang, L. W., Brosch, T., Li, D. K. B., Metz, L., Traboulsee, A., Tam, R., 2016. Deep learning of brain lesion patterns for predicting future disease activity in patients with early symptoms of multiple sclerosis. In: DLMIA. Vol. 10008 of Lect Notes Comput
Sci. pp. 86–94.
Ypsilantis, P.-P., Siddique, M., Sohn, H.-M., Davies, A., Cook, G., Goh, V., Montana, G., 2015. Predicting response to neoadjuvant chemotherapy with pet imaging using convolutional neural networks. PLoS ONE 10 (9), 1–18.
Yu, L., Chen, H., Dou, Q., Qin, J., Heng, P. A., 2016a. Automated melanoma recognition in dermoscopy images via very deep residual networks. IEEE Trans Med Imaging, in press.
Yu, L., Guo, Y., Wang, Y., Yu, J., Chen, P., Nov. 2016b. Segmentation of fetal left ventricle in echocardiographic sequences based on dynamic convolutional neural networks. IEEE Trans Biomed Eng, in press.
Yu, L., Yang, X., Chen, H., Qin, J., Heng, P. A., 2017. Volumetric convnets with mixed residual connections for automated prostate segmentation from 3D MR images. In: Thirty-First AAAI Conference on Artificial Intelligence.
Zeiler, M. D., Fergus, R., 2014. Visualizing and understanding convolutional networks. In: European Conference on Computer Vision. pp. 818–833.
Zhang, H., Li, L., Qiao, K., Wang, L., Yan, B., Li, L., Hu, G., 2016a.
Image prediction for limited-angle tomography via deep learning with convolutional neural network. arXiv:1607.08707.
Zhang, L., Gooya, A., Dong, B. H. R., Petersen, S. E., MedranoGracia, K. P., Frangi, A. F., 2016b. Automated quality assessment of cardiac MR images using convolutional neural networks. In:
SASHIMI. Vol. 9968 of Lect Notes Comput Sci. pp. 138–145.
Zhang, Q., Xiao, Y., Dai, W., Suo, J., Wang, C., Shi, J., Zheng, H., 2016c. Deep learning based classification of breast tumors with shear-wave elastography. Ultrasonics 72, 150–157.
Zhang, R., Zheng, Y., Mak, T. W. C., Yu, R., Wong, S. H., Lau, J.
Y. W., Poon, C. C. Y., Jan. 2017. Automatic detection and classification of colorectal polyps by transferring low-level CNN features from nonmedical domain. IEEE J Biomed Health Inform 21, 41–
Zhang, W., Li, R., Deng, H., Wang, L., Lin, W., Ji, S., Shen, D., 2015.
Deep convolutional neural networks for multi-modality isointense infant brain image segmentation. NeuroImage 108, 214–224.
Zhao, J., Zhang, M., Zhou, Z., Chu, J., Cao, F., Nov. 2016. Automatic detection and classification of leukocytes using convolutional neural networks. Medical & Biological Engineering & Computing.
Zhao, L., Jia, K., 2016. Multiscale CNNs for brain tumor segmentation and diagnosis. Computational and Mathematical Methods in Medicine 2016, 8356294.
Zheng, Y., Liu, D., Georgescu, B., Nguyen, H., Comaniciu, D., 2015.
3D deep learning for efficient and robust landmark detection in volumetric data. In: Med Image Comput Comput Assist Interv.
Vol. 9349 of Lect Notes Comput Sci. pp. 565–572.
Zhou, X., Ito, T., Takayama, R., Wang, S., Hara, T., Fujita, H., 2016.
Three-dimensional CT image segmentation by combining 2D fully convolutional network with 3D majority voting. In: DLMIA. Vol.
10008 of Lect Notes Comput Sci. pp. 111–120.
Zhu, Y., Wang, L., Liu, M., Qian, C., Yousuf, A., Oto, A., Shen, D., Jan. 2017. MRI based prostate cancer detection with high-level representation and hierarchical classification. Med Phys, in press.
Zilly, J., Buhmann, J. M., Mahapatra, D., 2017. Glaucoma detection using entropy sampling and ensemble learning for automatic optic cup and disc segmentation. Comput Med Imaging Graph 55, 28–
Zreik, M., Leiner, T., de Vos, B., van Hamersvelt, R., Viergever, M., Isgum, I., 2016. Automatic segmentation of the left ventricle in cardiac CT angiography using convolutional neural networks. In:
IEEE Int Symp Biomedical Imaging. pp. 40–43.Published as a conference paper at ICLR 2017
ADVERSARIALLY LEARNED INFERENCE
Vincent Dumoulin1, Ishmael Belghazi1, Ben Poole2
Olivier Mastropietro1, Alex Lamb1, Martin Arjovsky3
Aaron Courville1†
1 MILA, Université de Montréal, firstname.lastname@umontreal.ca.
2 Neural Dynamics and Computation Lab, Stanford, poole@cs.stanford.edu.
3 New York University, martinarjovsky@gmail.com.
†CIFAR Fellow.
ABSTRACT
We introduce the adversarially learned inference (ALI) model, which jointly learns a generation network and an inference network using an adversarial process. The generation network maps samples from stochastic latent variables to the data space while the inference network maps training examples in data space to the space of latent variables. An adversarial game is cast between these two networks and a discriminative network is trained to distinguish between joint latent/data-space samples from the generative network and joint samples from the inference network.
We illustrate the ability of the model to learn mutually coherent inference and generation networks through the inspections of model samples and reconstructions and confirm the usefulness of the learned representations by obtaining a performance competitive with state-of-the-art on the semi-supervised SVHN and CIFAR10 tasks.
INTRODUCTION
Deep directed generative model has emerged as a powerful framework for modeling complex highdimensional datasets. These models permit fast ancestral sampling, but are often challenging to learn due to the complexities of inference. Recently, three classes of algorithms have emerged as effective for learning deep directed generative models: 1) techniques based on the Variational
Autoencoder (VAE) that aim to improve the quality and efficiency of inference by learning an inference machine (Kingma & Welling, 2013; Rezende et al., 2014), 2) techniques based on Generative
Adversarial Networks (GANs) that bypass inference altogether (Goodfellow et al., 2014) and 3) autoregressive approaches (van den Oord et al., 2016b;c;a) that forego latent representations and instead model the relationship between input variables directly. While all techniques are provably consistent given infinite capacity and data, in practice they learn very different kinds of generative models on typical datasets.
VAE-based techniques learn an approximate inference mechanism that allows reuse for various auxiliary tasks, such as semi-supervised learning or inpainting. They do however suffer from a wellrecognized issue of the maximum likelihood training paradigm when combined with a conditional independence assumption on the output given the latent variables: they tend to distribute probability mass diffusely over the data space (Theis et al., 2015). The direct consequence of this is that image samples from VAE-trained models tend to be blurry (Goodfellow et al., 2014; Larsen et al., 2015).
Autoregressive models produce outstanding samples but do so at the cost of slow sampling speed and foregoing the learning of an abstract representation of the data. GAN-based approaches represent a good compromise: they learn a generative model that produces higher-quality samples than the best VAE techniques (Radford et al., 2015; Larsen et al., 2015) without sacrificing sampling speed and also make use of a latent representation in the generation process. However, GANs lack an efficient inference mechanism, which prevents them from reasoning about data at an abstract level.
For instance, GANs don't allow the sort of neural photo manipulations showcased in (Brock et al., 2016). Recently, efforts have aimed to bridge the gap between VAEs and GANs, to learn generative models with higher-quality samples while learning an efficient inference network (Larsen et al., 1 arXiv:1606.00704v3 [stat.ML] 21 Feb 2017
Published as a conference paper at ICLR 2017 x ∼ q(x)
ˆz ∼ q(z | x)
D(x, z)
˜x ∼ p(x | z) z ∼ p(z)
Gz(x)
Gx(z)(x, ˆz)(˜x, z)
Figure 1: The adversarially learned inference (ALI) game.
2015; Lamb et al., 2016; Dosovitskiy & Brox, 2016). While this is certainly a promising research direction, VAE-GAN hybrids tend to manifest a compromise of the strengths and weaknesses of both approaches.
In this paper, we propose a novel approach to integrate efficient inference within the GAN framework.
Our approach, called Adversarially Learned Inference (ALI), casts the learning of both an inference machine (or encoder) and a deep directed generative model (or decoder) in an GAN-like adversarial framework. A discriminator is trained to discriminate joint samples of the data and the corresponding latent variable from the encoder (or approximate posterior) from joint samples from the decoder while in opposition, the encoder and the decoder are trained together to fool the discriminator. Not only are we asking the discriminator to distinguish synthetic samples from real data, but we are requiring it to distinguish between two joint distributions over the data space and the latent variables.
With experiments on the Street View House Numbers (SVHN) dataset (Netzer et al., 2011), the CIFAR-10 object recognition dataset (Krizhevsky & Hinton, 2009), the CelebA face dataset (Liu et al., 2015) and a downsampled version of the ImageNet dataset (Russakovsky et al., 2015), we show qualitatively that we maintain the high sample fidelity associated with the GAN framework, while gaining the ability to perform efficient inference. We show that the learned representation is useful for auxiliary tasks by achieving results competitive with the state-of-the-art on the semi-supervised
SVHN and CIFAR10 tasks.
ADVERSARIALLY LEARNED INFERENCE
Consider the two following probability distributions over x and z:
• the encoder joint distribution q(x, z) = q(x)q(z | x), • the decoder joint distribution p(x, z) = p(z)p(x | z).
These two distributions have marginals that are known to us: the encoder marginal q(x) is the empirical data distribution and the decoder marginal p(z) is usually defined to be a simple, factorized distribution, such as the standard Normal distribution p(z) = N(0, I). As such, the generative process between q(x, z) and p(x, z) is reversed.
ALI's objective is to match the two joint distributions. If this is achieved, then we are ensured that all marginals match and all conditional distributions also match. In particular, we are assured that the conditional q(z | x) matches the posterior p(z | x).
In order to match the joint distributions, an adversarial game is played. Joint pairs (x, z) are drawn either from q(x, z) or p(x, z), and a discriminator network learns to discriminate between the two, while the encoder and decoder networks are trained to fool the discriminator.
The value function describing the game is given by: min
G max
D V (D, G) = Eq(x)[log(D(x, Gz(x)))] + Ep(z)[log(1 − D(Gx(z), z))]
=
�� q(x)q(z | x) log(D(x, z))dxdz
�� p(z)p(x | z) log(1 − D(x, z))dxdz.
Published as a conference paper at ICLR 2017
Algorithm 1 The ALI training procedure. θg, θd ← initialize network parameters repeat x(1),..., x(M) ∼ q(x)
▷ Draw M samples from the dataset and the prior z(1),..., z(M) ∼ p(z)
ˆz(i) ∼ q(z | x = x(i)), i = 1,..., M
▷ Sample from the conditionals
˜x(j) ∼ p(x | z = z(j)), j = 1,..., M ρ(i) q
← D(x(i), ˆz(i)), i = 1,..., M
▷ Compute discriminator predictions ρ(j) p
← D(˜x(j), z(j)), j = 1,..., M
Ld ← − 1
M
�M i=1 log(ρ(i) q ) −
M
�M j=1 log(1 − ρ(j) p )
▷ Compute discriminator loss
Lg ← − 1
M
�M i=1 log(1 − ρ(i) q ) −
M
�M j=1 log(ρ(j) p )
▷ Compute generator loss θd ← θd − ∇θdLd
▷ Gradient update on discriminator network θg ← θg − ∇θgLg
▷ Gradient update on generator networks until convergence
An attractive property of adversarial approaches is that they do not require that the conditional densities can be computed; they only require that they can be sampled from in a way that allows gradient backpropagation. In the case of ALI, this means that gradients should propagate from the discriminator network to the encoder and decoder networks.
This can be done using the the reparametrization trick (Kingma, 2013; Bengio et al., 2013b;a). Instead of sampling directly from the desired distribution, the random variable is computed as a deterministic transformation of some noise such that its distribution is the desired distribution. For instance, if q(z | x) = N(µ(x), σ2(x)I), one can draw samples by computing z = µ(x) + σ(x) ⊙ ϵ, ϵ ∼ N(0, I).
More generally, one can employ a change of variable of the form v = f(u, ϵ)(3) where ϵ is some random source of noise.
The discriminator is trained to distinguish between samples from the encoder (x, ˆz) ∼ q(x, z) and samples from the decoder (˜x, z) ∼ p(x, z). The generator is trained to fool the discriminator, i.e., to generate x, z pairs from q(x, z) or p(x, z) that are indistinguishable one from another. See Figure 1 for a diagram of the adversarial game and Algorithm 1 for an algorithmic description of the procedure.
In such a setting, and under the assumption of an optimal discriminator, the generator minimizes the Jensen-Shannon divergence (Lin, 1991) between q(x, z) and p(x, z). This can be shown using the same proof sketch as in the original GAN paper (Goodfellow et al., 2014).
RELATION TO GAN
ALI bears close resemblance to GAN, but it differs from it in the two following ways:
• The generator has two components: the encoder, Gz(x), which maps data samples x to z-space, and the decoder Gx(z), which maps samples from the prior p(z) (a source of noise) to the input space.
• The discriminator is trained to distinguish between joint pairs (x, ˆz = Gx(x)) and (˜x =
Gx(z), z), as opposed to marginal samples x ∼ q(x) and ˜x ∼ p(x).
ALTERNATIVE APPROACHES TO FEEDFORWARD INFERENCE IN GANS
The ALI training procedure is not the only way one could learn a feedforward inference network in a GAN setting.
In recent work, Chen et al. (2016) introduce a model called InfoGAN which minimizes the mutual information between a subset c of the latent code and x through the use of an auxiliary distribution
Published as a conference paper at ICLR 2017
Q(c | x). However, this does not correspond to full inference on z, as only the value for c is inferred.
Additionally, InfoGAN requires that Q(c | x) is a tractable approximate posterior that can be sampled from and evaluated. ALI only requires that inference networks can be sampled from, allowing it to represent arbitrarily complex posterior distributions.
One could learn the inverse mapping from GAN samples: this corresponds to learning an encoder to reconstruct z, i.e. finding an encoder such that Ez∼p(z)[∥z − Gz(Gx(z))∥2 ≈ 0. We are not aware of any work that reports results for this approach. This resembles the InfoGAN learning procedure but with a fixed generative model and a factorial Gaussian posterior with a fixed diagonal variance.
Alternatively, one could decompose training into two phases. In the first phase, a GAN is trained normally. In the second phase, the GAN's decoder is frozen and an encoder is trained following the ALI procedure (i.e., a discriminator taking both x and z as input is introduced). We call this post-hoc learned inference. In this setting, the encoder and the decoder cannot interact together during training and the encoder must work with whatever the decoder has learned during GAN training. Post-hoc learned inference may be suboptimal if this interaction is beneficial to modeling the data distribution.
GENERATOR VALUE FUNCTION
As with GANs, when ALI's discriminator gets too far ahead, its generator may have a hard time minimizing the value function in Equation 1. If the discriminator's output is sigmoidal, then the gradient of the value function with respect to the discriminator's output vanishes to zero as the output saturates.
As a workaround, the generator is trained to maximize
V ′(D, G) = Eq(x)[log(1 − D(x, Gz(x)))] + Ep(z)[log(D(Gx(z), z))](4) which has the same fixed points but whose gradient is stronger when the discriminator's output saturates.
The adversarial game does not require an analytical expression for the joint distributions. This means we can introduce variable changes without having to know the explicit distribution over the new variable. For instance, sampling from p(z) could be done by sampling ϵ ∼ N(0, I) and passing it through an arbitrary differentiable function z = f(ϵ).
However, gradient propagation into the encoder and decoder networks relies on the reparametrization trick, which means that ALI is not directly applicable to either applications with discrete data or to models with discrete latent variables.
DISCRIMINATOR OPTIMALITY
Proposition 1. Given a fixed generator G, the optimal discriminator is given by
D∗(x, z) = q(x, z) q(x, z) + p(x, z).
Proof. For a fixed generator G, the complete data value function is V (D, G) = Ex,z∼q(x,z)[log(D(x, z))] + Ex,z∼p(x,z)[log(1 − D(x, z))].
The result follows by the concavity of the log and the simplified Euler-Lagrange equation first order conditions on (x, z) → D(x, z).
RELATIONSHIP WITH THE JENSEN-SHANNON DIVERGENCE
Proposition 2. Under an optimal discriminator D∗, the generator minimizes the Jensen-Shanon divergence which attains its minimum if and only if q(x, z) = p(x, z).
Proof. The proof is a straightforward extension of the proof in Goodfellow et al. (2014).
Published as a conference paper at ICLR 2017(a) SVHN samples.(b) SVHN reconstructions.
Figure 2: Samples and reconstructions on the SVHN dataset. For the reconstructions, odd columns are original samples from the validation set and even columns are corresponding reconstructions (e.g., second column contains reconstructions of the first column's validation set samples).(a) CelebA samples.(b) CelebA reconstructions.
Figure 3: Samples and reconstructions on the CelebA dataset. For the reconstructions, odd columns are original samples from the validation set and even columns are corresponding reconstructions.(a) CIFAR10 samples.(b) CIFAR10 reconstructions.
Figure 4: Samples and reconstructions on the CIFAR10 dataset. For the reconstructions, odd columns are original samples from the validation set and even columns are corresponding reconstructions.
Published as a conference paper at ICLR 2017
INVERTIBILITY
Proposition 3. Assuming optimal discriminator D and generator G. If the encoder Gx is deterministic, then Gx = G−1 z and Gz = G−1 x almost everywhere.
Sketch of proof. Consider the event Rϵ = {x : ∥x − (Gx ◦ Gz)(x))∥ > ϵ} for some positive ϵ.
This set can be seen as a section of the (x, z) space over the elements z such that z = Gz(x).
The generator being optimal, the probabilities of Rϵ under p(x, z) and q(x, z) are equal. Now p(x | z) = δx−Gx(z), where δ is the Dirac delta distribution. This is enough to show that there are no x satisfying the event Rϵ and thus Gx = G−1 z almost everywhere. By symmetry, the same argument can be applied to show that Gz = G−1 x.
The complete proof is given in (Donahue et al., 2016), in which the authors independently examine the same model structure under the name Bidirectional GAN (BiGAN).
RELATED WORK
Other recent papers explore hybrid approaches to generative modeling. One such approach is to relax the probabilistic interpretation of the VAE model by replacing either the KL-divergence term or the reconstruction term with variants that have better properties. The adversarial autoencoder model (Makhzani et al., 2015) replaces the KL-divergence term with a discriminator that is trained to distinguish between approximate posterior and prior samples, which provides a more flexible approach to matching the marginal q(z) and the prior. Other papers explore replacing the reconstruction term with either GANs or auxiliary networks. Larsen et al. (2015) collapse the decoder of a VAE and the generator of a GAN into one network in order to supplement the reconstruction loss with a learned similarity metric. Lamb et al. (2016) use the hidden layers of a pre-trained classifier as auxiliary reconstruction losses to help the VAE focus on higher-level details when reconstructing. Dosovitskiy
& Brox (2016) combine both ideas into a unified loss function.
ALI's approach is also reminiscent of the adversarial autoencoder model, which employs a GAN to distinguish between samples from the approximate posterior distribution q(z | x) and prior samples.
However, unlike adversarial autoencoders, no explicit reconstruction loss is being optimized in ALI, and the discriminator receives joint pairs of samples (x, z) rather than marginal z samples.
Independent work by Donahue et al. (2016) proposes the same model under the name Bidirectional
GAN (BiGAN), in which the authors emphasize the learned features' usefulness for auxiliary supervised and semi-supervised tasks. The main difference in terms of experimental setting is that they use a deterministic q(z | x) network, whereas we use a stochastic network. In our experience, this does not make a big difference when x is a deterministic function of z as the stochastic inference networks tend to become determinstic as training progresses. When using stochastic mappings from z to x, the additional flexiblity of stochastic posteriors is critical.
EXPERIMENTAL RESULTS
We applied ALI to four different datasets, namely CIFAR10 (Krizhevsky & Hinton, 2009), SVHN(Netzer et al., 2011), CelebA (Liu et al., 2015) and a center-cropped, 64 × 64 version of the ImageNet dataset (Russakovsky et al., 2015).1
Transposed convolutions are used in Gx(z). This operation corresponds to the transpose of the matrix representation of a convolution, i.e., the gradient of the convolution with respect to its inputs. For more details about transposed convolutions and related operations, see Dumoulin & Visin (2016); Shi et al. (2016); Odena et al. (2016).
SAMPLES AND RECONSTRUCTIONS
For each dataset, samples are presented (Figures 2a, 3a 4a and 5a). They exhibit the same image fidelity as samples from other adversarially-trained models.
1 The code for all experiments can be found at https://github.com/IshmaelBelghazi/ALI.
Readers can also consult the accompanying website at https://ishmaelbelghazi.github.io/ALI.
Published as a conference paper at ICLR 2017(a) Tiny ImageNet samples.(b) Tiny ImageNet reconstructions.
Figure 5:
Samples and reconstructions on the Tiny ImageNet dataset. For the reconstructions, odd columns are original samples from the validation set and even columns are corresponding reconstructions.
We also qualitatively evaluate the fit between the conditional distribution q(z | x) and the posterior distribution p(z | x) by sampling ˆz ∼ q(z | x) and ˆx ∼ p(x | z = ˆz) (Figures 2b, 3b, 4b and 5b).
This corresponds to reconstructing the input in a VAE setting. Note that the ALI training objective does not involve an explicit reconstruction loss.
We observe that reconstructions are not always faithful reproductions of the inputs. They retain the same crispness and quality characteristic to adversarially-trained models, but oftentimes make mistakes in capturing exact object placement, color, style and (in extreme cases) object identity. The extent to which reconstructions deviate from the inputs varies between datasets: on CIFAR10, which arguably constitutes a more complex input distribution, the model exhibits less faithful reconstructions.
This leads us to believe that poor reconstructions are a sign of underfitting.
This failure mode represents an interesting departure from the bluriness characteristic to the typical
VAE setup. We conjecture that in the underfitting regime, the latent variable representation learned by ALI is potentially more invariant to less interesting factors of variation in the input and do not devote model capacity to capturing these factors.
LATENT SPACE INTERPOLATIONS
As a sanity check for overfitting, we look at latent space interpolations between validation set examples (Figure 6). We sample pairs of validation set examples x1 and x2 and project them into z1 and z2 by sampling from the encoder. We then linearly interpolate between z1 and z2 and pass the intermediary points through the decoder to plot the input-space interpolations.
We observe smooth transitions between pairs of examples, and intermediary images remain believable.
This is an indicator that ALI is not concentrating its probability mass exclusively around training examples, but rather has learned latent features that generalize well.
SEMI-SUPERVISED LEARNING
We investigate the usefulness of the latent representation learned by ALI through semi-supervised benchmarks on SVHN and CIFAR10.
We first compare with GAN on SVHN by following the procedure outlined in Radford et al. (2015).
We train an L2-SVM on the learned representations of a model trained on SVHN. The last three hidden layers of the encoder as well as its output are concatenated to form a 8960-dimensional feature vector. A 10,000 example held-out validation set is taken from the training set and is used for model selection. The SVM is trained on 1000 examples taken at random from the remainder of the training set. The test error rate is measured for 100 different SVMs trained on different random 1000-example training sets, and the average error rate is measured along with its standard deviation.
Published as a conference paper at ICLR 2017
Figure 6: Latent space interpolations on the CelebA validation set. Left and right columns correspond to the original pairs x1 and x2, and the columns in between correspond to the decoding of latent representations interpolated linearly from z1 to z2. Unlike other adversarial approaches like
DCGAN (Radford et al., 2015), ALI allows one to interpolate between actual data points.
Using ALI's inference network as opposed to the discriminator to extract features, we achieve a misclassification rate that is roughly 3.00 ± 0.50% lower than reported in Radford et al. (2015)(Table 1), which suggests that ALI's inference mechanism is beneficial to the semi-supervised learning task.
We then investigate ALI's performance when label information is taken into account during training.
We adapt the discriminative model proposed in Salimans et al. (2016). The discriminator takes x and z as input and outputs a distribution over K + 1 classes, where K is the number of categories. When label information is available for q(x, z) samples, the discriminator is expected to predict the label.
When no label information is available, the discriminator is expected to predict K + 1 for p(x, z) samples and k ∈ {1,..., K} for q(x, z) samples.
Interestingly, Salimans et al. (2016) found that they required an alternative training strategy for the generator where it tries to match first-order statistics in the discriminator's intermediate activations with respect to the data distribution (they refer to this as feature matching). We found that ALI did not require feature matching to obtain comparable results. We achieve results competitive with the state-of-the-art, as shown in Tables 1 and 2. Table 2 shows that ALI offers a modest improvement over Salimans et al. (2016), more specifically for 1000 and 2000 labeled examples.
Table 1: SVHN test set missclassification rate
Model
Misclassification rate
VAE (M1 + M2) (Kingma et al., 2014)
SWWAE with dropout (Zhao et al., 2015)
DCGAN + L2-SVM (Radford et al., 2015)
SDGM (Maaløe et al., 2016)
GAN (feature matching) (Salimans et al., 2016)
8.11 ± 1.3
ALI (ours, L2-SVM)
19.14 ± 0.50
ALI (ours, no feature matching)
7.42 ± 0.65
Table 2: CIFAR10 test set missclassification rate for semi-supervised learning using different numbers of trained labeled examples. For ALI, error bars correspond to 3 times the standard deviation.
Number of labeled examples
Model
Misclassification rate
Ladder network (Rasmus et al., 2015)
CatGAN (Springenberg, 2015)
GAN (feature matching) (Salimans et al., 2016)
21.83 ± 2.01
19.61 ± 2.09
18.63 ± 2.32
17.72 ± 1.82
ALI (ours, no feature matching)
19.98 ± 0.89
19.09 ± 0.44
17.99 ± 1.62
17.05 ± 1.49
Published as a conference paper at ICLR 2017
We are still investigating the differences between ALI and GAN with respect to feature matching, but we conjecture that the latent representation learned by ALI is better untangled with respect to the classification task and that it generalizes better.
CONDITIONAL GENERATION
We extend ALI to match a conditional distribution. Let y represent a fully observed conditioning variable. In this setting, the value function reads min
G max
D V (D, G) = Eq(x) p(y)[log(D(x, Gz(x, y), y))] + Ep(z) p(y)[log(1 − D(Gx(z, y), z, y))]
We apply the conditional version of ALI to CelebA using the dataset's 40 binary attributes. The attributes are linearly embedded in the encoder, decoder and discriminator. We observe how a single element of the latent space z changes with respect to variations in the attributes vector y. Conditional samples are shown in Figure 7.
Figure 7: Conditional generation sequence. We sample a single fixed latent code z. Each row has a subset of attributes that are held constant across columns. The attributes are male, attractive, young for row I; male, attractive, older for row II; female, attractive, young for row III; female, attractive, older for Row IV. Attributes are then varied uniformly over rows across all columns in the following sequence: (b) black hair; (c) brown hair; (d) blond hair; (e) black hair, wavy hair; (f) blond hair, bangs; (g) blond hair, receding hairline; (h) blond hair, balding; (i) black hair, smiling; (j) black hair, smiling, mouth slightly open; (k) black hair, smiling, mouth slightly open, eyeglasses; (l) black hair, smiling, mouth slightly open, eyeglasses, wearing hat.
IMPORTANCE OF LEARNING INFERENCE JOINTLY WITH GENERATION
To highlight the role of the inference network during learning, we performed an experiment on a toy dataset for which q(x) is a 2D gaussian mixture with 25 mixture components laid out on a grid. The covariance matrices and centroids have been chosen such that the distribution exhibits lots of modes separated by large low-probability regions, which makes it a decently hard task despite the 2D nature of the dataset.
We trained ALI and GAN on 100,000 q(x) samples. The decoder and discriminator architectures are identical between ALI and GAN (except for the input of the discriminator, which receives the concatenation of x and z in the ALI case). Each model was trained 10 times using Adam (Kingma &
Ba, 2014) with random learning rate and β1 values, and the weights were initialized by drawing from a gaussian distribution with a random standard deviation.
We measured the extent to which the trained models covered all 25 modes by drawing 10,000 samples from their p(x) distribution and assigning each sample to a q(x) mixture component according to the mixture responsibilities. We defined a dropped mode as one that wasn't assigned to any sample.
Using this definition, we found that ALI models covered 13.4 ± 5.8 modes on average (min: 8, max:
25) while GAN models covered 10.4 ± 9.2 modes on average (min: 1, max: 22).
Published as a conference paper at ICLR 2017
Figure 8: Comparison of (a) ALI, (b) GAN with an encoder learned to reconstruct latent samples (c)
GAN with an encoder learned through ALI, (d) variational autoencoder (VAE) on a 2D toy dataset.
The ALI model in (a) does a much better job of covering the latent space (second row) and producing good samples than the two GAN models (b, c) augmented with an inference mechanism.
We then selected the best-covering ALI and GAN models, and the GAN model was augmented with an encoder using the learned inverse mapping and post-hoc learned inference procedures outlined in subsection 2.2. The encoders learned for GAN inference have the same architecture as ALI's encoder.
We also trained a VAE with the same encoder-decoder architecture as ALI to outline the qualitative differences between ALI and VAE models.
We then compared each model's inference capabilities by reconstructing 10,000 held-out samples from q(x). Figure 8 summarizes the experiment. We observe the following:
• The ALI encoder models a marginal distribution q(z) that matches p(z) fairly well (row 2, column a). The learned representation does a decent job at clustering and organizing the different mixture components.
• The GAN generator (row 5, columns b-c) has more trouble reaching all the modes than the ALI generator (row 5, column a), even over 10 runs of hyperparameter search.
• Learning an inverse mapping from GAN samples does not work very well: the encoder has trouble covering the prior marginally and the way it clusters mixture components is not very well organized (row 2, column b). As discussed in subsection 2.2, reconstructions suffer from the generator dropping modes.
• Learning inference post-hoc doesn't work as well as training the encoder and the decoder jointly. As had been hinted at in subsection 2.2, it appears that adversarial training benefits
Published as a conference paper at ICLR 2017 from learning inference at training time in terms of mode coverage. This also negatively impacts how the latent space is organized (row 2, column c). However, it appears to be better at matching q(z) and p(z) than when inference is learned through inverse mapping from GAN samples.
• Due to the nature of the loss function being optimized, the VAE model covers all modes easily (row 5, column d) and excels at reconstructing data samples (row 3, column d).
However, they have a much more pronounced tendency to smear out their probability density(row 5, column d) and leave "holes" in q(z) (row 2, column d). Note however that recent approaches such as Inverse Autoregressive Flow (Kingma et al., 2016) may be used to improve on this, at the cost of a more complex mathematical framework.
In summary, this experiment provides evidence that adversarial training benefits from learning an inference mechanism jointly with the decoder. Furthermore, it shows that our proposed approach for learning inference in an adversarial setting is superior to the other approaches investigated.
CONCLUSION
We introduced the adversarially learned inference (ALI) model, which jointly learns a generation network and an inference network using an adversarial process. The model learns mutually coherent inference and generation networks, as exhibited by its reconstructions. The induced latent variable mapping is shown to be useful, achieving results competitive with the state-of-the-art on the semisupervised SVHN and CIFAR10 tasks.
ACKNOWLEDGMENTS
The authors would like to acknowledge the support of the following agencies for research funding and computing support: NSERC, Calcul Québec, Compute Canada. We would also like to thank the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012; Theano Development Team, 2016), Blocks and Fuel (van Merriënboer et al., 2015), which were used extensively for the paper. Finally, we would like to thank Yoshua Bengio, David Warde-Farley, Yaroslav Ganin and Laurent Dinh for their valuable feedback.
REFERENCES
Frédéric Bastien, Pascal Lamblin, Razvan Pascanu, James Bergstra, Ian Goodfellow, Arnaud Bergeron, Nicolas Bouchard, David Warde-Farley, and Yoshua Bengio. Theano: new features and speed improvements. arXiv preprint arXiv:1211.5590, 2012.
Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013a.
Yoshua Bengio, Eric Thibodeau-Laufer, Guillaume Alain, and Jason Yosinski. Deep generative stochastic networks trainable by backprop. arXiv preprint arXiv:1306.1091, 2013b.
James Bergstra, Olivier Breuleux, Frédéric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume
Desjardins, Joseph Turian, David Warde-Farley, and Yoshua Bengio. Theano: a cpu and gpu math expression compiler. In Proceedings of the Python for scientific computing conference (SciPy), volume 4, pp. 3. Austin, TX, 2010.
Andrew Brock, Theodore Lim, JM Ritchie, and Nick Weston. Neural photo editing with introspective adversarial networks. arXiv preprint arXiv:1609.07093, 2016.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan:
Interpretable representation learning by information maximizing generative adversarial nets. In
Advances in Neural Information Processing Systems, pp. 2172–2180, 2016.
Jeff Donahue, Philipp Krähenbühl, and Trevor Darrell. Adversarial feature learning. arXiv preprint arXiv:1605.09782, 2016.
Published as a conference paper at ICLR 2017
Alexey Dosovitskiy and Thomas Brox. Generating images with perceptual similarity metrics based on deep networks. arXiv preprint arXiv:1602.02644, 2016.
Vincent Dumoulin and Francesco Visin. A guide to convolution arithmetic for deep learning. arXiv preprint arXiv:1603.07285, 2016.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
Generative adversarial nets.
In Advances in Neural
Information Processing Systems, pp. 2672–2680, 2014.
Ian J Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. Maxout networks. arXiv preprint arXiv:1302.4389, 2013.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
Diederik P Kingma. Fast gradient-based inference with continuous latent variable models in auxiliary form. arXiv preprint arXiv:1306.0733, 2013.
Diederik P Kingma and Max Welling.
Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.
Diederik P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised learning with deep generative models. In Advances in Neural Information Processing Systems, pp.
3581–3589, 2014.
Diederik P Kingma, Tim Salimans, and Max Welling. Improving variational inference with inverse autoregressive flow. arXiv preprint arXiv:1606.04934, 2016.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images, 2009.
Alex Lamb, Vincent Dumoulin, and Aaron Courville. Discriminative regularization for generative models. arXiv preprint arXiv:1602.03220, 2016.
Anders Boesen Lindbo Larsen, Søren Kaae Sønderby, and Ole Winther. Autoencoding beyond pixels using a learned similarity metric. arXiv preprint arXiv:1512.09300, 2015.
Jianhua Lin. Divergence measures based on the shannon entropy. Information Theory, IEEE
Transactions on, 37(1):145–151, 1991.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In
Proceedings of the IEEE International Conference on Computer Vision, pp. 3730–3738, 2015.
Lars Maaløe, Casper Kaae Sønderby, Søren Kaae Sønderby, and Ole Winther. Auxiliary deep generative models. arXiv preprint arXiv:1602.05473, 2016.
Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, and Ian Goodfellow. Adversarial autoencoders. arXiv preprint arXiv:1511.05644, 2015.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. In NIPS workshop on deep learning and unsupervised feature learning, volume 2011, pp. 4. Granada, Spain, 2011.
Augustus Odena, Vincent Dumoulin, and Chris Olah. Deconvolution and checkerboard artifacts. http://distill.pub/2016/deconv-checkerboard/, 2016.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Antti Rasmus, Harri Valpola, Mikko Honkala, Mathias Berglund, and Tapani Raiko. Semi-supervised learning with ladder network. In Advances in Neural Information Processing Systems, 2015, 2015.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. arXiv preprint arXiv:1401.4082, 2014.
Published as a conference paper at ICLR 2017
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211–252, 2015.
Tim Salimans, Ian J. Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans. arXiv preprint arXiv:1606.03498, 2016.
Wenzhe Shi, Jose Caballero, Lucas Theis, Ferenc Huszar, Andrew Aitken, Christian Ledig, and Zehan Wang. Is the deconvolution layer the same as a convolutional layer? arXiv preprint arXiv:1609.07009, 2016.
Jost Tobias Springenberg. Unsupervised and semi-supervised learning with categorical generative adversarial networks. arXiv preprint arXiv:1511.06390, 2015.
Theano Development Team. Theano: A Python framework for fast computation of mathematical expressions. arXiv preprint arXiv:1605.02688, 2016.
Lucas Theis, Aron van den Oord, and Matthias Bethge. A note on the evaluation of generative models. arXiv preprint arXiv:1511.01844, 2015.
Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew W. Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio. arXiv preprint arXiv:1609.03499, 2016a.
Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. arXiv preprint arXiv:1601.06759, 2016b.
Aaron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, and Koray Kavukcuoglu.
Conditional image generation with pixelcnn decoders. arXiv preprint arXiv:1606.05328, 2016c.
Bart van Merriënboer, Dzmitry Bahdanau, Vincent Dumoulin, Dmitriy Serdyuk, David Warde-Farley, Jan Chorowski, and Yoshua Bengio. Blocks and fuel: Frameworks for deep learning. arXiv preprint arXiv:1506.00619, 2015.
Junbo Zhao, Michael Mathieu, Ross Goroshin, and Yann Lecun. Stacked what-where auto-encoders. arXiv preprint arXiv:1506.02351, 2015.
Published as a conference paper at ICLR 2017
A
HYPERPARAMETERS
Operation
Kernel
Strides
Feature maps
BN?
Dropout
Nonlinearity
Gz(x) – 3 × 32 × 32 input
Convolution
5 × 5
1 × 1
√
Leaky ReLU
Convolution
4 × 4
2 × 2
√
Leaky ReLU
Convolution
4 × 4
1 × 1
√
Leaky ReLU
Convolution
4 × 4
2 × 2
√
Leaky ReLU
Convolution
4 × 4
1 × 1
√
Leaky ReLU
Convolution
1 × 1
1 × 1
√
Leaky ReLU
Convolution
1 × 1
1 × 1
×
Linear
Gx(z) – 64 × 1 × 1 input
Transposed convolution
4 × 4
1 × 1
√
Leaky ReLU
Transposed convolution
4 × 4
2 × 2
√
Leaky ReLU
Transposed convolution
4 × 4
1 × 1
√
Leaky ReLU
Transposed convolution
4 × 4
2 × 2
√
Leaky ReLU
Transposed convolution
5 × 5
1 × 1
√
Leaky ReLU
Convolution
1 × 1
1 × 1
√
Leaky ReLU
Convolution
1 × 1
1 × 1
×
Sigmoid
D(x) – 3 × 32 × 32 input
Convolution
5 × 5
1 × 1
×
Maxout
Convolution
4 × 4
2 × 2
×
Maxout
Convolution
4 × 4
1 × 1
×
Maxout
Convolution
4 × 4
2 × 2
×
Maxout
Convolution
4 × 4
1 × 1
×
Maxout
D(z) – 64 × 1 × 1 input
Convolution
1 × 1
1 × 1
×
Maxout
Convolution
1 × 1
1 × 1
×
Maxout
D(x, z) – 1024 × 1 × 1 input
Concatenate D(x) and D(z) along the channel axis
Convolution
1 × 1
1 × 1
×
Maxout
Convolution
1 × 1
1 × 1
×
Maxout
Convolution
1 × 1
1 × 1
×
Sigmoid
Optimizer Adam (α = 10−4, β1 = 0.5, β2 = 10−3)
Batch size 100
Epochs 6475
Leaky ReLU slope, maxout pieces 0.1, 2
Weight, bias initialization Isotropic gaussian (µ = 0, σ = 0.01), Constant(0)
Table 3: CIFAR10 model hyperparameters (unsupervised). Maxout layers (Goodfellow et al., 2013) are used in the discriminator.
Published as a conference paper at ICLR 2017
Operation
Kernel
Strides
Feature maps
BN?
Dropout
Nonlinearity
Gz(x) – 3 × 32 × 32 input
Convolution
5 × 5
1 × 1
√
Leaky ReLU
Convolution
4 × 4
2 × 2
√
Leaky ReLU
Convolution
4 × 4
1 × 1
√
Leaky ReLU
Convolution
4 × 4
2 × 2
√
Leaky ReLU
Convolution
4 × 4
1 × 1
√
Leaky ReLU
Convolution
1 × 1
1 × 1
√
Leaky ReLU
Convolution
1 × 1
1 × 1
×
Linear
Gx(z) – 256 × 1 × 1 input
Transposed convolution
4 × 4
1 × 1
√
Leaky ReLU
Transposed convolution
4 × 4
2 × 2
√
Leaky ReLU
Transposed convolution
4 × 4
1 × 1
√
Leaky ReLU
Transposed convolution
4 × 4
2 × 2
√
Leaky ReLU
Transposed convolution
5 × 5
1 × 1
√
Leaky ReLU
Convolution
1 × 1
1 × 1
√
Leaky ReLU
Convolution
1 × 1
1 × 1
×
Sigmoid
D(x) – 3 × 32 × 32 input
Convolution
5 × 5
1 × 1
×
Leaky ReLU
Convolution
4 × 4
2 × 2
√
Leaky ReLU
Convolution
4 × 4
1 × 1
√
Leaky ReLU
Convolution
4 × 4
2 × 2
√
Leaky ReLU
Convolution
4 × 4
1 × 1
√
Leaky ReLU
D(z) – 256 × 1 × 1 input
Convolution
1 × 1
1 × 1
×
Leaky ReLU
Convolution
1 × 1
1 × 1
×
Leaky ReLU
D(x, z) – 1024 × 1 × 1 input
Concatenate D(x) and D(z) along the channel axis
Convolution
1 × 1
1 × 1
×
Leaky ReLU
Convolution
1 × 1
1 × 1
×
Leaky ReLU
Convolution
1 × 1
1 × 1
×
Sigmoid
Optimizer Adam (α = 10−4, β1 = 0.5, β2 = 10−3)
Batch size 100
Epochs 100
Leaky ReLU slope 0.01
Weight, bias initialization Isotropic gaussian (µ = 0, σ = 0.01), Constant(0)
Table 4: SVHN model hyperparameters (unsupervised).
Published as a conference paper at ICLR 2017
Operation
Kernel
Strides
Feature maps
BN?
Dropout
Nonlinearity
Gz(x) – 3 × 64 × 64 input
Convolution
2 × 2
1 × 1
√
Leaky ReLU
Convolution
7 × 7
2 × 2
√
Leaky ReLU
Convolution
5 × 5
2 × 2
√
Leaky ReLU
Convolution
7 × 7
2 × 2
√
Leaky ReLU
Convolution
4 × 4
1 × 1
√
Leaky ReLU
Convolution
1 × 1
1 × 1
×
Linear
Gx(z) – 512 × 1 × 1 input
Transposed convolution
4 × 4
1 × 1
√
Leaky ReLU
Transposed convolution
7 × 7
2 × 2
√
Leaky ReLU
Transposed convolution
5 × 5
2 × 2
√
Leaky ReLU
Transposed convolution
7 × 7
2 × 2
√
Leaky ReLU
Transposed convolution
2 × 2
1 × 1
√
Leaky ReLU
Convolution
1 × 1
1 × 1
×
Sigmoid
D(x) – 3 × 64 × 64 input
Convolution
2 × 2
1 × 1
√
Leaky ReLU
Convolution
7 × 7
2 × 2
√
Leaky ReLU
Convolution
5 × 5
2 × 2
√
Leaky ReLU
Convolution
7 × 7
2 × 2
√
Leaky ReLU
Convolution
4 × 4
1 × 1
√
Leaky ReLU
D(z) – 512 × 1 × 1 input
Convolution
1 × 1
1 × 1
×
Leaky ReLU
Convolution
1 × 1
1 × 1
×
Leaky ReLU
D(x, z) – 1024 × 1 × 1 input
Concatenate D(x) and D(z) along the channel axis
Convolution
1 × 1
1 × 1
×
Leaky ReLU
Convolution
1 × 1
1 × 1
×
Leaky ReLU
Convolution
1 × 1
1 × 1
×
Sigmoid
Optimizer Adam (α = 10−4, β1 = 0.5)
Batch size 100
Epochs 123
Leaky ReLU slope 0.02
Weight, bias initialization Isotropic gaussian (µ = 0, σ = 0.01), Constant(0)
Table 5: CelebA model hyperparameters (unsupervised).
Published as a conference paper at ICLR 2017
Operation
Kernel
Strides
Feature maps
BN?
Dropout
Nonlinearity
Gz(x) – 3 × 64 × 64 input
Convolution
4 × 4
2 × 2
√
Leaky ReLU
Convolution
4 × 4
1 × 1
√
Leaky ReLU
Convolution
4 × 4
2 × 2
√
Leaky ReLU
Convolution
4 × 4
1 × 1
√
Leaky ReLU
Convolution
4 × 4
2 × 2
√
Leaky ReLU
Convolution
4 × 4
1 × 1
√
Leaky ReLU
Convolution
1 × 1
1 × 1
√
Leaky ReLU
Convolution
1 × 1
1 × 1
√
Leaky ReLU
Convolution
1 × 1
1 × 1
×
Linear
Gx(z) – 256 × 1 × 1 input
Convolution
1 × 1
1 × 1
√
Leaky ReLU
Convolution
1 × 1
1 × 1
√
Leaky ReLU
Transposed convolution
4 × 4
1 × 1
√
Leaky ReLU
Transposed convolution
4 × 4
2 × 2
√
Leaky ReLU
Transposed convolution
4 × 4
1 × 1
√
Leaky ReLU
Transposed convolution
4 × 4
2 × 2
√
Leaky ReLU
Transposed convolution
4 × 4
1 × 1
√
Leaky ReLU
Transposed convolution
4 × 4
2 × 2
√
Leaky ReLU
Convolution
1 × 1
1 × 1
×
Sigmoid
D(x) – 3 × 64 × 64 input
Convolution
4 × 4
2 × 2
×
Leaky ReLU
Convolution
4 × 4
1 × 1
√
Leaky ReLU
Convolution
4 × 4
2 × 2
√
Leaky ReLU
Convolution
4 × 4
1 × 1
√
Leaky ReLU
Convolution
4 × 4
2 × 2
√
Leaky ReLU
Convolution
4 × 4
1 × 1
√
Leaky ReLU
D(z) – 256 × 1 × 1 input
Convolution
1 × 1
1 × 1
×
Leaky ReLU
Convolution
1 × 1
1 × 1
×
Leaky ReLU
D(x, z) – 2304 × 1 × 1 input
Concatenate D(x) and D(z) along the channel axis
Convolution
1 × 1
1 × 1
×
Leaky ReLU
Convolution
1 × 1
1 × 1
×
Leaky ReLU
Convolution
1 × 1
1 × 1
×
Sigmoid
Optimizer Adam (α = 10−4, β1 = 0.5, β2 = 10−3)
Batch size 128
Epochs 125
Leaky ReLU slope 0.01
Weight, bias initialization Isotropic gaussian (µ = 0, σ = 0.01), Constant(0)
Table 6: Tiny ImageNet model hyperparameters (unsupervised).
Published as a conference paper at ICLR 2017
B
A GENERATIVE STORY FOR ALI
Xavier's painting
Zelda's description
Mr. Discriminator
Xena's depiction
Zach's description
Figure 9: A Circle of Infinite Painters' view of the ALI game.
The Circle of Infinite Painters is a very prolific artistic group. Very little is known about the Circle, but what we do know is that it is composed of two very brilliant artists. It has produced new paintings almost daily for more than twenty years, each one more beautiful than the others. Not only are the paintings exquisite, but their title and description is by itself a literary masterpiece.
However, some scholars believe that things might not be as they appear: certain discrepancies in the Circle's body of work hints at the Circle being composed of more than one artistic duo. This is what Joseph Discriminator, art critique and world expert on the Circle, believes. He's recently been working intensively on the subject. Without knowing it, he's right: the Circle is not one, but two artistic duos.
Xavier and Zach Prior form the creative component of the group. Xavier is a painter and can, in one hour and starting from nothing, produce a painting that would make any great painter jealous.
Impossible however for him to explain what he's done: he works by intuition alone. Zach is an author and his literary talent equals Xavier's artistic talent. His verb is such that the scenes he describes could just as well be real.
By themselves, the Prior brothers cannot collaborate: Xavier can't paint anything from a description and Zach is bored to death with the idea of describing anything that does not come out of his head.
This is why the Prior brothers depend on the Conditional sisters so much.
Zelda Conditional has an innate descriptive talent: she can examine a painting and describe it so well that the original would seem like an imitation. Xena Conditional has a technical mastery of painting that allows her to recreate everything that's described to her in the most minute details. However, their creativity is inversely proportional to their talent: by themselves, they cannot produce anything of interest.
As such, the four members of the Circle work in pairs. What Xavier paints, Zelda describes, and what
Zach describes, Xena paints. They all work together to fulfill the same vision of a unified Circle of Infinite Painters, a whole greater than the sum of its parts.
This is why Joseph Discriminator's observations bother them so much. Secretly, the Circle put Mr.
Discriminator under surveillance. Whatever new observation he's made, they know right away and work on attenuating the differences to maintain the illusion of a Circle of Infinite Painters made of a single artistic duo.
Will the Circle reach this ideal, or will it be unmasked by Mr. Discriminator?Toward Deep Learning Software Repositories
Martin White, Christopher Vendome, Mario Linares-V´asquez, and Denys Poshyvanyk
Department of Computer Science
College of William and Mary
Williamsburg, Virginia 23187–8795
Email: {mgwhite, cvendome, mlinarev, denys}@cs.wm.edu
Abstract—Deep learning subsumes algorithms that automatically learn compositional representations. The ability of these models to generalize well has ushered in tremendous advances in many fields such as natural language processing (NLP).
Recent research in the software engineering (SE) community has demonstrated the usefulness of applying NLP techniques to software corpora. Hence, we motivate deep learning for software language modeling, highlighting fundamental differences between state-of-the-practice software language models and connectionist models. Our deep learning models are applicable to source code files (since they only require lexically analyzed source code written in any programming language) and other types of artifacts. We show how a particular deep learning model can remember its state to effectively model sequential data, e.g., streaming software tokens, and the state is shown to be much more expressive than discrete tokens in a prefix. Then we instantiate deep learning models and show that deep learning induces high-quality models compared to n-grams and cachebased n-grams on a corpus of Java projects. We experiment with two of the models' hyperparameters, which govern their capacity and the amount of context they use to inform predictions, before building several committees of software language models to aid generalization. Then we apply the deep learning models to code suggestion and demonstrate their effectiveness at a real SE task compared to state-of-the-practice models. Finally, we propose avenues for future work, where deep learning can be brought to bear to support model-based testing, improve software lexicons, and conceptualize software artifacts. Thus, our work serves as the first step toward deep learning software repositories.
Keywords—Software repositories, machine learning, deep learning, software language models, n-grams, neural networks
I.
INTRODUCTION
The field of natural language processing (NLP) has developed many prominent techniques to support speech recognition and statistical machine translation, among many other applications. One critical component to many of these techniques is a statistical language model, and the most prevalent class of statistical language models is simple Markov models called n-gram models (or "n-grams"). n-grams are useful abstractions for modeling sequential data where there are dependencies among the terms in a sequence. A corpus can be regarded as a sequence of sequences, and corpusbased models such as n-grams learn conditional probability distributions from the order of terms in a corpus. Corpusbased models can be used for many different types of tasks like discriminating instances of data or generating new data that are characteristic of a domain.
The terms in a sequence can represent different entities depending on the domain. In software engineering (SE), sequential data emerge from countless artifacts, e.g., source code files and execution traces, where the terms (or "words") can be software tokens or method calls, and the sequences (or
"sentences") can be lines of code or method call sequences.
While software tokens and method calls characterize two different lexicons, statistical language models such as n-grams can be applied to corpora from each domain because the models represent simple arrangements of terms. Consequently, models like n-grams can be used to predict the next term in a sequence.
Recent research in the SE community has examined and successfully applied n-grams to formal languages, like programming languages, and SE artifacts –. The breadth of these applications in SE research and practice underscores the importance of the ability to effectively learn from sequential data in software repositories. However, there is an apparent discrepancy between the representation power of models like n-grams for reaping information from repositories and the expressiveness that is produced and archived in repositories.
Consider the characteristics of modern software repositories and the requirements that these characteristics impose on models. Software repositories are massive depots of unstructured data, so good models require a lot of capacity to be able to learn from the voluminous scale rather than saturate after observing a fraction of the data that are available. Specifically, the kind of conceptual information that is buried in software repositories is very complex, requiring expressive models to manage this complexity. Moreover, software artifacts are laden with semantics, which means approaches that depend on matching lexemes are suboptimal. Finally, practical SE tasks require a lot of context—much more than short lists of the last two, three, and four terms in a sequence—whether the task is developing a feature or reproducing an issue. Capacity, expressiveness, semantics, and context are key concerns when mining sequential SE data and inducing software language models in particular. Nonetheless, n-grams have limited effective capacity. They are not expressive, because they are simply smoothed counts of term co-occurrences.
They have trouble with semantics and generalizing beyond the explicit features observed in training –. Lastly, language models, including software language models, based on n-grams are quickly overwhelmed by the curse of dimensionality, so the effective amount of context is limited.
How can we improve the performance at SE tasks (e.g., code suggestion) based on software language models? In order to improve the quality of software language models, we must improve the representation power of the abstractions we use, so the goal of this paper is to marry deep learning and software language modeling. The purpose of applying deep learning to software language modeling is to improve the quality of the underlying abstractions for numerous SE tasks, viz. code suggestion,, deriving readable string test inputs to reduce human oracle cost, predicting programmer comments to improve search over code bases and code categorization, improving error reporting, generating feasible test cases to improve coverage, improving stylistic consistency to aid readability and maintainability, and code migration –. Thus, we make the following contributions:
• We introduce deep learning to SE research, specifically, software language modeling. Deep learning, a nascent field in machine learning, will provide the SE community with new ways to mine and analyze sequential data to support SE tasks.
• We motivate deep learning algorithms for software language modeling by clearly distinguishing them from state-of-the-practice software language models.
• We show that deep learning induces high-quality software language models compared to state-of-the-practice models using an intrinsic evaluation metric. Then we demonstrate its effectiveness at a practical SE task.
• Our work is the first step in a new space of models and applications. While we focus on applying one deep architecture to one SE task, we believe deep learning is teeming with opportunities in SE research. We identify several avenues for future work, which highlight different ways that deep learning can be used to support practical SE tasks.
Sec. II will review background on software language modeling and deep learning for NLP. This section will define all the keywords (e.g., deep architecture, deep learning, deep software language model) and affirm the purpose of introducing these state-of-the-art approaches to SE research. Sec. III will pinpoint how this new class of software language models is poised to perform better at SE tasks by emphasizing their capacity and expressiveness, as well as their ability to model semantics and consider rich contexts. Sec. IV will use perplexity (PP), an intrinsic evaluation metric, to compare the quality of this new class of software language models to a state-of-the-practice baseline, and Sec. V will measure the models at a real SE task.
Sec. VI will discuss threats to the validity of our work. Sec. VII will describe several avenues for future work. One avenue proposes using deep software language models to support objectives other than code coverage in model-based testing.
Another avenue proposes using deep software language models to improve software lexicons. Sec. VIII concludes the paper.
II.
BACKGROUND AND RELATED WORK
In this section, we present background on statistical language models and preliminary research applying these models to software corpora. We focus on how current approaches to software language modeling can be improved, laying the foundation for Sec. III, where we show how deep learning can realize these improvements. Then we define all the keywords associated with deep learning, before presenting preliminary research applying deep learning to NLP.
A. Statistical Language Models
A statistical language model is a probability distribution over sentences in a language. This ostensibly simple abstraction is remarkably effective for NLP tasks such as speech recognition and statistical machine translation. In statistical language modeling, our goal is to find a tractable representation of a sentence s by way of the joint distribution: p(s) = m
� i=1 p(wi|wi−1
) ≈ m
� i=1 p(wi|wi−1 i−n+1).
In practice, we generalize the model's maximum likelihood estimates using one of many smoothing techniques. Essentially, these probabilistic automata (i.e., n-grams) measure the degree of membership of every conceivable sentence in the language. Sentences frequently observed—in a generative sense—in the training corpus are judged to be more fluent than sentences observed less frequently (or not at all). In other words, we expect a good model to assign a high probability to a representative test document, or, equivalently, in-domain out-of-sample cases should have low cross entropy, Hp(s) ≈ − 1 m m
� i=1 log2 p(wi|wi−1 i−n+1).
Cross entropy is an empirical estimate of how well a language model predicts terms in a sequence. Likewise, PP = 2Hp estimates the average number of tokens at each point in the test document. In language modeling, PP is a proxy for quality, and—as noted by Tu et al. —good quality language models show great promise in SE applications. Our goal is to propose powerful abstractions novel to software language modeling using PP as empirical validation of their efficacy and capacity to support SE tasks.
B. Applications of Statistical Software Language Models
Hindle et al. demonstrated that language models over software corpora emit a "naturalness" in the sense that real programs written by real people have useful statistical properties, encapsulated in statistical language models, that can leverage SE tasks. This work was an important first step in applying natural language abstractions to software corpora, but n-grams are simple approaches that do not have the capacity to learn representations that reliably generalize beyond the explicit features in a training corpus. Furthermore, these models build limited domain abstractions, and they are quickly overwhelmed by the curse of dimensionality,, –. The expectation in software language modeling research is that performance at SE tasks will improve with models more sophisticated than n-grams. The purpose of our work is to introduce compositional representations that are designed to process data in stages in a complex architecture.
Each stage transforms internal representations as information flows from one layer of the architecture to the next. The feature spaces in a deep learning model are fundamentally different than the conditional probability tables that constitute an n-gram model, and the power lies in the fact that these representations generalize well,.
Allamanis and Sutton estimated an n-gram from a software corpus with more than one billion tokens, but we regard the massive scale as an organic smoothing technique.
The model's effectiveness is still subject to token distances in the corpus, where clues behind the n-gram's relatively short prefix (or "history") are elided from the model's context,,. Moreover, the massive scale does not truly solve the problem of considering tokens' semantic similarity,,. The approach for software language modeling that we present in Sec. III is designed to consider an arbitrary number of levels of context, where context takes on a much deeper meaning than concatenated tokens in a prefix. In our work, the deep learning model encodes context in a continuous-valued state vector, encapsulating much richer semantics. Finally, Allamanis and Sutton conducted experiments where they collapsed the vocabulary by having the tokenizer replace identifiers and literals with generic tokens, which was a novel way to measure the model's performance on structural aspects of the code. However, we regard this approach as feature engineering. In this case, the token types in the corpus are engineered to solve the specific problem of modeling syntax.
But the essence of deep learning, which underpins our work, is to design approaches that can automatically discover these feature spaces, to—for instance—capture regularities at the syntactic, type, scope, and semantic levels.
Although Allamanis' giga-token model over source code demonstrated improvements in quality, a drawback to estimating n-grams over a massive corpus is losing resolution in the model. Good resolution yields regularities "endemic" to particular granularities, e.g., methods, classes, or modules. Of course, if the training corpus is too small, then the language model will be brittle for any practical application,.
A cache-based language model, is designed to solve this optimization problem by interpolating a static model with a dynamic cache component. Recently, Tu et al. applied cache models to software corpora: p(wi|wi−1 i−n+1, c) = αpN(wi|wi−1 i−n+1)+βpC(wi|wi−1 i−n+1) (3) where 0 ≤ α, β and α + β = 1, c is the list of n-grams that are stored in the cache, pN is a static n-gram model, and pC is a dynamic cache model. The cache component encapsulates endemic and specific patterns in source code.
While the cache component is a mechanism for capturing local context, the context we recur in a deep learning model will be an expressive continuous-valued state vector, which is capable of characterizing domain concepts, rather than simply storing auxiliary conditional probability tables like pC. Consequently, cache-based language models still suffer from the inability to understand semantic similarity, because they are fundamentally look-up tables, whereas deep learning models induce similar representations for token types used in similar ways.
C. Artificial Neural Networks
Connectionism includes an expansive and deep body of knowledge that pervades artificial intelligence, cognitive psychology, neuroscience, and philosophy, and a rigorous treatment is well beyond the scope of this paper. Connectionist models comprise neuron-like processing units, and each unit has an activity level computed from its inputs. In a feed-forward topology, information in the artificial neural network flows from input units through hidden units to output units along connections with adjustable weights. A neural network architecture specifies intrinsic characteristics such as the number of units in the input, hidden, and output layers as well as the number of hidden layers in the network. A deep architecture comprises many hidden layers. Supervised learning algorithms discriminatively train the weights to achieve the desired input-output behavior, so the hidden units automatically learn to represent important features of the domain. This process of training the weights in a deep architecture is known as deep learning, and we refer to software language models based on deep learning as deep software language models. Accordingly, deep learning models, including deep software language models, comprise multiple levels of nonlinear transformations. The canonical learning algorithm for neural networks is the back-propagation procedure, which allows an arbitrarily connected neural network to develop internal representations of its environment. These neural activation patterns, or distributed representations, harness formidable and efficient internal representations of domain concepts. Units can "participate" in the representation of more than one concept, which gives way to representational efficiency (where different pools of units encode different concepts) and aids generalization,.
A simple two-layer feed-forward neural network, with one hidden layer and one output layer, cannot reliably learn beyond first-order temporal dependencies. This architecture can be augmented with a short-term memory by recurring the hidden layer, which encapsulates the network's state, back to the input layer. The directed cycle provides context for the current prediction, and this continuous-valued state vector is fundamentally different than a discrete token in an n-gram's history. We can provide more context by extending the recurrence and considering an arbitrary number of levels of context.
From a temporal perspective, this recurrent neural network(RNN) can be viewed as a very deep neural network, –, where depth is the length of the longest path from an input node to an output node, and the purpose of the depth in this case is to reliably model temporal dependencies. The depth of a RNN is evident when you unfold the recurrence in time and measure the path from any unit in the deepest state vector to any output unit. Deep architectures like RNNs lie at the forefront of machine learning and NLP, but we are not indiscriminately introducing complexity. We expect these approaches will yield tremendous advances in SE as they already have in other fields.
D. Applications of Neural Network Language Models
Connectionist models for NLP go back at least as far as
Elman, who used them to represent lexical categories, and Miikkulainen and Dyer, who developed a mechanism for building distributed representations for communication in a parallel distributed processing network. Bengio et al. proposed a statistical model of natural language based on neural networks to learn distributed representations for words to allay the curse of dimensonality: One training sentence increases the probability of a combinatorial number of similar sentences. Sequences of words were modeled by agglutinating the word representations of consecutive words in the corpus into a single pattern to be presented to the network.
Bengio also constructed model ensembles by combining a neural network language model with low-order n-grams and observed that mixing the neural network's posterior distribution with an interpolated trigram improved the performance.
This work also measured the performance of the model after adding direct connections from nodes in the projection layer to output nodes, but the topology of this network does not constitute a deep architecture. This model represents history by presenting n-gram patterns to the network, whereas our work is based on a network which considers an arbitrary number of contextual levels to inform predictions.
Our primary related work is the work by Mikolov, who excised the projection layer in Bengio's architecture and added recurrent connections from the hidden layer back to the input layer to form a RNN. Representing context with recurrent connections rather than patterns of n-grams is what distinguishes Mikolov's recurrent architecture from
Bengio's feed-forward architecture. Mikolov reported improvements using RNNs over feed-forward neural networks and implemented a toolkit for training, evaluating, and using RNN language models. The package implements several heuristics for controlling the computational complexity of training RNNs. Recently, Raychev et al. proposed a tool based in part on Mikolov's package, RNNs, and program analysis techniques for synthesizing API completions.
III.
A DEEP SOFTWARE LANGUAGE MODEL
In this section, we specify a deep architecture for software language modeling and pinpoint how this new class of models is poised to improve the performance at SE tasks that use language models. We begin with the ubiquitous two-layer feedforward neural network. As noted in Sec. II, these models cannot reliably learn beyond first-order temporal dependencies, so Elman networks augment the architecture with a short-term memory mechanism. RNNs extend Elman networks by considering an arbitrary number of levels of context. RNNs are state-of-the-art models for NLP, but they are expensive to train, so a number of heuristics have been developed to control the complexity. One heuristic is designed to reduce the complexity of computing the posterior distribution for each training example by factorizing the output layer and organizing the tokens into classes,. Another heuristic involves training a maximum entropy model with a RNN by implementing direct connections between input units and output units.
Feed-forward networks. A pattern is presented to a feedforward neural network by setting the value of each unit in the network's input layer x to the pattern's corresponding value.
For instance, given a software corpus C of lexically analyzed tokens, we can represent a token w in the vocabulary VC using one-hot encoding1 and set wi = xi. The token is projected onto a feature space F by an affine transformation pj = ajixi +bj.
This transformation (or "pre-activation") is a fundamental point of divergence from models like n-grams. Then each pj is transformed by a differentiable, nonlinear function f such that zj = f(pj) where zj are the units that comprise the hidden layer z. The size of z (i.e., |z|) is an example of a "hyperparameter", and adjusting this hyperparameter will regulate the model's capacity such that models with larger hidden layers yield more capacity,. Practical choices for f include the logistic sigmoid, the hyperbolic tangent, and the rectifier. These "activation" functions enable highly nonlinear and supremely expressive models.
1In a one-hot encoded vector, one component is equal to one, and every other component is equal to zero, e.g., w = (0,..., 0, 1, 0,..., 0).
After learning weights from x to z, when a fresh token is presented to the network, the units zj will fire with varying intensities—analyzing the learned features—and ascribe a point in F to the token, effectively inducing clusters of examples in F. These clusters enable a connectionist software language model to generalize beyond simple Markov chains in C like n-grams and model semantics. The hidden units are transformed qk = βkjzj (omitting all bias terms going forward) and activated by a function g in the output layer y such that yk = g(qk). For multinomial classification problems, such as predicting the next token in source code, the softmax function activates values in the output layer such that p(yk|w) = g(qk). In software language modeling, propagating a token w(t) from x through z to y yields a posterior distribution over VC, and the model predicts the next token w(t + 1) in a sequence:
ˆw(t + 1) = argmax k p(yk|w(t)).
We require an algorithm for learning θ = {a, β} from C, i.e., maximizing the likelihood function, L(θ) =
|C|
� t=1 p(w(t + 1)|w(t), θ).
Equivalently, we can minimize the negative log-likelihood by training θ using stochastic gradient descent. For each w ∈ C, we compute the gradient of the error in the output layer, using a cross entropy criterion, and propagate this error back through the network, using the chain rule to evaluate partial derivatives of the error function with respect to the weights, before updating the weights. Overfitting is a concern since these models have the capacity to learn very complex representations, so θ is typically regularized,,.
Elman networks. The immediate concern with the model(Eq. (4)) is the inability to reliably learn beyond first-order temporal dependencies. n-grams encode "temporal" dependencies by learning tables of smoothed conditional probability distributions over a large number of prefixes. On the other hand, connectionist models can represent histories much more compactly. Specifically, an Elman network augments a feedforward network with a simple short-term memory mechanism.
The short-term memory is realized by copying the hidden state z(t−1) back to the input layer x(t) and learning more weights γ to influence the hidden activations. This recurrence provides context for the current prediction. Thus, the input layer x in an Elman network is essentially a concatenation of w(t) and z(t − 1), i.e., xi(t) = (w(t), z(t − 1))i. In a feed-forward network, the pre-activation in z took the form pj = ajixi, but after recurring the state vector, the pre-activation in z takes the form pj(t) = ajiwi(t) + γjjzj(t − 1) = αjixi(t), where α is simply a concatenation of aji and γjj, and the model becomes θ = {α, β}. The cost of learning an additional
O(m2) parameters, where m = |z|, is met with improved representation power over sequences of software tokens.
Recurrent networks. A RNN (Fig. 1) extends the memory bank in an Elman network for an arbitrary number of levels of context—though there may be practical limits to the depth of the recurrence. Therefore, because of the gradient problem, we typically truncate the back-propagation through time procedure. Parenthetically, there are other ways a β γ w z y w(1) w(2) w(3) w(4) w(5) w(6) w(7) w(8) int
[
] list
= null
</s>
Fig. 1.
RNN UNFOLDED IN TIME. The depth of a RNN is evident when the recurrence is unfolded in time. Time steps correspond to software tokens w(t) in a corpus, where w(0) = <s>. Each node in the figure represents a vector of units. White nodes are one-hot token representations; gray nodes are continuous-valued, hidden states; black nodes represent posterior distributions over the vocabulary. Units in the state vectors (gray nodes) compute their activation as a function of the current token and the previous state. Regarding the depth in this notional model, y(8) is a function of w(8), z(7), yet z(7) is a function of w(7), z(6), etc. Hence, predictions are informed by processing data in the past using multiple levels of nonlinear transformations. to control this problem,,, but we omit these implementation details here. As the error is back-propagated through time in a RNN, each level of temporal context has an abating amount of influence on training γ, which is shared across time. This weight sharing yields an efficient representation compared to n-grams—which are hampered by the curse of dimensionality as they try to encode deeper contexts—and persisting a sequence of state vectors is much more expressive, in terms of discriminative power, than hardcoded prefixes. Interestingly, if τ is the number of time steps the error is back-propagated through time, the network is still capable of learning information longer than τ steps.
However, while a RNN is capable of learning powerful representations, it has some computationally expensive components, e.g., the posterior distribution in the output layer. Massive software repositories have a daunting challenge that arguably far exceeds the same problem in natural languages (including highly inflective natural languages), which is the size of the vocabulary. Computing the softmax function over extremely large vocabularies |C| × ε times, where ε is the number of training epochs, is nontrivial. One solution to controlling this complexity is to factorize the output layer,,,. Using class-based output layers has been shown to yield
15–30 times speed-up. Direct connections are another implementation detail designed to improve performance. Bengio et al. implemented direct connections from units in the projection layer to output units. The authors reported the connections did not help the model generalize from their relatively small corpus, but the connections did help reduce the training time. Mikolov proposed direct connections from input units to output units and cast these connections as a maximum entropy model which can be trained with the neural network using stochastic gradient descent. The only change to the model specification is the addition of a term in the output pre-activation to account for the connections.
The direct connections are reported to have yielded significant performance gains with respect to PP and word error rate.
Now, we are ready to present a deep architecture for software language modeling, specified in Eq. (6)–(8), without
TABLE I.
STATISTICS ON THE CORPORA USED FOR THE STUDY
Corpus
Projects
Tokens
Unique
Training
Development
Testing
Total
125,181 implementation details like class-based output layers and direct connections for clarity: xi(t) = (w(t), z(t − 1))i(6) zj(t) = f(αjixi(t))(7) yk(t) = g(βkjzj(t))(8) where α = concatenate(a, γ), f(uj) = sigmoid(uj), and g(uk) = softmax(uk). The model is similar to Eq. (4), except we present more than the current token to the network, i.e., x(t) rather than simply w(t). For example, in Fig. 1, the input layer x(5) comprises the two red nodes (Eq. (6)), α concatenates the linear transformations represented by the dashed arrows, and the network computes (Eq. (7)–(8)) a posterior distribution (blue node). The argument of the maximum of this distribution is the network's prediction, which (in this case) should be the Java literal null.
IV.
EMPIRICAL VALIDATION
The goal of our empirical study was to evaluate the effectiveness and expressiveness of deep software language models with the purpose of providing better abstractions for software language modeling and its associated SE tasks. We used PP, an intrinsic evaluation metric that estimates the average number of tokens to choose from at each point in a sequence, as the criterion. We began by computing the PP of several different n-gram configurations by varying the order n and adding a dynamic cache component to establish a state-of-the-practice baseline in software language modeling. Then we instantiated several deep software language models and computed the PP of these models with varying amounts of capacity and context over the same software corpus. Next, we selected the most performant architectures and interpolated several model instances to aid generalization and assess the performance of committees of software language models. Finally, deep software language models are capable online learners, so we also measured the performance of models that learned as they tested on out-of-domain samples.
A. Methodology and Data Collection
To build the corpora for our study, we used the JFlex scanner generator, which was packaged with a full Java lexical analyzer, to tokenize the source code in a repository of 16,221 Java projects cloned from GitHub. We augmented the production rules in the lexer since it originally did not support Java annotations. After tokenizing the files in each project, we sampled projects from the repository without replacement, querying enough projects to gather over seven million tokens. Then we randomly partitioned the projects into mutually exclusive training, development, and testing sets where approximately five million tokens were allotted for training, one million tokens for development, and one
Fig. 2.
PP V. ORDER. PP of back-off (red) and interpolated (blue) models at different orders (from two to nine). The 5-gram and 8-gram were the top performing back-off and interpolated models, respectively. million tokens for testing. The purpose of a training set is to learn a useful representation of the domain. For example, a high-quality software language model is useful, because it can effectively predict the next token in a sequence. In a supervised setting, the training set couples input and its corresponding target to guide the form of the representation. To learn a good model, the supervised learning algorithm presents a token w(t) to the model and, in the case of deep software language models, the back-propagation through time algorithm(with a gradient descent step) trains the model using the next token w(t + 1) in the sequence. Generally, the purpose of a development set is to govern hyperparameters such as the learning rate in gradient searches. The model is evaluated on the development set after each training epoch, and its relative performance on the development set can be used to judge convergence. It is important to note that data in the development set are not used to learn any of the model's parameters. For example, in the case of deep software language models, none of the weights are modified as the model is evaluated on the development set. Once the model is trained, it can be evaluated on a testing set. Notably, by partitioning the projects into mutually exclusive training, development, and testing sets, all of our experiments simulated new project settings (or "greenfield development"). Training and testing on distinct domains presents unique challenges for corpusbased models like software language models.
For each set of projects, we removed blank lines from the files and randomly agglutinated the source files to form a training corpus, a development corpus, and a testing corpus.
Tab. I lists summary statistics for each corpus, including the number of projects used to form each corpus, the total number of tokens in each corpus, and the number of unique tokens in each corpus. The total unique tokens denotes the number of unique tokens in a concatenated corpus of all three corpora.
From these corpora, we used standard text normalization techniques that are used in the NLP community. We used regular expressions to replace integers, real numbers, exponential notation, and hexadecimal numbers with a generic
<num> token. After replacing numbers, we replaced singletons in each corpus as well as every token in the development and testing corpora that did not appear in the training corpus with <unk> to build an open vocabulary system with a vocabulary size of 71,293.
B. State-of-the-Practice Software Language Models
In practice, n-grams' maximum likelihood estimates are discounted, and the probability mass gleaned from the observed n-grams is redistributed using either back-off or interpolation. We used SRILM to estimate back-off
Fig. 3.
PP V. CACHE SIZE. PP of 5-gram back-off (red) and interpolated 8gram interpolated (blue) models with unigram caches varying in size from 10 to 10,000. The 100-token cache yielded the best result for each model type. and interpolated n-grams, from our training corpus, varying the order from two to nine. Each model was smoothed using modified Kneser-Ney with an unknown token and no cutoffs. Fig. 2 plots PP versus order for each model. The models' results on the test corpus are virtually indistinguishable for this dataset, and both models appear to saturate near order five. This saturation is consistent with other studies on similar corpora. With respect to PP, the most performant back-off model was the 5-gram (PP = 19.8911) and the most performant interpolated model was the 8-gram (PP = 19.9815). We augmented each of the models with a unigram cache, varying the size of the cache from 10 to 10,000. The dynamic unigram cache model was linearly interpolated with the static n-gram model using a mixing coefficient of 0.05. Fig. 3 plots PP versus unigram cache size for both models. The 100-token unigram cache component effectively improves the performance for both the 5-gram back-off model (PP = 12.3170) and the interpolated 8-gram model (PP = 12.2209). These performance gains from using a dynamic cache component are consistent with previous empirical studies.
We used the interpolated 8-gram model with a 100-token unigram cache (PP = 12.2209) as the baseline.
C. State-of-the-Art Software Language Models
After computing a baseline using state-of-the-practice software language models, we configured a RNN. These models have expansive design spaces spanned by several hyperparameters. We chose to measure the performance by varying the size m of the hidden layer and the number of steps τ in the truncated back-propagation through time algorithm. To train and test RNNs, we used the RNNLM Toolkit. We instantiated 10 models with the same random seed, but we varied m from 50 to 500 units with sigmoid activations.2 For each model, we truncated the back-propagation through time algorithm at τ = 10 levels, updating the context every 10 iterations, and factorized the output layer into 268 classes.
We used the default starting learning rate of 0.1 and the default ℓ2 regularization parameter of 10−6. The learning rate was annealed during training by monitoring the model's performance on the development set: After each training epoch, PP on the development set was computed to govern the learning rate schedule. Finally, to determine the number of direct connections from input nodes to output nodes, we built a frequency distribution of the token types in the training
2While the RNNLM Toolkit uses sigmoid activations, the toolkit implements a simple mechanism to control the gradient problem by limiting the maximum size of gradients of errors that get accumulated in the hidden units.
Levels 5 10 15 20
Fig. 4.
PP V. HIDDEN SIZE AND DEPTH. PP of RNNs with hidden layers varying in size. Initially, we fixed the number of levels of context at 10. Then, for each 200 ≤ m ≤ 400, we varied the number of levels of context. corpus. We found that 995 token types covered 80.0% of the tokens in the training corpus, so we set the number of direct connections equal to 1,000. Fig. 4 plots PP versus m, where the deep models are shown to outperform—without a dynamic auxiliary component like a cache during testing— the baseline on this dataset, with the best results between 200 and 400 units. Next, we selected the five models with m between 200 and 400. For each model, we varied the number of levels of context τ = 5, 15, 20, keeping the same configuration for every other parameter. Fig. 4 plots PP for each 200 ≤ m ≤ 400 at four different values for τ. For our dataset, the most performant models in our (m, τ) design space were (300, 20) (PP = 10.1960) and (400, 5) (PP = 10.1721).
Deep learning (PP = 10.1721) beat the baseline.
D. Committees of Deep Software Language Models
Neural network language models are initialized with small random weights. Different instantiations will likely lead to models finding different local minima on the error surface, and models converging to different local minima may have different perspectives on the task. Therefore, we can construct committees of software language models by simply averaging p(y|x) for each model instance. Bengio et al. reported performance gains by combining a neural network language model with an interpolated trigram, and the authors noted the performance gains suggest that the models make errors in different places. Likewise, Schwenk and Gauvain interpolated neural network language models with back-off models to improve the performance in a speech recognition system. Mikolov reported performance gains by combining several RNN language models. We instantiated five
RNN-(300, 20) models and five RNN-(400, 5) models with different random seeds. Tab. II lists the results of combining several software language models on our dataset, e.g., RNN(300, 20)-1,2 denotes the linear interpolation of two RNN(300, 20) models—one model instantiated with random seed
1 and the another instantiated with random seed 2—where the coefficients in the mixture are 0.50. N denotes an interpolated
8-gram model with a 100-token unigram cache. So, RNN(300, 20)-1,2,3,4,5,N represents the combination of five deep models and an interpolated n-gram model, where the combination of deep models has a weight of 0.60 in the mixture and the n-gram has a weight of 0.40. The top performing committee, RNN-(300, 20)-1,2,3,4,5,N, achieves PP = 7.8512, which is equivalent to a cross-entropy score of 2.9729 bits.
Recall these performance scores are computed using a training corpus of 732 randomly chosen projects, and the test corpus is TABLE II.
COMMITTEES OF SOFTWARE LANGUAGE MODELS
Committee
Coefficients
PP
RNN-(300, 20)-1,2
RNN-(300, 20)-1,2,3
RNN-(300, 20)-1,2,3,4
RNN-(300, 20)-1,2,3,4,5
RNN-(300, 20)-1,2,3,4,5,N
RNN-(400, 5)-1,2
RNN-(400, 5)-1,2,3
RNN-(400, 5)-1,2,3,4
RNN-(400, 5)-1,2,3,4,5
RNN-(400, 5)-1,2,3,4,5,N
7.9346 another random collection of 173 out-of-domain projects. The committee improves the performance as compared to instances of each model, e.g., RNN-(300, 20)-1 (PP = 10.1960) and N(PP = 12.2209).
Constructing committees of deep software language models can aid generalization and improve performance.
E. Deep Software Language Models Online
Cache-based language models separate static concerns from dynamic concerns using a convex combination of components, where a large static model is interpolated with a small dynamic model. In software language modeling, this small dynamic model has been used to capture local patterns in source code. Neural network language models are capable of learning online by back-propagating the error for each test document and performing a gradient search thereby enabling adaptation. Tab. III lists the results of evaluating models online on our dataset. RNNs denotes a static model;
RNNd denotes a dynamic model; RNNc denotes a committee of static and dynamic models using the corresponding mixture coefficient. N denotes an interpolated 8-gram model with a 100-token unigram cache as above. For example, RNNs(300, 20)-5 denotes a static model with 300 hidden units and 20 levels of context instantiated with random seed 5, and RNNd(300, 20)-5 denotes a similarly configured model that learns as it tests. RNNc-(300, 20)-5 denotes a committee comprising the static and dynamic models whose votes are weighted by the coefficients. The static and dynamic models were equally weighted in our experiments. Evaluating models online, where the deep software models can learn as they test, significantly improved the performance on our dataset.
For deep software models online (PP = 3.5958), the cross entropy scores are on the order of two bits.
When deep software language models are online, they can be incrementally trained. Thus, in new project settings, online learners are able to automatically adapt as the project is being developed. In the committee of static and dynamic models, the static component can be regarded as weak prior knowledge of the domain in new project settings, and the dynamic component acts as an incremental learner, which adapts as the project is being developed. Although the dynamic models performed noticeably better on our dataset, one potential
TABLE III.
ONLINE MODELS
Model
Coefficients
PP
RNNs-(300, 20)-5RNNd-(300, 20)-5RNNc-(300, 20)-5
RNNs-(400, 5)-1RNNd-(400, 5)-1RNNc-(400, 5)-1
3.7480 benefit of sacrificing some of the performance gain by using a committee is that static models can serve as anchors and help prevent the model from being "poisoned," i.e., degenerated by learning unreliable information online.
V.
CASE STUDY: CODE SUGGESTION
In Sec. IV, our intrinsic evaluation compared the quality of deep software language models to state-of-the-practice models.
In this section, we conduct an extrinsic evaluation to measure the performance of deep software language models at a real SE task, code suggestion, and we show that deep learning improves the performance at an SE task based on software language models. A code suggestion engine recommends the next token given the context,. The goal of the study was to measure the accuracy of deep software language models for code suggestion with the purpose of providing better tools and automated techniques to aid software development and maintenance. The context of the study consisted of the same corpora listed in Tab. I. The quality focus concerned code suggestion of tokens in the testing corpus. We examined the following research questions:
RQ1
Do deep learning models (Sec. III) significantly outperform state-of-the-practice models (Sec. II) at code suggestion on our dataset?
RQ2
Are there any distinguishing characteristics of the test documents on which the deep learning models achieve considerably better performance as compared to state-of-the-practice models?
RQ1. We used Top-k accuracy to compare deep software language models to state-of-the-practice models at code suggestion. Top-k accuracy has been used in previous code suggestion studies,. Tab. IV lists our Top-k results, where k = 1, 5, 10, for the most performant static and dynamic models of each model type. The deep learning models appear to outperform the n-grams at each level, so we designed comparative experiments to measure the statistical significance of our results. The treatments in our experimental design were the language models. The experimental units were the sentences in the test corpus, and the responses were the Topk scores. The null hypothesis stated there was no difference in performance. The (two-tailed) research hypothesis stated there was a difference in performance. We tested these hypotheses at α = 0.05 using the Wilcoxon test, a nonparameteric test, to determine whether the reported differences were statistically significant. Comparing the best deep software language model (RNNd-(300, 20)-5) to the best n-gram model(interpolated 8-gram), we found p ≤ 2.2 × 10−16 < 0.05 = α in all three cases (i.e., Top-1, Top-5, and Top-10); therefore, we rejected the null hypothesis, suggesting that a statistically
TABLE IV.
TOP-K ACCURACY (%)
Model
Top-1
Top-5
Top-10
Interpolated 8-gram
Interpolated 8-gram 100-cache
RNNs-(400, 5)-1
RNNd-(300, 20)-5
92.0 significant difference existed. We interpreted the difference as deep learning realizing an improvement at the code suggestion task. Regarding the effect size (Cliff's δ), we observed a medium effect size for Top-1, a large effect size for Top-5, and a medium effect size for Top-10.
Deep learning significantly outperformed n-grams at code suggestion on our dataset.
RQ2. After assessing the significance of applying deep learning to a real SE task, we conducted an exploratory study on the performance results. We began by sorting all the sentences in the test corpus by their Top-10% (according to the n-gram), i.e., the ratio of the number of tokens (including
</s>) in the sentence suggested in the Top-10 divided by the total number of tokens in the sentence. We observed that the sentences at the top of the list with low Top-10 scores were relatively short in length. Some of these sentences only comprised annotations (e.g., @Before and @Test) and others only comprised keywords (e.g., else, try, and finally). Given the poor performance of n-grams on these test documents, we were interested in comparing the performance of deep software language models on these sentences. We designed another set of experiments to compare the performance of the two models; however, in these experiments, the experimental units were sentences of length one, two, or three, respectively. Each experiment compared the models' Top-k performances at each sentence length. The null hypothesis for each comparative experiment stated there was no difference in performance. The(two-tailed) research hypothesis stated there was a difference in performance. We tested these hypotheses as above. All comparisons yielded statistically significant differences, where p ≤ 2.2 × 10−16 < 0.05 = α; therefore, we rejected the null hypothesis (for each comparison) and interpreted the difference as improved performance. Regarding the effect size (Cliff's δ), we only found large effect sizes for Top-5, where the sentence length was equal to two, and for Top-10, where the sentence length was equal to two or three.
Our results show that deep learning improves the performance at a SE task based on software language models. Moreover, there may be interesting cases in software corpora where deep learning outperforms models like n-grams. Sentences of length one or two tokens are arguably more germane to software corpora than natural language corpora.
VI.
THREATS TO VALIDITY
Threats to construct validity concern the relationship between theory and observation and relate to possible measurement imprecisions when extracting data used in a study. In mining the Git repositories and collecting the projects for our analysis, we relied on both the GitHub API and the git utility.
These tools are under active development with a community supporting them. Additionally, the GitHub API is the primary interface to extract project information. We cannot exclude imprecisions due to the implementation of such an API.
Threats to internal validity can be related to confounding factors internal to a study that could have affected the results.
In our study, we relied on RNNLM to train and evaluate deep software language models. While RNNLM is a reliable implementation that has been used in a number of NLP experiments,,, it is still an evolving project.
However, our results and trends are in line with those that have been obtained in the field of NLP; thus, we are confident that the results are reliable.
Threats to external validity represent the ability to generalize the observations in a study. We do not claim that the obtained results can be observed across other repositories or projects, especially projects written in other programming languages. Additionally, our dataset is representative of only repositories hosted on GitHub, so we do not claim that the results generalize to all Java projects. GitHub's exponential growth and popularity as a public forge indicates that it represents a large portion of the open source community. While
GitHub contains a large number of repositories, it may not necessarily be a comprehensive set of all open source projects or even all Java projects. However, we analyzed the diversity of the projects from the proposed metrics in Nagappan et al. and compared our dataset to the projects available on
Boa and found 1,556 projects out of the 16,221 projects.
We also analyzed the diversity of the 1,030 tokenized projects in our training, development, and test corpora, and we were able to match 128 projects. Our entire dataset had a diversity score of 0.3455, and the subset that we used to conduct our language modeling experiments had a diversity score of 0.2208. According to our dimensions, these values suggest that approximately 10% of our entire dataset covers one-third of the open source projects, and approximately 10% of our corpus covers one-fifth of open source projects. In our diversity analysis, we considered six metrics: programming languages, developers, project age, number of committers, number of revisions, and number of programming languages. For the entire dataset, we had scores of 0.45, 0.99, 1.00, 0.99, 0.96, and 0.99, respectively. For the study corpora, we had scores of 0.38, 0.98, 1.00, 0.98, 0.92, and 1.00, respectively. These results indicate that both our dataset and our corpora have high-dimensional diversity coverage for the relevant dimensions to our study.
Since we consider only Java projects, it is expected that our representativeness would be rather low in the programming languages dimension. Thus, our results are representative of a proportion of the open source community; in particular, we have high coverage within our key dimensions. Further evaluation of projects across other open source repositories and other programming languages would be necessary to validate our observations in a more general context. It is also important to note that we only consider open source projects.
VII.
AVENUES FOR FUTURE WORK
There are two principal research components in our future work on deep software language modeling and, generally, using deep learning to mine sequential SE data. One research component examines extensions of the models. One set of extensions involves search problems, such as hyperparameter optimization, designed to improve deep learning-based approaches (e.g., our deep software language models) for mining sequential SE data. Another set of extensions involves entirely new architectures and models, such as stacked RNNs and recursive neural networks, for mining sequential SE data. The other research component examines applications of deep architectures to SE tasks. We present three of these applications, informally organized according to features—from(concrete) token types to (abstract) conceptualizations. The first application, model-based testing, is an example of an SE task that can benefit from our deep software language models, where the raw features do not have to be words per se. As we noted in Sec. I, the nature of the terms depends on the domain, and this application supports that claim. The next application, software lexicon, shows that deep software language models are not simply useful for their high-quality output. We can also use their internal representations and feature detectors to support SE tasks. Thus, the same model that serves as a code suggestion engine can also be used to improve the software lexicon. Finally, while RNNs are deep in time, our last application, conceptualizing software artifacts, suggests that deep learning can be used to learn hierarchies of features to gradually abstract SE artifacts from token streams to useful concepts to support software maintenance and evolution.
Hyperparameter Optimization. Deep architectures comprise multiple levels of nonlinear transformations. Different subspaces in a deep architecture are trained as information flows forward and supervision propagates back through the network, but models like RNNs entail a considerable number of hyperparameters for governing different facets of the architecture, e.g., the size of the hidden layer, the number of levels before truncating the back-propagation through time algorithm, the number of classes for partitioning token types in the output layer, the learning rate, and the amount of regularization. We believe this translates to an expansive design space with new and important search problems for SE research to optimize these complex architectures over SE datasets for SE tasks.
SE research has examined similar problems in different contexts. Additionally, recent research in the machine learning community has proposed methodologies for automatically configuring the optimal set of hyperparameters,, but these approaches have not been measured using SE datasets in the context of SE tasks.
Model-based Testing. Tonella et al. demonstrated how interpolating a spectrum of low-order Markov models inferred from an event log can be used to improve code coverage, since
"backing off" increases the likelihood of deriving feasible test cases. Conceptually, the work by Tonella et al. uses "naturalness" (Sec. II) at different scales to improve code coverage, but we envision much more opportunity in model-based testing by exploiting the posterior distribution in the output layer of a deep software language model. While the posterior can specify natural event sequences, we can also infer unnatural event sequences from this model. Our work will segment the posterior's domain into a natural space and an unnatural space. In this sense, we propose a novel interpretation of deep software language models as natural bits to support other aspects of software testing, e.g., destructive testing and regression testing. Moreover, the natural space in this model is w z1... zn y w(1) w(2) w(3)
· · · w(t)
· · ·
Fig. 5.
STACKED RNN UNFOLDED IN TIME. While RNNs are deep in time, stacking RNNs yield architectures that are deep in both time and space. The depth in space (e.g., red nodes at time t) enables hierarchical processing which potentially learns information across multiple time scales,. not fixed. We envision a framework for adapting this space by dynamically toggling event sequences as natural or unnatural depending on the evidence to steer the model online according to specific objectives (other than code coverage).
Software Lexicon. Software maintenance is hard. Since program comprehension is one contributing factor, improving the software lexicon is one way to support critical maintenance tasks. We envision a novel lexicon broker to negotiate commits with the expressed goal of supporting program comprehension by consolidating concepts and, to this end, serving as a recommendation engine in cases where developers' implementations can be improved. How can we enable this broker?
While a deep software language model can effectively support many different SE tasks, the architecture's components may be used for other pertinent SE tasks such as learning software synonyms in massive repositories –. Recall the deep software language model embeds a token vector in a lowdimensional subspace using a linear projection a (Sec. III).
This is perhaps best understood by thinking of the vector ajiwi as a linear combination of the columns of a, i.e., ajiwi = �n j=1 wja·j = a·N ∈ Rm×1 where 1 ≤ N ≤ n, m = |z|, and the last equality is because w is one-hot encoded.
So, each column in a represents one token in the vocabulary.
These are contextualized feature vectors which can leverage intelligent recommendations on improving the lexicon. After conducting our empirical validation (Sec. IV) and observing how well the deep software language models performed on our dataset, we conducted a cursory study of the models' internal representations. The purpose of this small exploratory study was to begin to understand how these models can be analyzed to address other SE concerns, e.g., token similarity. We extracted the token embeddings a·j ∈ R300×1 from our RNNs(300, 20)-1 model (Sec. IV). Tab. V lists the two closest tokens, using Euclidean distance, for three distinct queries. Of the 71,293 token types in the vocabulary (Sec. IV), the two closest tokens to getX were two other getter methods that appear to be related to position or size. Again, of the 71,293 token types, the closest tokens to transient were two other Java keywords.
Finally, the two closest tokens to @BeforeClass were two other
Java annotations. While these are intriguing anecdotes, this is very preliminary work in this space, but we believe these
TABLE V.
QUERYING SIMILAR TOKENS USING TOKEN EMBEDDINGS
Query
Closest Tokens getX getY, getWidth transient native, volatile
@BeforeClass
@AfterClass, @Extension cursory observations warrant a deeper and much more rigorous examination in different SE contexts. Finally, it is important to note that architectures, e.g. stacked RNNs (Fig. 5), that are deep in both time and space, provide more opportunity for using components for other SE tasks.
Language Models for Software Evolution. Sutskever et al. trained a type of RNN for character-level language modeling and found the model learned to balance parentheses and quotes over distances as long as 30 characters. They note that a character-level n-gram language model could only do this by modeling 31-grams. We believe this has some particularly important implications for modeling software corpora because of software concerns like scope and encapsulation. If a software language model is capable of balancing { and } in source code, then perhaps deep architectures can be designed with enough capacity to reliably predict the next word in a sequence "in time," yet—at a higher level of abstraction— the architecture is capable of representing invariant features of the evolution of the software system "in space." We see these feature hierarchies as gateways to entirely novel methods for classifying software systems in many different software maintenance and evolution contexts.
VIII.
CONCLUSION
State-of-the-practice software language models are bound to the n-gram features that are apparent by simply scanning a corpus and aggregating counts of specific and discrete token sequences (Sec. II). On the other hand, deep learning uses expressive, continuous-valued representations that are capable of learning more robust models (Sec. III). We propose that SE research, with a wealth of unstructured data, is a unique opportunity to employ these state-of-the-art approaches. By empirically demonstrating that a relatively simple RNN configuration can outperform n-grams and cache-based n-grams with respect to PP on a Java corpus, deep software language models are shown to be high-quality software language models (Sec. IV), capable of showing great promise in SE applications.
We also demonstrate an improvement in performance at an
SE task (Sec. V). Finally, we identify avenues for future work using deep software language models to conduct modelbased testing, improve software lexicons, and support software maintenance and evolution (Sec. VII). Computer vision, speech recognition, and other fields have occupied the attention of the approaches we propose in this paper. Our work is one step— the first step—toward deep learning software repositories.
ACKNOWLEDGMENT
We would like to thank Abram Hindle from the University of Alberta for sharing his tools and data from his empirical study. This work is supported in part by the NSF CCF1218129 and NSF-1253837 grants. Any opinions, findings, and conclusions expressed herein are the authors' and do not necessarily reflect those of the sponsors.
REFERENCES
 
F. Jelinek, Statistical Methods for Speech Recognition.
Cambridge, MA, USA: MIT Press, 1997.
 
P. Koehn, Statistical Machine Translation, 1st ed.
New York, NY, USA: Cambridge University Press, 2010.
 
J. Goodman, "Classes for fast maximum entropy training," CoRR, vol. cs.CL/0108006, 2001.
 
D. Jurafsky and J. H. Martin, Speech and Language Processing, 2nd ed.
Upper Saddle River, NJ, USA: Prentice-Hall, Inc., 2009.
 
A. Hindle, E. T. Barr, Z. Su, M. Gabel, and P. Devanbu, "On the naturalness of software," in Proceedings of the 34th International
Conference on Software Engineering, ser. ICSE '12.
Piscataway, NJ, USA: IEEE Press, 2012, pp. 837–847.
 
T. T. Nguyen, A. T. Nguyen, H. A. Nguyen, and T. N. Nguyen, "A statistical semantic language model for source code," in Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering, ser. ESEC/FSE '13.
New York, NY, USA: ACM, 2013, pp. 532–542.
 
S. Afshan, P. McMinn, and M. Stevenson, "Evolving readable string test inputs using a natural language model to reduce human oracle cost," in Proceedings of the 2013 IEEE Sixth International Conference on Software Testing, Verification and Validation, ser. ICST '13.
Washington, DC, USA: IEEE Computer Society, 2013, pp. 352–361.
 
D. Movshovitz-Attias and W. W. Cohen, "Natural language models for predicting programming comments," in ACL.
Sofia, Bulgaria:
Association for Computational Linguistics, August 2013.
 
M. Allamanis and C. A. Sutton, "Mining source code repositories at massive scale using language modeling," in MSR, 2013, pp. 207–216.
 
J. C. Campbell, A. Hindle, and J. N. Amaral, "Syntax errors just aren't natural: Improving error reporting with language models," in Proceedings of the 11th Working Conference on Mining Software
Repositories, ser. MSR '14.
New York, NY, USA: ACM, 2014, pp.
252–261.
 
P. Tonella, R. Tiella, and D. C. Nguyen, "Interpolated n-grams for model based testing," in ICSE, 2014, pp. 562–572.
 
Z. Tu, Z. Su, and P. Devanbu, "On the localness of software," in Proceedings of the 22nd ACM SIGSOFT International Symposium on
Foundations of Software Engineering, ser. FSE '14.
New York, NY, USA: ACM, 2014, pp. 269–280.
 
M. Allamanis, E. T. Barr, C. Bird, and C. Sutton, "Learning natural coding conventions," in Proceedings of the 22nd ACM SIGSOFT
International Symposium on Foundations of Software Engineering, ser.
FSE '14.
New York, NY, USA: ACM, 2014, pp. 281–293.
 
R. Rosenfeld, "Two decades of statistical language modeling: Where do we go from here?" in Proceedings of the IEEE, vol. 88, 2000, pp.
1270–1278.
 
A. Mnih and Y. W. Teh, "A fast and simple algorithm for training neural probabilistic language models," in Proceedings of the 29th International
Conference on Machine Learning, 2012, pp. 1751–1758.
 
Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin, "A neural probabilistic language model," J. Mach. Learn. Res., vol. 3, pp. 1137–1155, Mar. 2003.
 
F. Morin and Y. Bengio, "Hierarchical probabilistic neural network language model," in Proceedings of the Tenth International Workshop on
Artificial Intelligence and Statistics.
Society for Artificial Intelligence and Statistics, 2005, pp. 246–252.
 
H. Schwenk and J.-L. Gauvain, "Training neural network language models on very large corpora," in Proceedings of the Conference on
Human Language Technology and Empirical Methods in Natural Language Processing, ser. HLT '05.
Stroudsburg, PA, USA: Association for Computational Linguistics, 2005, pp. 201–208.
 
A. T. Nguyen, T. T. Nguyen, and T. N. Nguyen, "Lexical statistical machine translation for language migration," in Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering, ser.
ESEC/FSE '13.
New York, NY, USA: ACM, 2013, pp. 651–654.
 
——, "Migrating code with statistical machine translation," in Companion Proceedings of the 36th International Conference on Software
Engineering, ser. ICSE Companion '14.
New York, NY, USA: ACM, 2014, pp. 544–547.
 
A. T. Nguyen, H. A. Nguyen, T. T. Nguyen, and T. N. Nguyen, "Statistical learning approach for mining API usage mappings for code migration," in Proceedings of the 29th ACM/IEEE International
Conference on Automated Software Engineering, ser. ASE '14.
New
York, NY, USA: ACM, 2014, pp. 457–468.
 
S. F. Chen and J. Goodman, "An empirical study of smoothing techniques for language modeling," in Proceedings of the 34th Annual
Meeting on Association for Computational Linguistics, ser. ACL '96.
Stroudsburg, PA, USA: Association for Computational Linguistics, 1996, pp. 310–318.
 
I. Sutskever, J. Martens, and G. Hinton, "Generating text with recurrent neural networks," in Proceedings of the 28th International Conference on Machine Learning (ICML-11), ser. ICML '11.
New York, NY, USA: ACM, June 2011, pp. 1017–1024.
 
E. Arisoy, T. N. Sainath, B. Kingsbury, and B. Ramabhadran, "Deep neural network language models," in Proceedings of the NAACL-HLT
2012 Workshop: Will We Ever Really Replace the N-gram Model?
On the Future of Language Modeling for HLT.
Montr´eal, Canada:
Association for Computational Linguistics, June 2012, pp. 20–28.
 
T. Mikolov, "Statistical language models based on neural networks,"
Ph.D. dissertation, 2012.
 
Y. Bengio, "Learning deep architectures for AI," Found. Trends Mach.
Learn., vol. 2, no. 1, pp. 1–127, Jan. 2009.
 
T. Mikolov, W. tau Yih, and G. Zweig, "Linguistic regularities in continuous space word representations." in HLT-NAACL.
The Association for Computational Linguistics, 2013, pp. 746–751.
 
R. Rosenfeld, "A maximum entropy approach to adaptive statistical language modeling," Computer, Speech and Language, vol. 10, pp. 187–
 
Y. Bengio, "Deep learning of representations: Looking forward," in Proceedings of the First International Conference on Statistical Language and Speech Processing, ser. SLSP '13.
Berlin, Heidelberg: SpringerVerlag, 2013, pp. 1–37.
 
B.-J. P. Hsu, "Language modeling for limited-data domains," Ph.D. dissertation, Cambridge, MA, USA, 2009.
 
R. Kuhn and R. De Mori, "A cache-based natural language model for speech recognition," IEEE Trans. Pattern Anal. Mach. Intell., vol. 12, no. 6, pp. 570–583, Jun. 1990.
 
P. Clarkson and A. Robinson, "Language model adaptation using mixtures and an exponentially decaying cache," in In Proceedings of ICASSP-97, 1997, pp. 799–802.
 
G. E. Hinton, "Connectionist learning procedures," Artif. Intell., vol. 40, no. 1-3, pp. 185–234, 1989.
 
C. M. Bishop and J. Lasserre, "Generative or discrimative? Getting the best of both worlds," in Bayesian Statistics 8, International Society for
Bayesian Analysis.
Oxford University Pres, 2007, pp. 3–24.
 
D. E. Rumelhart, G. E. Hinton, and R. J. Williams, "Neurocomputing:
Foundations of research."
Cambridge, MA, USA: MIT Press, 1988, ch. Learning Representations by Back-propagating Errors, pp. 696–699.
 
G. E. Hinton, "Learning distributed representations of concepts," in Proceedings of the Eighth Annual Conference of the Cognitive Science
Society.
Hillsdale, NJ: Erlbaum, 1986, pp. 1–12.
 
G. E. Hinton, J. L. McClelland, and D. E. Rumelhart, "Distributed representations," in Parallel Distributed Processing. Volume 1: Foundations.
Cambridge, MA: MIT Press, 1986, ch. 3, pp. 77–109.
 
N. K. Sinha and M. M. Gupta, Soft Computing and Intelligent Systems:
Theory and Applications, 1st ed.
San Francisco, CA, USA: Morgan
Kaufmann Publishers Inc., 1999.
 
I. Sutskever, J. Martens, G. E. Dahl, and G. E. Hinton, "On the importance of initialization and momentum in deep learning," in Proceedings of the 30th International Conference on Machine Learning (ICML-13), vol. 28, no. 3.
JMLR Workshop and Conference Proceedings, May
2013, pp. 1139–1147.
 
M. Hermans and B. Schrauwen, "Training and analysing deep recurrent neural networks," in Advances in Neural Information Processing
Systems 26.
Curran Associates, Inc., 2013, pp. 190–198.
 
R. Pascanu, C¸. G¨ulc¸ehre, K. Cho, and Y. Bengio, "How to construct deep recurrent neural networks," CoRR, vol. abs/1312.6026, 2013.
 
O. ˙Irsoy and C. Cardie, "Opinion mining with deep recurrent neural networks," in Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2014, pp. 720–728.
 
J. L. Elman, "Finding structure in time," COGNITIVE SCIENCE, vol. 14, no. 2, pp. 179–211, 1990.
 
R. Miikkulainen and M. G. Dyer, "Natural language processing with modular neural networks and distributed lexicon," Cognitive Science, vol. 15, pp. 343–399, 1991.
 
M. I. Jordan, "Serial order: A parallel distributed processing approach,"
Institute for Cognitive Science, University of California, San Diego, Tech. Rep. ICS Report 8604, 1986.
 
T. Mikolov, S. Kombrink, A. Deoras, L. Burget, and J. Cernock´y, "RNNLM - Recurrent neural network language modeling toolkit," in Proceedings of ASRU 2011.
IEEE Signal Processing Society, 2011, pp. 1–4.
 
T. Mikolov, A. Deoras, D. Povey, L. Burget, and J. Cernock´y, "Strategies for training large scale neural network language models," in ASRU, 2011, pp. 196–201.
 
V. Raychev, M. Vechev, and E. Yahav, "Code completion with statistical language models," in Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation, ser. PLDI
'14.
New York, NY, USA: ACM, 2014, pp. 419–428.
 
T. Mikolov, S. Kombrink, L. Burget, J. Cernock´y, and S. Khudanpur, "Extensions of recurrent neural network language model," in Proceedings of the 2011 IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2011. IEEE Signal Processing Society, 2011, pp. 5528–5531.
 
Y. Shi, W. Zhang, J. Liu, and M. T. Johnson, "RNN language model with word clustering and class-based output layer," EURASIP J. Audio, Speech and Music Processing, vol. 2013, p. 22, 2013.
 
Y. Bengio, "Practical recommendations for gradient-based training of deep architectures," CoRR, vol. abs/1206.5533, 2012.
 
Y. Bengio, A. C. Courville, and P. Vincent, "Unsupervised feature learning and deep learning: A review and new perspectives," CoRR, vol. abs/1206.5538, 2012.
 
X. Glorot, A. Bordes, and Y. Bengio, "Deep sparse rectifier neural networks," in JMLR W&CP: Proceedings of the Fourteenth International
Conference on Artificial Intelligence and Statistics (AISTATS 2011), Apr. 2011.
 
Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel, "Backpropagation applied to handwritten zip code recognition," Neural Comput., vol. 1, no. 4, pp. 541–551, Dec.
 
Y. Bengio, P. Simard, and P. Frasconi, "Learning long-term dependencies with gradient descent is difficult," Trans. Neur. Netw., vol. 5, no. 2, pp. 157–166, Mar. 1994.
 
P. Werbos, "Backpropagation through time: what does it do and how to do it," in Proceedings of IEEE, vol. 78, no. 10, 1990, pp. 1550–1560.
 
J. Schmidhuber, "Learning complex, extended sequences using the principle of history compression," Neural Comput., vol. 4, no. 2, pp.
234–242, Mar. 1992.
 
R. Pascanu, T. Mikolov, and Y. Bengio, "On the difficulty of training recurrent neural networks," in Proceedings of the 30th International
Conference on Machine Learning, ICML 2013, Atlanta, GA, USA, 1621 June 2013, 2013, pp. 1310–1318.
 
A. Mnih and G. Hinton, "Three new graphical models for statistical language modelling," in Proceedings of the 24th International Conference on Machine Learning, ser. ICML '07.
New York, NY, USA:
ACM, 2007, pp. 641–648.
 (2014) JFlex. [Online]. Available: http://jflex.de/
 
S. M. Katz, "Estimation of probabilities from sparse data for the language model component of a speech recognizer," in IEEE Transactions on Acoustics, Speech and Signal Processing, 1987, pp. 400–401.
 
F. Jelinek and R. L. Mercer, "Interpolated estimation of markov source parameters from sparse data," in In Proceedings of the Workshop on
Pattern Recognition in Practice, May 1980, pp. 381–397.
 
A. Stolcke, "SRILM - an extensible language modeling toolkit." in INTERSPEECH.
ISCA, 2002.
 
C. M. Bishop, Pattern Recognition and Machine Learning (Information
Science and Statistics).
Secaucus, NJ, USA: Springer-Verlag New
York, Inc., 2006.
 
D. Sheskin, Handbook of Parametric and Nonparametric Statistical
Procedures., 2nd ed.
Chapman & Hall/CRC, 2000.
 
M. Nagappan, T. Zimmermann, and C. Bird, "Diversity in software engineering research," ser. ESEC/FSE '13, 2013, pp. 466–476.
 
R. Dyer, H. A. Nguyen, H. Rajan, and T. N. Nguyen, "Boa: A language and infrastructure for analyzing ultra-large-scale software repositories," ser. ICSE '13, 2013, pp. 422–431.
 
R. Socher, C. C.-Y. Lin, A. Y. Ng, and C. D. Manning, "Parsing natural scenes and natural language with recursive neural networks." in ICML.
Omnipress, 2011, pp. 129–136.
 
A. Panichella, B. Dit, R. Oliveto, M. Di Penta, D. Poshyvanyk, and A. De Lucia, "How to effectively use topic models for software engineering tasks? An approach based on genetic algorithms," in Proceedings of the 2013 International Conference on Software Engineering, ser. ICSE '13.
Piscataway, NJ, USA: IEEE Press, 2013, pp. 522–531.
 
J. Bergstra and Y. Bengio, "Random search for hyper-parameter optimization," J. Mach. Learn. Res., vol. 13, no. 1, pp. 281–305, Feb.
 
J. Snoek, H. Larochelle, and R. P. Adams, "Practical bayesian optimization of machine learning algorithms," in Advances in Neural
Information Processing Systems 25, F. Pereira, C. Burges, L. Bottou, and K. Weinberger, Eds. Curran Associates, Inc., 2012, pp. 2951–2959.
 
J. Yang and L. Tan, "Inferring semantically related words from software context," in Proceedings of the 9th IEEE Working Conference on Mining
Software Repositories, ser. MSR '12.
Piscataway, NJ, USA: IEEE
Press, 2012, pp. 161–170.
 
M. J. Howard, S. Gupta, L. Pollock, and K. Vijay-Shanker, "Automatically mining software-based, semantically-similar words from comment-code mappings," in Proceedings of the 10th Working Conference on Mining Software Repositories, ser. MSR '13.
Piscataway, NJ, USA: IEEE Press, 2013, pp. 377–386.
 
Y. Tian, D. Lo, and J. L. Lawall, "Automated construction of a software-specific word similarity database," in 2014 Software Evolution
Week - IEEE Conference on Software Maintenance, Reengineering, and Reverse Engineering, CSMR-WCRE 2014, Antwerp, Belgium, February
3-6, 2014, 2014, pp. 44–53.
 
Y. Tian, D. Lo, and J. Lawall, "Sewordsim: Software-specific word similarity database," in Companion Proceedings of the 36th International
Conference on Software Engineering, ser. ICSE Companion '14.
New
York, NY, USA: ACM, 2014, pp. 568–571.
 
J. Yang and L. Tan, "Swordnet: Inferring semantically related words from software context," Empirical Softw. Engg., vol. 19, no. 6, pp. 1856–
1886, Dec. 2014.
 
L. N. Trefethen and D. Bau, Numerical Linear Algebra.
SIAM, 1997.Invariant Information Clustering for
Unsupervised Image Classification and Segmentation
Xu Ji
University of Oxford xuji@robots.ox.ac.uk
João F. Henriques
University of Oxford joao@robots.ox.ac.uk
Andrea Vedaldi
University of Oxford vedaldi@robots.ox.ac.uk
Abstract
We present a novel clustering objective that learns a neural network classifier from scratch, given only unlabelled data samples. The model discovers clusters that accurately match semantic classes, achieving state-of-the-art results in eight unsupervised clustering benchmarks spanning image classification and segmentation. These include STL10, an unsupervised variant of ImageNet, and CIFAR10, where we significantly beat the accuracy of our closest competitors by 6.6 and 9.5 absolute percentage points respectively.
The method is not specialised to computer vision and operates on any paired dataset samples; in our experiments we use random transforms to obtain a pair from each image. The trained network directly outputs semantic labels, rather than high dimensional representations that need external processing to be usable for semantic clustering. The objective is simply to maximise mutual information between the class assignments of each pair. It is easy to implement and rigorously grounded in information theory, meaning we effortlessly avoid degenerate solutions that other clustering methods are susceptible to. In addition to the fully unsupervised mode, we also test two semi-supervised settings.
The first achieves 88.8% accuracy on STL10 classification, setting a new global state-of-the-art over all existing methods (whether supervised, semi-supervised or unsupervised).
The second shows robustness to 90% reductions in label coverage, of relevance to applications that wish to make use of small amounts of labels. github.com/xu-ji/IIC
1. Introduction
Most supervised deep learning methods require large quantities of manually labelled data, limiting their applicability in many scenarios. This is true for large-scale image classification and even more for segmentation (pixelwise classification) where the annotation cost per image is very high.
Unsupervised clustering, on the other hand, aims to group data points into classes entirely
Figure 1:
Models trained with IIC on entirely unlabelled data learn to cluster images (top, STL10) and patches (bottom, Potsdam-3). The raw clusters found directly correspond to semantic classes (dogs, cats, trucks, roads, vegetation etc.) with state-of-the-art accuracy. Training is end-toend and randomly initialised, with no heuristics used at any stage. without labels.
Many authors have sought to combine mature clustering algorithms with deep learning, for example by bootstrapping network training with k-means style objectives.
However, trivially combining clustering and representation learning methods often leads to degenerate solutions.
It is precisely to prevent such degeneracy that cumbersome pipelines — involving pre-training, feature post-processing (whitening or PCA), clustering mechanisms external to the network — have evolved.
In this paper, we introduce Invariant Information Clustering (IIC), a method that addresses this issue in a more principled manner. IIC is a generic clustering algorithm that
9865 directly trains a randomly initialised neural network into a classification function, end-to-end and without any labels.
It involves a simple objective function, which is the mutual information between the function's classifications for paired data samples. The input data can be of any modality and, since the clustering space is discrete, mutual information can be computed exactly.
Despite its simplicity, IIC is intrinsically robust to two issues that affect other methods. The first is clustering degeneracy, which is the tendency for a single cluster to dominate the predictions or for clusters to disappear (which can be observed with k-means, especially when combined with representation learning ). Due to the entropy maximisation component within mutual information, the loss is not minimised if all images are assigned to the same class. At the same time, it is optimal for the model to predict for each image a single class with certainty (i.e. one-hot) due to the conditional entropy minimisation (fig. 3). The second issue is noisy data with unknown or distractor classes (present in STL10 for example). IIC addresses this issue by employing an auxiliary output layer that is parallel to the main output layer, trained to produce an overclustering (i.e. same loss function but greater number of clusters than the ground truth) that is ignored at test time. Auxiliary overclustering is a general technique that could be useful for other algorithms. These two features of IIC contribute to making it the only method amongst our unsupervised baselines that is robust enough to make use of the noisy unlabelled subset of STL10, a version of ImageNet specifically designed as a benchmark for unsupervised clustering.
In the rest of the paper, we begin by explaining the difference between semantic clustering and intermediate representation learning (section 2), which separates our method from the majority of work in unsupervised deep learning.
We then describe the theoretical foundations of IIC in statistical learning (section 3), demonstrating that maximising mutual information between pairs of samples under a bottleneck is a principled clustering objective which is equivalent to distilling their shared abstract content (co-clustering). We propose that for static images, an easy way to generate pairs with shared abstract content from unlabelled data is to take each image and its random transformation, or each patch and a neighbour. We show that maximising MI automatically avoids degenerate solutions and can be written as a convolution in the case of segmentation, allowing for efficient implementation with any deep learning library.
We perform experiments on a large number of datasets (section 4) including STL, CIFAR, MNIST, COCO-Stuff and Potsdam, setting a new state-of-the-art on unsupervised clustering and segmentation in all cases, with results of 59.6%, 61.7% and 72.3% on STL10, CIFAR10 and COCO-Stuff-3 beating the closest competitors (53.0%, 52.2%, 54.0%) with significant margins. Note that train𝐼(𝑧, 𝑧′)
CNN
CNN 𝑥 𝑥′ = 𝑔𝑥
𝑃(𝑧, 𝑧′)
Objective
Cluster probabilities
𝑃(𝑧|𝑥) 𝜎
FC 𝜎
FC
𝑃(𝑧′|𝑥′)
FC
FC 𝜎 𝜎
𝑃(𝑦, 𝑦′)
𝐼(𝑦, 𝑦′)
Optional overclustering 𝑔
Figure 2: IIC for image clustering. Dashed line denotes shared parameters, g is a random transformation, and I denotes mutual information (eq. (3)). ing deep neural networks to perform large scale, real-world segmentations from scratch, without labels or heuristics, is a highly challenging task with negligible precedent. We also perform an ablation study and additionally test two semisupervised modes, setting a new global state-of-the-art of 88.8% on STL10 over all supervised, semi-supervised and unsupervised methods, and demonstrating the robustness in semi-supervised accuracy when 90% of labels are removed.
2. Related work
Co-clustering and mutual information.
The use of information as a criterion to learn representations is not new.
One of the earliest works to do so is by Becker and Hinton. More generally, learning from paired data has been explored in co-clustering and in other works that build on the information bottleneck principle.
Several recent papers have used information as a tool to train deep networks in particular.
IMSAT maximises mutual information between data and its representation and DeepINFOMAX maximizes information between spatially-preserved features and compact features.
However, IMSAT and DeepINFOMAX combine information with other criteria, whereas in our method information is the only criterion used. Furthermore, both IMSAT and DeepINFOMAX compute mutual information over continuous random variables, which requires complex estimators, whereas IIC does so for discrete variables with simple and exact computations. Finally, DeepINFOMAX considers the information I(x, f(x)) between the features x and a deterministic function f(x) of it, which is in principle the same as the entropy H(x); in contrast, in IIC information does not trivially reduce to entropy.
Semantic clustering versus intermediate representation learning.
In semantic clustering, the learned function directly outputs discrete assignments for high level (i.e.
Figure 3: Training with IIC on unlabelled MNIST in successive epochs from random initialisation (left). The network directly outputs cluster assignment probabilities for input images, and each is rendered as a coordinate by convex combination of 10 cluster vertices. There is no cherry-picking as the entire dataset is shown in every snapshot. Ground truth labelling (unseen by model) is given by colour. At each cluster the average image of its assignees is shown.
With neither labels nor heuristics, the clusters discovered by IIC correspond perfectly to unique digits, with one-hot certain prediction (right). semantic) clusters.
Intermediate representation learners, on the other hand, produce continuous, distributed, highdimensional representations that must be post-processed, for example by k-means, to obtain the discrete lowcardinality assignments required for unsupervised semantic clustering. The latter includes objectives such as generative autoencoder image reconstruction, triplets and spatial-temporal order or context prediction, for example predicting patch proximity, solving jigsaw puzzles and inpainting.
Note it also includes a number of clustering methods (DeepCluster, exemplars ) where the clustering is only auxiliary; a clustering-style objective is used but does not produce groups with semantic correspondence. For example, DeepCluster is a state-of-the-art method for learning highlytransferable intermediate features using overclustering as a proxy task, but does not automatically find semantically meaningful clusters. As these methods use auxiliary objectives divorced from the semantic clustering objective, it is unsurprising that they perform worse than IIC (section 4), which directly optimises for it, training the network end-toend with the final clusterer implicitly wrapped inside.
Optimising image-to-image distance.
Many approaches to deep clustering, whether semantic or auxiliary, utilise a distance function between input images that approximates a given grouping criterion. Agglomerative clustering and partially ordered sets of HOG features have been used to group images, and exemplars define a group as a set of random transformations applied to a single image. Note the latter does not scale easily, in particular to image segmentation where a single 200 × 200 image would call for 40k classes. DAC, JULE, DeepCluster, ADC and DEC rely on the inherent visual consistency and disentangling properties of CNNs to produce cluster assignments, which are processed and reinforced in each iteration. The latter three are based on k-means style mechanisms to refine feature centroids, which is prone to degenerate solutions and thus needs explicit prevention mechanisms such as pre-training, cluster-reassignment or feature cleaning via PCA and whitening.
Invariance as a training objective.
Optimising for function outputs to be persistent through spatio-temporal or non-material distortion is an idea shared by IIC with several works, including exemplars, IMSAT, proximity prediction, the denoising objective of Tagger, temporal slowness constraints, and optimising for features to be invariant to local image transformations.
More broadly, the problem of modelling data transformation has received significant attention in deep learning, one example being the transforming autoencoder.
3. Method
First we introduce a generic objective, Invariant Information Clustering, which can be used to cluster any kind of unlabelled paired data by training a network to predict cluster identities (section 3.1). We then apply it to image clustering (section 3.2, fig. 2 and fig. 3) and segmentation (section 3.3), by generating the required paired data using random transformations and spatial proximity.
3.1. Invariant Information Clustering
Let x, x′ ∈ X be a paired data sample from a joint probability distribution P(x, x′). For example, x and x′ could be different images containing the same object. The goal of Invariant Information Clustering (IIC) is to learn a representation Φ : X → Y that preserves what is in common between x and x′ while discarding instance-specific details.
The former can be achieved by maximizing the mutual information between encoded variables: max
Φ
I(Φ(x), Φ(x′)), (1) which is equivalent to maximising the predictability of Φ(x) from Φ(x′) and vice versa.
An effect of equation eq. (1), in general, is to make representations of paired samples the same. However, it is not the same as merely minimising representation distance, as done for example in methods based on k-means : the presence of entropy within I allows us to avoid degeneracy, as discussed in detail below.
If Φ is a neural network with a small output capacity(often called a "bottleneck"), eq. (1) also has the effect of discarding instance-specific details from the data. Clustering imposes a natural bottleneck, since the representation
9867 space is Y = {1,..., C}, a finite set of class indices (as opposed to an infinite vector space). Without a bottleneck, i.e. assuming unbounded capacity, eq. (1) is trivially solved by setting Φ to the identity function because of the data processing inequality, i.e. I(x, x′) ≥ I(Φ(x), Φ(x′)).
Since our goal is to learn the representation with a deep neural network, we consider soft rather than hard clustering, meaning the neural network Φ is terminated by a (differentiable) softmax layer. Then the output Φ(x) ∈ C can be interpreted as the distribution of a discrete random variable z over C classes, formally given by P(z = c|x) =
Φc(x). Making the output probabilistic amounts to allowing for uncertainty in the cluster assigned to an input.
Consider now a pair of such cluster assignment variables z and z′ for two inputs x and x′ respectively. Their conditional joint distribution is given by P(z = c, z′ = c′|x, x′) = Φc(x) · Φc′(x′). This equation states that z and z′ are independent when conditioned on specific inputs x and x′; however, in general they are not independent after marginalization over a dataset of input pairs (xi, x′ i), i = 1,..., n. For example, for a trained classification network Φ and a dataset of image pairs where each image contains the same object of its pair but in a randomly different position, the random variable constituted by the class of the first of each pair, z, will have a strong statistical relationship with the random variable for the class of the second of each pair, z′; one is predictive of the other (in fact identical to it, in this case) so they are highly dependent. After marginalization over the dataset (or batch, in practice), the joint probability distribution is given by the C × C matrix
P, where each element at row c and column c′ constitutes
Pcc′ = P(z = c, z′ = c′):
P = 1 n n
� i=1
Φ(xi) · Φ(x′ i)⊤.
The marginals Pc = P(z = c) and Pc′ = P(z′ = c′) can be obtained by summing over the rows and columns of this matrix. As we generally consider symmetric problems, where for each (xi, x′ i) we also have (x′ i, xi), P is symmetrized using (P + P⊤)/2.
Now the objective function eq. (1) can be computed by plugging the matrix P into the expression for mutual information, which results in the formula:
I(z, z′) = I(P) =
C
� c=1
C
� c′=1
Pcc′ · ln
Pcc′
Pc · Pc′.
Why degenerate solutions are avoided.
Mutual information (3) expands to I(z, z′) = H(z) − H(z|z′). Hence, maximizing this quantity trades-off minimizing the conditional cluster assignment entropy H(z|z′) and maximising individual cluster assignments entropy H(z). The smallest value of H(z|z′) is 0, obtained when the cluster assignments are exactly predictable from each other.
The largest value of H(z) is ln C, obtained when all clusters are equally likely to be picked. This occurs when the data is assigned evenly between the clusters, equalizing their mass. Therefore the loss is not minimised if all samples are assigned to a single cluster (i.e. output class is identical for all samples). Thus as maximising mutual information naturally balances reinforcement of predictions with mass equalization, it avoids the tendency for degenerate solutions that algorithms which combine k-means with representation learning are susceptible to. For further discussion of entropy maximisation, and optionally how to prioritise it with an entropy coefficient, see supplementary material.
Meaning of mutual information.
The reader may now wonder what are the benefits of maximising mutual information, as opposed to merely maximising entropy. Firstly, due to the soft clustering, entropy alone could be maximised trivially by setting all prediction vectors Φ(x) to uniform distributions, resulting in no clustering. This is corrected by the conditional entropy component, which encourages deterministic one-hot predictions. For example, even for the degenerate case of identical pairs x = x′, the IIC objective encourages a deterministic clustering function (i.e. Φ(x) is a one-hot vector) as this results in null conditional entropy
H(z|z′) = 0. Secondly, the objective of IIC is to find what is common between two data points that share redundancy, such as different images of the same object, explicitly encouraging distillation of the common part while ignoring the rest, i.e. instance details specific to one of the samples.
This would not be possible without pairing samples.
3.2. Image clustering
IIC requires a source of paired samples (x, x′), which are often unavailable in unsupervised image clustering applications. In this case, we propose to use generated image pairs, consisting of image x and its randomly perturbed version x′ = gx. The objective eq. (1) can thus be written as: max
Φ
I(Φ(x), Φ(gx)), (4) where both image x and transformation g are random variables. Useful g could include scaling, skewing, rotation or flipping (geometric), changing contrast and colour saturation (photometric), or any other perturbation that is likely to leave the content of the image intact. IIC can then be used to recover the factor which is invariant to which of the pair is picked. The effect is to learn a function that partitions the data such that clusters are closed to the perturbations, without dropping clusters. The objective is simple enough to be written in six lines of PyTorch code (fig. 4).
Auxiliary overclustering.
For certain datasets(e.g.
STL10), training data comes in two types: one known to contain only relevant classes and the other known to contain irrelevant or distractor classes. It is desirable to train a 9868 def IIC(z, zt, C=10):
P = (z.unsqueeze(2) * zt.unsqueeze(1)).sum(dim=0)
P = ((P + P.t()) / 2) / P.sum()
P[(P < EPS).data] = EPS
Pi = P.sum(dim=1).view(C, 1).expand(C, C)
Pj = P.sum(dim=0).view(1, C).expand(C, C) return (P * (log(Pi) + log(Pj) - log(P))).sum()
Figure 4: IIC objective in PyTorch. Inputs z and zt are n × C matrices, with C predicted cluster probabilities for n sampled pairs (i.e. CNN softmaxed predictions). For example, the prediction for each image in a dataset and its transformed version (e.g. using standard data augmentation). clusterer specialised for the relevant classes, that still benefits from the context provided by the distractor classes, since the latter is often much larger (for example 100K compared to 13K for STL10). Our solution is to add an auxiliary overclustering head to the network (fig. 2) that is trained with the full dataset, whilst the main output head is trained with the subset containing only relevant classes. This allows us to make use of the noisy unlabelled subset despite being an unsupervised clustering method. Other methods are generally not robust enough to do so and thus avoid the 100k-samples unlabelled subset of STL10 when training for unsupervised clustering ( ). Since the auxiliary overclustering head outputs predictions over a larger number of clusters than the ground truth, whilst still maintaining a predictor that is matched to ground truth number of clusters (the main head), it can be useful in general for increasing expressivity in the learned feature representation, even for datasets where there are no distractor classes.
3.3. Image segmentation
IIC can be applied to image segmentation identically to image clustering, except for two modifications. Firstly, since predictions are made for each pixel densely, clustering is applied to image patches (defined by the receptive field of the neural network for each output pixel) rather than whole images. Secondly, unlike with whole images, one has access to the spatial relationships between patches. Thus, we can add local spatial invariance to the list of geometric and photometric invariances in section 3.2, meaning we form pairs of patches not only via synthetic perturbations, but also by extracting pairs of adjacent patches in the image.
In detail, let the RGB image x ∈ R3×H×W be a tensor, u ∈ Ω = {1,..., H} × {1,..., W} a pixel location, and xu a patch centered at u. We can form a pair of patches(xu, xu+t) by looking at location u and its neighbour u + t at some small displacement t ∈ T ⊂ Z2.
The cluster probability vectors for all patches xu can be read off as the column vectors Φ(xu) = Φu(x) ∈ C of the tensor
Φ(x) ∈ C×H×W, computed by a single application of the convolutional network Φ. Then, to apply IIC, one simply substitutes pairs (Φu(x), Φu+t(x)) in the calculation of the joint probability matrix (2).
The geometric and photometric perturbations used before for whole image clustering can be applied to individual patches too. Rather than transforming patches individually, however, it is much more efficient to transform all of them in parallel by perturbing the entire image. Any number or combination of these invariances can be chained and learned simultaneously; the only detail is to ensure indices of the original image and transformed image class probability tensors line up, meaning that predictions from patches which are intended to be paired together do so.
Formally, if the image transformation g is a geometric transformation, the vector of cluster probabilities Φu(x) will not correspond to Φu(gx); rather, it will correspond to Φg(u)(gx) because patch xu is sent to patch xg(u) by the transformation. All vectors can be paired at once by applying the reverse transformation g−1 to the tensor Φ(gx), as
[g−1Φ(gx)]u = Φg(u)(gx). For example, flipping the input image will require flipping the resulting probability tensor back. In general, the perturbation g can incorporate geometric and photometric transformations, and g−1 only needs to undo geometric ones. The segmentation objective is thus: max
Φ
|T|
� t∈T
I(Pt), Pt =
1 n|G||Ω| n
� i=1
� g∈G
Convolution
�
��
�
� u∈Ω
Φu(xi) · [g−1Φ(gxi)]⊤ u+t.
Hence the goal is to maximize the information between each patch label Φu(xi) and the patch label [g−1Φ(gxi)]u+t of its transformed neighbour patch, in expectation over images i = 1,..., n, patches u ∈ Ω within each image, and perturbations g ∈ G. Information is in turn averaged over all neighbour displacements t ∈ T (which was found to perform slightly better than averaging over t before computing information; see supplementary material).
Implementation.
The joint distribution of eq. (5) for all displacements t ∈ T can be computed in a simple and highly efficient way. Given two network outputs for one batch of image pairs y = Φ(x), y′ = Φ(gx) where y, y′ ∈
Rn×C×H×W, we first bring y′ back into the coordinatespace of y by using a bilinear resampler1, which inverts any geometrical transforms in g, y′ ← g−1y′. Then, the inner summation in eq. (5) reduces to the convolution of the two tensors. Using any standard deep learning framework, this can be achieved by swapping the first two dimensions of each of y and y′, computing P = y ∗ y′ (a 2D convolution with padding d in both dimensions), and normalising the result to produce P ∈ C×C×(2d+1)×(2d+1).
4. Experiments
We apply IIC to fully unsupervised image clustering and segmentation, as well as two semi-supervised settings. Ex1The core differentiable operator in spatial transformer networks.
STL10
CIFAR10 CFR100-20 MNIST
Random network
K-means †
Spectral clustering 
Triplets ‡
AE ‡
Sparse AE ‡
Denoising AE ‡
Variational Bayes AE ‡
SWWAE 2015 ‡
GAN 2015 ‡
JULE 2016 
DEC 2016 †
DAC 2017 
DeepCluster 2018 † ‡
33.4⋆
37.4⋆
18.9⋆
65.6 ⋆
ADC 2018 
16.0⋆
IIC (lowest loss sub-head)
IIC (avg sub-head ± STD)
± 0.844
± 5.01
± 0.462
± 0.652
Table 1: Unsupervised image clustering. Legend: †Method based on k-means. ‡Method that does not directly learn a clustering function and requires further application of k-means to be used for image clustering.
⋆Results obtained using our experiments with authors' original code.
STL10
No auxiliary overclustering
Single sub-head (h = 1)
No sample repeats (r = 1)
Unlabelled data segment ignored
Full setting
Table 2: Ablations of IIC (unsupervised setting). Each row shows a single change from the full setting. The full setting has auxiliary overclustering, 5 initialisation heads, 5 sample repeats, and uses the unlabelled data subset of STL10. isting baselines are outperformed in all cases. We also conduct an analysis of our method via ablation studies. For minor details see supplementary material.
4.1. Image clustering
Datasets.
We test on STL10, which is ImageNet adapted for unsupervised classification, as well as CIFAR10, CIFAR100-20 and MNIST. The main setting is pure unsupervised clustering (IIC) but we also test two semisupervised settings: finetuning and overclustering. For unsupervised clustering, following previous work, we train on the full dataset and test on the labelled part; for the semi-supervised settings, train and test sets are separate.
As for DeepCluster, we found Sobel filtering to be beneficial, as it discourages clustering based on trivial cues such as colour and encourages using more meaningful cues such as shape.
Additionally, for data augmentation, we repeat images within each batch r times; this means that multiple image pairs within a batch contain the same original image, each paired with a different transformation, which encourages greater distillation since there are more examples of which visual details to ignore (section 3.1). We set r ∈ for all experiments. Images are rescaled and cropped for training (prior to applying transforms g, consisting of random additive and multiplicative colour transformations and horizontal flipping) and a single center crop is used at test time for all experiments except semi-supervised finetuning, where 10 crops are used.
Architecture.
All networks are randomly initialised and consist of a ResNet or VGG11-like base b (see sup. mat.), followed by one or more heads (linear predictors). Let the number of ground truth clusters be kgt and the output channels of a head be k. For IIC, there is a main output head with k = kgt and an auxiliary overclustering head (fig. 2) with k > kgt. For semi-supervised overclustering there is one output head with k > kgt. For increased robustness, each head is duplicated h = 5 times with a different random initialisation, and we call these concrete instantiations subheads. Each sub-head takes features from b and outputs a probability distribution for each batch element over the relevant number of clusters. For semi-supervised finetuning (table 3), the base is copied from a semi-supervised overclustering network and combined with a single randomly initialised linear layer where k = kgt.
Training.
We use the Adam optimiser with learning rate 10−4. For IIC, the main and auxiliary heads are trained by maximising eq. (3) in alternate epochs.
For semi-supervised overclustering, the single head is trained by maximising eq. (3). Semi-supervised finetuning uses a standard logistic loss.
Evaluation.
We evaluate based on accuracy (true positives divided by sample size). For IIC we follow the standard protocol of finding the best one-to-one permutation mapping between learned and ground-truth clusters (from the main output head; auxiliary overclustering head is ignored) using linear assignment. While this step uses labels, it does not constitute learning as it merely makes the metric invariant to the order of the clusters. For semisupervised overclustering, each ground-truth cluster may correspond to the union of several predicted clusters. Evaluation thus requires a many-to-one discrete map from k to kgt, since k > kgt. This extracts some information from the labels and thus requires separated training and test set.
Note this mapping is found using the training set (accuracy is computed on the test set) and does not affect the network parameters as it is used for evaluation only. For semi-supervised finetuning, output channel order matches ground truth so no mapping is required.
Each sub-head is assessed independently; we report average and best subhead (as chosen by lowest IIC loss) performance.
Unsupervised learning analysis.
IIC is highly capable of discovering clusters in unlabelled data that accurately correspond to the underlying semantic classes, and outperforms all competing baselines at this task (table 1), with significant margins of 6.6% and 9.5% in the case of STL10 and CICat
Dog
Bird
Deer
Monkey
Car
Plane
Truck
Figure 5: Unsupervised image clustering (IIC) results on STL10. Predicted cluster probabilities from the best performing head are shown as bars.
Prediction corresponds to tallest, ground truth is green, incorrectly predicted classes are red, and all others are blue. The bottom row shows failure cases.
STL10
Dosovitskiy 2015 †
SWWAE 2015 †
Dundar 2015 
Cutout* 2017 
Oyallon* 2017 †
Oyallon* 2017 
DeepCluster 2018 
73.4⋆
ADC 2018 
56.7⋆
DeepINFOMAX 2018 
IIC plus finetune†
IIC plus finetune
Table 3: Fully and semi-supervised classification.
Legend:
*Fully supervised method. ⋆Our experiments with authors' code. †Multi-fold evaluation.
Figure 6: Semi-supervised overclustering. Training with IIC loss to overcluster (k > kgt) and using labels for evaluation mapping only. Performance is robust even with 90%-75% of labels discarded (left and center). STL10-r denotes networks with output k = ⌈1.4r⌉. Overall accuracy improves with the number of output clusters k (right). For further details see supplementary material.
FAR10. As mentioned in section 2, this underlines the advantages of end-to-end optimisation instead of using a fixed external procedure like k-means as with many baselines.
The clusters found by IIC are highly discriminative (fig. 5), although note some failure cases; as IIC distills purely visual correspondences within images, it can be confused by instances that combine classes, such as a deer with the coat pattern of a cat. Our ablations (table 2) illustrate the contributions of various implementation details, and in particular the accuracy gain from using auxiliary overclustering.
Semi-supervised learning analysis.
For semi-supervised learning, we establish a new state-of-the-art on STL10 out of all reported methods by finetuning a network trained in an entirely unsupervised fashion with the IIC objective(recall labels in semi-supervised overclustering are used for evaluation and do not influence the network parameters).
This explicitly validates the quality of our unsupervised learning method, as we beat even the supervised state-of-the-art (table 3). Given that the bulk of parameters within semi-supervised overclustering are trained unsupervised (i.e. all network parameters), it is unsurprising that Figure 6 shows a 90% drop in the number of available labels for STL10 (decreasing the amount of labelled data available from 5000 to 500 over 10 classes) barely impacts performance, costing just ∼10% drop in accuracy. This setting has lower label requirements than finetuning because whereas the latter learns all network parameters, the former only needs to learn a discrete map between k and kgt, making it an important practical setting for applications with small amounts of labelled data.
4.2. Segmentation
Datasets.
Large scale segmentation on real-world data using deep neural networks is extremely difficult without labels or heuristics, and has negligible precedent. We establish new baselines on scene and satellite images to highlight performance on textural classes, where the assumption of spatially proximal invariance (section 3.3) is most valid. COCO-Stuff is a challenging and diverse segmentation dataset containing "stuff" classes ranging from buildings to bodies of water. We use the 15 coarse labels and 164k images variant, reduced to 52k by taking only images with at least 75% stuff pixels. COCO-Stuff-3 is a subset of COCO-Stuff with only sky, ground and plants labelled. For both COCO datasets, input images are shrunk by two thirds and cropped to 128 × 128 pixels, Sobel preprocessing is applied for data augmentation, and predictions for non-stuff pixels are ignored. Potsdam is divided into 8550 RGBIR 200 × 200 px satellite images, of which
3150 are unlabelled. We test both the 6-label variant (roads and cars, vegetation and trees, buildings and clutter) and a 3-label variant (Potsdam-3) formed by merging each of the 3 pairs. All segmentation training and testing sets have been released with our code.
Figure 7: Example segmentation results (un- and semi-supervised). Left: COCO-Stuff-3 (non-stuff pixels in black), right: Potsdam-3. Input images, IIC(fully unsupervised segmentation) and IIC* (semi-supervised overclustering) results are shown, together with the ground truth segmentation (GT).
COCO-Stuff-3 COCO-Stuff Potsdam-3 Potsdam
Random CNN
K-means †
SIFT ‡
Doersch 2015 ‡
Isola 2016 ‡
DeepCluster 2018 † ‡
IIC
Table 4: Unsupervised segmentation. IIC experiments use a single subhead. Legend: †Method based on k-means. ‡Method that does not directly learn a clustering function and requires further application of k-means to be used for image clustering.
Architecture.
All networks are randomly initialised and consist of a base CNN b (see sup. mat.) followed by head(s), which are 1 × 1 convolution layers. Similar to section 4.1, overclustering uses k 3-5 times higher than kgt.
Since segmentation is much more expensive than image clustering (e.g. a single 200 × 200 Potsdam image contains
40,000 predictions), all segmentation experiments were run with h = 1 and r = 1 (sec. 4.1).
Training.
The convolutional implementation of IIC(eq. (5)) was used with d = 10. For Potsdam-3 and COCOStuff-3, the optional entropy coefficient (section 3.1 and sup. mat.) was used and set to 1.5. Using the coefficient made slight improvements of 1.2%-3.2% on performance.
These two datasets are balanced in nature with very large sample volume (e.g. 40, 000 × 75 predictions per batch for
Potsdam-3) resulting in stable and balanced batches, justifying prioritisation of equalisation. Other training details are the same as section 4.1.
Evaluation.
Evaluation uses accuracy as in section 4.1, computed per-pixel. For the baselines, the original authors' code was adapted from image clustering where available, and the architectures are shared with IIC for fairness. For baselines that required application of k-means to produce per-pixel predictions (table 4), k-means was trained with randomly sampled pixel features from the training set (10M for Potsdam, Potsdam-3; 50M for COCO-Stuff, COCOStuff-3) and tested on the full test set to obtain accuracy.
Analysis.
Without labels or heuristics to learn from, and given just the cluster cardinality (3), IIC automatically partitions COCO-Stuff-3 into clusters that are recognisable as sky, vegetation and ground, and learns to classify vegetation, roads and buildings for Potsdam-3 (fig. 7).
The segmentations are notably intricate, capturing fine detail, but are at the same time locally consistent and coherent across all images. Since spatial smoothness is built into the loss (section 3.3), all our results are able to use raw network outputs without post-processing (avoiding e.g. CRF smoothing ).
Quantitatively, we outperform all baselines (table 4), notably by 18.3% in the case of COCOStuff-3.
The efficient convolutional formulation of the loss (eq. (5)) allows us to optimise over all pixels in all batch images in parallel, converging in fewer epochs (passes of the dataset) without paying the price of reduced computational speed for dense sampling. This is in contrast to our baselines which, being not natively adapted for segmentation, required sampling a subset of pixels within each batch, resulting in increased loss volatility and training speeds that were up to 3.3× slower than IIC.
5. Conclusions
We have shown that it is possible to train neural networks into semantic clusterers without using labels or heuristics.
The novel objective presented relies on statistical learning, by optimising mutual information between related pairsa relationship that can be generated by random transforms
- and naturally avoids degenerate solutions. The resulting models classify and segment images with state-of-the-art levels of semantic accuracy. Being not specific to vision, the method opens up many interesting research directions, including optimising information in datastreams over time.
Acknowledgments.
We are grateful to ERC StG
IDIU-638009 and EPSRC
AIMS
CDT for support.
References
 Miguel A Bautista, Artsiom Sanakoyeu, and Bjorn Ommer.
Deep unsupervised similarity learning using partially ordered sets. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7130–7139, Miguel
A
Bautista, Artsiom
Sanakoyeu, Ekaterina
Tikhoncheva, and Bjorn Ommer.
Cliquecnn: Deep unsupervised exemplar learning.
In Advances in Neural
Information Processing Systems, pages 3846–3854, 2016. 3
 Suzanna Becker and Geoffrey E Hinton.
Self-organizing neural network that discovers surfaces in random-dot stereograms. Nature, 355(6356):161, 1992. 2
 Ishmael Belghazi, Sai Rajeswar, Aristide Baratin, R Devon
Hjelm, and Aaron Courville. Mine: mutual information neural estimation. arXiv preprint arXiv:1801.04062, 2018. 2
 Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo
Larochelle. Greedy layer-wise training of deep networks. In
Advances in neural information processing systems, pages
153–160, 2007. 6
 Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. Cocostuff: Thing and stuff classes in context. arXiv preprint arXiv:1612.03716, 2016. 7
 Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised learning of visual features. arXiv preprint arXiv:1807.05520, 2018.
 Jianlong Chang, Lingfeng Wang, Gaofeng Meng, Shiming
Xiang, and Chunhong Pan. Deep adaptive image clustering.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5879–5887, 2017. 3, 5, 6
 Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE transactions on pattern analysis and machine intelligence, 40(4):834–848, 2018. 8
 Adam Coates, Andrew Ng, and Honglak Lee.
An analysis of single-layer networks in unsupervised feature learning.
In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pages 215–223, 2011. 2
 Thomas M Cover and Joy A Thomas. Elements of information theory. John Wiley & Sons, 2012. 4
 Rodrigo Santa Cruz, Basura Fernando, Anoop Cherian, and Stephen Gould. Deeppermnet: Visual permutation learning. arXiv preprint arXiv:1704.02729, 2017. 3
 Navneet Dalal and Bill Triggs. Histograms of oriented gradients for human detection. In Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, volume 1, pages 886–893. IEEE, 2005.
 Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee, 2009. 2
 Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017. 7
 Inderjit S Dhillon, Subramanyam Mallela, and Dharmendra S Modha. Information-theoretic co-clustering. In Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 89–98.
ACM, 2003. 2
 Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation learning by context prediction. In
Proceedings of the IEEE International Conference on Computer Vision, pages 1422–1430, 2015. 1, 3, 8
 Alexey Dosovitskiy, Philipp Fischer, Jost Tobias Springenberg, Martin Riedmiller, and Thomas Brox. Discriminative unsupervised feature learning with exemplar convolutional neural networks. IEEE transactions on pattern analysis and machine intelligence, 38(9):1734–1747, 2015. 1, 3, 7
 Aysegul Dundar, Jonghoon Jin, and Eugenio Culurciello.
Convolutional clustering for unsupervised learning. arXiv preprint arXiv:1511.06241, 2015. 7
 Nir Friedman, Ori Mosenzon, Noam Slonim, and Naftali
Tishby. Multivariate information bottleneck. In Proceedings of the Seventeenth conference on Uncertainty in artificial intelligence, pages 152–161. Morgan Kaufmann Publishers Inc., 2001. 2
 Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra
Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages
580–587, 2014. 1
 Klaus Greff, Antti Rasmus, Mathias Berglund, Tele Hao, Harri Valpola, and Juergen Schmidhuber. Tagger: Deep unsupervised perceptual grouping. In Advances in Neural Information Processing Systems, pages 4484–4492, 2016. 3
 Klaus Greff, Rupesh Kumar Srivastava, and Jürgen Schmidhuber. Binding via reconstruction clustering. arXiv preprint arXiv:1511.06418, 2015. 3
 Philip Haeusser, Johannes Plapp, Vladimir Golkov, Elie Aljalbout, and Daniel Cremers. Associative deep clustering:
Training a classification network with no labels. In German
Conference on Pattern Recognition, pages 18–32. Springer, John A Hartigan. Direct clustering of a data matrix. Journal of the american statistical association, 67(337):123–129, Geoffrey E Hinton, Alex Krizhevsky, and Sida D Wang.
Transforming auto-encoders. In International Conference on
Artificial Neural Networks, pages 44–51. Springer, 2011. 3
 R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam Trischler, and Yoshua
Bengio.
Learning deep representations by mutual information estimation and maximization. arXiv preprint arXiv:1808.06670, 2018. 2, 7
 Weihua Hu, Takeru Miyato, Seiya Tokui, Eiichi Matsumoto, and Masashi Sugiyama. Learning discrete representations via information maximizing self-augmented training. arXiv preprint arXiv:1702.08720, 2017. 2, 3
 Ka Yu Hui. Direct modeling of complex invariances for visual object features. In International Conference on Machine
Learning, pages 352–360, 2013. 3
 Phillip Isola, Daniel Zoran, Dilip Krishnan, and Edward H
Adelson.
Learning visual groups from co-occurrences in space and time. arXiv preprint arXiv:1511.06811, 2015. 3, ISPRS.
ISPRS
2D
Semantic
Labeling
Contest. http://www2.isprs.org/commissions/comm3/ wg4/semantic-labeling.html.
[Online; accessed
10-October-2018]. 7
 Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al.
Spatial transformer networks. In Advances in neural information processing systems, pages 2017–2025, 2015. 5
 Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 6
 Harold W Kuhn. The hungarian method for the assignment problem. In 50 Years of Integer Programming 1958-2008, pages 29–47. Springer, 2010. 6
 Erik G Learned-Miller. Entropy and mutual information. 4
 Hsin-Ying Lee, Jia-Bin Huang, Maneesh Singh, and MingHsuan Yang. Unsupervised representation learning by sorting sequences. In 2017 IEEE International Conference on
Computer Vision (ICCV), pages 667–676. IEEE, 2017. 3
 Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence
Zitnick. Microsoft COCO: Common objects in context. In
Proc. ECCV, pages 740–755. Springer, 2014. 1
 David G Lowe.
Distinctive image features from scaleinvariant keypoints. International journal of computer vision, 60(2):91–110, 2004. 8
 Andrew Ng. Sparse autoencoder. CS294A Lecture notes, pages 1–19, 2011. 6
 Mehdi Noroozi and Paolo Favaro.
Unsupervised learning of visual representations by solving jigsaw puzzles.
In
European Conference on Computer Vision, pages 69–84.
Springer, 2016. 3
 Edouard
Oyallon, Eugene
Belilovsky, and Sergey
Zagoruyko.
Scaling the scattering transform:
Deep hybrid networks. In Proceedings of the IEEE international conference on computer vision, pages 5618–5627, 2017. 7
 Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor
Darrell, and Alexei A Efros.
Context encoders: Feature learning by inpainting.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages
2536–2544, 2016. 3
 Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu
Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-learn: Machine learning in python. Journal of machine learning research, 12(Oct):2825–2830, 2011. 8
 Alec Radford, Luke Metz, and Soumith Chintala.
Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015. 6
 Matthew Schultz and Thorsten Joachims. Learning a distance metric from relative comparisons. In Advances in neural information processing systems, pages 41–48, 2004. 3, Kihyuk Sohn and Honglak Lee.
Learning invariant representations with local transformations. arXiv preprint arXiv:1206.6418, 2012. 3
 Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua
Bengio, and Pierre-Antoine Manzagol. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. Journal of Machine
Learning Research, 11(Dec):3371–3408, 2010. 3, 6
 Jianfeng Wang, Jingdong Wang, Jingkuan Song, Xin-Shun
Xu, Heng Tao Shen, and Shipeng Li. Optimized cartesian k-means. IEEE Transactions on Knowledge & Data Engineering, (1):1–1, 2015. 6
 Pu Wang, Carlotta Domeniconi, and Kathryn Blackmond
Laskey. Information bottleneck co-clustering. Citeseer. 2
 Junyuan Xie, Ross Girshick, and Ali Farhadi. Unsupervised deep embedding for clustering analysis.
In International conference on machine learning, pages 478–487, 2016. 1, Jianwei Yang, Devi Parikh, and Dhruv Batra. Joint unsupervised learning of deep representations and image clusters.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5147–5156, 2016. 3, 6
 Lihi Zelnik-Manor and Pietro Perona. Self-tuning spectral clustering.
In Advances in neural information processing systems, pages 1601–1608, 2005. 6
 Junbo Zhao, Michael Mathieu, Ross Goroshin, and Yann
Lecun. Stacked what-where auto-encoders. arXiv preprint arXiv:1506.02351, 2015. 6, 7
 Will Zou, Shenghuo Zhu, Kai Yu, and Andrew Y Ng. Deep learning of invariant features via simulated fixations in video.
In Advances in neural information processing systems, pages
3203–3211, 2012. 3Generative Moment Matching Networks
Yujia Li1
YUJIALI@CS.TORONTO.EDU
Kevin Swersky1
KSWERSKY@CS.TORONTO.EDU
Richard Zemel1,2
ZEMEL@CS.TORONTO.EDU
1Department of Computer Science, University of Toronto, Toronto, ON, CANADA
2Canadian Institute for Advanced Research, Toronto, ON, CANADA
Abstract
We consider the problem of learning deep generative models from data. We formulate a method that generates an independent sample via a single feedforward pass through a multilayer perceptron, as in the recently proposed generative adversarial networks (Goodfellow et al., 2014).
Training a generative adversarial network, however, requires careful optimization of a difficult minimax program.
Instead, we utilize a technique from statistical hypothesis testing known as maximum mean discrepancy (MMD), which leads to a simple objective that can be interpreted as matching all orders of statistics between a dataset and samples from the model, and can be trained by backpropagation.
We further boost the performance of this approach by combining our generative network with an auto-encoder network, using MMD to learn to generate codes that can then be decoded to produce samples.
We show that the combination of these techniques yields excellent generative models compared to baseline approaches as measured on MNIST and the Toronto Face Database.
1. Introduction
The most visible successes in the area of deep learning have come from the application of deep models to supervised learning tasks. Models such as convolutional neural networks (CNNs), and long short term memory (LSTM) networks are now achieving impressive results on a number of tasks such as object recognition (Krizhevsky et al., 2012;
Sermanet et al., 2014; Szegedy et al., 2014), speech recognition (Graves & Jaitly, 2014; Hinton et al., 2012a), image caption generation (Vinyals et al., 2014; Fang et al., 2014;
Proceedings of the 32 nd International Conference on Machine
Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).
Kiros et al., 2014), machine translation (Cho et al., 2014;
Sutskever et al., 2014), and more. Despite their successes, one of the main bottlenecks of the supervised approach is the difficulty in obtaining enough data to learn abstract features that capture the rich structure of the data. It is well recognized that a promising avenue is to use unsupervised learning on unlabelled data, which is far more plentiful and cheaper to obtain.
A long-standing and inherent problem in unsupervised learning is defining a good method for evaluation. Generative models offer the ability to evaluate generalization in the data space, which can also be qualitatively assessed.
In this work we propose a generative model for unsupervised learning that we call generative moment matching networks (GMMNs). GMMNs are generative neural networks that begin with a simple prior from which it is easy to draw samples. These are propagated deterministically through the hidden layers of the network and the output is a sample from the model. Thus, with GMMNs it is easy to quickly draw independent random samples, as opposed to expensive MCMC procedures that are necessary in other models such as Boltzmann machines (Ackley et al., 1985;
Hinton, 2002; Salakhutdinov & Hinton, 2009). The structure of a GMMN is most analogous to the recently proposed generative adversarial networks (GANs) (Goodfellow et al., 2014), however unlike GANs, whose training involves a difficult minimax optimization problem, GMMNs are comparatively simple; they are trained to minimize a straightforward loss function using backpropagation.
The key idea behind GMMNs is the use of a statistical hypothesis testing framework called maximum mean discrepancy (Gretton et al., 2007). Training a GMMN to minimize this discrepancy can be interpreted as matching all moments of the model distribution to the empirical data distribution. Using the kernel trick, MMD can be represented as a simple loss function that we use as the core training objective for GMMNs. Using minibatch stochastic gradient descent, training can be kept efficient, even with large datasets.
Generative Moment Matching Networks
As a second contribution, we show how GMMNs can be used to bootstrap auto-encoder networks in order to further improve the generative process. The idea behind this approach is to train an auto-encoder network and then apply a GMMN to the code space of the auto-encoder. This allows us to leverage the rich representations learned by auto-encoder models as the basis for comparing data and model distributions. To generate samples in the original data space, we simply sample a code from the GMMN and then use the decoder of the auto-encoder network.
Our experiments show that this relatively simple, yet very flexible framework is effective at producing good generative models in an efficient manner. On MNIST and the Toronto Face Dataset (TFD) we demonstrate improved results over comparable baselines, including GANs. Source code for training GMMNs is available at https:// github.com/yujiali/gmmn.
2. Maximum Mean Discrepancy
Suppose we are given two sets of samples X = {xi}N i=1 and Y = {yj}M j=1 and are asked whether the generating distributions PX = PY. Maximum mean discrepancy is a frequentist estimator for answering this question, also known as the two sample test (Gretton et al., 2007; 2012a).
The idea is simple: compare statistics between the two datasets and if they are similar then the samples are likely to come from the same distribution.
Formally, the following MMD measure computes the mean squared difference of the statistics of the two sets of samples.
LMMD2 =
������
N
N
� i=1 φ(xi) − 1
M
M
� j=1 φ(yj)
������
=
N 2
N
� i=1
N
� i′=1 φ(xi)⊤φ(xi′) −
NM
N
� i=1
M
� j=1 φ(xi)⊤φ(yj)
M 2
M
� j=1
M
� j′=1 φ(yj)⊤φ(yj′)
Taking φ to be the identity function leads to matching the sample mean, and other choices of φ can be used to match higher order moments.
Written in this form, each term in Equation (2) only involves inner products between the φ vectors, and therefore the kernel trick can be applied.
LMMD2 =
N 2
N
� i=1
N
� i′=1 k(xi, xi′) −
NM
N
� i=1
M
� j=1 k(xi, yj)
M 2
M
� j=1
M
� j′=1 k(yj, yj′)
The kernel trick implicitly lifts the sample vectors into an infinite dimensional feature space. When this feature space corresponds to a universal reproducing kernel Hilbert space, it is shown that asymptotically, MMD is 0 if and only if PX = PY (Gretton et al., 2007; 2012a).
For universal kernels like the Gaussian kernel, defined as k(x, x′) = exp(− 1
2σ|x − x′|2), where σ is the bandwidth parameter, we can use a Taylor expansion to get an explicit feature map φ that contains an infinite number of terms and covers all orders of statistics. Minimizing MMD under this feature expansion is then equivalent to minimizing a distance between all moments of the two distributions.
3. Related Work
In this work we focus on generative models due to their ability to capture the salient properties and structure of data. Deep generative models are particularly appealing because they are capable of learning a latent manifold on which the data has high density. Learning this manifold allows smooth variations in the latent space to result in non-trivial transformations in the original space, effectively traversing between high density modes through low density areas (Bengio et al., 2013a). They are also capable of disentangling factors of variation, which means that each latent variable can become responsible for modelling a single, complex transformation in the original space that would otherwise involve many variables (Bengio et al., 2013a).
Even if we restrict ourselves to the field of deep learning, there are a vast array of approaches to generative modelling. Below, we outline some of these methods.
One popular class of generative models used in deep learning are undirected graphical models, such as Boltzmann machines (Ackley et al., 1985), restricted Boltzmann machines (Hinton, 2002), and deep Boltzmann machines(Salakhutdinov & Hinton, 2009). These models are normalized by a typically intractable partition function, making training, evaluation, and sampling extremely difficult, usually requiring expensive Markov-chain Monte Carlo(MCMC) procedures.
Next there is the class of fully visible directed models such as fully visible sigmoid belief networks (Neal, 1992) and the neural autoregressive distribution estimator (Larochelle
& Murray, 2011). These admit efficient log-likelihood calculation, gradient-based learning and efficient sampling, but require that an ordering be imposed on the observable variables, which can be unnatural for domains such as images and cannot take advantage of parallel computing methods due to their sequential nature.
More related to our own work, there is a line of research devoted to recovering density models from auto-encoder networks using MCMC procedures (Rifai et al., 2012; Bengio
Generative Moment Matching Networks et al., 2013b; 2014). These attempt to use contraction operators, or denoising criteria in order to generate a Markov chain by repeated perturbations during the encoding phase, followed by decoding.
Also related to our own work is the class of deep, variational networks (Rezende et al., 2014; Kingma & Welling, 2014; Mnih & Gregor, 2014). These are also deep, directed generative models, however they make use of an additional neural network that is designed to approximate the posterior over the latent variables. Training is carried out via a variational lower bound on the log-likelihood of the model distribution. These models are trained using stochastic gradient descent, however they either require that the latent representation is continuous (Kingma & Welling, 2014), or require many secondary networks to sufficiently reduce the variance of gradient estimates in order to produce a sufficiently good learning signal (Mnih & Gregor, 2014).
Finally there is some early work that proposed the idea of using feed-forward neural networks to learn generative models. MacKay (1995) proposed a model that is closely related to ours, which also used a feed-forward network to map the prior samples to the data space. However, instead of directly outputing samples, an extra distribution is associated with the output. Sampling was used extensively for learning and inference in this model. MagdonIsmail & Atiya (1998) proposed to use a neural network to learn a transformation from the data space to another space where the transformed data points are uniformly distributed. This transformation network then learns the cumulative density function.
In recent independent work, Dziugaite et al. (2015) proposed an idea very similar to our's, which trains a feed-forward neural network generative model by optimizing MMD. A more thorough comparison with their approach is left to future work.
4. Generative Moment Matching Networks
4.1. Data Space Networks
The high-level idea of the GMMN is to use a neural network to learn a deterministic mapping from samples of a simple, easy to sample distribution, to samples from the data distribution. The architecture of the generative network is exactly the same as a generative adversarial network (Goodfellow et al., 2014). However, we propose to train the network by simply minimizing the MMD criterion, avoiding the hard minimax objective function used in generative adversarial network training.
More specifically, in the generative network we have a stochastic hidden layer h ∈ RH with H hidden units at the top with a prior uniform distribution on each unit indeUniform Prior
ReLU
ReLU
ReLU
Sigmoid
GMMN
Sample Generation
ReLU
Uniform Prior
Sigmoid
Sigmoid
Sigmoid
Input Data
Reconstruction
Auto-Encoder
GMMN
Dropout
Dropout
Sample Generation
ReLU
ReLU
ReLU
Sigmoid
ReLU(a) GMMN(b) GMMN+AE
Figure 1. Example architectures of our generative moment matching networks.(a) GMMN used in the input data space.(b)
GMMN used in the code space of an auto-encoder. pendently, p(h) =
H
� j=1
U(hj)
Here U(h) =
2I[−1 ≤ h ≤ 1] is a uniform distribution in [−1, 1], where I[.] is an indicator function. Other choices for the prior are also possible, as long as it is a simple enough distribution from which we can easily draw samples.
The h vector is then passed through the neural network and deterministically mapped to a vector x ∈ RD in the D dimensional data space. x = f(h; w)(5) f is the neural network mapping function, which can contain multiple layers of nonlinearities, and w represents the parameters of the neural network. One example architecture for f is illustrated in Figure 1(a), which has 4 intermediate ReLU (Nair & Hinton, 2010) nonlinear layers and one logistic sigmoid output layer.
The prior p(h) and the mapping f(h; w) jointly defines a distribution p(x) in the data space. To generate a sample x ∼ p(x) we only need to sample from the uniform prior p(h) and then pass the sample h through the neural net to get x = f(h; w).
Goodfellow et al. (2014) proposed to train this network by using an extra discriminative network, which tries to distinguish between model samples and data samples. The generative network is then trained to counteract this in order to make the samples indistinguishable to the discriminative network. The gradient of this objective can be backpropagated through the generative network. However, because of the minimax nature of the formulation, it is easy to get
Generative Moment Matching Networks stuck at a local optimum. So the training of generative network and the discriminative network must be interleaved and carefully scheduled. By contrast, our learning algorithm simply involves minimizing the MMD objective.
Assume we have a dataset of training examples xd
1,..., xd
N(d for data), and a set of samples generated from our model xs
1,..., xs
M (s for samples). The MMD objective LMMD2 is differentiable when the kernel is differentiable. For example for Gaussian kernels k(x, y) = exp
�
− 1
2σ||x − y||2�, the gradient of xs ip has a simple form
∂LMMD2
∂xs ip
= 2
M 2
M
� j=1
1 σ k(xs i, xs j)(xs jp − xs ip)
−
MN
N
� j=1
1 σ k(xs i, xd j)(xd jp − xs ip)
This gradient can then be backpropagated through the generative network to update the parameters w.
4.2. Auto-Encoder Code Space Networks
Real-world data can be complicated and high-dimensional, which is one reason why generative modelling is such a difficult task. Auto-encoders, on the other hand, are designed to solve an arguably simpler task of reconstruction.
If trained properly, auto-encoder models can be very good at representing data in a code space that captures enough statistical information that the data can be reliably reconstructed.
The code space of an auto-encoder has several advantages for creating a generative model. The first is that the dimensionality can be explicitly controlled. Visual data, for example, while represented in a high dimensional space, often exists on a low-dimensional manifold. This is beneficial for a statistical estimator like MMD because the amount of data required to produce a reliable estimate grows with the dimensionality of the data (Ramdas et al., 2015). The second advantage is that each dimension of the code space can end up representing complex variations in the original data space. This concept is referred to in the literature as disentangling factors of variation (Bengio et al., 2013a).
For these reasons, we propose to bootstrap auto-encoder models with a GMMN to create what we refer to as the GMMN+AE model.
These operate by first learning an auto-encoder and producing code representations of the data, then freezing the auto-encoder weights and learning a GMMN to minimize MMD between generated codes and data codes. A visualization of this model is given in Figure
1(b).
Our method for training a GMMN+AE proceeds as follows:
1. Greedy layer-wise pretraining of the auto-encoder(Bengio et al., 2007).
2. Fine-tune the auto-encoder.
3. Train a GMMN to model the code layer distribution using an MMD objective on the final encoding layer.
We found that adding dropout to the encoding layers can be beneficial in terms of creating a smooth manifold in code space. This is analogous to the motivation behind contractive and denoising auto-encoders (Rifai et al., 2011; Vincent et al., 2008).
4.3. Practical Considerations
Here we outline some design choices that we have found to improve the peformance of GMMNs.
Bandwidth Parameter. The bandwidth parameter in the kernel plays a crucial role in determining the statistical efficiency of MMD, and optimally setting it is an open problem. A good heuristic is to perform a line search to obtain the bandwidth that produces the maximal distance (Sriperumbudur et al., 2009), other more advanced heuristics are also available (Gretton et al., 2012b). As a simpler approximation, for most of our experiments we use a mixture of K kernels spanning multiple ranges. That is, we choose the kernel to be: k(x, x′) =
K
� q=1 kσq(x, x′)(7) where kσq is a Gaussian kernel with bandwidth parameter σq. We found that choosing simple values for these such as
1, 5, 10, etc. and using a mixture of 5 or more was sufficient to obtain good results. The weighting of different kernels can be further tuned to achieve better results, but we kept them equally weighted for simplicity.
Square Root Loss. In practice, we have found that better results can be obtained by optimizing LMMD = √LMMD2.
This loss can be important for driving the difference between the two distributions as close to 0 as possible. Compared to LMMD2 which flattens out when its value gets close to 0, LMMD behaves much better for small LMMD values. Alternatively, this can be understood by writing down the gradient of LMMD with respect to w
∂LMMD
∂w
=
2√LMMD2
∂LMMD2
∂w
The 1/(2√LMMD2) term automatically adapts the effective learning rate. This is especially beneficial when both
LMMD2 and ∂LMMD2
∂w become small, where this extra factor can help by maintaining larger gradients.
Generative Moment Matching Networks
Algorithm 1: GMMN minibatch training
Input : Dataset {xd
1,..., xd
N}, prior p(h), network f(h; w) with initial parameter w(0)
Output: Learned parameter w∗
1 while Stopping criterion not met do
Get a minibatch of data Xd ← {xd i1,..., xd ib}
Get a new set of samples Xs ← {xs
1,..., xs b}
Compute gradient ∂LMMD
∂w on Xd and Xs
Take a gradient step to update w
6 end
Minibatch Training. One of the issues with MMD is that the usage of kernels means that the computation of the objective scales quadratically with the amount of data. In the literature there have been several alternative estimators designed to overcome this (Gretton et al., 2012a). In our case, we found that it was sufficient to optimize MMD using minibatch optimization. In each weight update, a small subset of data is chosen, and an equal number of samples are drawn from the GMMN. Within a minibatch, MMD is applied as usual. As we are using exact samples from the model and the data distribution, the minibatch MMD is still a good estimator of the population MMD. We found this approach to be both fast and effective. The minibatch training algorithm for GMMN is shown in Algorithm 1.
5. Experiments
We trained GMMNs on two benchmark datasets: MNIST(LeCun et al., 1998) and the Toronto Face Dataset (TFD)(Susskind et al., 2010). For MNIST, we used the standard test set of 10,000 images, and split out 5000 from the standard 60,000 training images for validation. The remaining
55,000 were used for training. For TFD, we used the same training and test sets and fold splits as used by (Goodfellow et al., 2014), but split out a small set of the training data and used it as the validation set. For both datasets, rescaling the images to have pixel intensities between 0 and 1 is the only preprocessing step we did.
On both datasets, we trained the GMMN network in both the input data space and the code space of an auto-encoder.
For all the networks we used in this section, a uniform distribution in [−1, 1]H was used as the prior for the H-dimensional stochastic hidden layer at the top of the GMMN, which was followed by 4 ReLU layers, and the output was a layer of logistic sigmoid units.
The autoencoder we used for MNIST had 4 layers, 2 for the encoder and 2 for the decoder. For TFD the auto-encoder had 6 layers in total, 3 for the encoder and 3 for the decoder. For both auto-encoders the encoder and the decoder had mirrored architectures. All layers in the auto-encoder network used sigmoid nonlinearities, which also guaranteed that the code space dimensions lay in, so that they could match the GMMN outputs. The network architectures for MNIST are shown in Figure 1.
The auto-encoders were trained separately from the GMMN. Cross entropy was used as the reconstruction loss.
We first did standard layer-wise pretraining, then fine-tuned all layers jointly. Dropout (Hinton et al., 2012b) was used on the encoder layers. After training the auto-encoder, we fixed it and passed the input data through the encoder to get the corresponding codes. The GMMN network was then trained in this code space to match the statistics of generated codes to the statistics of codes from data examples.
When generating samples, the generated codes were passed through the decoder to get samples in the input data space.
For all experiments in this section the GMMN networks were trained with minibatches of size 1000, for each minibatch we generated a set of 1000 samples from the network. The loss and gradient were computed from these
2000 points. We used the square root loss function LMMD throughout.
Evaluation of our model is not straight-forward, as we do not have an explicit form for the probability density function, it is not easy to compute the log-likelihood of data.
However, sampling from our model is easy. We therefore followed the same evaluation protocol used in related models (Bengio et al., 2013a), (Bengio et al., 2014), and (Goodfellow et al., 2014). A Gaussian Parzen window (kernel density estimator) was fit to 10,000 samples generated from the model. The likelihood of the test data was then computed under this distribution. The scale parameter of the Gaussians was selected using a grid search in a fixed range using the validation set.
The hyperparameters of the networks, including the learning rate and momentum for both auto-encoder and GMMN training, dropout rate for the auto-encoder, and number of hidden units on each layer of both auto-encoder and GMMN, were tuned using Bayesian optimization (Snoek et al., 2012; 2014)1 to optimize the validation set likelihood under the Gaussian Parzen window density estimation.
The log-likelihood of the test set for both datasets are shown in Table 1. The GMMN is competitive with other approaches, while the GMMN+AE significantly outperforms the other models. This shows that despite being relatively simple, MMD, especially when combined with an effective decoder, is a powerful objective for training good generative models.
Some samples generated from the GMMN models are
1We used the service provided by https://www. whetlab.com
Generative Moment Matching Networks(e) GMMN nearest neighbors for MNIST samples(a) GMMN MNIST samples(b) GMMN TFD samples(f) GMMN+AE nearest neighbors for MNIST samples(g) GMMN nearest neighbors for TFD samples(c) GMMN+AE MNIST samples(d) GMMN+AE TFD samples(h) GMMN+AE nearest neighbors for TFD samples
Figure 2. Independent samples and their nearest neighbors in the training set for the GMMN+AE model trained on MNIST and TFD datasets. For (e)(f)(g) and (h) the top row are the samples from the model and the bottom row are the corresponding nearest neighbors from the training set measured by Euclidean distance.
Model
MNIST
TFD
DBN
138 ± 2
1909 ± 66
Stacked CAE
121 ± 1.6
2110 ± 50
Deep GSN
214 ± 1.1
1890 ± 29
Adversarial nets
225 ± 2
2057 ± 26
GMMN
147 ± 2
2085 ± 25
GMMN+AE
282 ± 2
2204 ± 20
Table 1. Log-likelihood of the test sets under different models.
The baselines are Deep Belief Net (DBN) and Stacked Contractive Auto-Encoder (Stacked CAE) from (Bengio et al., 2013a), Deep Generative Stochastic Network (Deep GSN) from (Bengio et al., 2014) and Adversarial nets (GANs) from (Goodfellow et al., 2014). shown in Figure 2(a-d).
The GMMN+AE produces the most visually appealing samples, which are reflected in its
Parzen window log-likelihood estimates. The likely explanation is that any perturbations in the code space correspond to smooth transformations along the manifold of the data space. In that sense, the decoder is able to "correct" noise in the code space.
To determine whether the models learned to merely copy the data, we follow the example of (Goodfellow et al., 2014) and visualize the nearest neighbour of several samples in terms of Euclidean pixel-wise distance in Figure
2(e-h). By this metric, it appears as though the samples are not merely data examples.
One of the interesting aspects of a deep generative model such as the GMMN is that it is possible to directly explore the data manifold. Using the GMMN+AE model, we randomly sampled 5 points in the uniform space and show their corresponding data space projections in Figure 3. These points are highlighted by red boxes. From left to right, top to bottom we linearly interpolate between these points in the uniform space and show their corresponding projections in data space. The manifold is smooth for the most part, and almost all of the projections correspond to realistic looking data. For TFD in particular, these transformations involve complex attributes, such as the changing of pose, expression, lighting, gender, and facial hair. More results can be found at http://www.cs. toronto.edu/˜yujiali/proj/gmmn.html.
6. Conclusion and Future Work
In this paper we provide a simple and effective framework for training deep generative models called generative moment matching networks. Our approach is based off of optimizing maximum mean discrepancy so that samples generated from the model are indistinguishable from data examples in terms of their moment statistics. As is standard with
MMD, the use of the kernel trick allows a GMMN to avoid explicitly computing these moments, resulting in a simple
Generative Moment Matching Networks(a) MNIST interpolation(b) TFD interpolation
Figure 3. Linear interpolation between 5 uniform random points from the GMMN+AE prior projected through the network into data space for (a) MNIST and (b) TFD. The 5 random points are highlighted with red boxes, and the interpolation goes from left to right, top to bottom. The final two rows represent an interpolation between the last highlighted image and the first highlighted image. training objective, and the use of minibatch stochastic gradient descent allows the training to scale to large datasets.
Our second contribution combines MMD with autoencoders for learning a generative model of the code layer.
The code samples from the model can then be fed through the decoder in order to generate samples in the original space.
The use of auto-encoders makes the generative model learning a much simpler problem.
Combined with MMD, pretrained auto-encoders can be readily bootstrapped into a good generative model of data. On the MNIST and Toronto Face Database, the GMMN+AE model achieves superior performance compared to other approaches. For these datasets, we demonstrate that the GMMN+AE is able to discover the implicit manifold of the data.
There are many interesting directions for research using
MMD. One such extension is to consider alternatives to the standard MMD criterion in order to speed up training. One such possibility is the class of linear-time estimators that has been developed recently in the literature (Gretton et al., 2012a).
Another possibility is to utilize random features (Rahimi
& Recht, 2007). These are randomized feature expansions whose inner product converges to a kernel function with an increasing number of features. This idea was recently explored for MMD in (Zhao & Meng, 2014). The advantage of this approach would be that the cost would no longer grow quadratically with minibatch size because we could use the original objective given in Equation 2. Another advantage of this approach is that the data statistics could be pre-computed from the entire dataset, which would reduce the variance of the objective gradients.
Another direction we would like to explore is joint training of the auto-encoder model with the GMMN. Currently, these are treated separately, but joint training may encourage the learning of codes that are both suitable for reconstruction as well as generation.
While a GMMN provides an easy way to sample data, the posterior distribution over the latent variables is not readily available. It would be interesting to explore ways in which to infer the posterior distribution over the latent space. A straightforward way to do this is to learn a neural network to predict the latent vector given a sample. This is reminiscent of the recognition models used in the wake-sleep algorithm (Hinton et al., 1995), or variational auto-encoders(Kingma & Welling, 2014).
An interesting application of MMD that is not directly related to generative modelling comes from recent work on learning fair representations (Zemel et al., 2013). There, the objective is to train a prediction method that is invariant to a particular sensitive attribute of the data. Their solution is to learn an intermediate clustering-based representation.
MMD could instead be applied to learn a more powerful, distributed representation such that the statistics of the representation do not change conditioned on the sensitive variable. This idea can be further generalized to learn representations invariant to known biases.
Finally, the notion of utilizing an auto-encoder with the GMMN+AE model provides new avenues for creating generative models of even more complex datasets. For example, it may be possible to use a GMMN+AE with convolutional auto-encoders (Zeiler et al., 2010; Masci et al., 2011;
Generative Moment Matching Networks
Makhzani & Frey, 2014) in order to create generative models of high resolution color images.
Acknowledgements
We thank David Warde-Farley for helpful clarifications regarding (Goodfellow et al., 2014), and Charlie Tang for providing relevant references. We thank CIFAR, NSERC, and Google for research funding.
References
Ackley, David H, Hinton, Geoffrey E, and Sejnowski, Terrence J. A learning algorithm for boltzmann machines.
Cognitive science, 9(1):147–169, 1985.
Bengio, Yoshua, Lamblin, Pascal, Popovici, Dan, Larochelle, Hugo, et al. Greedy layer-wise training of deep networks. In Advances in Neural Information Processing Systems (NIPS), 2007.
Bengio, Yoshua, Mesnil, Gr´egoire, Dauphin, Yann, and Rifai, Salah.
Better mixing via deep representations.
In Proceedings of the 28th International Conference on
Machine Learning (ICML), 2013a.
Bengio, Yoshua, Yao, Li, Alain, Guillaume, and Vincent, Pascal. Generalized denoising auto-encoders as generative models. In Advances in Neural Information Processing Systems, pp. 899–907, 2013b.
Bengio, Yoshua, Thibodeau-Laufer, Eric, Alain, Guillaume, and Yosinski, Jason. Deep generative stochastic networks trainable by backprop. In Proceedings of the 29th International Conference on Machine Learning(ICML), 2014.
Cho, Kyunghyun, van Merrienboer, Bart, Gulcehre, Caglar, Bougares, Fethi, Schwenk, Holger, and Bengio, Yoshua.
Learning phrase representations using rnn encoderdecoder for statistical machine translation. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2014.
Dziugaite, Gintare Karolina, Roy, Daniel M., and Ghahramani, Zoubin. Training generative neural networks via maximum mean discrepancy optimization.
In Uncertainty in Artificial Intelligence (UAI), 2015.
Fang, Hao, Gupta, Saurabh, Iandola, Forrest, Srivastava, Rupesh, Deng, Li, Doll´ar, Piotr, Gao, Jianfeng, He, Xiaodong, Mitchell, Margaret, Platt, John, Zitnick, C. Lawrence, and Zweig, Geoffrey. From captions to visual concepts and back. arXiv preprint arXiv:1411.4952, Goodfellow, Ian, Pouget-Abadie, Jean, Mirza, Mehdi, Xu, Bing, Warde-Farley, David, Ozair, Sherjil, Courville, Aaron, and Bengio, Yoshua. Generative adversarial nets.
In Advances in Neural Information Processing Systems, pp. 2672–2680, 2014.
Graves, Alex and Jaitly, Navdeep.
Towards end-to-end speech recognition with recurrent neural networks. In
Proceedings of the 31st International Conference on Machine Learning (ICML-14), pp. 1764–1772, 2014.
Gretton, Arthur, Borgwardt, Karsten M, Rasch, Malte, Sch¨olkopf, Bernhard, and Smola, Alex J.
A kernel method for the two-sample-problem.
In Advances in Neural Information Processing Systems (NIPS), 2007.
Gretton, Arthur, Borgwardt, Karsten M, Rasch, Malte J, Sch¨olkopf, Bernhard, and Smola, Alexander. A kernel two-sample test. The Journal of Machine Learning Research, 13(1):723–773, 2012a.
Gretton, Arthur, Sejdinovic, Dino, Strathmann, Heiko, Balakrishnan, Sivaraman, Pontil, Massimiliano, Fukumizu, Kenji, and Sriperumbudur, Bharath K. Optimal kernel choice for large-scale two-sample tests. In Advances in Neural Information Processing Systems, pp. 1205–1213, 2012b.
Hinton, Geoffrey E. Training products of experts by minimizing contrastive divergence. Neural Computation, 14(8):1771–1800, 2002.
Hinton, Geoffrey E, Dayan, Peter, Frey, Brendan J, and Neal, Radford M. The "wake-sleep" algorithm for unsupervised neural networks. Science, 268(5214):1158–
Hinton, Geoffrey E., Deng, Li, Yu, Dong, Dahl, George E., Mohamed, Abdel-rahman, Jaitly, Navdeep, Senior, Andrew, Vanhoucke, Vincent, Nguyen, Patrick, Sainath, Tara N., and Kingsbury, Brian. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups.
IEEE Signal Process.
Mag., 29(6):82–97, 2012a.
Hinton, Geoffrey E, Srivastava, Nitish, Krizhevsky, Alex, Sutskever, Ilya, and Salakhutdinov, Ruslan R. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580, 2012b.
Kingma, Diederik P. and Welling, Max.
Auto-encoding variational Bayes.
In International Conference on
Learning Representations, 2014.
Kiros, Ryan, Salakhutdinov, Ruslan, and Zemel, Richard S.
Unifying visual-semantic embeddings with multimodal neural language models. arXiv preprint arXiv:1411.2539, 2014.
Generative Moment Matching Networks
Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E.
Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing
Systems (NIPS), 2012.
Larochelle, Hugo and Murray, Iain. The neural autoregressive distribution estimator.
In roceedings of the 14th
International Conference on Artificial Intelligence and Statistics (AISTATS), 2011.
LeCun, Yann, Bottou, L´eon, Bengio, Yoshua, and Haffner, Patrick.
Gradient-based learning applied to document recognition.
Proceedings of the IEEE, 86(11):2278–
MacKay, David JC. Bayesian neural networks and density networks. Nuclear Instruments and Methods in Physics
Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment, 354(1):73–80, 1995.
Magdon-Ismail, Malik and Atiya, Amir. Neural networks for density estimation. In NIPS, pp. 522–528, 1998.
Makhzani, Alireza and Frey, Brendan. A winner-take-all method for training sparse convolutional autoencoders.
In NIPS Deep Learning Workshop, 2014.
Masci, Jonathan, Meier, Ueli, Cires¸an, Dan, and Schmidhuber, J¨urgen. Stacked convolutional auto-encoders for hierarchical feature extraction. In Artificial Neural Networks and Machine Learning–ICANN 2011, pp. 52–59.
Springer, 2011.
Mnih, Andriy and Gregor, Karol. Neural variational inference and learning in belief networks. In International
Conference on Machine Learning, 2014.
Nair, Vinod and Hinton, Geoffrey E. Rectified linear units improve restricted boltzmann machines. In International
Conference on Machine Learning, pp. 807–814, 2010.
Neal, Radford M.
Connectionist learning of belief networks. Artificial intelligence, 56(1):71–113, 1992.
Rahimi, Ali and Recht, Benjamin. Random features for large-scale kernel machines. In Advances in Neural Information Processing Systems (NIPS), 2007.
Ramdas, Aaditya, Reddi, Sashank J, Poczos, Barnabas, Singh, Aarti, and Wasserman, Larry. On the decreasing power of kernel and distance based nonparametric hypothesis tests in high dimensions. In The Twenty-Ninth
AAAI Conference on Artificial Intelligence (AAAI-15), Rezende, Danilo Jimenez, Mohamed, Shakir, and Wierstra, Daan. Stochastic backpropagation and approximate inference in deep generative models. In International Conference on Machine Learning, pp. 1278–1286, 2014.
Rifai, Salah, Vincent, Pascal, Muller, Xavier, Glorot, Xavier, and Bengio, Yoshua. Contractive auto-encoders:
Explicit invariance during feature extraction.
In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pp. 833–840, 2011.
Rifai, Salah, Bengio, Yoshua, Dauphin, Yann, and Vincent, Pascal. A generative process for sampling contractive auto-encoders. In International Conference on Machine
Learning (ICML), 2012.
Salakhutdinov, Ruslan and Hinton, Geoffrey E. Deep boltzmann machines. In International Conference on Artificial Intelligence and Statistics, 2009.
Sermanet, Pierre, Eigen, David, Zhang, Xiang, Mathieu, Micha¨el, Fergus, Rob, and LeCun, Yann.
Overfeat:
Integrated recognition, localization and detection using convolutional networks. In International Conference on
Learning Representations, 2014.
Snoek, Jasper, Larochelle, Hugo, and Adams, Ryan P.
Practical Bayesian optimization of machine learning algorithms. In Advances in Neural Information Processing
Systems, 2012.
Snoek, Jasper, Swersky, Kevin, Zemel, Richard S., and Adams, Ryan P. Input warping for bayesian optimization of non-stationary functions. In International Conference on Machine Learning, 2014.
Sriperumbudur, Bharath K, Fukumizu, Kenji, Gretton, Arthur, Lanckriet, Gert RG, and Sch¨olkopf, Bernhard.
Kernel choice and classifiability for rkhs embeddings of probability distributions. In Advances in Neural Information Processing Systems, pp. 1750–1758, 2009.
Susskind, Joshua, Anderson, Adam, and Hinton, Geoffrey E. The toronto face dataset. Technical report, Department of Computer Science, University of Toronto, Sutskever, Ilya, Vinyals, Oriol, and Le, Quoc VV.
Sequence to sequence learning with neural networks. In
Advances in Neural Information Processing Systems, pp.
3104–3112, 2014.
Szegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet, Pierre, Reed, Scott, Anguelov, Dragomir, Erhan, Dumitru, Vanhoucke, Vincent, and Rabinovich, Andrew.
Going deeper with convolutions. arXiv preprint arXiv:1409.4842, 2014.
Vincent, Pascal, Larochelle, Hugo, Bengio, Yoshua, and Manzagol, Pierre-Antoine.
Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th international conference on Machine learning, pp. 1096–1103. ACM, 2008.
Generative Moment Matching Networks
Vinyals, Oriol, Toshev, Alexander, Bengio, Samy, and Erhan, Dumitru. Show and tell: A neural image caption generator. arXiv preprint arXiv:1411.4555, 2014.
Zeiler, Matthew D, Krishnan, Dilip, Taylor, Graham W, and Fergus, Robert. Deconvolutional networks. In Computer
Vision and Pattern Recognition, pp. 2528–2535. IEEE, Zemel, Richard, Wu, Yu, Swersky, Kevin, Pitassi, Toni, and Dwork, Cynthia. Learning fair representations. In International Conference on Machine Learning, pp. 325–333, Zhao, Ji and Meng, Deyu. Fastmmd: Ensemble of circular discrepancy for efficient two-sample test. arXiv preprint arXiv:1405.2664, 2014.Objects that Sound
Relja Arandjelovi´c1 and Andrew Zisserman1,2
1 DeepMind
2 VGG, Department of Engineering Science, University of Oxford
Abstract. In this paper our objectives are, first, networks that can embed audio and visual inputs into a common space that is suitable for cross-modal retrieval; and second, a network that can localize the object that sounds in an image, given the audio signal. We achieve both these objectives by training from unlabelled video using only audio-visual correspondence (AVC) as the objective function. This is a form of crossmodal self-supervision from video.
To this end, we design new network architectures that can be trained for cross-modal retrieval and localizing the sound source in an image, by using the AVC task. We make the following contributions: (i) show that audio and visual embeddings can be learnt that enable both within-mode(e.g. audio-to-audio) and between-mode retrieval; (ii) explore various architectures for the AVC task, including those for the visual stream that ingest a single image, or multiple images, or a single image and multi-frame optical flow; (iii) show that the semantic object that sounds within an image can be localized (using only the sound, no motion or flow information); and (iv) give a cautionary tale on how to avoid undesirable shortcuts in the data preparation.
Introduction
There has been a recent surge of interest in cross-modal learning from images and audio [1–4]. One reason for this surge is the availability of virtually unlimited training material in the form of videos (e.g. from YouTube) that can provide both an image stream and a (synchronized) audio stream, and this cross-modal information can be used to train deep networks. Cross-modal learning itself has a long history in computer vision, principally in the form of images and text [5–7].
Although audio and text share the fact that they are both sequential in nature, the challenges of using audio to partner images are significantly different to those of using text. Text is much closer to a semantic annotation than audio. With text, e.g. in the form of a provided caption of an image, the concepts (such as 'a dog') are directly available and the problem is then to provide a correspondence between the noun 'dog' and a spatial region in the image. Whereas, for audio, obtaining the semantics is less direct, and has more in common with image classification, in that the concept dog is not directly available from the signal but requires something like a ConvNet to obtain it (think of classifying an image as to whether it contains a dog or not, and classifying an audio clip as to whether it contains the sound of a dog or not). arXiv:1712.06651v2 [cs.CV] 25 Jul 2018
R. Arandjelovi´c and A. Zisserman(a) Input image with sound(b) Where is the sound?
Fig. 1. Where is the sound? Given an input image and sound clip, our method learns, without a single labelled example, to localize the object that makes the sound.
In this paper our interest is in cross-modal learning from images and audio [1–4, 9–12]. In particular, we use unlabelled video as our source material, and employ audio-visual correspondence (AVC) as the training objective. In brief, given an input pair of a video frame and 1 second of audio, the AVC task requires the network to decide whether they are in correspondence or not.
The labels for the positives (matching) and negatives (mismatched) pairs are obtained directly, as videos provide an automatic alignment between the visual and the audio streams – frame and audio coming from the same time in a video are positives, while frame and audio coming from different videos are negatives.
As the labels are constructed directly from the data itself, this is an example of "self-supervision" [13–22], a subclass of unsupervised methods.
The AVC task stimulates the learnt visual and audio representations to be both discriminative, to distinguish between matched and mismatched pairs, and semantically meaningful. The latter is the case because the only way for a network to solve the task is if it learns to classify semantic concepts in both modalities, and then judge whether the two concepts correspond. Recall that the visual network only sees a single frame of video and therefore it cannot learn to cheat by exploiting motion information.
In this paper we propose two networks that enable new functionalities: in Section 3 we propose a network architecture that produces embeddings directly suitable for cross-modal retrieval; in Section 4 we design a network and a learning procedure capable of localizing the sound source, i.e. answering the basic question
– "Which object in an image is making the sound?". An example is shown in Figure 1. Both of these are trained from scratch with no labels whatsoever, using the same unsupervised audio-visual correspondence task (AVC).
Dataset
Throughout the paper we use the publicly available AudioSet dataset. It consists of 10 second clips from YouTube with an emphasis on audio events, and video-level audio class labels (potentially more than 1 per video) are available, Objects that Sound
3 but are noisy; the labels are organized in an ontology. To make the dataset more manageable and interesting for our purposes, we filter it for sounds of musical instruments, singing and tools, yielding 110 audio classes (the full list is given in Appendix A.1), removing uninteresting classes like breathing, sine wave, sound effect, infrasound, silence, etc. The videos are challenging as many are of poor quality, the audio source is not always visible, and the audio stream can be artificially inserted on top of the video, e.g. it is often the case that a video is compiled of a musical piece and an album cover, text naming the song, still frame of the musician, or even completely unrelated visual motifs like a landscape, etc.
The dataset already comes with a public train-test split, and we randomly split the public training set into training and validation sets in 90%-10% proportions.
The final AudioSet-Instruments dataset contains 263k, 30k and 4.3k 10 s clips in the train, val and test splits, respectively.
We re-emphasise that no labels whatsoever are used for any of our methods since we treat the dataset purely as a collection of label-less videos. Labels are only used for quantitative evaluation purposes, e.g. to evaluate the quality of our unsupervised cross-modal retrieval (Section 3.1).
Cross-modal retrieval
In this section we describe a network architecture capable of learning good visual and audio embeddings from scratch and without labels. Furthermore, the two embeddings are aligned in order to enable querying across modalities, e.g. using an image to search for related sounds.
The Audio-Visual Embedding Network (AVE-Net) is designed explicitly to facilitate cross-modal retrieval. The input image and 1 second of audio (represented as a log-spectrogram) are processed by vision and audio subnetworks(Figures 2a and 2b), respectively, followed by feature fusion whose goal is to determine whether the image and the audio correspond under the AVC task. The architecture is shown in full detail in Figure 2c. To enforce feature alignment, the AVE-Net computes the correspondence score as a function of the Euclidean distance between the normalized visual and audio embeddings. This information bottleneck, the single scalar value that summarizes whether the image and the audio correspond, forces the two embeddings to be aligned. Furthermore, the use of the Euclidean distance during training is crucial as it makes the features
"aware" of the distance metric, therefore making them amenable to retrieval.
The two subnetworks produce a 128-D L2 normalized embedding for each of the modalities. The Euclidean distance between the two 128-D features is computed, and this single scalar is passed through a tiny FC, which scales and shifts the distance to calibrate it for the subsequent softmax. The bias of the FC essentially learns the threshold on the distance above which the two features are deemed not to correspond.
Relation to previous works. The L3-Net introduced in and shown in Figure 2d, was also trained using the AVC task. However, the L3-Net audio and visual
R. Arandjelovi´c and A. Zisserman
224x224x3 conv1_1 3x3 stride 2
112x112x64 conv1_2 3x3
112x112x64 pool1 2x2
56x56x64 conv2_1 3x3
56x56x128 conv2_2 3x3
56x56x128 pool2 2x2
28x28x128 conv3_1 3x3
28x28x256 conv3_2 3x3
28x28x256 pool3 2x2
14x14x256 conv4_1 3x3
14x14x512 conv4_2 3x3
14x14x512(a) Vision ConvNet conv1_1 3x3 stride 2
128x100x64 conv1_2 3x3
128x100x64 pool1 2x2
64x50x64 conv2_1 3x3
64x50x128 conv2_2 3x3
64x50x128 pool2 2x2
32x25x128 conv3_1 3x3
32x25x256 conv3_2 3x3
32x25x256 pool3 2x2
16x12x256 conv4_1 3x3
16x12x512 conv4_2 3x3
16x12x512
257x200x1 log-spectrogram
1 second 48kHz audio(b) Audio ConvNet
224x224x3
257x200x1 log-spectrogram
1 second 48kHz audio
Vision subnetwork
Audio subnetwork softmax
Euclidean distance
1 fc3 1x2
Corresponds: yes/no?
Audio ConvNet
16x12x512 pool4 16x12
1x1x512 fc1 512x128
128 fc2 128x128
L2 normalization
Image ConvNet
14x14x512 fc1 512x128
128 fc2 128x128
128 pool4 14x14
1x1x512
L2 normalization(c) AVE-Net
224x224x3
257x200x1 log-spectrogram
1 second 48kHz audio
Vision subnetwork
Audio subnetwork concat
1024 fc1 1024x128
128 fc2 128x2
2 softmax
Corresponds: yes/no? pool4 14x14
1x1x512
Image ConvNet
14x14x512 pool4 16x12
1x1x512
Audio ConvNet
16x12x512(d) L3-Net 
Fig. 2. ConvNet architectures. Each blocks represents a single layer with text providing more information – first row: layer name and optional kernel size, second row: output feature map size. Each convolutional layer is followed by batch normalization and a ReLU nonlinearity, and the first fully connected layer (fc1) is followed by ReLU. All pool layers perform max pooling and their strides are equal to the kernel sizes. (a) and (b) show the vision and audio ConvNets which perform initial feature extraction from the image and audio inputs, respectively. (c) Our AVE-Net is designed to produce aligned vision and audio embeddings as the only information, a single scalar, used to decide whether the two inputs correspond is the Euclidean distance between the embeddings. (d) In contrast, the L3-Net architecture combines the two modalities by concatenation and a couple of fully connected layers which produce the corresponds or not classification scores. features are inadequate for cross-modal retrieval (as will be shown in the results of Section 3.1) as they are not aligned in any way – the fusion is performed by concatenating the features and the correspondence score is computed only after the fully connected layers. In contrast, the AVE-Net moves the fully connected layers into the vision and audio subnetworks and directly optimizes the features for cross-modal retrieval.
The training bears resemblance to metric learning via the contrastive loss, but (i) unlike contrastive loss which requires tuning of the margin hyper-parameter, ours is parameter-free, and (ii) it explicitly computes the corresponds-or-not output, thus making it directly comparable to the L3-Net while contrastive loss would require another hyper-parameter for the distance threshold. Wang et al. also train a network for cross-modal retrieval but use a triplet loss which also contains the margin hyper-parameter, they use pretrained networks, and consider different modalities (image-text) with fully supervised correspondence labels. In concurrent work, Hong et al. use a similar technique with
Objects that Sound
5 pretrained networks and triplet loss for joint embedding of music and video.
Recent work of also trains networks for cross-modal retrieval, but uses an
ImageNet pretrained network as a teacher. In our case, we train the entire network from scratch.
Evaluation and results
The architectures are trained on the AudioSet-Instruments train-val set, and evaluated on the AudioSet-Instruments test set described in Section 2. Implementation details are given below in Section 3.3.
On the audio-visual correspondence task, AVE-Net achieves an accuracy of 81.9%, beating slightly the L3-Net which gets 80.8%. However, AVC performance is not the ultimate goal since the task is only used as a proxy for learning good embeddings, so the real test of interest here is the retrieval performance.
To evaluate the intra-modal (e.g. image-to-image) and cross-modal retrieval, we use the AudioSet-Instruments test dataset. A single frame and surrounding 1 second of audio are sampled randomly from each test video to form the retrieval database. All combinations of image/audio as query and image/audio as database are tested, e.g. audio-to-image uses the audio embedding as the query vector to search the database of visual embeddings, answering the question "Which image could make this sound?"; and image-to-image uses the visual embedding as the query vector to search the same database.
Evaluation metric. The performance of a retrieval system is assessed using a standard measure – the normalized discounted cumulative gain (nDCG). It measures the quality of the ranked list of the top k retrieved items (we use k = 30 throughout) normalized to the range, where 1 signifies a perfect ranking in which items are sorted in a non-increasing relevance-to-query order. For details on the definition of the relevance, refer to Appendix A.2. Each item in the test dataset is used as a query and the average nDCG@30 is reported as the final retrieval performance. Recall that the labels are noisy, and note that we only extract a single frame / 1s audio per video and can therefore miss the relevant event, so the ideal nDCG of 1 is highly unlikely to be achievable.
Baselines. We compare to the L3-Net as it is also trained in an unsupervised manner, and we train it using an identical procedure and training data to our method. As the L3-Net is expected not to work for cross-modal retrieval since the representation are not aligned in any way, we also test the L3-Net representations aligned with CCA as a baseline. In addition, vision features extracted from the last hidden layer of the VGG-16 network trained in a fully-supervised manner on ImageNet are evaluated as well. For cross-modal retrieval, the VGG16-ImageNet visual features are aligned with the L3-Net audio features using CCA, which is a strong baseline as the vision features are fully-supervised while the audio features are state-of-the-art. Note that the vanilla L3-Net produces 512-D representations, while VGG16 yields a 4096-D visual descriptor.
For computational reasons, and for fair comparison with our AVE-Net which
R. Arandjelovi´c and A. Zisserman
Table 1. Cross-modal and intra-modal retrieval. Comparison of our method with unsupervised and supervised baselines in terms of the average nDCG@30 on the AudioSet-Instruments test set. The columns headers denote the modalities of the query and the database, respectively, where im stands for image and aud for audio.
Our AVE-Net beats all baselines convincingly.
Method im-im im-aud aud-im aud-aud
Random chance
L3-Net 
L3-Net with CCA
VGG16-ImageNet 
–
–
–
VGG16-ImageNet + L3-Audio CCA.493
AVE-Net.665 produces 128-D embeddings, all CCA-based methods use 128 components. For all cases the representations are L2-normalized as we found this to significantly improve the performance; note that AVE-Net includes L2-normalization in the architecture and therefore the re-normalization is redundant.
Results. The nDCG@30 for all combinations of query-database modalities is shown in Table 1. For intra-modal retrieval (image-image, audio-audio) our AVENet is better than all baselines including slightly beating VGG16-ImageNet for image-image, which was trained in a fully supervised manner on another task.
It is interesting to note that our network has never seen same-modality pairs during training, so it has not been trained explicitly for image-image and audioaudio retrieval. However, intra-modal retrieval works because of transitivity – an image of a violin is close in feature space to the sound of a violin, which is in turn close to other images of violins. Note that despite learning essentially the same information on the same task and training data as the L3-Net, our
AVE-Net outperforms the L3-Net because it is Euclidean distance "aware", i.e. it has been designed and trained with retrieval in mind.
For cross-modal retrieval (image-audio, audio-image), AVE-Net beats all baselines, verifying that our unsupervised training is effective. The L3-Net representations are clearly not aligned across modalities as their cross-modal retrieval performance is on the level of random chance. The L3-Net features aligned with
CCA form a strong baseline, but the benefits of directly training our network for alignment are apparent. It is interesting that aligning vision features trained on ImageNet with state-of-the-art L3-Net audio features using CCA performs worse than other methods, demonstrating a case for unsupervised learning from a more varied dataset, as it is not sufficient to just use ImageNet-pretrained networks as black-box feature extractors.
Figure 3 shows some qualitative retrieval results, illustrating the efficacy of our approach. The system generally does retrieve relevant items from the database, while making reasonable mistakes such as confusing the sound of a zither with an acoustic guitar.
Objects that Sound
Query
Top 5 retrieved items
Fig. 3. Cross-modal and intra-modal retrieval. Each column shows one query and retrieved results. Purely for visualization purposes, as it is hard to display sound, the frame of the video that is aligned with the sound is shown instead of the actual sound form. The sound icon or lack of it indicates the audio or vision modality, respectively.
For example, the last column illustrates query by image into an audio database, thus answering the question "Which sounds are the most plausible for this query image?"
Note that many audio retrieval items are indeed correct despite the fact that their corresponding frames are unrelated – e.g. the audio of the blue image with white text does contain drums – this is just an artefact of how noisy real-world YouTube videos are.
Extending the AVE-Net to multiple frames
It is also interesting to investigate whether using information from multiple frames can help solving the AVC task. For these results only, we evaluate two modifications to the architecture from Figure 2a to handle a different visual input – multiple frames (AVE+MF) and optical flow (AVE+OF). For conciseness, the details of the architectures are explained in Appendix C, but the overall idea is that for AVE+MF we input 25 frames and convert convolution layers from
2D to 3D, while for AVE+OF we combine information from a single frame and 10 frames of optical flow using a two-stream network in the style of.
The performance of the AVE+MF and AVE+OF networks on the AVC task are 84.7% and 84.9%, respectively, compared to our single input image network's
81.9%. However, when evaluated on retrieval, they fail to provide a boost, e.g. the AVE+OF network achieves 0.608, 0.558, 0.588, and 0.665 for im-im, im-aud, aud-im and aud-aud, respectively; this is comparable to the performance of the R. Arandjelovi´c and A. Zisserman vanilla AVE-Net that uses a single frame as input (Table 1). One explanation of this underwhelming result is that, as is the case with most unsupervised approaches, the performance on the training objective is not necessarily in perfect correlation with the quality of learnt features and their performance on the task of interest. More specifically, the AVE+MF and AVE+OF could be using the motion information available at input to solve the AVC task more easily by exploiting some lower-level information (e.g. changes in the motion could be correlated with changes in sound, such as when seeing the fingers playing a guitar or flute), which in turn provides less incentive for the network to learn good semantic embeddings. For this reason, a single frame input is used for all other experiments.
Preventing shortcuts and Implementation
Preventing shortcuts. Deep neural networks are notorious for finding subtle data shortcuts to exploit in order to "cheat" and thus not learn to solve the task in the desired manner; an example is the misuse of chromatic aberration in to solve the relative-position task. To prevent such behaviour, we found it important to carefully implement the sampling of AVC negative pairs to be as similar as possible to the sampling of positive pairs. In detail, a positive pair is generated by sampling a random video, picking a random frame in that video, and then picking a 1 second audio with the frame at its mid-point. It is tempting to generate a negative pair by randomly sampling two different videos and picking a random frame from one and a random 1 second audio clip from the other.
However, this produces a slight statistical difference between positive and negative audio samples, in that the mid-point of the positives is always aligned with a frame and is thus at a multiple of 0.04 seconds (the video frame rate is 25fps), while negatives have no such restrictions. This allows a shortcut as it appears the network is able to learn to recognize audio samples taken at multiples of 0.04s, therefore distinguishing positives from negatives. It probably does so by exploiting low-level artefacts of MPEG encoding and/or audio resampling. Therefore, with this naive implementation of negative pair generation the network has less incentive to strongly learn semantically meaningful information.
To prevent this from happening, the audio for the negative pair is also sampled only from multiples of 0.04s. Without shortcut prevention, the AVE-Net achieves an artificially high accuracy of 87.6% on the AVC task, compared to
81.9% with the proper sampling safety mechanism in place, but the performance of the network without shortcut prevention on the retrieval task is consistently
1-2% worse. Note that, for fairness, we train the L3-Net with shortcut prevention as well.
The L3-Net training in does not encounter this problem due to performing additional data augmentation by randomly misaligning the audio and the frame by up to 1 second for both positives and negatives. We apply this augmentation as well, but our observation is important to keep in mind for future unsupervised approaches where exact alignment might be required, such as audio-visual synchronization.
Objects that Sound
Implementation details. We follow the same setup and implementation details as in. Namely, the input frame is a 224×224 colour image, while the 1 second of audio is resampled at 48 kHz, converted into a log-spectrogram (window length
0.01s and half-window overlap) and treated as a 257 × 200 greyscale image.
Standard data augmentation is used – random cropping, horizontal flipping and brightness and saturation jittering for vision, and random clip-level amplitude jittering for audio. The network is trained with cross-entropy loss for the binary classification task – whether the image and the audio correspond or not – using the Adam optimizer, weight decay 10−5, and learning rate obtained by grid search. Training is done using 16 GPUs in parallel with synchronous updates implemented in TensorFlow, where each worker processes a 128-element batch, thus making the effective batch size 2048.
Note that the only small differences from the setup of are that: (i) We use a stride of 2 pixels in the first convolutional layers as we found it to not affect the performance while yielding a 4× speedup and saving in GPU memory, thus enabling the use of 4× larger batches (the extra factor of 2× is through use of a better GPU); and (ii) We use a learning rate schedule in the style of where the learning rate is decreased by 6% every 16 epochs. With this setup we are able to fully reproduce the L3-Net results of, achieving even slightly better performance (+0.5% on the ESC-50 classification benchmark ), probably due to the improved learning rate schedule and the use of larger batches.
Localizing objects that sound
A system which understands the audio-visual world should associate appearance of an object with the sound it makes, and thus be able to answer "where is the object that is making the sound?" Here we outline an architecture and a training procedure for learning to localize the sounding object, while still operating in the scenario where there is no supervision, neither on the object location level nor on their identities. We again make use of the AVC task, and show that by designing the network appropriately, it is possible to learn to localize sounding objects in this extremely challenging label-less scenario.
In contrast to the standard AVC task where the goal is to learn a single embedding of the entire image which explains the sound, the goal in sound localization is to find regions of the image which explain the sound, while other regions should not be correlated with it and belong to the background. To operationalize this, we formulate the problem in the Multiple Instance Learning (MIL) framework. Namely, local region-level image descriptors are extracted on a spatial grid and a similarity score is computed between the audio embedding and each of the vision descriptors. For the goal of finding regions which correlate well with the sound, the maximal similarity score is used as the measure of the image-audio agreement. The network is then trained in the same manner as for the AVC task, i.e. predicting whether the image and the audio correspond.
For corresponding pairs, the method encourages one region to respond highly and therefore localize the object, while for mismatched pairs the maximal score
R. Arandjelovi´c and A. Zisserman
224x224x3
257x200x1 log-spectrogram
1 second 48kHz audio conv5 1x1
14x14x128 conv6 1x1
14x14x128
Image ConvNet
14x14x512
Audio ConvNet
16x12x512 pool4 16x12
1x1x512 fc1 512x128
128 fc2 128x128
128 all pairwise scalar products
14x14x1 sigmoid
14x14x1 conv7 1x1
14x14x1
Vision subnetwork
Audio subnetwork maxpool 14x14
Corresponds: yes/no?
Corresponds: where?
14x14 per-location correspondence scores
Fig. 4. Audio-Visual Object Localization (AVOL-Net). The notation and some building blocks are shared with Figure 2. The audio subnetwork is the same as in AVENet (Figure 2c). The vision network, instead of globally pooling the feature tensor, continues to operate at the 14 × 14 resolution, with relevant FCs (vision-fc1, visionfc2, fc3) converted into their "fully convolutional" equivalents (i.e. 1 × 1 convolutions conv5, conv6, conv7). The similarities between the audio and all vision embeddings reveal the location of the object that makes the sound, while the maximal similarity is used as the correspondence score. should be low thus making the entire score map low, indicating, as desired, there is no object which makes the input sound. In essence, the audio representation forms a filter which "looks" for relevant image patches in a similar manner to an attention mechanism.
Our Audio-Visual Object Localization Network (AVOL-Net) is depicted in Figure 4. Compared to the AVE-Net (Figure 2c), the vision subnetwork does not pool conv4 2 features but keeps operating on the 14 × 14 resolution. To enable this, the two fully connected layers fc1 and fc2 of the vision subnetwork are converted to 1 × 1 convolutions conv5 and conv6. Feature normalization is removed to enable features to have a low response on background regions.
Similarities between each of the 14 × 14 128-D visual descriptors and the single
128-D audio descriptor are computed via a scalar product, producing a 14 × 14 similarity score map. Similarly to the AVE-Net, the scores are calibrated using a tiny 1 × 1 convolution (fc3 converted to be "fully convolutional"), followed by a sigmoid which produces the localization output in the form of the imageaudio correspondence score for each spatial location. Max pooling over all spatial
Objects that Sound
11 locations is performed to obtain the final correspondence score, which is then used for training on the AVC task using the logistic loss.
Relation to previous works. While usually hinting at object localization, previous cross-modal works fall short from achieving this goal. Harwath et al. demonstrate localizing objects in the audio domain of a spoken text, but do not design their network for localization. In, the network, trained from scratch, internally learns object detectors, but has never been demonstrated to be able to answer the question "Where is the object that is making the sound?", nor, unlike our approach, was it trained with this ability in mind. Rather, their heatmaps are produced by examining responses of its various neurons given only the input image. The output is computed completely independently of the sound and therefore cannot answer "Where is the object that is making the sound?".
Our approach has similarities with and who used max and average pooling, respectively, to learn object detectors without bounding box annotations in the single visual modality setting, but use ImageNet pretrained networks and image-level labels. The MIL-based approach also has connections with attention mechanisms as it can be viewed as "infinitely hard" attention. Note that we do not use information from multiple audio channels which could aid localization because (i) this setup generally requires known calibration of the multi-microphone rig which is unknown for unconstrained YouTube videos, (ii) the number of channels changes across videos, (iii) quality of audio on YouTube varies significantly while localization methods based on multi-microphone information are prone to noise and reverberation, and (iv) we desire that our system learns to detect semantic concepts rather than localize by "cheating" through accessing multi-microphone information. Finally, a similar technique to ours appears in the concurrent work of, while later works of are also relevant.
Evaluation and results
First, the accuracy of the localization network (AVOL-Net) on the AVC task is the same as that of the AVE-Net embedding network in Section 3, which is encouraging as it means that switching to the MIL setup does not cause a loss in accuracy and the ability to detect semantic concepts in the two modalities.
The ability of the network to localize the object(s) that sound is demonstrated in Figure 5. It is able to detect a wide range of objects in different viewpoints and scales, and under challenging imaging conditions. A more detailed discussion including the analysis of some failure cases is available in the figure caption. As expected from an unsupervised method, it is not necessarily the case that it detects the entire object but can focus only on specific discriminative parts such as the interface between the hands and the piano keyboard. This interacts with the more philosophical question of what is an object and what is it that is making the sound – the body of the piano and its strings, the keyboard, the fingers on the keyboard, the whole human together with the instrument, or the R. Arandjelovi´c and A. Zisserman
Fig. 5. What is making the sound? Localization output of the AVOL-Net on the unseen test data; see Figure 1 and https://goo.gl/JVsJ7P for more. Recall that the network sees a single frame and therefore cannot "cheat" by using motion information.
Each pair of images shows the input frame (left) and the localization output for the input frame and 1 second of audio around it, overlaid over the frame (right). Note the wide range of detectable objects, such as keyboards, accordions, drums, harps, guitars, violins, xylophones, people's mouths, saxophones, etc. Sounding objects are detected despite significant clutter and variations in lighting, scale and viewpoint. It is also possible to detect multiple relevant objects: two violins, two people singing, and an orchestra. The final row shows failure cases, where the first two likely reflects the noise in the training data as many videos contain just music sheets or text overlaid with music playing, in columns 3-4 the network probably just detects the salient parts of the scene, while in columns 5-6 it fails to detect the sounding objects. entire orchestra? How should a gramophone or a radio be handled by the system, as they can produce arbitrary sounds?
From the impressive results in Figure 5, one question that comes to mind is whether the network is simply detecting the salient object in the image, which is not the desired behaviour. To test this hypothesis we can provide mismatched frame and audio pairs as inputs to interrogate the network to answer "what would make this sound?", and check if salient objects are still highlighted regardless of the irrelevant sound. Figure 6 shows that this is indeed not the case, as when, for example, drums are played on top of an image of a violin, the localization map is empty. In contrast, when another violin is played, the network highlights the violin. Furthermore, to completely reject the saliency hypothesis – in the case of an image depicting a piano and a flute, it is possible to play a flute sound and the network will pick the flute, while if a piano is played, the piano is highlighted in the image. Therefore, the network has truly learnt to disentangle
Objects that Sound
Fig. 6. What would make this sound? Similarly to Figure 5, the AVOL-Net localization output is shown given an input image frame and 1s of audio. However, here the frame and audio are mismatched. Each triplet of images shows the (left) input audio, (middle) input frame, and (right) localization output overlaid over the frame. Purely for visualization purposes, as it is hard to display sound, the frame of the video that is aligned with the sound is shown instead of the actual sound form (left). On the example of the first triplet: (left) flute sound illustrated by an image of a flute, (middle) image of a piano and a flute, (right) the flute from the middle image is highlighted as our network successfully answers the question "What in the piano-flute image would make a flute sound?" In each row the input frame is fixed while the input audio varies, showing that object localization does depend on the sound and therefore our system is not just detecting salient objects in the scene but is achieving the original goal – localizing the object that sounds. multiple objects in an image and maintain a discriminative embedding for each of them.
To evaluate the localization performance quantitatively, 500 clips are sampled randomly from the validation data and the middle frame annotated with the localization of the instrument producing the sound. We then compare two methods of predicting the localization (as in ): first, a baseline method that always predicts the center of the image; second, the mode of the AVOL-Net heatmap produced by inputting the sound of the clip. The baseline achieves 57.2%, whilst
AVOL-Net achieves 81.7%. This demonstrates that the AVOL-NET is not simply highlighting the salient object at the center of the image. Failure cases are mainly due to the problems with the AudioSet dataset described in Section 2.
Note, it is necessary to annotate the data, rather than using a standard benchmark, since datasets such as PASCAL VOC, COCO, DAVIS, KITTI, do not contain musical instruments. This also means that off-the-shelf object detectors for instruments are not available, so could not be used to annotate AudioSet frames with bounding boxes.
Finally, Figure 7 shows the localization results on videos. Note that each video frame and surrounding audio are processed completely independently, so no motion information is used, nor is there any temporal smoothing. The results reiterate the ability of the system to detect an object under a variety of poses, R. Arandjelovi´c and A. Zisserman
Fig. 7. What is making the sound? The visualization is the same as for Figure 5 but here each column contains frames from a single video, taken 1 second apart. The frames are processed completely independently, motion information is not used, nor there is any temporal smoothing. Our method reliably detects the sounding object across varying poses (columns 1-2), and shots (column 3). Furthermore, it is able to switch between objects that are making the sound such as interleaved speech and guitar during a guitar lesson (column 4). and to highlight different objects depending on the varying audio context. Please see this YouTube playlist (https://goo.gl/JVsJ7P) for more video results.
Conclusions and future work
We have demonstrated that the unsupervised audio-visual correspondence task enables, with appropriate network design, two entirely new functionalities to be learnt: cross-modal retrieval, and semantic based localization of objects that sound. The AVE-Net was shown to perform cross-modal retrieval even better than supervised baselines, while the AVOL-Net exhibits impressive object localization capabilities. Potential improvements could include modifying the AVOLNet to have an explicit soft attention mechanism, rather than the max-pooling used currently.
Acknowledgements. We thank Carl Doersch for useful insights regarding preventing shortcuts.
Bibliography
 Aytar, Y., Vondrick, C., Torralba, A.: SoundNet: Learning sound representations from unlabeled video. In: NIPS. (2016)
 Harwath, D., Torralba, A., Glass, J.R.: Unsupervised learning of spoken language with visual context. In: NIPS. (2016)
 Owens, A., Jiajun, W., McDermott, J., Freeman, W., Torralba, A.: Ambient sound provides supervision for visual learning. In: Proc. ECCV. (2016)
 Arandjelovi´c, R., Zisserman, A.: Look, listen and learn. In: Proc. ICCV.
 Barnard, K., Duygulu, P., de Freitas, N., Forsyth, D., Blei, D., Jordan, M.:
Matching words and pictures. JMLR 3 (Feb 2003) 1107–1135
 Duygulu, P., Barnard, K., de Freitas, J.F.G., Forsyth, D.A.: Object recognition as machine translation: Learning a lexicon for a fixed image vocabulary.
In: Proc. ECCV. (2002)
 Frome, A., Corrado, G.S., Shlens, J., Bengio, S., Dean, J., Ranzato, M.A., Mikolov, T.: Devise: A deep visual-semantic embedding model. In: NIPS.
 Xu, K., Ba, J., Kiros, R., Courville, A., Salakhutdinov, R., Zemel, R., Bengio, Y.: Show, attend and tell: Neural image caption generation with visual attention. arXiv preprint arXiv:1502.03044 (2015)
 de Sa, V.R.: Learning classification from unlabelled data. In: NIPS. (1994)
 Kidron, E., Schechner, Y.Y., Elad, M.: Pixels that sound. In: Proc. CVPR.
 Owens, A., Isola, P., McDermott, J.H., Torralba, A., Adelson, E.H., Freeman, W.T.: Visually indicated sounds. In: Proc. CVPR. (2016) 2405–2413
 Aytar, Y., Vondrick, C., Torralba, A.: See, hear, and read: Deep aligned representations. CoRR abs/1706.00932 (2017)
 Dosovitskiy, A., Springenberg, J.T., Riedmiller, M., Brox, T.: Discriminative unsupervised feature learning with convolutional neural networks. In:
NIPS. (2014)
 Doersch, C., Gupta, A., Efros, A.A.: Unsupervised visual representation learning by context prediction. In: Proc. CVPR. (2015)
 Agrawal, P., Carreira, J., Malik, J.: Learning to see by moving. In: Proc.
ICCV. (2015)
 Wang, X., Gupta, A.: Unsupervised learning of visual representations using videos. In: Proc. ICCV. (2015) 2794–2802
 Zhang, R., Isola, P., Efros, A.A.: Colorful image colorization. In: Proc.
ECCV, Springer (2016) 649–666
 Misra, I., Zitnick, C.L., Herbert, M.: Shuffle and learn: Unsupervised learning using temporal order verification. In: Proc. ECCV. (2016)
 Pathak, D., Kr¨ahenb¨uhl, P., Donahue, J., Darrell, T., Efros, A.A.: Context encoders: Feature learning by inpainting. In: Proc. CVPR. (2016) 2536–2544
R. Arandjelovi´c and A. Zisserman
 Noroozi, M., Favaro, P.: Unsupervised learning of visual representations by solving jigsaw puzzles. In: Proc. ECCV. (2016)
 Fernando, B., Bilen, H., Gavves, E., Gould, S.: Self-supervised video representation learning with odd-one-out networks. In: Proc. ICCV. (2017)
 Doersch, C., Zisserman, A.: Multi-task self-supervised visual learning. In:
Proc. ICCV. (2017)
 Gemmeke, J.F., Ellis, D.P.W., Freedman, D., Jansen, A., Lawrence, W., Moore, R.C., Plakal, M., Ritter, M.: Audio Set: An ontology and humanlabeled dataset for audio events. In: ICASSP. (2017)
 Ioffe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by reducing internal covariate shift. In: Proc. ICML. (2015)
 Arandjelovi´c, R., Gronat, P., Torii, A., Pajdla, T., Sivic, J.: NetVLAD:
CNN architecture for weakly supervised place recognition.
IEEE PAMI
 Chopra, S., Hadsell, R., LeCun, Y.: Learning a similarity metric discriminatively, with application to face verification. In: Proc. CVPR. Volume 1., IEEE (2005) 539–546
 Wang, L., Li, Y., Lazebnik, S.: Learning deep structure-preserving imagetext embeddings. In: Proc. CVPR. (2016)
 Hong, S., Im, W., S. Yang, H.:
CBVMR: Content-Based Video-Music
Retrieval Using Soft Intra-Modal Structure Constraint. In: ACM ICMR.
 Simonyan, K., Zisserman, A.: Very deep convolutional networks for largescale image recognition. In: International Conference on Learning Representations. (2015)
 Simonyan, K., Zisserman, A.: Two-stream convolutional networks for action recognition in videos. In: NIPS. (2014)
 Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In:
Proc. ICLR. (2015)
 Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., Rabinovich, A.: Going deeper with convolutions. In:
Proc. CVPR. (2015)
 Piczak, K.J.: ESC: Dataset for environmental sound classification. In: Proc.
ACMM. (2015)
 Dietterich, T.G., Lathrop, R.H., Lozano-Perez, T.:
Solving the multiple instance problem with axis-parallel rectangles. Artificial Intelligence 89(12) (1997) 31–71
 Oquab, M., Bottou, L., Laptev, I., Sivic, J.: Is object localization for free? weakly-supervised learning with convolutional neural networks. In: Proc.
CVPR. (2015)
 Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., Torralba, A.: Learning deep features for discriminative localization. In: Proc. CVPR. (2016)
 Bahdanau, D., Cho, K., Bengio, Y.: Neural machine translation by jointly learning to align and translate. Proc. ICLR (2015)
 Shivappa, S.T., Rao, B.D., Trivedi, M.M.: Audio-visual fusion and tracking with multilevel iterative decoding: Framework and experimental evaluation.
IEEE Journal of Selected Topics in Signal Processing (2010)
Objects that Sound
 Senocak, A., Oh, T.H., Kim, J., Yang, M.H., Kweon, I.S.:
On learning association of sound source and visual scenes. In: Proc. CVPR. (2018)
 Zhao, H., Gan, C., Rouditchenko, A., Vondrick, C., McDermott, J., Torralba, A.: The sound of pixels. In: Proc. ECCV. (2018)
 Owens, A., Efros, A.A.: Audio-visual scene analysis with self-supervised multisensory features. In: Proc. ECCV. (2018)
 Zach, C., Pock, T., Bischof, H.:
A duality based approach for realtime
TV-L1 optical flow. Pattern Recognition (2007)
A
AudioSet-Instruments
Throughout the paper we use the publicly available AudioSet dataset (Section 2). It contains video-level audio class labels (potentially more than 1 per video) which are organized in an ontology; recall that no labels are used for training, just for evaluation. This section describes the AudioSet-Instruments subset we use, and further details needed for evaluation of retrieval performance.
A.1
Classes
To make the dataset more manageable and interesting for our purposes, we filter it for sounds of musical instruments, singing and tools, i.e. we use all videos which contain at least one label that is a descendant of one of those three classes according in the AudioSet ontology. This yields the following 110 audio classes:
Accordion; Acoustic guitar; Alto saxophone; Bagpipes; Banjo; Bass (instrument role); Bass drum; Bass guitar; Bassoon; Bell; Bicycle bell; Bowed string instrument; Brass instrument; Bugle; Cello; Change ringing (campanology); Chant;
Child singing; Chime; Choir; Church bell; Clarinet; Clavinet; Cornet; Cowbell; Crash cymbal; Cymbal; Dental drill, dentist's drill; Didgeridoo; Double bass; Drill; Drum; Drum kit; Drum machine; Drum roll; Electric guitar; Electric piano; Electronic organ; Female singing; Filing (rasp); Flute; French horn;
Glockenspiel; Gong; Guitar; Hammer; Hammond organ; Harmonica; Harp; Harpsichord; Hi-hat; Jackhammer; Jingle bell; Keyboard (musical); Male singing;
Mallet percussion; Mandolin; Mantra; Maraca; Marimba, xylophone; Mellotron;
Musical ensemble; Musical instrument; Oboe; Orchestra; Organ; Percussion; Piano; Pizzicato; Plucked string instrument; Power tool; Rapping; Rattle (instrument); Rhodes piano; Rimshot; Sampler; Sanding; Sawing; Saxophone; Scratching (performance technique); Shofar; Singing; Singing bowl; Sitar; Snare drum;
Soprano saxophone; Steel guitar, slide guitar; Steelpan; String section; Strum;
Synthesizer; Synthetic singing; Tabla; Tambourine; Tapping (guitar technique);
Theremin; Timpani; Tools; Trombone; Trumpet; Tubular bells; Tuning fork;
Ukulele; Vibraphone; Violin, fiddle; Wind chime; Wind instrument, woodwind instrument; Wood block; Yodeling; Zither.
R. Arandjelovi´c and A. Zisserman
A.2
Relevance
As described in Section 3.1, the AudioSet ontology is taken into account when evaluating the retrieval performance, as, for example, an ideal system should rank the 'electric guitar' higher than 'drums' when querying with an 'acoustic guitar'.
We use the standard evaluation metric for this scenario where retrieved results have varying relevance – the normalized discounted cumulative gain (nDCG).
Here, we define the relevance between of one video to another. Recall that AudioSet contains only video-level labels and that videos generally have multiple labels. Therefore, we first define the relevance of individual classes, followed by the definition of the video (i.e. set of classes) relevance.
Class relevance. An appropriate measure of distance between two classes organized in an ontology is the tree distance, d, i.e. the length of the shortest path between the two classes. For example, the distances between 'acoustic guitar' and 'acoustic guitar', 'electric guitar', and 'drums' are 0, 2 and 5, respectively.
The relevance of one class to another is then defined as the negative of their tree distance, but offset by a constant to make sure relevances are not negative.
Specifically, relevance is computed as r = C − d, where C = 20 as this is the longest possible distance between two classes.
Video relevance. Since videos generally contain multiple labels, we define the relevance of one video to another as the maximal relevance across all pairs of classes in the two videos. The motivation behind using the maximal relevance, as opposed to for example the minimal or the average, is that AudioSet labels are only provided on the video-level. Since we use only single frames or 1 second audio clips throughout, it is not guaranteed that these contain all of the video classes (in fact they could even contain none), so using a measure other than the maximal relevance would over-penalize perfectly relevant results. For example, consider the case of a video which has a person 'singing' followed by an 'electric guitar', and imagine we use a frame from the second half of the video as a query. The ground truth only tells us that there is 'singing' and 'electric guitar' somewhere in the video, so we do not know which one of the two (if any) does the frame depict. Therefore, retrieving a video which contains 'electric guitar' without 'singing' is a perfectly acceptable result.
B
Initialization for the AVE-Net
In its vanilla form, there is actually nothing forcing the network to make the distances between corresponding features small and non-corresponding large – it could equally learn anti-aligned embeddings where a large distance between the visual and audio features signifies high similarity. To stimulate the desired behaviour where small distance means large similarity, one simply needs to enforce the correct sign of the weights in the tiny fc3 layer. We found it to be sufficient to just initialize the layer with weights of the correct sign and not enforce this during training.
Objects that Sound
19 conv4_1 3x3
14x14x512 conv4_2 3x3
14x14x512 conv. concat
14x14x512 conv1_1 3x3 stride 2
112x112x64 conv1_2 3x3
112x112x64 pool1 2x2
56x56x64 conv2_1 3x3
56x56x128 conv2_2 3x3
56x56x128 pool2 2x2
28x28x128 conv3_1 3x3
28x28x256 conv3_2 3x3
28x28x256 pool3 2x2
14x14x256
224x224x(10x2) conv1_1 3x3 stride 2
112x112x64 conv1_2 3x3
112x112x64 pool1 2x2
56x56x64 conv2_1 3x3
56x56x128 conv2_2 3x3
56x56x128 pool2 2x2
28x28x128 conv3_1 3x3
28x28x256 conv3_2 3x3
28x28x256 pool3 2x2
14x14x256
224x224x3
Vision-Frame subnetwork
Vision-Flow subnetwork
Fig. 8. AVE+OF: Vision ConvNet. The notation and some building blocks are shared with Figure 2. The vision subnetwork of the AVE+OF network is a two-stream network, where the image and flow streams are processed independently with 3 conv-conv-pool blocks each, followed by concatenating their outputs in the 'channel' dimension, and passing through another conv-conv block. The image is a single RGB frame, while there are 10 frames of flow (concatenated in the 'channel' dimension) where each spatial location contains a 2-D vector of horizontal and vertical displacements.
C
AVE+OF architecture
Section 3.2 of the main paper discusses versions of the AVE-Net that use multiple frames as input. Here we give details of the better performing network, AVE+OF, which, along with a frame and 1 second of audio, ingests 10 frames of optical flow as well (computed using the TV-L1 algorithm ). The network follows the same architecture as the AVE-Net shown in Figure 2d of the main paper, but with the vision subnetwork (input: single RGB frame) replaced with the network shown in Figure 8 (input: single RGB frame and 10 optical flow frames). The new vision subnetwork is a two-stream architecture, i.e. the frame and flow streams are fused by concatenation followed by two convolutional layers. The output of this network has the same dimensions as the original vision ConvNet
R. Arandjelovi´c and A. Zisserman(Figure 2a of the main paper), and is therefore readily pluggable into the AVENet architecture (Figure 2d of the main paper).
D
Additional AVE-Net results
Figures 9 and 10 complement Section 3.1, and contain additional cross-modal retrieval results, further demonstrating the superiority of AVE-Net versus all baselines.
Dimensionality
0.58 nDCG@30
Image-Audio
AVE-Net
L^3-Net with CCA
VGG-ImageNet + L^3-Audio CCA
Fig. 9. Cross-modal retrieval vs embedding dimensionality. Comparison of our method with baselines in terms of the average nDCG@30 on the AudioSet-Instruments test set. Our AVE-Net beats all baselines regardless for all sizes of baseline embeddings.
Note that CCA does not necessarily work better with increased dimensionality due to denoising properties of dimensionality reduction.
Number of retrieved items K
0.60 nDCG@K
Image-Audio
AVE-Net
L^3-Net with CCA
VGG-ImageNet + L^3-Audio CCA
L^3-Net
Number of retrieved items K
0.65 nDCG@K
Audio-Image
AVE-Net
L^3-Net with CCA
VGG-ImageNet + L^3-Audio CCA
L^3-Net
Fig. 10. Cross-modal retrieval. Comparison of our method with baselines in terms of the average nDCG@K for various values of K, on the AudioSet-Instruments test set.
Our AVE-Net beats all baselines for all K.