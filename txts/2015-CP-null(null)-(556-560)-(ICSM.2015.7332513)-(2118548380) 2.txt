Exploring the Use of Deep Learning
for Feature Location
Christopher S. Corley
The University of Alabama
Tuscaloosa, AL, USA
cscorley@ua.edu
Kostadin Damevski
Virginia Commonwealth University
Richmond, VA, USA
damevski@acm.org
Nicholas A. Kraft
ABB Corporate Research
Raleigh, NC, USA
nicholas.a.kraft@us.abb.com
Abstract—Deep learning models can infer complex patterns
present in natural language text. Relative to n-gram models, deep
learning models can capture more complex statistical patterns
based on smaller training corpora. In this paper we explore the
use of a particular deep learning model, document vectors (DVs),
for feature location. DVs seem well suited to use with source code,
because they both capture the influence of context on each term in
a corpus and map terms into a continuous semantic space that
encodes semantic relationships such as synonymy. We present
preliminary results that show that a feature location technique
(FLT) based on DVs can outperform an analogous FLT based
on latent Dirichlet allocation (LDA) and then suggest several
directions for future work on the use of deep learning models to
improve developer effectiveness in feature location.
Index Terms—deep learning; neural networks; document vec-
tors; feature location
I. INTRODUCTION
When starting a maintenance task, software developers
commonly need to locate the relevant features in a potentially
large and unfamiliar code base. Due to the difficulty and
importance of this task, researchers have proposed a number
of approaches to improve developers' effectiveness in locating
features, largely based upon applying natural language analysis
or text retrieval techniques to source code [1]. Most of the
proposed feature location techniques have treated source code
as an unordered set of natural language terms (i.e., as a bag-
of-words), even though recent fundamental results have shown
that source code contains context and flow that is even more
pronounced than natural language text [2].
In this paper, we explore the use of deep learning, a particular
class of neural networks that has shown promising results in
modeling natural language [3], for feature location. In particular,
we investigate the efficacy of document vectors [4] (DVs).
DVs capture the influence of the surrounding context, and
its order, on each term, which can improve the ranking of
results retrieved for a developer query [5]. For example, in
the statement diagram.redraw() the word diagram is relevant
to the word redraw and this relationship is captured by DVs.
Therefore, when querying for diagram, program elements where
redraw is also present are considered more relevant and thus
are boosted in the rankings.
Deep learning models such as DVs also create a novel
notion of semantic similarity between the source code terms.
Semantic similarity is the result of mapping the corpus terms
into a continuous semantic space, where synonyms, antonyms,
and other semantic relations are encoded and easily composed
together [4].
In our preliminary evaluation, we compare a feature location
technique (FLT) based on DVs to an FLT based on Latent
Dirichlet Allocation (LDA) using the benchmark by Dit et
al. [6]. LDA is a popular and promising approach for modeling
natural language text in source code [7]. The benchmark
comprises 633 features from six versions of four open source
Java projects, and our results show that for many of the features,
the DV-based FLT outperforms the LDA-based FLT. Our results
also show that less time is needed for model training and
inference in the DV-based FLT as compared to the LDA-
based FLT. We also suggest directions for future work on
the use of DVs (or other deep learning models) to improve the
effectiveness of feature location engines.
II. BACKGROUND
Feature location systems retrieve a ranked list of program
elements (e.g., methods or classes) for a developer query. In
the training phase, feature location systems construct a model
of the software, at the granularity of program elements, based
on the natural language embedded in identifiers and comments.
In the retrieval phase, given a natural language query, feature
location systems use the model to retrieve the relevant program
elements with high similarity to the query.
A. Feature Location Workflow
A feature location system based on deep learning, during its
training phase, creates a contextual representation of the natural
language terms embedded in the source code. This contextual
representation includes influence from terms preceding and
following each term, relative to their distance from that term.
More intuitively, such models incorporate mutual influence
between terms in the same method, while terms that are closer
in distance (e.g. occur the same statement) influence each other
more strongly.
Deep learning is based on a multi-stage neural network,
consisting of several hidden layers in addition to single input
and output layers. The input layer consists of an ordered
sequences of identifiers extracted from the code. The multiple
hidden layers serve to capture the context for each encountered
term, representing the complex patterns of term contexts
978-1-4673-7532-0/15 c⃝ 2015 IEEE
ICSME 2015, Bremen, Germany
Accepted for publication by IEEE. c
⃝ 2015 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/
republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.
556
[1,0,0,0]
interface DiagramNode extends DiagramElement {
diagram
node
diagram
element
hidden layer(s)
diagram
node
element
[-4,2,7,
-3,-1,9]
[1,-2,6
8,8,-6]
[9,5,0
7,-9,1]
[0,1,0,0]
[0,0,1,0]
[0,0,0,1]
Fig. 1: A deep learning neural network can encode source code
identifiers, in the order they appear in the source code, in its input
layer. Using a deep structure of hidden layers, each term and its
context receives a semantic vector representation. The output layer
consists of vector for each term in the corpus; the vector feature size
is arbitrary and does not need to relate to the number of terms in the
corpus.
occurring in the corpus. The output layer consists of a vector for
each term, which has been shown to carry semantic meaning.
An example of this architecture for a single line of code is
shown in Figure 1. Recent advances in this area have stemmed
from the use of novel neural network architectures, including
recurrent neural networks that connect the hidden layers back
to the input layer, among other strategies. The systems are
trained using backpropagation and gradient descent, techniques
common to many neural network based models.
An extension to learning the semantic vector representation
of words is the use of an additional vector that will encode
the representation of a larger body of text, such as a paragraph
or an entire document [4]. While comparing word vectors
indicates semantic relations between two terms, comparing
two document vectors carries a similar semantic connotation
at the document level. For instance, the approach has been
applied for determining the sentiment (i.e. positive, negative)
of reviews on a popular movie recommendation site.
A number of preprocessing steps are commonly performed
before the training phase of feature location systems. The steps
commonly used are [8], [9]:
• Splitting: separate tokens into constituent words based on
common coding style conventions (e.g., the use of camel
case or underscores) and on the presence of non-letters
(e.g., punctuation or digits)
• Normalizing: replace each upper case letter with the
corresponding lower case letter
• Filtering: remove common words such as articles (e.g.,
'an' or 'the'), programming language keywords, standard
library entity names, or short words
During the retrieval state of feature location, a similarity
measure (e.g. cosine similarity) between the words in the
query and words in each program element is computed. The
program elements are ranked based on this similarity metric
and presented to the developer in descending order.
If document vectors are used then a special inference process
is needed to infer a vector representation for the entire query,
treated as a document in the corpus. Following this, the vectors
of the query and each program element (i.e. method or class)
can be compared to produce the final ranking.
B. Semantic Similarity
The result of the deep learning models described in this
paper are vectors representing each term in the corpus [3].
Similar vectors can also be computed to represent an entire
paragraph or document [4]. Semantic similarity is the notion
that these vectors are composable semantically, e.g., the result
of the operation vec("The Eiffel Tower") - vec("Paris") +
vec("London") is closest to vec("Big Ben"). This capability is
established automatically by the system, without any additional
processing or supervised input.
In feature location, as in general information retrieval, the
retrieval quality can only be as good as the quality of the
developer query. Problems such as the dictionary mismatch
problem, as well as the propensity of users to issue short queries,
have previously been observed as common difficulties in using
feature location tools in the field [10], [11]. Semantic similarity
can be a useful capability in mitigating these problems, by
performing query recommendation that allows the user to
extend his or her queries with terms from the same corpus, or
automatic query extension.
To illustrate this capability for source code-based corpora, we
provide a set of illustrative examples gathered on the ArgoUML
v0.22 in Table I. In many cases semantic similarity provides
reasonable results for similar terms, though exceptions exist
largely due to the limited appearance of certain words in the
corpus. We anticipate that larger corpora, based on larger code
bases, or leveraging related code bases or documents, could
improve the results of semantic similarity even further.
III. PRELIMINARY STUDY
In this section we describe the design of a study in which
we compare a deep-learning-based FLT to a baseline topic-
modeling-based FLT. In particular, we use a DV-based FLT
and an LDA-based FLT, respectively.
A. Subject software systems
We employ the dataset of six software systems by Dit et
al. [6]. The dataset contains 633 queries for method-level
goldsets, as seen in Table II, and was automatically extracted
from changesets that relate to the queries (issue reports).
ArgoUML is a UML diagramming tool1. jEdit is a text
editor2. JabRef is a BibTeX bibliography management tool3.
muCommander is a cross-platform file manager4.
1 http://argouml.tigris.org/
2 http://www.jedit.org/
3 http://jabref.sourceforge.net/
4 http://www.mucommander.com/
557
TABLE I: Examples of semantically similar terms and their weight for a deep model trained on the ArgoUML code base. Only terms with
weight >= 0.6 are included.
Term(s)
Semantically Similar Terms and Weight
save
(saved 0.69), (pcs 0.64), (exists 0.63), (projects 0.60), (close 0.61), (file 0.60)
file
(filter 0.78), (zip 0.74), (exists 0.74), (persister 0.71), (files 0.69), (directory 0.69)
file + save
(exists 0.77), (saved 0.74), (filter 0.73), (zip 0.72), (unable 0.67), (projects 0.67), (persister 0.67),
(files 0.66), (cant 0.66), (scheme 0.65)
explorer + diagrams - creating
(nodes 0.71), (deletion 0.67), (perspectives 0.65), (perspective 0.60), (updated 0.60), (modified
0.60)
TABLE II: Subject System Sizes and Queries
Subject System
Methods
Queries
ArgoUML v0.22
12353
91
ArgoUML v0.24
13064
52
ArgoUML v0.26.2
16880
209
Jabref v2.6
5357
39
jEdit v4.3
7305
150
muCommander v0.8.5
8799
92
Total
63758
633
B. Setting
We implemented our approach in Python v2.7 using the topic
modeling library, Gensim [12] and our ANTLR v3 Java-based
tool, Teaser5, for parsing source code.
To build our corpora, we extract the documents representing
methods from every Java file in the snapshot. The text of
an inner method (e.g., a method inside an anonymous class)
is only attributed to that method, and not the containing
one. Comments, literals, and identifiers within a method are
considered as text of the method. Block comments immediately
preceding an method are also included in this text.
After extracting documents and tokenizing, we split the
tokens based on camel case, underscores, and non-letters. We
only keep the split tokens; original tokens are discarded. We
normalize to lower case before filtering non-letters, English
stop words [13], Java keywords, and words shorter than three
characters long. We do not stem words. Here, we must be
careful to not lose ordering of the words, as this is crucial for
the deep learning approach.
For LDA, the approach is straightforward. We train the LDA
model and query it using it's built-in inference methods.
In Gensim, the DV deep learning model is known as
Doc2Vec. We train this DV model on the corpora, and
query it to obtain rankings of methods related to the query.
Because DV is a neural network with multiple layers (i.e., the
document vector layer and the word vector layer), there are
two approaches for measuring document similarity.
For the document-based layer, we can infer the document
vector of the query and perform pair-wise similarity of it to
the each method's document vector within the model. We
also found it useful to consider the word-based layer. For this,
we can get the word vector for each word in the query and
sum them. We then take the query's summed vector and also
5 https://github.com/nkraft/teaser
TABLE III: Model training times for 100 topics
LDA
DV
ArgoUML v0.22
0m 58.207s
0m 02.070s
ArgoUML v0.24
1m 05.507s
0m 02.267s
ArgoUML v0.26.2
1m 21.176s
0m 02.736s
JabRef v2.6
0m 29.504s
0m 01.280s
jEdit v4.3
0m 36.701s
0m 01.519s
muCommander v0.8.5
0m 42.897s
0m 01.696s
perform pair-wise similarity to each document vector in the
model.
Regarding configuration of the two models, we choose
various sizes of K for each, where K for LDA is the number
of topics and for DV is the number of features (i.e., document
vector length).
C. Data Collection and Analysis
To evaluate the performance of an FLT we cannot use
measures such as precision and recall. This is because the
FLT creates the rankings pairwise, causing every entity being
searched to appear in the rankings. Similar to the study of
Poshyvanyk et al. [14], for an effectiveness measure we use
the rank of the first relevant document, which represents the
number of source code entities a developer would have to view
before reaching a relevant one. The Mean Reciprocal Rank
(MRR) [15] is defined as:
MRR =
1
|Q|
|Q|
�
i=1
1
ei
(1)
where Q is the set of queries and ei is the effectiveness measure
for some query Qi.
We also want to explore the differences in terms of
computational overhead for using each model. We collect
information such as time taken to train the model and the
time taken to process the goldsets, i.e., the average time to
query and rank each subject system. We train and query all
models on a system running ArchLinux with an Intel Core-i7
4GHz processor and 32 GB of memory.
D. Results and Discussion
Table V summarizes the results of each subject system for
each model and query approach. We bold which of the three
approaches is greatest for each value K at steps of 100. To
the right of Table V is Figure 2, a graphical view of the all
values of K at steps of 25.
558
TABLE V: MRR
System
Approach
100
200
300
400
500
LDA
0.0175
0.0295
0.0271
0.0611
0.0220
ArgoUML v0.22
DV Inference
0.0115
0.0105
0.0096
0.0184
0.0162
DV Summation
0.0775
0.0570
0.0625
0.0587
0.0601
LDA
0.0441
0.0373
0.0655
0.0779
0.0344
ArgoUML v0.24
DV Inference
0.0246
0.0152
0.0260
0.0258
0.0380
DV Summation
0.0827
0.0906
0.0874
0.0691
0.0942
LDA
0.0493
0.0628
0.0857
0.0703
0.0811
ArgoUML v0.26.2
DV Inference
0.0404
0.0218
0.0290
0.0364
0.0403
DV Summation
0.0847
0.0890
0.0813
0.0834
0.0805
LDA
0.0055
0.0364
0.1304
0.0781
0.0548
JabRef v2.6
DV Inference
0.0262
0.0463
0.0318
0.0289
0.0234
DV Summation
0.0450
0.0373
0.0455
0.0382
0.0428
LDA
0.0670
0.0432
0.0641
0.0693
0.0607
jEdit v4.3
DV Inference
0.0341
0.0282
0.0369
0.0354
0.0450
DV Summation
0.0872
0.0791
0.0825
0.0814
0.0679
LDA
0.0392
0.0217
0.0198
0.0559
0.0329
muCommander v0.8.5
DV Inference
0.0977
0.0771
0.0800
0.0665
0.0838
DV Summation
0.0652
0.0623
0.0703
0.0606
0.0538
100
200
300
400
DV Inf.
DV Sum.
LDA
Fig. 2: MRR
TABLE IV: Average time to rank per query for 100 topics
LDA
DV Inf.
DV Sum.
ArgoUML v0.22
0.943362s
0.225868s
2.118835s
ArgoUML v0.24
1.686980s
0.259615s
1.824923s
ArgoUML v0.26.2
0.744468s
0.300956s
2.715062s
JabRef v2.6
0.957000s
0.124128s
0.720589s
jEdit v4.3
0.405700s
0.134080s
1.017060s
muCommander v0.8.5
0.685543s
0.165663s
1.056108s
There are a handful of clear trends. For example, the
ArgoUML projects perform similarily across each version. For
all three versions, the DV word vector summation performs
best for many values of K, with few exceptions in favor of
LDA. Interestingly, DV inference has the worst performance
across all three versions. This trend continues with JabRef
and jEdit, but does not for muCommander. Surprisingly, DV
inference always performs best for muCommander.
A second trend is how few features (K) DV needs to perform
well. As shown in Figure 2, many of the projects achieve high
DV summation performance by K = 100, and plateau after
that. By contrast, LDA can require as many as 300 to 500
topics. This observation is noteworthy because fewer topics
generally implies less training time.
Tables III and IV summarize the time taken to train each
model on the corpus and the average query and rank time for
all queries in the goldset.
Little time is required to train a DV model — training
finishes in under 3 seconds for all systems. On the other hand,
training an LDA model can take up to about 90 seconds. The
average time to query and rank all queries in the goldset shows
that DV inference is fastest, while the word vector summation
is on par or worse than LDA.
IV. RELATED WORK
Statistical natural language models, such as the n-gram
model, have seen widespread use for a variety of natural
language processing tasks due to their simplicity and effective-
ness when trained with a substantial corpus of text. Software
engineering researchers have shown n-gram models to be
even more effective for source code than for natural language
documents [2].
Recent research in natural language modeling has introduced
deep learning methods, consisting of specific classes and
architectures of neural networks, that can produce capable
statistical models of natural language text able to capture
more complex patterns while being trained using smaller
corpora relative to the n-gram model [3], [4]. Using a set of
optimizations strategies such models can be built at reasonable
computational costs. In this paper we examine the effectiveness
and potential applicability of these deep learning models for
the problem of feature location in software engineering.
White et al. [16] reported promising results in applying deep
learning models for source code, outperforming models based
on similar n-gram configurations on code completion tasks.
This work establishes the use of deep learning for software
engineering problems, sketching out several avenues for future
use of such models, including code completion and the building
of synonym dictionaries, among others. Our work explores
these models for the feature location problem during software
maintenance. A notable difference to the prior experiments
conducted by White et al. is that the deep models in this paper
are based solely on terms found in identifiers and comments
559
in the source code of a single software project, while White et
al. considered a larger corpus consisting of the entire source
listing of several combined projects.
Previous feature location techniques that incorporated sur-
rounding context in the model have shown high degrees of
effectiveness. Examples include the use of verb and direct
object pairs [17] and statement level markov random fields [5].
Deep learning approaches allow the inclusion of broader context
than these previous approaches.
Howard at al. [18] utilized rules based on natural language
text in leading method comments to mine synonym dictionaries
from source code repositories. One of the additional capability
of deep language models is the ability to mine such terms at a
greater scale relative to the their approach.
V. CONCLUSIONS AND FUTURE WORK
In this work we present a preliminary study of a deep-
learning-based FLT using document vectors (DV). We find that
training the DV model has low computational overhead (i.e., is
fast), while maintaining accuracy on par with LDA. We find DV
to be a promising solution to implementing smarter developer
search tools in the IDE, a task to which more computationally-
intensive models such as LDA are less well-suited.
One direction for future work is to explore the effects
of parameter tuning on the performance of DV. Multiple
studies [7], [19] show that selection of appropriate parameter
values is key to the performance of an LDA-based FLT. Thus,
the first question to address is whether the same is true of a
DV-based FLT, and if so, the next question is how to select DV
parameters for a particular subject system. The results of our
study show that a DV-based FLT can provide better accuracy
than an LDA-based FLT while requiring fewer computational
resources, and more intelligent parameter selection for DV
could further improve its accuracy.
Another direction for future work is to use the semantic
relations encoded by a DV model for query recommendation or
refinement. We would like to investigate the efficacy of these
semantic relations for recommending query terms based on a
partial query. That is, while a developer is entering a query, a
recommender could use a DV model of the subject system to
recommend additional query terms. Such an approach would be
similar (in function) to the ones implemented in the Sando [20]
and CONQUER [21]. Similarly, given its relative computational
efficiency, using DV as the basis for an IDE-based search tool
is a realistic goal.
Further directions for future work include extending the
DV model to incorporate program structure information, an
approach which has shown useful when combined with text
retrieval models [22], or extending the DV model to incorporate
natural language information such as part-of-speech tags [17]
or phrasal representations [23].
REFERENCES
[1] B. Dit, M. Revelle, M. Gethers, and D. Poshyvanyk, "Feature location
in source code: a taxonomy and survey," Journal of Software: Evolution
and Process, vol. 25, no. 1, pp. 53–95, 2013.
[2] A. Hindle, E. T. Barr, Z. Su, M. Gabel, and P. Devanbu, "On the
Naturalness of Software," in Proceedings of the 34th ACM/IEEE
International Conference on Software Engineering, 2012, pp. 837–847.
[3] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, "Dis-
tributed Representations of Words and Phrases and their Compositionality,"
in Advances in Neural Information Processing Systems 26, 2013, pp.
3111–3119.
[4] Q. Le and T. Mikolov, "Distributed Representations of Sentences and
Documents," in Proceedings of the 31st International Conference on
Machine Learning, T. Jebara and E. P. Xing, Eds., 2014, pp. 1188–1196.
[5] E. Hill, B. Sisman, and A. Kak, "On the use of positional proximity in IR-
based feature location," in Proc. of IEEE Conf. on Software Maintenance,
Reengineering and Reverse Engineering, 2014, pp. 318–322.
[6] B. Dit, A. Holtzhauer, D. Poshyvanyk, and H. Kagdi, "A dataset from
change history to support evaluation of software maintenance tasks," in
Proceedings of the 10th IEEE Working Conference on Mining Software
Repositories, 2013, pp. 131–134.
[7] A. Panichella, B. Dit, R. Oliveto, M. Di Penta, D. Poshyvanyk, and
A. De Lucia, "How to effectively use topic models for software engineer-
ing tasks? an approach based on genetic algorithms," in Proceedings of
the 2013 International Conference on Software Engineering, ser. ICSE
'13.
Piscataway, NJ, USA: IEEE Press, 2013, pp. 522–531.
[8] A. Marcus, A. Sergeyev, V. Rajlich, and J. I. Maletic, "An information
retrieval approach to concept location in source code," in Proc. of 11th
Working Conf. on Reverse Engineering, 2004, pp. 214–223.
[9] A. Marcus and T. Menzies, "Software is data too," in Proceedings of the
FSE/SDP workshop on Future of software engineering research.
ACM,
2010, pp. 229–232.
[10] S. Haiduc and A. Marcus, "On the Effect of the Query in IR-based
Concept Location," in Proceedings of the 19th IEEE International
Conference on Program Comprehension, 2011, pp. 234–237.
[11] K. Damevski, D. Shepherd, and L. Pollock, "A field study of how devel-
opers locate features in source code," Empirical Software Engineering,
pp. 1–24, 2015.
[12] R. ˇReh˚uˇrek and P. Sojka, "Software framework for topic modelling with
large corpora," in Proceedings of the LREC 2010 Workshop on New
Challenges for NLP Frameworks, May 2010, pp. 45–50.
[13] C. Fox, "Lexical analysis and stoplists," in Information Retrieval: Data
Structures and Algorithms, W. Frakes and R. Baeza-Yates, Eds. Prentice-
Hall, 1992, pp. 102–130.
[14] D. Poshyvanyk, Y.-G. Guéhéneuc, A. Marcus, G. Antoniol, and
V. Rajlich, "Feature location using probabilistic ranking of methods
based on execution scenarios and information retrieval," IEEE Trans. on
Software Engineering, vol. 33, no. 6, pp. 420–432, 2007.
[15] E. M. Voorhees, "The trec-8 question answering track report." in TREC,
vol. 99, 1999, pp. 77–82.
[16] M. White, C. Vendome, M. Linares-Vásquez, and D. Poshyvanyk,
"Toward deep learning software repositories," in Proceedings of the
12th Working Conference on Mining Software Repositories, 2015.
[17] D. Shepherd, Z. P. Fry, E. Hill, L. Pollock, and K. Vijay-Shanker,
"Using natural language program analysis to locate and understand action-
oriented concerns."
ACM Press, 2007, p. 212.
[18] M. J. Howard, S. Gupta, L. Pollock, and K. Vijay-Shanker, "Automatically
Mining Software-based, Semantically-similar Words from Comment-code
Mappings," in Proceedings of the 10th Working Conference on Mining
Software Repositories, 2013, pp. 377–386.
[19] L. R. Biggers, C. Bocovich, R. Capshaw, B. P. Eddy, L. H. Etzkorn,
and N. A. Kraft, "Configuring latent dirichlet allocation based feature
location," Empirical Software Engineering, vol. 19, no. 3, pp. 465–500,
2014.
[20] D. Shepherd, K. Damevski, B. Ropski, and T. Fritz, "Sando: an extensible
local code search framework," in Proc. of ACM SIGSOFT 20th Int'l Sym.
on the Foundations of Software Engineering, 2012.
[21] M. Roldan-Vega, G. Mallet, E. Hill, and J. Fails, "CONQUER: A tool
for nl-based query refinement and contextualizing source code search
results," in Proceedings of the 29th IEEE International Conference on
Software Maintenance, 2013.
[22] B. Bassett and N. A. Kraft, "Structural information based term weighting
in text retrieval for feature location," in Proc. 21st IEEE Int'l Conf.
Program Comprehension, 2013, pp. 133–141.
[23] E. Hill, L. Pollock, and K. Vijay-Shanker, "Improving source code search
with natural language phrasal representations of method signatures," in
Proceedings of the 26th IEEE International Conference on Automated
Software Engineering, 2011.
560
