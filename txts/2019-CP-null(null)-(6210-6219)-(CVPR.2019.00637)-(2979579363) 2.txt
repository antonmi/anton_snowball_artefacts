Unsupervised Embedding Learning via Invariant and Spreading
Instance Feature
Mang Ye†
Xu Zhang‡
Pong C. Yuen†
Shih-Fu Chang‡
† Hong Kong Baptist University, Hong Kong
‡ Columbia University, New York
{mangye,pcyuen}@comp.hkbu.edu.hk, {xu.zhang,sc250}@columbia.edu
Abstract
This paper studies the unsupervised embedding learn-
ing problem, which requires an effective similarity mea-
surement between samples in low-dimensional embedding
space. Motivated by the positive concentrated and nega-
tive separated properties observed from category-wise su-
pervised learning, we propose to utilize the instance-wise
supervision to approximate these properties, which aims at
learning data augmentation invariant and instance spread-
out features. To achieve this goal, we propose a novel in-
stance based softmax embedding method, which directly op-
timizes the 'real' instance features on top of the softmax
function. It achieves significantly faster learning speed and
higher accuracy than all existing methods. The proposed
method performs well for both seen and unseen testing cat-
egories with cosine similarity. It also achieves competitive
performance even without pre-trained network over sam-
ples from fine-grained categories.
1. Introduction
Deep embedding learning is a fundamental task in com-
puter vision [14], which aims at learning a feature embed-
ding that has the following properties: 1) positive concen-
trated, the embedding features of samples belonging to the
same category are close to each other [32]; 2) negative sep-
arated, the embedding features of samples belonging to d-
ifferent categories are separated as much as possible [52].
Supervised embedding learning methods have been studied
to achieve such objectives and demonstrate impressive ca-
pabilities in various vision tasks [28, 30, 53]. However,
annotated data needed for supervised methods might be dif-
ficult to obtain. Collecting enough annotated data for differ-
ent tasks requires costly human efforts and special domain
expertise. To address this issue, this paper tackles the unsu-
pervised embedding learning problem (a.k.a. unsupervised
metric learning in [21]), which aims at learning discrimina-
tive embedding features without human annotated labels.
CNN
Input Images
Output Features
Figure 1: Illustration of our basic idea. The features of the same
instance under different data augmentations should be invariant,
while features of different image instances should be separated.
Unsupervised embedding learning usually requires that
the similarity between learned embedding features is con-
sistent with the visual similarity or category relations of in-
put images. In comparison, general unsupervised feature
learning usually aims at learning a good "intermediate" fea-
ture representation from unlabelled data [6, 26, 31, 34]. The
learned feature is then generalized to different tasks by us-
ing a small set of labelled training data from the target task
to fine-tune models (e.g., linear classifier, object detector,
etc.) for the target task [3]. However, the learned feature
representation may not preserve visual similarity and its
performance drops dramatically for similarity based tasks,
e.g. nearest neighbor search [46, 48, 50].
The main challenge of unsupervised embedding learning
is to discover visual similarity or weak category information
from unlabelled samples. Iscen et al. [21] proposed to mine
hard positive and negative samples on manifolds. However,
its performance heavily relies on the quality of the initial-
ized feature representation for label mining, which limits
the applicability for general tasks. In this paper, we pro-
pose to utilize the instance-wise supervision to approximate
the positive concentrated and negative separated proper-
ties mentioned earlier. The learning process only relies on
instance-wise relationship and does not rely on relations be-
tween pre-defined categories, so it can be well generalized
to samples of arbitrary categories that have not been seen
before (unseen testing categories) [12].
6210
For positive concentration: it is usually infeasible to
mine reliable positive information with randomly initialized
network. Therefore, we apply a random data augmentation
(e.g., transformation, scaling) to each image instance and
use the augmented image as a positive sample. In other
words, features of each image instance under different data
augmentations should be invariant. For negative separa-
tion: since unlabelled data are usually highly imbalanced
[27, 49], the number of negative samples for each image in-
stance is much larger than that of positive samples. There-
fore, a small batch of randomly selected instances can be ap-
proximately treated as negative samples for each instance.
With such assumption, we try to separate each instance from
all the other sampled instances within the batch, resulting
in a spread-out property [52]. It is clear that such assump-
tion may not always hold, and each batch may contain a
few false negatives. However, through our extensive exper-
iments, we observe that the spread-out property effectively
improves the discriminability. In summary, our main idea is
to learn a discriminative instance feature, which preserves
data augmentation invariant and spread-out properties for
unsupervised embedding learning, as shown in Fig. 1.
To achieve these goals, we introduce a novel instance
feature-based softmax embedding method. Existing soft-
max embedding is usually built on classifier weights [8] or
memorized features [46], which has limited efficiency and
discriminability. We propose to explicitly optimize the fea-
ture embedding by directly using the inner products of in-
stance features on top of softmax function, leading to signif-
icant performance and efficiency gains. The softmax func-
tion mines hard negative samples and takes full advantage
of relationships among all sampled instances to improve the
performance. The number of instance is significantly larger
than the number of categories, so we introduce a Siamese
network training strategy.
We transform the multi-class
classification problem to a binary classification problem and
use maximum likelihood estimation for optimization.
The main contributions can be summarized as follows:
• We propose a novel instance feature-based softmax
embedding method to learn data augmentation invari-
ant and instance spread-out features. It achieves signif-
icantly faster learning speed and higher accuracy than
all the competing methods.
• We show that both the data augmentation invariant
and instance spread-out properties are important for
instance-wise unsupervised embedding learning. They
help capture apparent visual similarity between sam-
ples and generalizes well on unseen testing categories.
• The proposed method achieves the state-of-the-art per-
formances over other unsupervised learning methods
on comprehensive image classification and embedding
learning experiments.
2. Related Work
General Unsupervised Feature Learning.
Unsuper-
vised feature learning has been widely studied in litera-
ture. Existing works can be roughly categorized into three
categories [3]: 1) generative models, this approach aims
at learning a parameterized mapping between images and
predefined noise signals, which constrains the distribution
between raw data and noises [46]. Bolztmann Machines
(RBMs) [24, 40], Auto-encoders [20, 42] and generative
adversarial network (GAN) [7, 10, 11] are widely stud-
ied. 2) Estimating Between-image Labels, it usually esti-
mates between-image labels using the clustering technique
[3, 9, 26] or kNN-based methods [41], which provide label
information. Then label information and feature learning
process are iteratively updated. 3) Self-supervised Learn-
ing, this approach designs pretext tasks/signals to generate
"pseudo-labels" and then formulate it as a prediction task to
learn the feature representations. The pretext task could be
the context information of local patches [6], the position of
randomly rearranged patches [31], the missing pixels of an
image [34] or the color information from gray-scale images
[51]. Some attempts also use video information to provide
weak supervision to learn feature representations [1, 44].
As we discussed in Section 1, general unsupervised fea-
ture learning usually aims at learning a good "intermediate"
feature representation that can be well generalized to oth-
er tasks. The intermediate feature representation may not
preserve visual similar property. In comparison, unsuper-
vised embedding learning requires additional visual simi-
larity property of the learned features.
Deep Embedding Learning. Deep embedding learning
usually learns an embedding function by minimizing the
intra-class variation and maximizing the inter-class varia-
tion [32, 37, 45, 47]. Most of them are designed on top of
pairwise [12, 30] or triplet relationships [13, 29]. In partic-
ular, several sampling strategies are widely investigated to
improve the performance, such as hard mining [16], semi-
hard mining [35], smart mining [13] and so on. In compari-
son, softmax embedding achieves competitive performance
without sampling requirement [18].
Supervised learning
has achieved superior performance on various tasks, but
they still rely on enough annotated data.
Unsupervised Embedding Learning. According to the
evaluation protocol, it can be categorized into two cases,
1) the testing categories are the same with the training cat-
egories (seen testing categories), and 2) the testing cate-
gories are not overlapped with the training categories (un-
seen testing categories). The latter setting is more challeng-
ing. Without category-wise labels, Iscen et al. [21] pro-
posed to mine hard positive and negative samples on mani-
folds, and then train the feature embedding with triplet loss.
However, it heavily relies on the initialized representation
for label mining.
6211
3. Proposed Method
Our goal is to learn a feature embedding network fθ(·)
from a set of unlabelled images X = {x1, x2, · · · , xn}.
fθ(·) maps the input image xi into a low-dimensional em-
bedding feature fθ(xi) ∈ Rd, where d is the feature dimen-
sion. For simplicity, the feature representation fθ(xi) of an
image instance is represented by fi, and we assume that all
the features are ℓ2 normalized, i.e. ∥fi∥2 = 1. A good fea-
ture embedding should satisfy: 1) the embedding features of
visual similar images are close to each other; 2) the embed-
ding features of dissimilar image instances are separated.
Without category-wise labels, we utilize the instance-
wise supervision to approximate the positive concentrated
and negative seperated properties. In particular, the em-
bedding features of the same instance under different data
augmentations should be invariant, while the features of d-
ifferent instances should be spread-out. In the rest of this
section, we first review two existing instance-wise feature
learning methods, and then propose a much more efficient
and discriminative instance feature-based softmax embed-
ding. Finally, we will give a detailed rationale analysis and
introduce our training strategy with Siamese network.
3.1. Instance-wise Softmax Embedding
Softmax Embedding with Classifier Weights. Exemplar
CNN [8] treats each image as a distinct class.
Follow-
ing the conventional classifier training, it defines a matrix
W = [w1, w2, · · · , wn]T ∈ Rn×d, where the j-th column
wj is called the corresponding classifier weight for the j-th
instance. Exemplar CNN ensures that image instance under
different image transformations can be correctly classified
into its original instance with the learned weight. Based on
Softmax function, the probability of sample xj being rec-
ognized as the i-th instance can be represented as
P(i|xj) =
exp(wT
i fj)
�n
k=1 exp(wT
k fj).
(1)
At each step, the network pulls sample feature fi towards
its corresponding weight wi, and pushes it away from the
classifier weights wk of other instances. However, classifier
weights prevent explicitly comparison over features, which
results in limited efficiency and discriminability.
Softmax Embedding with Memory Bank. To improve the
inferior efficiency, Wu et al. [46] propose to set up a mem-
ory bank to store the instance features fi calculated in the
previous step. The feature stored in the memory bank is de-
noted as vi, which serves as the classifier weight for the
corresponding instance in the following step. Therefore,
the probability of sample xj being recognized as the i-th
instance can be written as
P(i|xj) =
exp(vT
i fj/τ)
�n
k=1 exp(vT
k fj/τ),
(2)
where τ is the temperature parameter controlling the con-
centration level of the sample distribution [17]. vT
i fj mea-
sures the cosine similarity between the feature fj and the
i-th memorized feature vi. For instance xi at each step,
the network pulls its feature fi towards its corresponding
memorized vector vi, and pushes it away from the memo-
rized vectors of other instances. Due to efficiency issue, the
memorized feature vi corresponding to instance xi is only
updated in the iteration which takes xi as input. In other
words, the memorized feature vi is only updated once per
epoch. However, the network itself is updated in each iter-
ation. Comparing the real-time instance feature fi with the
outdated memorized feature vi would cumber the training
process. Thus, the memory bank scheme is still inefficient.
A straightforward idea to improve the efficiency is di-
rectly optimizing over feature itself, i.e. replacing the
weight {wi} or memory {vi} with fi. However, it is im-
plausible due to two reasons: 1) Considering the probability
P(i|xi) of recognizing xi to itself, since f T
i fi=1, i.e. the fea-
ture and "pseudo classifier weight" (the feature itself) are al-
ways perfectly aligned, optimizing the network will not pro-
vide any positive concentrated property; 2) It's impractical
to calculate the feature of all the samples (fk, k = 1, . . . , n)
on-the-fly in order to calculate the denominator in Eq. (2),
especially for large-scale instance number dataset.
3.2. Softmax Embedding on 'Real' Instance Feature
To address above issues, we propose a softmax embed-
ding variant for unsupervised embedding learning, which
directly optimizes the real instance feature rather than clas-
sifier weights [8] or memory bank [46]. To achieve the goal
that features of the same instance under different data aug-
mentations are invariant, while the features of different in-
stances are spread-out, we propose to consider 1) both the
original image and its augmented image, 2) a small batch of
randomly selected samples instead of the full dataset.
For each iteration, we randomly sample m instances
from the dataset.
To simplify the notation, without
loss of generality, the selected samples are denoted by
{x1, x2, · · · , xm}. For each instance, a random data aug-
mentation operation T(·) is applied to slightly modify the
original image. The augmented sample T(xi) is denoted by
ˆxi, and its embedding feature fθ(ˆxi) is denoted by ˆfi. In-
stead of considering the instance feature learning as a multi-
class classification problem, we solve it as binary classifica-
tion problem via maximum likelihood estimation (MLE). In
particular, for instance xi, the augmented sample ˆxi should
be classified into instance i, and other instances xj, j ̸= i
shouldn't be classified into instance i. The probability of ˆxi
being recognized as instance i is defined by
P(i|ˆxi) =
exp(f T
i ˆfi/τ)
�m
k=1 exp(f T
k ˆfi/τ)
.
(3)
6212
CNN
FC
L2 Norm
FC
L2 Norm
CNN
Low-dim
Low-dim
Embedding Space
f1
f2
f3
 f1
 f2
 f3
𝐱1
𝐱2
𝐱3
 𝐱1
 𝐱2
 𝐱3
Data 
Augmentation
Share Weights
Figure 2: The framework of the proposed unsupervised learning method with Siamese network. The input images are projected into
low-dimensional normalized embedding features with the CNN backbone. Image features of the same image instance with different data
augmentations are invariant, while embedding features of different image instances are spread-out.
On the other hand, the probability of xj being recognized
as instance i is defined by
P(i|xj) =
exp(f T
i fj/τ)
�m
k=1 exp(f T
k fj/τ), j ̸= i
(4)
Correspondingly, the probability of xj not being recognized
as instance i is 1 − P(i|xj).
Assuming different instances being recognized as in-
stance i are independent, the joint probability of ˆxi being
recognized as instance i and xj, j ̸= i not being classified
into instance i is
Pi = P(i|ˆxi)
�
j̸=i
(1 − P(i|xj))
(5)
The negative log likelihood is given by
Ji = − log P(i|ˆxi) −
�
j̸=i
log(1 − P(i|xj))
(6)
We solve this problem by minimizing the sum of the neg-
ative log likelihood over all the instances within the batch,
which is denoted by
J = −
�
i
log P(i|ˆxi) −
�
i
�
j̸=i
log(1 − P(i|xj)).
(7)
3.3. Rationale Analysis
This section gives a detailed rationale analysis about why
minimizing Eq. (6) could achieve the augmentation invari-
ant and instance spread-out feature. Minimizing Eq. (6) can
be viewed as maximizing Eq. (3) and minimizing Eq. (4).
Considering Eq. (3), it can be rewritten as
P(i|ˆxi) =
exp(f T
i ˆfi/τ)
exp(f T
i ˆfi/τ) + �
k̸=i exp(f T
k ˆfi/τ)
,
(8)
Maximizing Eq. (3) requires maximizing exp(f T
i ˆfi/τ) and
minimizing exp(f T
k ˆfi/τ), k ̸= i. Since all the features are
ℓ2 normalized, maximizing exp(f T
i ˆfi/τ) requires increas-
ing the inner product (cosine similarity) between fi and ˆfi,
resulting in a feature that is invariant to data augmentation.
On the other hand, minimizing exp(f T
k ˆfi/τ) ensures ˆfi and
other instances {fk} are separated. Considering all the in-
stances within the batch, the instances are forced to be sep-
arated from each other, resulting in the spread-out property.
Similarly, Eq. (4) can be rewritten as,
P(i|xj) =
exp(f T
i fj/τ)
exp(f T
j fj/τ) + �
k̸=j exp(f T
k fj/τ),
(9)
Note that the inner product f T
j fj is 1 and the value of τ
is generally small (say 0.1 in the experiment). Therefore,
exp(f T
j fj/τ) generally determines the value of the whole
denominator. Minimizing Eq. (4) means that exp(f T
i fj/τ)
should be minimized, which aims at separating fj from fi.
Thus, it further enhances the spread-out property.
3.4. Training with Siamese Network
We proposed a Siamese network to implement the pro-
posed algorithm as shown in Fig. 2. At each iteration, m
randomly selected image instances are fed into in the first
branch, and the corresponding augmented samples are fed
into the second branch. Note that data augmentation is al-
so be used in the first branch to enrich the training sam-
ples. For implementation, each sample has one randomly
augmented positive sample and 2N − 2 negative samples
to compute Eq. (7), where N is the batch size. The pro-
posed training strategy greatly reduces the computational
cost. Meanwhile, this training strategy also takes full ad-
vantage of relationships among all instances sampled in a
mini-batch [32]. Theoretically, we could also use a multi-
branch network by considering multiple augmented images
for each instance in the batch.
6213
Methods
kNN
RandomCNN
32.1
DeepCluster (10) [3]
44.4
DeepCluster (1000) [3]
67.6
Exemplar [8]
74.5
NPSoftmax [46]
80.8
NCE [46]
80.4
Triplet
57.5
Triplet (Hard)
78.4
Ours
83.6
Table 1: kNN accuracy (%) on CIFAR-10 dataset.
4. Experimental Results
We have conducted the experiments with two different
settings to evaluate the proposed method1. The first setting
is that the training and testing sets share the same categories
(seen testing category). This protocol is widely adopted for
general unsupervised feature learning. The second setting
is that the training and testing sets do not share any com-
mon categories (unseen testing category). This setting is
usually used for supervised embedding learning [32]. Fol-
lowing [21], we don't use any semantic label in the training
set. The latter setting is more challenging than the former
setting and it could apparently demonstrate the quality of
learned features on unseen categories.
4.1. Experiments on Seen Testing Categories
We follow the experimental settings in [46] to conduct
the experiments on CIFAR-10 [23] and STL-10 [4] dataset-
s, where training and testing set share the same categories.
Specifically, ResNet18 network [15] is adopted as the back-
bone and the output embedding feature dimension is set to
128. The initial learning rate is set to 0.03, and it is de-
cayed by 0.1 and 0.01 at 120 and 160 epoch. The network is
trained for 200 epochs. The temperature parameter τ is set
to 0.1. The algorithm is implemented on PyTorch with SGD
optimizer with momentum. The weight decay parameter is
5×10−4 and momentum is 0.9. The training batch size is
set to 128 for all competing methods on both datasets. Four
kinds of data augmentation methods (RandomResizedCrop,
RandomGrayscale, ColorJitter, RandomHorizontalFlip) in
PyTorch with default parameters are adopted.
Following [46], we adopt weighted kNN classifier to e-
valuate the performance. Given a test sample, we retrieve
its top-k (k = 200) nearest neighbors based on cosine simi-
larity, then apply weighted voting to predict its label [46].
4.1.1
CIFAR-10 Dataset
CIFAR-10 datset [23] contains 50K training images and
10K testing images from the same ten classes. The image
size are 32 × 32. Five methods are included for compari-
son: DeepCluster [3] with different cluster numbers, Exem-
1Code
is
available
at
https://github.com/mangye16/
Unsupervised_Embedding_Learning
0 10 20
40
80
120
160
200
Training Epochs
10
20
30
40
50
60
70
80
90
kNN Accuracy (%)
Ours
DeepCluster [3]
NCE [46]
Exemplar [8]
Figure 3: Evaluation of the training efficiency on CIFAR-10
dataset. kNN accuracy (%) at each epoch is reported, demonstrat-
ing the learning speed of different methods.
plar CNN [8], NPSoftmax [46], NCE [46] and Triplet loss
with and without hard mining. Triplet (hard) is the online
hard negative sample within each batch for training [16],
and the margin parameter is set to 0.5. DeepCluster [3] and
NCE [46] represent the state-of-the-art unsupervised feature
learning methods. The results are shown in Table 1.
Classification Accuracy. Table 1 demonstrates that our
proposed method achieves the best performance (83.6%)
with kNN classifier.
DeepCluster [3] performs well in
learning good "intermediate" features with large-scale un-
labelled data, but the performance with kNN classification
drops dramatically. Meanwhile, it is also quite sensitive
to cluster numbers, which is unsuitable for different tasks.
Compared to Exemplar CNN [8] which uses the classifi-
er weights for training, the proposed method outperforms
it by 9.1%. Compared to NPSoftmax [46] and NCE [46],
which use memorized feature for optimizing, the proposed
method outperform by 2.8% and 3.2% respectively. The
performance improvement is clear due to the idea of direct-
ly performing optimization over feature itself. Compared
to triplet loss, the proposed method also outperforms it by
a clear margin. The superiority is due to the hard mining
nature in Softmax function.
Efficiency. We plot the learning curves of the compet-
ing methods at different epochs in Fig. 3. The proposed
method takes only 2 epochs to get a kNN accuracy of 60%
while [46] takes 25 epochs and [8] takes 45 epochs to reach
the same accuracy. It is obvious that our learning speed is
much faster than the competitors. The efficiency is guar-
anteed by directly optimization on instance features rather
than classifier weights [8] or memory bank [46].
4.1.2
STL-10 Dataset
STL-10 dataset [4] is an image recognition dataset with col-
ored images of size 96 × 96, which is widely used in unsu-
pervised learning. Specifically, this dataset is originally de-
signed with three splits: 1) train, 5K labelled images in ten
6214
Methods
Training
Linear
kNN
RandomCNN
None
-
22.4
k-MeansNet∗ [5]
105K
60.1
-
HMP∗ [2]
105K
64.5
-
Satck∗ [54]
105K
74.3
-
Exemplar∗ [8]
105K
75.4
-
NPSoftmax [46]
5K
62.3
66.8
NCE [46]
5K
61.9
66.2
DeepCluster(100) [3]
5K
56.5
61.2
Ours
5K
69.5
74.1
Ours
105K
77.9
81.6
Table 2: Classification accuracy (%) with linear classifier and
kNN classifier on STL-10 dataset.
∗Results are taken from [33],
the baseline network is different.
classes for training, 2) test, 8K images from the same ten
classes for testing, 3) unlabelled, 100K unlabelled images
which share similar distribution with labelled data for un-
supervised learning. We follow the same experimental set-
ting as CIFAR-10 dataset and report classification accuracy
(%) with both Linear Classifier (Linear) and kNN classier
(kNN) in Table 2. Linear classifier means training a SVM
classifier on the learned features and the labels of training
samples. The classifier is used to predict the label of test
samples. We implement NPSoftmax [46], NCE [46] and
DeepCluster [3] (cluster number 100) under the same set-
tings with their released code. By default, we only use 5K
training images without using labels for training. The per-
formances of some state-of-the-art unsupervised methods
(k-MeansNet [5], HMP [2], Satck [54] and Exemplar [8])
are also reported. Those results are taken from [33].
As shown in Table 2 , when only using 5K training im-
ages for learning, the proposed method achieves the best ac-
curacy with both classifiers (kNN: 74.1%, Linear: 69.5%),
which are much better than NCE [46] and DeepCluster [3]
under the same evaluation protocol. Note that kNN mea-
sures the similarity directly with the learned features and
Linear requires additional classifier learning with the la-
belled training data. When 105K images are used for train-
ing, the proposed method also achieves the best perfor-
mance for both kNN classifier and linear classifier. In par-
ticular, the kNN accuracy is 74.1% for 5K training images,
and it increases to 81.6% for full 105K training images. The
classification accuracy with linear classifier also increases
from 69.5% to 77.9%. This experiment verifies that the pro-
posed method can benefit from more training samples.
4.2. Experiments on Unseen Testing Categories
This section evaluates the discriminability of the learned
feature embedding when the semantic categories of training
samples and testing samples are not overlapped. We follow
the experimental settings described in [32] to conduct ex-
periments on CUB200-2011(CUB200) [43], Stanford On-
line Product (Product) [32] and Car196 [22] datasets. No
semantic label is used for training. Caltech-UCSD Birds
Methods
R@1
R@2
R@4
R@8
NMI
Initial (FC)
39.2
52.1
66.1
78.2
51.4
Supervised Learning
Lifted [32]
43.6
56.6
68.6
79.6
56.5
Clustering[38]
48.2
61.4
71.8
81.9
59.2
Triplet+ [13]
45.9
57.7
69.6
79.8
58.1
Smart+ [13]
49.8
62.3
74.1
83.3
59.9
Unsupervised Learning
Cyclic [25]
40.8
52.8
65.1
76.0
52.6
Exemplar [8]
38.2
50.3
62.8
75.0
45.0
NCE [46]
39.2
51.4
63.7
75.8
45.1
DeepCluster[3]
42.9
54.1
65.6
76.2
53.0
MOM [21]
45.3
57.8
68.6
78.4
55.0
Ours
46.2
59.0
70.1
80.2
55.4
Table 3: Results (%) on CUB200 dataset.
Methods
R@1
R@10
R@100
NMI
Initial (FC)
40.8
56.7
72.1
84.0
Exemplar [8]
45.0
60.3
75.2
85.0
NCE [46]
46.6
62.3
76.8
85.8
DeepCluster[3]
34.6
52.6
66.8
82.8
MOM [21]
43.3
57.2
73.2
84.4
Ours
48.9
64.0
78.0
86.0
Table 4: Results (%) on Product dataset.
200 (CUB200) [43] is a fine-grained bird dataset. Follow-
ing [32], the first 100 categories with 5,864 images are used
for training, while the other 100 categories with 5,924 im-
ages are used for testing. Stanford Online Product (Product)
[32] is a large-scale fine-grained product dataset. Similar-
ly, 11,318 categories with totally 59,551 images are used
for training, while the other 11,316 categories with 60,502
images are used for testing. Cars (Car196) dataset [22] is
a fine-grained car category dataset. The first 98 categories
with 8,054 images are used for training, while the other 98
categories with 8,131 images are used for testing.
Implementation Details. We implement the proposed
method on PyTorch. The pre-trained Inception-V1 [39] on
ImageNet is used as the backbone network following exist-
ing methods [30, 32, 37]. A 128-dim fully connected layer
with ℓ2 normalization is added after the pool5 layer as the
feature embedding layer. All the input images are firstly re-
sized to 256 × 256. For data augmentation, the images are
randomly cropped at size 227×227 with random horizontal
flipping following [21, 30]. Since the pre-trained network
performs well on CUB200 dataset, we randomly select the
augmented instance and its corresponding nearest instance
as positive. In testing phase, a single center-cropped im-
age is adopted for fine-grained recognition as in [30]. We
adopt the SGD optimizer with 0.9 momentum. The initial
learning rate is set to 0.001 without decay. The temperature
parameter τ is set to 0.1. The training batch size is set to 64.
Evaluation Metrics. Following existing works on su-
pervised deep embedding learning [13, 32], the retrieval
performance and clustering quality of the testing set are e-
valuated. Cosine similarity is adopted for similarity mea-
6215
Methods
R@1
R@2
R@4
R@8
NMI
Initial (FC)
35.1
47.4
60.0
72.0
38.3
Exemplar [8]
36.5
48.1
59.2
71.0
35.4
NCE [46]
37.5
48.7
59.8
71.5
35.6
DeepCluster[3]
32.6
43.8
57.0
69.5
38.5
MOM [21]
35.5
48.2
60.6
72.4
38.6
Ours
41.3
52.3
63.6
74.9
35.8
Table 5: Results (%) on Car196 dataset.
surement. Given a query image from the testing set, R@K
measures the probability of any correct matching (with
same category label) occurs in the top-k retrieved ranking
list [32]. The average score is reported for all testings sam-
ples. Normalized Mutual Information (NMI) [36] is utilized
to measure the clustering performance of the testing set.
Comparison to State-of-the-arts. The results of all the
competing methods on three datasets are listed in Table 3,
4 and 5, respectively. MOM [21] is the only method that
claims for unsupervised metric learning. We implement the
other three state-of-the-art unsupervised methods (Exem-
plar [8], NCE [46] and DeepCluster [3]) on three dataset-
s with their released code under the same setting for fair
comparison. Note that these methods are originally eval-
uated for general unsupervised feature learning, where the
training and testing set share the same categories. We al-
so list some results of supervised learning (originate from
[21]) on CUB200 dataset as shown in Table 3.
Generally, the instance-wise feature learning methods
(NCE [46], Examplar [8], Ours) outperform non-instance-
wise feature learning methods (DeepCluster [3], MOM
[21]), especially on Car196 and Product datasets, which in-
dicates instance-wise feature learning methods have good
generalization ability on unseen testing categories. Among
all the instance-wise feature learning methods, the proposed
method is the clear winner, which also verifies the effective-
ness of directly optimizing over feature itself. Moreover, the
proposed unsupervised learning method is even competitive
to some supervised learning methods on CUB200 dataset.
Qualitative Result. Some retrieved examples with co-
sine similarity on CUB200 dataset at different training e-
pochs are shown in Fig. 4. The proposed algorithm can
iteratively improve the quality of the learned feature and
retrieve more correct images.
Although there are some
wrongly retrieved samples from other categories, most of
the top retrieved samples are visually similar to the query.
Training from Scratch. We also evaluate the perfor-
mance using a network (ResNet18) without pre-training.
The results on the large-scale Product dataset are shown in
Table 6. The proposed method is also a clear winner. In-
terestingly, MOM [21] fails in this experiment. The main
reason is that the feature from randomly initialized network
provides limited information for label mining. Therefore,
MOM cannot estimate reliable labels for training.
Methods
R@1
R@10
R@100
NMI
Random
18.4
29.4
46.0
79.8
Exemplar [8]
31.5
46.7
64.2
82.9
NCE [46]
34.4
49.0
65.2
84.1
MOM [21]
16.3
27.6
44.5
80.6
Ours
39.7
54.9
71.0
84.7
Table 6: Results (%) on Product dataset using network without
pre-trained parameters.
4.3. Ablation Study
The proposed method imposes two important properties
for instance feature learning: data augmentation invariant
and instance spread-out. We conduct ablation study to show
the effectiveness of each property on CIFAR-10 dataset.
Strategy
Full
w/o R
w/o G
w/o C
w/o F
kNN Acc (%)
83.6
56.2
79.3
75.7
82.6
Table 7: Effects of each data augmentation operation on CIFAR-
10 dataset. 'w/o': Without. 'R': RandomResizedCrop, 'G': Ran-
domGrayscale, 'C': ColorJitter, 'F': RandomHorizontalFlip.
Strategy
Full
No DA
Hard
Easy
kNN Acc (%)
83.6
37.4
83.2
57.5
Table 8: Different sampling strategies on CIFAR-10 dataset.
To show the importance of data augmentation invariant
property, we firstly evaluate the performance by removing
each of the operation respectively from the data augmen-
tation set. The results are shown in Table 7. We observe
that all listed operations contribute to the remarkable per-
formance gain achieved by the proposed algorithm. In par-
ticular, RandomResizedCrop contributes the most. We al-
so evaluate the performance without data augmentation (No
DA) in Table 8, and it shows that performance drops sig-
nificantly from 83.6% to 37.4%. It is because when train-
ing without data augmentation, the network does not create
any positive concentration property. The features of visual-
ly similar images are falsely separated.
To show the importance of spread-out property, we eval-
uated two different strategies to choose negative samples:
1) selecting the top 50% instance features that are similar
to query instance as negative (hard negative); 2) selecting
the bottom 50% instance features that are similar to query
instance as negative (easy negative). The results are shown
as "Hard" and "Easy" in Table 8. The performance drops
dramatically when only using the easy negative. In com-
parison, the performance almost remains the same as the
full model when only using hard negative. It shows that
separating hard negative instances helps to improve the dis-
criminability of the learned embedding.
4.4. Understanding of the Learned Embedding
We calculate the cosine similarity between the query fea-
ture and its 5NN features from the same category (Positive)
6216
Query
Epoch 0
Epoch 1
Epoch 2
Figure 4: 4NN retrieval results of some example queries on CUB200-2011 dataset. The positive (negative) retrieved results are framed in
green (red). The similarity is measured with cosine similarity.
0.0
0.2
0.4
0.6
0.8
1.0
0
5
Positive
Negative
0.0
0.2
0.4
0.6
0.8
1.0
0
5
Positive
Negative
(a) Random Network
0.0
0.2
0.4
0.6
0.8
1.0
0
5
Positive
Negative
0.0
0.2
0.4
0.6
0.8
1.0
0
5
Positive
Negative
(c) Exemplar [8]
0.0
0.2
0.4
0.6
0.8
1.0
0
5
Positive
Negative
0.0
0.2
0.4
0.6
0.8
1.0
0
5
Positive
Negative
(b) NCE [46]
0.0
0.2
0.4
0.6
0.8
1.0
0
5
Positive
Negative
0.0
0.2
0.4
0.6
0.8
1.0
0
5
Positive
Negative
(d) Ours
Figure 5: The cosine similarity distributions on CIFAR-10 [23]
as well as 5NN features from different categories (Nega-
tive). The distributions of the cosine similarity of different
methods are shown in Fig. 5. A more separable distribution
indicates a better feature embedding. It shows that the pro-
posed method performs best to separate positive and nega-
tive samples. We could also observe that our learned feature
preserves the best spread-out property.
It is interesting to show how the learned instance-wise
feature helps the category label prediction. We report the
cosine similarity distribution based on other category defi-
nitions (attributes in [19]) instead of semantic label in Fig. 6.
The distribution clearly shows that the proposed method al-
so performs well to separate other attributes, which demon-
strates the generalization ability of the learned feature.
5. Conclusion
In this paper, we propose to address the unsupervised
embedding learning problem by learning a data augmen-
tation invariant and instance spread-out feature. In partic-
ular, we propose a novel instance feature based softmax
embedding trained with Siamese network, which explicit-
0.0
0.2
0.4
0.6
0.8
1.0
0
5
Positive
Negative
0.0
0.2
0.4
0.6
0.8
1.0
0
5
Positive
Negative
(a) Attribute "animals vs artifacts"
0.0
0.2
0.4
0.6
0.8
1.0
0
5
Positive
Negative
0.0
0.2
0.4
0.6
0.8
1.0
0
5
Positive
Negative
(b) Attribute "big vs small shape animal"
Figure 6: The cosine similarity distributions of randomly initial-
ized network (left column) and our learned model (right column)
with different attributes on CIFAR-10 [23].
ly pulls the features of the same instance under different
data augmentations close and pushes the features of differ-
ent instances away. Comprehensive experiments show that
directly optimizing over instance feature leads to significant
performance and efficiency gains. We empirically show that
the spread-out property is particularly important and it helps
capture the visual similarity among samples.
Acknowledgement
This work is partially supported by Research Grants
Council (RGC/HKBU12200518), Hong Kong. This work is
partially supported by the United States Air Force Research
Laboratory (AFRL) and the Defense Advanced Research
Projects Agency (DARPA) under Contract No. FA8750-16-
C-0166. Any opinions, findings and conclusions or recom-
mendations expressed in this material are solely the respon-
sibility of the authors and does not necessarily represent the
official views of AFRL, DARPA, or the U.S. Government.
6217
References
[1] Pulkit Agrawal, Joao Carreira, and Jitendra Malik. Learning
to see by moving. In ICCV, pages 37–45, 2015. 2
[2] Liefeng Bo, Xiaofeng Ren, and Dieter Fox. Unsupervised
feature learning for rgb-d based object recognition. In Ex-
perimental Robotics, pages 387–402. Springer, 2013. 6
[3] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and
Matthijs Douze. Deep clustering for unsupervised learning
of visual features. In ECCV, pages 132–149, 2018. 1, 2, 5,
6, 7
[4] Adam Coates, Andrew Ng, and Honglak Lee. An analysis
of single-layer networks in unsupervised feature learning. In
AISTATS, pages 215–223, 2011. 5
[5] Adam Coates and Andrew Y Ng. Selecting receptive fields
in deep networks. In NIPS, pages 2528–2536, 2011. 6
[6] Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsuper-
vised visual representation learning by context prediction. In
ICCV, pages 1422–1430, 2015. 1, 2
[7] Jeff Donahue, Philipp Kr¨ahenb¨uhl, and Trevor Darrell. Ad-
versarial feature learning. arXiv preprint arXiv:1605.09782,
2016. 2
[8] Alexey Dosovitskiy, Philipp Fischer, Jost Tobias Springen-
berg, Martin Riedmiller, and Thomas Brox. Discriminative
unsupervised feature learning with exemplar convolutional
neural networks. PAMI, 38(9):1734–1747, 2016. 2, 3, 5, 6,
7, 8
[9] Alexey Dosovitskiy, Jost Tobias Springenberg, Martin Ried-
miller, and Thomas Brox. Discriminative unsupervised fea-
ture learning with convolutional neural networks. In NIPS,
pages 766–774, 2014. 2
[10] Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Olivi-
er Mastropietro, Alex Lamb, Martin Arjovsky, and Aaron
Courville. Adversarially learned inference. arXiv preprint
arXiv:1606.00704, 2016. 2
[11] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing X-
u, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets. In NIPS, pages
2672–2680, 2014. 2
[12] Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimension-
ality reduction by learning an invariant mapping. In CVPR,
2006. 1, 2
[13] Ben Harwood, BG Kumar, Gustavo Carneiro, Ian Reid, Tom
Drummond, et al. Smart mining for deep metric learning. In
ICCV, pages 2821–2829, 2017. 2, 6
[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Delving deep into rectifiers: Surpassing human-level perfor-
mance on imagenet classification.
In ICCV, pages 1026–
1034, 2015. 1
[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition.
In CVPR,
pages 770–778, 2016. 5
[16] Alexander Hermans, Lucas Beyer, and Bastian Leibe. In de-
fense of the triplet loss for person re-identification. arXiv
preprint arXiv:1703.07737, 2017. 2, 5
[17] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling
the knowledge in a neural network.
arXiv preprint arX-
iv:1503.02531, 2015. 3
[18] Shota Horiguchi, Daiki Ikami, and Kiyoharu Aizawa. Sig-
nificance of softmax-based features in comparison to dis-
tance metric learning-based features.
arXiv preprint arX-
iv:1712.10151, 2017. 2
[19] Chen Huang, Chen Change Loy, and Xiaoou Tang. Unsu-
pervised learning of discriminative attributes and visual rep-
resentations. In CVPR, pages 5175–5184, 2016. 8
[20] Fu Jie Huang, Y-Lan Boureau, Yann LeCun, et al. Unsuper-
vised learning of invariant feature hierarchies with applica-
tions to object recognition. In CVPR, pages 1–8, 2007. 2
[21] Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, and Ondrej
Chum. Mining on manifolds: Metric learning without labels.
2018. 1, 2, 5, 6, 7
[22] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei.
3d object representations for fine-grained categorization. In
ICCVW, pages 554–561, 2013. 6
[23] Alex Krizhevsky. Learning multiple layers of features from
tiny images. Technical report, Citeseer, 2009. 5, 8
[24] Honglak Lee, Roger Grosse, Rajesh Ranganath, and An-
drew Y Ng. Convolutional deep belief networks for scal-
able unsupervised learning of hierarchical representations. In
ICML, pages 609–616, 2009. 2
[25] Dong Li, Wei-Chih Hung, Jia-Bin Huang, Shengjin Wang,
Narendra Ahuja, and Ming-Hsuan Yang. Unsupervised vi-
sual representation learning by graph-based consistent con-
straints. In ECCV, pages 678–694, 2016. 6
[26] Renjie Liao, Alex Schwing, Richard Zemel, and Raquel Ur-
tasun. Learning deep parsimonious representations. In NIPS,
pages 5076–5084, 2016. 1, 2
[27] Tongliang Liu and Dacheng Tao. Classification with noisy
labels by importance reweighting. IEEE TPAMI, 38(3):447–
461, 2016. 2
[28] Chaochao Lu and Xiaoou Tang. Surpassing human-level face
verification performance on lfw with gaussianface. In AAAI,
pages 3811–3819, 2015. 1
[29] R Manmatha, Chao-Yuan Wu, Alexander J Smola, and
Philipp Kr¨ahenb¨uhl. Sampling matters in deep embedding
learning. In ICCV, pages 2859–2867, 2017. 2
[30] Yair Movshovitz-Attias, Alexander Toshev, Thomas K Le-
ung, Sergey Ioffe, and Saurabh Singh. No fuss distance met-
ric learning using proxies. In ICCV, pages 360–368, 2017.
1, 2, 6
[31] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of
visual representations by solving jigsaw puzzles. In ECCV,
pages 69–84, 2016. 1, 2
[32] Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio
Savarese. Deep metric learning via lifted structured feature
embedding. In CVPR, pages 4004–4012, 2016. 1, 2, 4, 5, 6,
7
[33] Edouard
Oyallon,
Eugene
Belilovsky,
and
Sergey
Zagoruyko.
Scaling the scattering transform:
Deep
hybrid networks. In ICCV, pages 5619–5628, 2017. 6
[34] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor
Darrell, and Alexei A Efros.
Context encoders: Feature
learning by inpainting. In CVPR, pages 2536–2544, 2016.
1, 2
6218
[35] Florian Schroff, Dmitry Kalenichenko, and James Philbin.
Facenet: A unified embedding for face recognition and clus-
tering. In CVPR, pages 815–823, 2015. 2
[36] Hinrich Sch¨utze, Christopher D Manning, and Prabhakar
Raghavan. Introduction to information retrieval, volume 39.
Cambridge University Press, 2008. 7
[37] Kihyuk Sohn. Improved deep metric learning with multi-
class n-pair loss objective. In NIPS, pages 1857–1865, 2016.
2, 6
[38] Hyun Oh Song, Stefanie Jegelka, Vivek Rathod, and Kevin
Murphy. Deep metric learning via facility location. In CVPR,
pages 2206–2214, 2017. 6
[39] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincen-
t Vanhoucke, and Andrew Rabinovich. Going deeper with
convolutions. In CVPR, pages 1–9, 2015. 6
[40] Yichuan Tang, Ruslan Salakhutdinov, and Geoffrey Hinton.
Robust boltzmann machines for recognition and denoising.
In CVPR, pages 2264–2271, 2012. 2
[41] Daniel Tarlow,
Kevin Swersky,
Laurent Charlin,
Ilya
Sutskever, and Rich Zemel. Stochastic k-neighborhood s-
election for supervised and unsupervised learning. In ICML,
pages 199–207, 2013. 2
[42] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and
Pierre-Antoine Manzagol. Extracting and composing robust
features with denoising autoencoders. In ICML, pages 1096–
1103, 2008. 2
[43] Catherine Wah, Steve Branson, Peter Welinder, Pietro Per-
ona, and Serge Belongie. The caltech-ucsd birds-200-2011
dataset. 2011. 6
[44] Xiaolong Wang and Abhinav Gupta. Unsupervised learning
of visual representations using videos. In ICCV, pages 2794–
2802, 2015. 2
[45] Yandong Wen, Kaipeng Zhang, Zhifeng Li, and Yu Qiao. A
discriminative feature learning approach for deep face recog-
nition. In ECCV, pages 499–515, 2016. 2
[46] Zhirong Wu, Yuanjun Xiong, X Yu Stella, and Dahua Lin.
Unsupervised feature learning via non-parametric instance
discrimination. In CVPR, pages 3733–3742, 2018. 1, 2, 3, 5,
6, 7, 8
[47] Tong Xiao, Hongsheng Li, Wanli Ouyang, and Xiaogang
Wang. Learning deep feature representations with domain
guided dropout for person re-identification. In CVPR, pages
1249–1258, 2016. 2
[48] Mang Ye, Xiangyuan Lan, and Pong C. Yuen. Robust anchor
embedding for unsupervised video person re-identification in
the wild. In ECCV, pages 170–186, 2018. 1
[49] Mang Ye, Jiawei Li, Andy J Ma, Liang Zheng, and Pong C.
Yuen. Dynamic graph co-matching for unsupervised video-
based person re-identification. In IEEE Transactions on Im-
age Processing (TIP), 2019. 2
[50] Mang Ye, Andy J Ma, Liang Zheng, Jiawei Li, and Pong C.
Yuen. Dynamic label graph matching for unsupervised video
re-identification. In ICCV, pages 5142–5150, 2017. 1
[51] Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful
image colorization. In ECCV, pages 649–666, 2016. 2
[52] Xu Zhang, X Yu Felix, Sanjiv Kumar, and Shih-Fu Chang.
Learning spread-out local feature descriptors.
In ICCV,
pages 4605–4613, 2017. 1, 2
[53] Xuan Zhang, Hao Luo, Xing Fan, Weilai Xiang, Yixiao Sun,
Qiqi Xiao, Wei Jiang, Chi Zhang, and Jian Sun.
Aligne-
dreid: Surpassing human-level performance in person re-
identification. arXiv preprint arXiv:1711.08184, 2017. 1
[54] J Zhao, M Mathieu, R Goroshin, and Y Lecun.
S-
tacked what-where auto-encoders.
arXiv preprint arX-
iv:1506.02351. 6
6219
