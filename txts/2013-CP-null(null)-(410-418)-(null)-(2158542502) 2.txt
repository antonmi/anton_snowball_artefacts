Learning with Marginalized Corrupted Features
Laurens van der Maaten
lvdmaaten@gmail.com
Delft University of Technology, Mekelweg 4, 2628 CD Delft, THE NETHERLANDS
Minmin Chen
mc15@cec.wustl.edu
Stephen Tyree
swtyree@wustl.edu
Kilian Q. Weinberger
kilian@wustl.edu
Washington University, St. Louis, MO 63130, USA
Abstract
The goal of machine learning is to develop
predictors that generalize well to test data.
Ideally, this is achieved by training on very
large (infinite) training data sets that cap-
ture all variations in the data distribution.
In the case of finite training data, an effec-
tive solution is to extend the training set with
artificially created examples—which, how-
ever, is also computationally costly. We pro-
pose to corrupt training examples with noise
from known distributions within the expo-
nential family and present a novel learning
algorithm, called marginalized corrupted fea-
tures (MCF), that trains robust predictors
by minimizing the expected value of the loss
function under the corrupting distribution—
essentially learning with infinitely many (cor-
rupted) training examples. We show empiri-
cally on a variety of data sets that MCF clas-
sifiers can be trained efficiently, may general-
ize substantially better to test data, and are
more robust to feature deletion at test time.
1. Introduction
In the hypothetical scenario with infinite data drawn
from the data distribution P, even a simple classi-
fier such as nearest neighbor (Cover & Hart, 1967)
becomes close to optimal (viz. its error is twice the
Bayes error).
In the more realistic scenario with a
finite training set, some variations in the data distri-
bution will not be captured and the learned classifier
performs worse at test time than during training. In
Proceedings of the 30 th International Conference on Ma-
chine Learning, Atlanta, Georgia, USA, 2013.
JMLR:
W&CP volume 28. Copyright 2013 by the author(s).
this paper, we propose an algorithm to train a classifier
from infinite training data, by corrupting the existing
finite training examples with a fixed noise distribution.
So instead of approximating the exact statistics of P
with finite data, we approximate a slightly modified
data distribution P′ with infinite data.
Our augmented data distribution P′ follows a simple
stochastic rule: pick one of the finite training examples
uniformly at random and transform it with some pre-
defined corrupting distribution. Many corrupting dis-
tributions are possible, but we focus on: i) Poisson cor-
ruption and ii) blankout / dropout corruption (random
deletion of features). The Poisson corruption model is
of interest when the data comprises count vectors, e.g.,
in document classification. It is particularly appealing
as it introduces no additional hyper-parameters and, in
our results, improves the test accuracy on almost all
data sets and loss functions. Robustness to blankout
corruption is of interest in settings with heavy-tailed
feature distributions, and in the "nightmare at test
time" scenario (Globerson & Roweis, 2006) in which
some of the features are blanked out during testing
(e.g., due to sensors failing or because the feature com-
putation exceeds a time budget).
Previous work (Burges & Sch¨olkopf, 1997) explicitly
augments the training set with additional examples
that are corrupted through similar transformations.
Although the simplicity of such an approach is ap-
pealing, it lacks elegance and the computational cost
of processing the additional corrupted training exam-
ples is prohibitive for most real-world problems. We
show that it is efficient to train predictors on an infi-
nite amount of corrupted copies of the training data by
marginalizing out the corrupting distribution. In par-
ticular, we focus on empirical risk minimization and
derive analytical solutions for a large family of cor-
rupting distributions and a variety of loss functions.
Learning with Marginalized Corrupted Features
In summary, we make the following contributions: i)
we introduce learning with marginalized corrupted fea-
tures (MCF), a framework that regularizes classifiers
by marginalizing out feature corruptions; ii) we derive
analytical solutions for quadratic, exponential, and lo-
gistic loss functions for a range of corrupting distribu-
tions; and iii) on several real-world data sets, we show
that training with MCF may lead to better classifiers
than training with common l1 or l2-norm regularizers.
2. Related work
Next
to
work
that
explicitly
corrupts
training
data (Burges & Sch¨olkopf, 1997), several prior studies
consider implicit approaches to classifying objects that
are subject to corruptions. Most of these studies mini-
mize the loss under an adversarial worst-case scenario.
In particular, Globerson & Roweis (2006) and Dekel &
Shamir (2008) propose minimax formulations in which
the maximum loss of an example over all
�D
K
�
possi-
ble corrupted examples that blank out K features is
minimized. Others (Bhattacharyya et al., 2004; Shiv-
aswamy et al., 2006; Trafalis & Gilbert, 2007; Xu et al.,
2009) also use minimax approaches that minimize the
loss under a worst-case scenario, but corrupt the data
by adding a constant that is uniformly drawn from
[−u, u] to each of the features.
Chechik et al. (2008) propose an algorithm that max-
imizes the margin in the subspace of the observed fea-
tures for each training instance to deal with randomly
deleted features. Teo et al. (2008) generalize the worst-
case scenario to obtain invariances to transformations
such as image rotations or translations. Their frame-
work incorporates several prior formulations on learn-
ing with invariants as special cases; e.g., Herbrich &
Graepel (2004), who generalized SVMs to be invariant
under polynomial input transformations.
Br¨uckner
et al. (2012) study a worst-case scenario in which an
adversary changes the data distribution to minimize a
function that may be antagonistic to the loss.
Prior work differs from MCF in that the corruption is
not computed analytically in expectation. In partic-
ular, existing approaches have two disadvantages: i)
they are complex and computationally expensive and
ii) they minimize the loss of a worst-case scenario that
is unlikely to be encountered in practice. By contrast,
MCF scales linearly in the number of training exam-
ples and considers an average-case instead of a worst-
case scenario.
Moreover, MCF can readily be used
with a variety of loss functions and corruption models.
Bishop (1995) and Webb (1994) proposed approaches
that can be viewed as approximations to MCF for ad-
ditive noise with small variance. By contrast, MCF is
exact and can be used with noise distributions with po-
tentially very high variance as well. MCF was inspired
by recent successes of denoising autoencoders (Glorot
et al., 2011; Vincent et al., 2008). Since autoencoders
are non-linear, the marginalization over the corrupting
distribution cannot be performed analytically in such
models. Linear denoising autoencoders (Chen et al.,
2012) can be viewed as a special case of MCF that aim
to minimize the expected value of the reconstruction
error under blankout corruption.
3. Learning with Marginalized
Corrupted Features (MCF)
To derive the marginalized corrupted features (MCF)
framework, we start by defining a corrupting distri-
bution that specifies how training observations x are
transformed into corrupted versions ˜x.
We assume
that the corrupting distribution factorizes over dimen-
sions1 and that each individual distribution is a mem-
ber of the natural exponential family:
p(˜x|x)=
D
�
d=1
PE(˜xd|xd; ηd),
(1)
where ηd indicates (user-defined) parameters of the
corrupting distribution on dimension d. We also as-
sume that the corrupting distribution is unbiased, i.e.
that E[˜x]p(˜x|x) = x. This assumption is necessary be-
cause biases in the corrupting distribution may lead
to undesired scale changes in the parameters ηd. Most
corrupting distributions of interest are unbiased or can
easily be made so; examples for PE are the unbi-
ased "blankout" noise (Vincent et al., 2008), Gaussian
noise, Laplace noise, and Poisson noise.
Explicit corruption. A simple approach to improv-
ing the generalization of a classifier using a corrupt-
ing distribution is to follow the spirit of Burges &
Sch¨olkopf (1997) by selecting each element of the train-
ing set D = {(xn,yn)}N
n=1 and corrupting it M times,
following (1). For each xn, this results in correspond-
ing corrupted observations ˜xnm (with m = 1, . . . , M).
This leads to the construction of a new data set ˜D of
size | ˜D| = MN. This extended data set can be used
for training by minimizing:
L( ˜D; Θ) =
N
�
n=1
1
M
M
�
m=1
L(˜xnm, yn; Θ),
with ˜xnm ∼ p(˜xnm|xn), Θ the set of model parame-
ters, and L(x, y; Θ) the loss function of the model.
1For Gaussian noise models, this assumption is unneces-
sary: we can also work with non-isotropic Gaussian noise.
Learning with Marginalized Corrupted Features
Distribution
PDF
E[˜xnd]p(˜xnd|xnd)
V [˜xnd]p(˜xnd|xnd)
Blankout noise
p(˜xnd = 0) = qd
p(˜xnd =
1
1−qd xnd) = 1−qd
xnd
qd
1−qd x2
nd
Gaussian noise
p(˜xnd|xnd) = N(˜xnd|xnd, σ2)
xnd
σ2
Laplace noise
p(˜xnd|xnd) = Lap(˜xnd|xnd, λ)
xnd
2λ2
Poisson noise
p(˜xnd|xnd) = Poisson(˜xnd|xnd)
xnd
xnd
Table 1. The probability density function (PDF), mean, and variance of corrupting distributions of interest.
These
quantities can be plugged into Eq. (3) to obtain the expected value under the corrupting distribution of the quadratic
loss.
Implicit corruption. Although such an approach is
effective, it lacks elegance and comes with high com-
putational costs, as the minimization of L( ˜D; Θ) scales
linearly in the number of corrupted observations. It
is, however, of interest to consider the limiting case in
which M →∞. In this case, we can apply the weak law
of large numbers and rewrite
1
M
�M
m=1 L(˜xm, ym; Θ)
as its expectation (Duda et al., 2001, §2.10.2):
L(D; Θ) =
N
�
n=1
E[L(˜xn, yn; Θ)]p(˜xn|xn).
(2)
Minimizing the expected value of the loss under the
corruption model leads to a new approach for training
predictors that we refer to as learning with marginal-
ized corrupted features (MCF).
3.1. Specific loss functions
The tractability of (2) depends on the choice of loss
function and corrupting distribution PE. In this sec-
tion, we show that for linear predictors that employ
a quadratic or exponential loss function, the required
expectations under p(˜x|x) in (2) can be computed an-
alytically for all corrupting distributions in the natural
exponential family. For linear predictors with logistic
loss, we derive a practical upper bound on the expected
loss under p(˜x|x), which serves as surrogate loss.
Quadratic
loss.
Assuming2
a
label
variable
y∈{−1, +1}, the expected value of the quadratic loss
under corrupting distribution p(˜x|x) is given by:
L(D; w) =
N
�
n=1
E
��
wT˜xn − yn
�2�
p(˜xn|xn)
= wT
�
N
�
n=1
E[˜xn]E[˜xn]T + V [˜xn]
�
w
− 2
�
N
�
n=1
ynE [˜xn]
�T
w + N,
(3)
2The same derivations are applicable to regression set-
tings in which y is a continuous variable.
where V [x] is a diagonal D×D matrix storing the vari-
ances of x, and all expectations are under p(˜xn|xn).
Eq. (3) is convex irrespective of what corruption model
is used; the optimal solution w∗ is given by:
w∗ =
�
N
�
n=1
E[˜xn]E[˜xn]T + V [˜xn]
�−1�
N
�
n=1
ynE [˜xn]
�
.
To minimize the expected quadratic loss under the cor-
ruption model, we only need to compute the variance
of the corrupting distribution, which is practical for all
exponential-family distributions. The mean is always
xnd because our corrupting distributions are unbiased.
Table 1 gives an overview of the variances of corrupt-
ing distributions of interest. (In blankout corruption,
we scale the value of "preserved" features by
1
1−qd to
ensure that the corrupting distribution is unbiased.)
An interesting setting of MCF with quadratic loss oc-
curs when the corrupting distribution p(˜x|x) is an
isotropic Gaussian distribution with mean x and vari-
ance σ2I. For such a Gaussian corruption model, we
obtain as special case (Chapelle et al., 2000):
L(D; w) = wT
�
N
�
n=1
xnxT
n
�
w
− 2
�
N
�
n=1
ynxn
�T
w + σ2NwTw + N,
which is the standard l2-regularized quadratic loss
with regularization parameter σ2N. Interestingly, us-
ing MCF with Laplace noise also leads to ridge regres-
sion (with regularization parameter 2λ2N).
Exponential loss. The expected value of the expo-
nential loss under corruption model p(˜x|x) is:
L(D; w) =
N
�
n=1
E
�
e−ynwT˜xn�
p(˜xn|xn)
=
N
�
n=1
D
�
d=1
E
�
e−ynwd˜xnd�
p(˜xnd|xnd) ,
(4)
Learning with Marginalized Corrupted Features
Distribution
E[exp(−ynwd˜xnd)]p(˜xnd|xnd)
Blankout noise
qd + (1 − qd) exp(−ynwd
1
1−qd xnd)
Gaussian noise
exp(−ynwdxnd + 1
2σ2y2
nw2
d)
Laplace noise
(1 − λ2y2
nw2
d)−1 exp(−ynwdxnd)
Poisson noise
exp(xnd(exp(−ynwd) − 1))
Table 2. Moment-generating functions (MGFs) of various
corrupting distributions. These quantities can be plugged
into equations (4) and (5) to obtain the expected value of
the loss (or surrogate) under the corrupting distribution of
the exponential and logistic loss functions, respectively.
where we used the assumption that the corruption is
independent across features. The above equation can
be recognized as a product of moment-generating func-
tions E[exp(tnd˜xnd)] with tnd =−ynwd. The moment-
generating function (MGF) can be computed for all
corrupting distributions in the natural exponential
family.
An overview of these MGFs is given in Ta-
ble 2. Because the expected exponential loss is a con-
vex combination of convex functions, it is itself convex
irrespective of what corruption model is used.
The derivation above can readily be extended to multi-
class exponential loss (Zhu et al., 2006) (with K
classes) by replacing the weight vector w by a D×K
weight matrix W, and by replacing the labels y by
label vectors y = {1, −
1
K−1}K with �K
k=1 yk = 0.
Logistic loss.
In the case of the logistic loss, the
solution to (2) cannot be computed in closed form.
Instead, we derive an upper bound, which can be min-
imized as a surrogate loss:
L(D; w) =
N
�
n=1
E
�
log
�
1 + e−ynwT˜xn
��
p(˜xn|xn)
≤
N
�
n=1
log
�
1 +
D
�
d=1
E
�
e−ynwd˜xnd�
p(˜xnd|xnd)
�
.
(5)
Herein, we have made use of Jensen's inequality to
upper-bound E[log z]. In the upper bound, we again
recognize a product of MGFs, which can be computed
in closed-form for corrupting distributions in the natu-
ral exponential family (see Table 2). The upper bound
on the expected logistic loss is convex whenever the
moment-generating function is log-linear in wd, e.g.,
for blankout corruption.
Again, the above derivation can readily be extended
to multi-class logistic loss (by redefining the labels y
to be label vectors y ∈ {0, 1}K with �K
k=1 yk = 1; and
by defining the loss as the logarithm of the softmax-
probability of making the correct prediction).
Case study. As an illustrative example, we show the
upper bound of the logistic loss (5), where inputs are
corrupted with the Poisson distribution:
L(D; w) ≤
N
�
n=1
log
�
1 + exp
� D
�
d=1
xnd(e−ynwd − 1)
��
.
Remarkably, this regularized loss does not have any
additional hyper-parameters. Further, the sum over
all features can still be computed efficiently for sparse
data by summing only over non-zero entries in xn.
Computational complexity. The use of MCF does
not impact the computational complexity of training:
the complexity of the training algorithms remains lin-
ear in N. The additional training time for minimiz-
ing quadratic loss with MCF is negligible, because the
computation time is dominated by the inversion of a
D ×D-matrix. For exponential and logistic loss, we
empirically found that computing the gradient of the
MCF loss was 2× and 10× slower than computing the
gradient of the "normal" loss, respectively.
4. Experiments
We perform experiments comparing MCF predictors
with standard predictors on three tasks:
i) docu-
ment classification based on word-count features us-
ing MCF with blankout and Poisson corruption; ii)
image classification based on bag-of-visual-word fea-
tures using MCF with blankout and Poisson corrup-
tion; and iii) classification of objects in the "night-
mare at test time" scenario using MCF with blank-
out corruption. The three experiments are described
separately below.
Code to reproduce the results of
our experiments is available from http://homepage.
tudelft.nl/19j49/mcf.
4.1. Document classification
We first test MCF predictors with blankout and
Poisson corruption on document classification tasks.
Specifically, we focus on three data sets: the Dmoz
data set, the Reuters data set, and the Amazon re-
view benchmark set (Blitzer et al., 2007).
Data sets. The Dmoz open directory (http://www.
dmoz.org) contains a large collection of webpages ar-
ranged into a tree hierarchy. The subset we use con-
tains N = 8, 980 webpages from the K = 16 cate-
gories in the top level of the hierarchy. Each webpage
is represented by a bag-of-words representation with
D=16, 498 words. The Reuters data set is a collection
of documents that appeared on the Reuters newswire
in 1987. Documents with multiple labels were removed
from the data, resulting in a set of N = 8, 293 doc-
uments from K = 65 categories.
The bag-of-words
Learning with Marginalized Corrupted Features
0
0.2
0.4
0.6
0.8
1
0.15
0.16
0.17
0.18
 
0
0.2
0.4
0.6
0.8
1
0.1
0.11
0.12
 
Classification error
0
0.2
0.4
0.6
0.8
1
0.34
0.36
0.38
0.4
0.42
0.44
0.46
0.48
 
0
0.2
0.4
0.6
0.8
1
0.13
0.14
0.15
0.16
0.17
0.18
 
DMOZ
0
0.2
0.4
0.6
0.8
1
0.06
0.08
0.1
0.12
0.14
0.16
0.18
 
Reuters
Blankout corruption level q
0
0.2
0.4
0.6
0.8
1
0.11
0.12
0.13
0.14
0.15
 
 
 
Quadratic loss
Exponential loss
Logistic loss
Blankout MCF (Qua.)
Blankout MCF (Exp.)
Blankout MCF (Log.)
Poisson MCF (Qua.)
Poisson MCF (Exp.)
Poisson MCF (Log.)
Amazon / Kitchen
Amazon / DVD
Amazon / Electronics
Amazon / Books
Figure 1. Classification errors of MCF predictors using blankout and Poisson corruption – as a function of the blankout
corruption level q – on the Amazon, Dmoz, and Reuters data sets for l2-regularized quadratic, exponential, and quadratic
loss functions. Classification errors are represented on the y-axis, whereas the blankout corruption level q is represented
on the x-axis. The case of MCF with blankout corruption and q = 0 corresponds to a standard l2-regularized classifier.
Figure best viewed in color.
representation contains D = 18, 933 words for each
document. The four Amazon data sets consist of ap-
proximately N = 6, 000 reviews of four types of prod-
ucts:
books, DVDs, electronics, and kitchen appli-
ances. Each review is represented by a bag-of-words
representation of the D=20, 000 most common words.
On the Dmoz and Reuters data sets, the task is to
classify the documents into one of the predefined cate-
gories. On the Amazon data set, the task is to decide
whether a review is positive or negative.
Setup. On the Dmoz and Reuters data sets, we use
a fixed training set of 75% of the data, and evaluate
the performance of our predictors on a fixed test set of
25% of the data. On the Amazon data set, we follow
the experimental setup of Blitzer et al. (2007) by using
a fixed division of the data into approximately 2, 000
training examples and about 4, 000 test examples (the
exact numbers vary slightly between tasks). All ex-
periments are performed using linear classifiers that
are trained with l2-regularization; the amount of l2-
regularization is determined via cross-validation. We
consider quadratic, exponential, and logistic loss func-
tions (both with and without MCF). The minimiza-
tion of the (expected) exponential and logistic losses
is performed by running Mark Schmidt's minFunc-
implementation of L-BFGS until convergence or until
a predefined maximum number of iterations is reached.
All predictors included a bias term that is neither reg-
ularized nor corrupted. In our experiments with MCF
using blankout corruption, we use the same noise level
for each feature, i.e. we assume that ∀d : qd = q. On
all data sets, we first investigate the performance of
MCF as a function of the corruption level q (but we
still cross-validate over the l2-regularizer). In a second
set of experiments, we cross-validate over the blankout
corruption parameter q and study to what extent the
performance (improvements) of MCF depend on the
amount of available training data. (MCF with Pois-
son corruption has no additional hyper-parameters, as
a result of which it requires no extra cross-validations.)
Results. Figure 1 shows the test error of our MCF
predictors on all data sets as a function of the blank-
out corruption level q. Herein, corruption level q = 0
corresponds to the baseline predictors, i.e.
to pre-
dictors that do not employ MCF. The results show:
i) that MCF improves over standard predictors for
both blankout corruption (for all corruption levels q)
and Poisson corruption on five out of six tasks; ii)
that MCF with Poisson corruption leads to signifi-
Learning with Marginalized Corrupted Features
cant performance improvements over standard clas-
sifiers whilst introducing no additional hyperparam-
eters; and iii) that the best performance tends to be
achieved by MCF with blankout corruption with high
corruption levels, i.e. when q is in the order of 0.8. The
best-performing MCF classifiers reduce the test errors
by up to 22% on the Amazon data if q is properly set.
In many of the experiments with MCF-trained losses
(in particular, when blankout corruption is used), we
also observe that the optimal level of l2-regularization
is 0. This shows that MCF has a regularizing effect in
itself, rendering additional regularization superfluous.
Figure 2 presents the results of a second set of experi-
ments on Dmoz and Reuters in which we study how the
performance of MCF depends on the amount of train-
ing data. For each training set size, we repeat the ex-
periment five times with randomly sub-sampled train-
ing sets; the figure reports the mean test errors and the
corresponding standard deviations. The results show
that classifiers trained with MCF (solid curves) signif-
icantly outperform their counterparts without MCF
(dashed curves).
The performance improvement is
consistent irrespective of the training set size, viz. up
to 25% on the Dmoz data set.
Explicit vs.
implicit feature corruption.
Fig-
ure 3 shows the classification error on Amazon (books)
when a classifier without MCF is trained on the data
set with additional explicitly corrupted samples, as for-
mulated in (3). Specifically, we use the blankout cor-
ruption model with q set by cross-validation for each
setting, and we train the classifiers with quadratic loss
and l2-regularization. The graph shows a clear trend
that the error decreases when the training set contains
more corrupted versions of the original training data,
i.e. with higher M in eq. (3). The graph illustrates
that the best performance is obtained as M approaches
infinity, which is equivalent to MCF with blankout cor-
ruption (marker in the bottom right; q=0.9).
4.2. Image classification
We performed image-classification experiments on the
CIFAR-10 data set (Krizhevsky, 2009), which is a sub-
set of the 80 million tiny images (Torralba et al., 2008).
The data set contains RGB images with 10 classes of
size 32×32, and consists of a fixed training and test
set of 50, 000 and 10, 000 images, respectively.
Setup. We follow the experimental setup of Coates
et al. (2011): we whiten the training images and ex-
tract a set of 7 × 7 image patches on which we ap-
ply k-means clustering (with k =2048) to construct a
codebook. Next, we slide a 7×7 pixel window over
each image and identify the nearest prototype in the
# of labeled training data
Blankout / DMOZ
Poisson / DMOZ
Blankout / Reuters
Poisson / Reuters
100
200
400
800
1600
3200
7184
0.4
0.5
0.6
0.7
0.8
 
 
 
Quadratic loss (L2)
Exponential loss (L2)
Logistic loss (L2)
Blankout MCF (qua.)
Blankout MCF (exp.)
Blankout MCF (log.)
100
200
400
800
1600
3200
5946
0.1
0.15
0.2
0.25
0.3
 
100
200
400
800
1600
3200
7184
0.4
0.5
0.6
0.7
0.8
 
 
 
Quadratic loss (L2)
Exponential loss (L2)
Logistic loss (L2)
Poisson MCF (qua.)
Poisson MCF (exp.)
Poisson MCF (log.)
100
200
400
800
1600
3200
5946
0.1
0.15
0.2
0.25
0.3
 
Classification error
Figure 2. The performance of standard and MCF classi-
fiers with blankout and Poisson corruption models as a
function of training set size on the Dmoz and Reuters data
sets. Both the standard and MCF predictors employ l2-
regularization. Figure best viewed in color.
Learning with Marginalized Corrupted Features
0
1
2
4
8
16
32
64 128 256
inf
0.13
0.14
0.15
0.16
0.17
0.18
0.19
Noise
Error
 
 
Explicit corruption
Implicit corruption (MCF)
# of corrupted copies
Classification error
…... 11
Figure 3. Comparison between MCF and explicitly adding
corrupted examples to the training set (for quadratic loss)
on the Amazon (books) data using blankout corruption.
Training with MCF is equivalent to using infinitely many
corrupted copies of the training data.
codebook for each window location. We construct an
image descriptor3 by subdividing the image into four
equally sized quadrants and counting the number of
times each prototype occurs in each quadrant. This
leads to a descriptor of dimensionality D = 4×2048.
We do not normalize the descriptors, because all im-
ages have the same size.
We train MCF predictors
with blankout and Poisson corruption on the full set
of training images, cross-validating over a range of l2-
regularization parameters. The generalization error is
evaluated on the test set.
Results. The results are reported in Table 3. The
baseline classifiers (without MCF) are comparable to
the 68.8% accuracy reported by Coates et al. (2011)
with exactly the same experimental setup (except for
exponential loss). The results illustrate the potential
of MCF classifiers to improve the prediction perfor-
mance on bag-of-visual-words features, in particular,
when using quadratic or logistic loss in combination
with a Poisson corruption model. Although our focus
in this section is to merely illustrate the potential of
MCF on image classification tasks, it is worth noting
that the best results in Table 3 match those of a highly
non-linear mean-covariance RBMs trained on the same
data (Ranzato & Hinton, 2010), despite our use of very
simple visual features and of linear classifiers.
4.3. Nightmare at test time
To test the performance of our MCF predictors with
blankout corruption under the "nightmare at test
3This way of extracting the image features is referred to
by Coates et al. (2011) as k-means with hard assignment,
average pooling, patch size 7×7, and stride 1.
Quadr.
Expon.
Logist.
No MCF
32.6%
39.7%
32.5%
Poisson MCF
29.1%
39.5%
30.0%
Blankout MCF
32.3%
37.9%
29.4%
Table 3. Classification errors obtained on the CIFAR-10
data set with MCF classifiers trained on simple spatial-
pyramid bag-of-visual-words features.
time" scenario, we perform experiments on the MNIST
handwritten digits data set. The MNIST data set con-
tains N = 60, 000 training and 10, 000 test images of
size D=28×28=784 pixels with K =10 classes.
Setup. We train our predictors on the full training
set, and evaluate their performance on versions of the
test set in which a certain percentage of the pixels are
randomly blanked out, i.e. set to zero. We compare
the performance of our MCF-predictors (using blank-
out corruption) with that of standard predictors that
use l1 or l2-regularized quadratic, exponential, logistic,
and hinge loss. As before, we use cross-validation to
determine the optimal value of the regularization pa-
rameter. For MCF predictors, we also cross-validate
over the blankout corruption level q (again, we use the
same noise level for each feature, i.e. ∀d : qd = q).
In addition to the comparisons with standard predic-
tors, we also compare the performance of MCF with
that of FDROP (Globerson & Roweis, 2006), which is
a state-of-the-art algorithm for the "nightmare at test
time" setting that minimizes the hinge loss under an
adversarial worst-case scenario.
The performances are reported as a function of the
feature-deletion percentage in the test set, i.e. as a
function of the probability with which a pixel in the
test set is switched off.
Following the experimental
setting of Globerson & Roweis (2006), we perform the
cross-validation for each deletion percentage indepen-
dently, i.e. we create a small validation set with the
same feature-deletion level and use it to determine the
best regularization parameters and blankout corrup-
tion level q for that percentage of feature deletions.
Results. Figure 4 shows the performance of our pre-
dictors as a function of the percentage of deletions in
the test images.
The figure shows the performance
for all three loss functions with MCF (solid lines) and
without MCF (dashed lines). The performance of a
standard predictor using hinge loss is shown as a red
dashed line; the performance of FDROP is shown as
a black dashed line. The results presented in Figure 4
clearly illustrate the ability of MCF with blankout
corruption to produce predictors that are robust to
the "nightmare at test time" scenario: MCF improves
the performance substantially for all three loss func-
Learning with Marginalized Corrupted Features
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
0.1
0.2
0.3
0.4
0.5
 
 
Quadratic loss (L2)
Exponential loss (L2)
Logistic loss (L2)
Hinge loss (L2)
Hinge loss (FDROP)
MCF quadratic loss
MCF exponential loss
MCF logistic loss
Figure 4. Classification errors of standard and MCF pre-
dictors with a blankout corruption model – trained us-
ing three different losses – and of FDROP (Globerson &
Roweis, 2006) on the MNIST data set using the "night-
mare at test time" scenario. Classification errors are rep-
resented on the y-axis, whereas the percent of features that
are deleted at test time is represented on the x-axis. Figure
best viewed in color.
tions considered. For instance, in the case in which
50% of the pixels in the test images is deleted, the
performance improvements obtained using MCF for
quadratic, exponential, and logistic loss are 40%, 47%,
and 60%, respectively. Further, the results also indi-
cate that MCF-losses may outperform FDROP: our
MCF logistic loss outperforms FDROP's worst-case
hinge loss across the board4. This is particularly im-
pressive as the standard hinge loss performs surpris-
ingly better than standard logistic loss on this data set
(the improvement of FDROP over the generic hinge-
loss is relatively modest). This result suggests that it
is better to consider an average-case than a worst-case
scenario in the "nightmare at test time" setting.
5. Discussion and Future Work
We presented an approach to learn classifiers by
marginalizing corrupted features (MCF). Specifically,
MCF trains predictors by introducing corruption on
the training examples, which is marginalized out in
the expectation of the loss function. We minimize the
expected loss with respect to the model parameters.
Our experimental results show that MCF predictors
with blankout and Poisson corruption perform very
well in the context of bag-of-words features.
MCF
with Poisson corruption is particularly interesting for
such count features, as it improves classification per-
4Quadratic and exponential losses perform somewhat
worse because they are less appropriate for linear classifiers,
but even they outperform FDROP for large numbers of
feature deletions in the test data.
formances without introducing any additional hyper-
parameters. As a disclaimer, care must be taken when
applying MCF with Poisson corruption on data sets
with outliers. Poisson corruption may emphasize out-
liers in the expected loss because the variance of a
Poisson distribution is equal to its mean, and because
our loss functions are not robust to outliers. A solu-
tion to this problem may be to redefine the corruption
distribution to p(˜xd|xd) = Pois(˜xnd| min{xnd, u}) for
some cutoff parameter u≥0.
In most of our experiments with MCF, the l2-
regularizer
parameter
(which
was
set
by
cross-
validation) ended up very close to zero. This implies
that MCF in itself has a regularizing effect. At the
same time, MCF with blankout corruption also ap-
pears to prevent weight undertraining (Sutton et al.,
2005): it encourages the weight on each feature to be
non-zero, in case this particular feature survives the
corruption.
Our experiments also reveal that MCF
with blankout corruption produces predictors that are
more robust to the "nightmare at test time" scenario,
making it useful in learning settings in which features
in the test data may be missing. Learning with MCF
is quite different from previous approaches for this
setting (Dekel & Shamir, 2008; Globerson & Roweis,
2006): it does not learn under a worst-case scenario,
but the (arguably) more common average-case sce-
nario by considering all possible corrupted observa-
tions. This has the advantage that it is computation-
ally much cheaper and that it allows for incorporating
prior knowledge. For instance, if the data is generated
by a collection of unreliable sensors, knowledge on the
sensor reliability may be used to set the qd-parameters.
In future work, we intend to explore extensions of MCF
to regression and structured prediction, as well as to
investigate if MCF can be employed for kernel ma-
chines. We also plan to explore in more detail what
corruption models p(˜x|x) are useful for what types of
data, and how these corruption models regularize clas-
sifiers (Ng, 2004). Further, MCF could be used in the
training of neural networks with a single layer of hid-
den units: blankout noise on the hidden nodes can im-
prove the performance of the networks (LeCun et al.,
1990; Sietsma & Dow, 1991; Hinton et al., 2012) and
can be marginalized out analytically. A final interest-
ing direction is to investigate the effect of marginaliz-
ing corrupted labels (Lawrence & Sch¨olkopf, 2001).
Acknowledgements
LvdM is supported by FP7 Social Signal Processing (SSP-
Net). KQW, MC, and ST are supported by NIH grant U01
1U01NS073457-01 and NSF grants 1149882 and 1137211.
The authors thank Fei Sha for helpful discussions.
Learning with Marginalized Corrupted Features
References
Bhattacharyya, C., Pannagadatta, K.S., and Smola, A.J.
A second order cone programming formulation for clas-
sifying missing data. In Advances in Neural Information
Processing Systems, pp. 153–160, 2004.
Bishop, C.M. Training with noise is equivalent to tikhonov
regularization. Neural Computation, 7(1):108–116, 1995.
Blitzer, J., Dredze, M., and Pereira, F. Biographies, Bol-
lywood, boom-boxes and blenders: Domain adaptation
for sentiment classification. In Association for Compu-
tational Linguistics, volume 45, pp. 440, 2007.
Br¨uckner, M., Kanzow, C., and Scheffer, T. Static predic-
tion games for adversarial learning problems. Journal of
Machine Learning Research, 12:2617–2654, 2012.
Burges, C.J.C. and Sch¨olkopf, B. Improving the accuracy
and speed of support vector machines. Advances in Neu-
ral Information Processing Systems, 9:375–381, 1997.
Chapelle, O., Weston, J., Bottou, L., and Vapnik, V. Vic-
inal risk minimization. In Advances in Neural Informa-
tion Processing Systems, pp. 416–422, 2000.
Chechik, G., Heitz, G., Elidan, G., Abbeel, P., and Koller,
D. Max-margin classification of data with absent fea-
tures. Journal of Machine Learning Research, 9(Jan):
1–21, 2008.
Chen, M., Xu, Z., Weinberger, K.Q., and Sha, F. Marginal-
ized denoising autoencoders for domain adaptation. In
Proceedings of the International Conference on Machine
Learning, pp. 767–774, 2012.
Coates, A., Lee, H., and Ng, A.Y. An analysis of single-
layer networks in unsupervised feature learning. In Pro-
ceedings of the International Conference on Artificial In-
telligence & Statistics, JMLR W&CP 15, pp. 215–223,
2011.
Cover, T. and Hart, P. Nearest neighbor pattern classifi-
cation. IEEE Transactions on Information Theory, 13
(1):21–27, 1967.
Dekel, O. and Shamir, O. Learning to classify with missing
and corrupted features. In Proceedings of the Interna-
tional Conference on Machine Learning, pp. 216–223,
2008.
Duda, R.O., Hart, P.E., and Stork, D.G. Pattern Classifi-
cation. Wiley Interscience Inc., 2001.
Globerson, A. and Roweis, S. Nightmare at test time: Ro-
bust learning by feature deletion. In Proceedings of the
International Conference on Machine Learning, pp. 353–
360, 2006.
Glorot, X., Bordes, A., and Bengio, Y. Domain adaptation
for large-scale sentiment classification: A deep learning
approach. In Proceedings of the International Conference
on Machine Learning, pp. 513–520, 2011.
Herbrich, R. and Graepel, T. Invariant pattern recognition
by semidefinite programming machines. In Advances in
Neural Information Processing Systems, volume 16, pp.
33, 2004.
Hinton, G.E., Srivastava, N., Krizhevsky, A., Sutskever, I.,
and Salakhutdinov, R.R. Improving neural networks by
preventing co-adaptation of feature detectors, 2012.
Krizhevsky, A. Learning multiple layers of features from
tiny images.
Technical report, University of Toronto,
2009.
Lawrence, N.D. and Sch¨olkopf, B.
Estimating a kernel
Fisher discriminant in the presence of label noise.
In
Proceedings of the International Conference in Machine
Learning, pp. 306–313, 2001.
LeCun, Y., Denker, J.S., and Solla, S.A. Optimal brain
damage. In Advances in Neural Information Processing
Systems, pp. 598–605, 1990.
Ng, A.Y. Feature selection, l1 vs. l2 regularization, and
rotational invariance.
In Proceedings of International
Conference on Machine Learning, pp. 78–85, 2004.
Ranzato, M. and Hinton, G.E. Modeling pixel means and
covariances using factorized third-order Boltzmann ma-
chines. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pp. 2551–2558,
2010.
Shivaswamy, P.K., Bhattacharyya, C., and Smola, A.J.
Second order cone programming approaches for handling
missing and uncertain data. Journal of Machine Learn-
ing Research, 7:1283–1314, 2006.
Sietsma, J. and Dow, R.J.F. Creating artificial neural net-
works that generalize. Neural Networks, 4:67–79, 1991.
Sutton, C., Sindelar, M., and McCallum, A. Feature bag-
ging: Preventing weight undertraining in structured dis-
criminative learning. Technical Report IR-402, Univer-
sity of Massachusetts, 2005.
Teo, C.H., Globerson, A., Roweis, S., and Smola, A. Con-
vex learning with invariances. Advances in Neural In-
formation Processing Systems, 20:1489–1496, 2008.
Torralba, A., Fergus, R., and Freeman, W.T. 80 million
tiny images: A large dataset for non-parametric object
and scene recognition. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 30(11):1958–1970,
2008.
Trafalis, T. and Gilbert, R. Robust support vector ma-
chines for classification and computational issues. Opti-
mization Methods and Software, 22(1):187–198, 2007.
Vincent, P., Larochelle, H., Bengio, Y., and Manzagol, P.A.
Extracting and composing robust features with denois-
ing autoencoders.
In Proceedings of the International
Conference on Machine Learning, pp. 1096–1103, 2008.
Webb, A.R. Functional approximation by feed-forward net-
works: a least-squares approach to generalization. IEEE
Transactions on Neural Networks, 5(3):363–371, 1994.
Xu, H., Caramanis, C., and Mannor, S. Robustness and
regularization of support vector machines.
Journal of
Machine Learning Research, 10:1485–1510, 2009.
Zhu, J., Rosset, S., Zou, H., and Hastie, T. Multi-class Ad-
aBoost. Technical Report 430, Department of Statistics,
University of Michigan, 2006.
