Deeply-Supervised Nets
Chen-Yu Lee ∗
Saining Xie ∗
Patrick W. Gallagher
Dept. of ECE, UCSD
chl260@ucsd.edu
Dept. of CSE and CogSci, UCSD
s9xie@ucsd.edu
Dept. of CogSci, UCSD
patrick.w.gallagher@gmail.com
Zhengyou Zhang
Zhuowen Tu
Microsoft Research
zhang@microsoft.com
Dept. of CogSci, UCSD
ztu@ucsd.edu
Abstract
We propose deeply-supervised nets (DSN), a
method that simultaneously minimizes clas-
sification error and improves the directness
and transparency of the hidden layer learn-
ing process. We focus our attention on three
aspects of traditional convolutional-neural-
network-type (CNN-type) architectures: (1)
transparency in the effect intermediate layers
have on overall classification; (2) discrimina-
tiveness and robustness of learned features,
especially in early layers; (3) training ef-
fectiveness in the face of "vanishing" gradi-
ents. To combat these issues, we introduce
"companion" objective functions at each hid-
den layer, in addition to the overall objec-
tive function at the output layer (an inte-
grated strategy distinct from layer-wise pre-
training). We also analyze our algorithm us-
ing techniques extended from stochastic gra-
dient methods. The advantages provided by
our method are evident in our experimen-
tal results, showing state-of-the-art perfor-
mance on MNIST, CIFAR-10, CIFAR-100,
and SVHN.
1
Introduction
Neural networks have recently resurged, most promi-
nently in deep learning (DL). The application of DL
techniques to large training sets has yielded signifi-
cant performance gains in image classification [12, 15]
and speech recognition [5].
However, even though
hierarchical neural networks [8, 11, 16] have shown
great promise in automatically learning thousands or
Appearing in Proceedings of the 18th International Con-
ference on Artificial Intelligence and Statistics (AISTATS)
2015, San Diego, CA, USA.
JMLR: W&CP volume 38.
Copyright 2015 by the authors. [*equal contribution]
even millions of features for pattern recognition, there
nonetheless remain many fundamental open questions
about DL.
These open questions arise with various aspects of cur-
rent DL frameworks: the features learned at hidden
layers (early hidden layers in particular) are not al-
ways "transparent" in their meaning and at times dis-
play reduced discriminativeness [31]; "vanishing" gra-
dients can sometimes lead to training difficulty [9, 20];
despite some theoretical work [7], mathematical un-
derstanding of DL is at an early stage.
Notwith-
standing such issues, DL has proven capable of auto-
matically learning rich hierarchical features combined
within an integrated network. Recent techniques such
as dropout [12], dropconnect [17], pre-training [5], and
data augmentation [22] bring enhanced performance
from various perspectives. Beyond this, recent code-
and experience-sharing [12, 6, 2] has greatly sped the
adoption and advance of DL in and out of machine
learning.
In this paper, we present a new DL approach we
call deeply-supervised nets (DSN). The central idea
of DSN is to provide integrated direct supervision to
the hidden layers, rather than the standard approach
of providing supervision only at the output layer and
propagating this supervision back to earlier layers. We
provide this integrated direct hidden layer supervision
by introducing companion objective functions for each
hidden layer; these companion objective functions can
be seen as an additional (soft) constraint (or as a new
regularization) within the learning process. Our ex-
periments show significant performance gains relative
to existing methods. In addition, we use analysis tech-
niques from stochastic gradient methods to investigate
a restricted setting in which incorporating companion
objective functions directly leads to improved conver-
gence rate.
The advantage of such integrated deep
562
Deeply-Supervised Nets
supervision is evident: (1) for small training data
and relatively shallower networks, deep supervi-
sion functions as a strong "regularization" for classi-
fication accuracy and learned features; (2) for large
training data and deeper networks deep super-
vision makes it convenient to exploit the significant
performance gains that extremely deep networks can
bring by improving otherwise problematic convergence
behavior.
The major characteristic of DSN is its integrated for-
mulation of deep supervision via companion objec-
tives, which in the simplest case are of a form nearly
identical to the objective function at the output layer,
as shown in Eq.
(4).
This integrated formulation
of deep supervision by means of companion objec-
tives immediately sets us apart from all previous ap-
proaches. In effect, we perform network optimization
incorporating companion objectives as a proxy for hid-
den layer feature quality. The combined learning in
our integrated formulation stands in contrast to previ-
ous research in which greedy layer-wise pre-training
was performed separately as either initialization or
fine-tuning, a strategy that resulted in overfitting [1].
There are a few other relevant examples: in [26], they
introduce an additional term only at the output layer
(to use the discriminative label information to assist
with learning the generative model), in strong distinc-
tion to our approach of deep supervision in a super-
vised learning setting; in [30] semi-supervised learning
is carried out by way of an embedding and no super-
vised labels are involved, in contrast to our supervised
approach; the emphasis of [23] is on providing the out-
put classifier with features learned from the hidden
layers — crucially, their technique does not target the
training and regularization benefits of our DSN. We
also note a parallel development bearing some similar-
ity to DSN: GoogLeNet [28] uses supervision on 2 hid-
den layers in a 22-layer CNN, but the two approaches
differ in focus, formulation, emphasis, and analysis.
On the theoretical side, analysis of deep learning is still
at an early stage of development; we mention [7] as a
recent example. Finally, we mention [4]: this work re-
expresses the original output layer classification prob-
lem in an equivalent decouple-able form. Their pur-
pose is to apply a standard optimization technique to
this decoupled equation; this approach is thus quite
distinct from ours. In particular, this approach does
not leverage any label information to achieve benefits
such as regularization. Our experiments are primarily
based on L2SVM objectives, previously used in DL by
[29] for the output layer only; we also show that our
approach of introducing companion objectives is not
dependent on the use of L2SVM objectives — when
using softmax we find similar performance improve-
ments. Our experiments show consistent improvement
of DSN-L2SVM and DSN-Softmax over CNN-L2SVM
and CNN-Softmax, respectively.
We use MNIST as the primary dataset in our exper-
iments; in these MNIST experiments we build DSN
on top of Theano [2]. For experiments on non-MNIST
datasets we build DSN on top of NIN [18]. Our DSN
framework achieves state-of-the-art results on each of
the datasets investigated: MNIST, CIFAR-10, CIFAR-
100, and SVHN. Finally, we note that our approach is
independent of techniques such as averaging [22], drop-
connect [17], and Maxout [10]; thus, we expect that
combining DSN with these techniques might result in
even greater classification error reduction.
2
Deeply-Supervised Nets
Our approach is built using the infrastructure provided
by existing supervised CNN-type frameworks [16, 6,
2]. We extend them by introducing a classifier, either
SVM or Softmax, at hidden layers.
2.1
Motivation
Our motivation for introducing classifiers at hidden
layers comes from the following observation: in gen-
eral, a discriminative classifier trained on highly dis-
criminative features will display better performance
than a discriminative classifier trained on less discrim-
inative features. If the features in question are the hid-
den layer feature maps of a deep network, this obser-
vation means that the performance of a discriminative
classifier trained using these hidden layer feature maps
can serve as a proxy for the quality/discriminativeness
of those hidden layer feature maps. We also expect this
deep supervision to alleviate the common problem of
"vanishing" gradients. One concern with direct pur-
suit of feature discriminativeness at all hidden layers is
that this might interfere with the overall network per-
formance; our experimental results indicate that this is
not the case. Our additional deep feedback is brought
in by associating a "companion" classification output
with each hidden layer. We may think of this com-
panion output as analogous to the final output that a
truncated network would have produced. Backpropa-
gation of error proceeds as usual, with the crucial dif-
ference that we backpropagate not only from the final
layer but also from our local companion output. The
empirical results suggest the following main properties
of the companion objective: (1) it is a type of feature
regularization (albeit an unusual one) — it leads to re-
duction in testing error, while not necessarily reducing
the training error; (2) it results in improved conver-
gence behavior, requiring much less manual tweaking
(particularly for very deep networks).
563
Chen-Yu Lee*, Saining Xie*, Patrick W. Gallagher, Zhengyou Zhang, Zhuowen Tu
2.2
Formulation
Here we focus on the supervised learning case. We de-
note our input training data set by S = {(Xi, yi), i =
1 . . . N}, where sample Xi ∈ Rn denotes the raw input
data and yi ∈ {1, . . . , K} denotes the corresponding
ground truth label for sample Xi. We subsequently
drop the subscript i for notational simplicity, since
we consider each sample independently. The goal of
deep neural networks, specifically convolutional neu-
ral networks (CNN) [16], is to learn layers of filters
and weights for minimization of output layer classifi-
cation error. In our present discussion, we absorb the
bias term into the weight parameters and do not differ-
entiate weights from filters. Furthermore, we denote a
recursive function for each layer m = 1 . . . M as
Z(m) = f(Q(m)),
and
Z(0) ≡ X,
(1)
Q(m) = W(m) ∗ Z(m−1),
(2)
where
M
denotes
the
total
number
of
layers,
W(m), m = 1 . . . M are the filters/weights to be
learned, Z(m−1) is the feature map produced at layer
m − 1, Q(m) refers to the convolved/filtered responses
on the previous feature map, and f(·) is a pooling
function on Q. Combining all layers of weights gives
W = (W(1), . . . , W(M)).
We next associate one classifier with each hidden
layer1. We denote the corresponding weights by
w = (w(1), . . . , w(M−1)),
in addition to the W from the standard CNN frame-
work. For ease of reference, we write the total objec-
tive function as
F(W) ≡ P(W) + Q(W),
(3)
in which we have the output objective P(W)
≡
∥w(out)∥2 + L(W, w(out)) and the summed companion
objectives Q(W) ≡ ∑M−1
m=1 αm[∥w(m)∥2+ℓ(W, w(m))−
γ]+. We denote the classifier weights for the output
layer by w(out). Our total combined objective function
is then
∥w(out)∥2+L(W,w(out))+∑M−1
m=1 αm[∥w(m)∥2+ℓ(W,w(m))−γ]+,
(4)
where
L(W,w(out))=∑
yk̸=y[1−<w(out),ϕ(Z(M),y)−ϕ(Z(M),yk)>]2
+
(5)
and
ℓ(W,w(m))=∑
yk̸=y[1−<w(m),ϕ(Z(m),y)−ϕ(Z(m),yk)>]2
+
(6)
1Our derivation is stated for L2SVM but we later show
experimental results for both L2SVM and softmax.
We
also tested L1SVM, finding results that differed negligibly
from those of L2SVM.
We train the DSN model using SGD. The gradient
w.r.t W follows the conventional CNN-based model,
plus the gradient that comes from the hidden layer
direct supervision; we also use companion objective
zero-ing, described in the following.
We refer to L(W, w(out)) as the overall loss (asso-
ciated with the output layer) and to ℓ(W, w(m)) as
the companion loss (associated with the hidden lay-
ers); in the L2SVM setting these are both (squared)
hinge losses of prediction errors. Intuitively, in addi-
tion to learning convolution kernels and weights, W
(as in the standard CNN model [16]) we incorporate
an additional objective at each hidden layer associated
with good label prediction for that layer; this addi-
tional objective strongly favors features that are dis-
criminative/sensible at each hidden layer. In Eq. (4),
∥w(out)∥2 and L(W, w(out)) are, respectively, the mar-
gin and the (squared) hinge loss of the classifier out-
put layer; we omit the balance parameter C in front
of the (squared) hinge loss for notational simplicity.
In Eq. (5), ∥w(m)∥2 and ℓ(W, w(m)) are, respectively,
the margin and the (squared) hinge loss of the hidden
layer classifier.
Note that for each ℓ(W, w(m)), the w(m) directly
depends on Z(m), which is in turn dependent on
W1, .., Wm
up to the mth layer.
Analogously,
L(W, w(out)) depends on w(out), which is decided by
the entire W.
The second term in Eq.
(4) often is
sent to zero in the course of training; this means that
the overall goal of producing good classification of the
output layer is not fundamentally altered and the com-
panion objective acts as a type of regularization or as
a proxy for discriminative features. One method by
which we pursue this companion objective zero-ing is
by having γ as a threshold (a hyper parameter) in
the second term of Eq. (4): once the companion ob-
jective of each hidden layer falls to γ (or below), it
vanishes and no longer contributes to the gradient up-
date in the learning process.
The mth balance pa-
rameter αm represents a trade-off between the out-
put objective and the corresponding companion objec-
tive. An alternative approach to companion objective
zero-ing is to use of a simple decay function such as
αm × 0.1 × (1 − t/N) → αm to enforce that the sec-
ond term vanish after a certain number of iterations,
where t is the epoch step and N is the total number
of epochs. We have obtained promising preliminary
results from each of these two approaches; this issue
remains to be more fully explored.
The main difference between Eq.
(4) and previous
research using layer-wise supervised training is that
we perform the optimization together with a term
serving as a proxy for the hidden layer feature qual-
ity. This integrated learning is in contrast to previous
564
Deeply-Supervised Nets
��������: Filters
������: ���������
������: ��������� maps
��������: Filters
������: ���������
������: ��������� maps
Input
Output
…
���
���
������������ ���
������
Q�����
�������)
(a) DSN illustration
(b) functions
Figure 1: Network architecture for the proposed deeply-supervised nets (DSN). The total objective function F(W) ≡
P(W) + Q(W), in which we have the output objective P(W) ≡ ∥w(out)∥2 + L(W, w(out)) and the summed companion
objectives Q(W) ≡ ∑M−1
m=1 αm[∥w(m)∥2 + ℓ(W, w(m)) − γ]+.
research in which greedy layer-wise pre-training was
performed separately as either initialization or fine-
tuning, a strategy that resulted in overfitting [1]. The
state-of-the-art benchmark results show that the deep
supervision approach does not display harmful levels of
overfitting: as seen in Figure (2.c), while the training
error for both CNN and DSN is eventually near zero,
DSN shows lower test error and so demonstrates its
advantage in generalization over standard CNN. This
is a matter for further investigation, but the present
results are very promising.
2.3
Stochastic Gradient Descent View
In addition to the present observation that learned fea-
tures in CNN are not always intuitive or discriminative
[31], broader discussion of deep neural network train-
ing difficulties has appeared in [9, 20]. As seen in Eq.
(1) and (2), the change of the bottom layer weights gets
propagated through many layers; this can lead to "van-
ishing" gradients [20]. Various techniques and param-
eter tuning tricks have been proposed to better train
deep neural networks, such as pre-training. This diffi-
culty in training partially raises some concerns about
the DL frameworks. Here we provide analysis of our
proposed formulation, in a hope to understand some
aspects of its experimentally-observed advantage in ef-
fectiveness and efficiency over existing CNN-style ap-
proaches.
The objective function in deep neural networks is
highly non-convex, a characteristic contributing to the
current lack of a clear mathematical/statistical analy-
sis of DL frameworks. A common approach is to sac-
rifice some realism for increased tractability, typically
by restricting attention only to settings in which lo-
cal convexity holds. Here we follow this example by
supposing that our objective functions are locally λ-
strongly convex and following analysis techniques from
stochastic gradient descent [3].
The definition of λ-strong convexity is standard: A
function F(W) is λ-strongly convex on a set W if
∀ W, W′ ∈ W and any subgradient g at W,
F(W′) ≥ F(W)+ < g, W′ − W > +λ
2 ∥W′ − W∥2. (7)
The Stochastic Gradient Descent (SGD) update rule
at step t is Wt+1 = ΠW(Wt − ηtˆg), where ηt = Θ(1/t)
refers to the step factor and ΠW denotes projection
onto the set W. Let W⋆ be the optimal solution, sup-
pose that there are upper bounds for E[∥WT − W⋆∥2]
and E[F(WT ) − F(W⋆)] in the strongly convex func-
tion setting [21], or for E[F(WT ) − F(W⋆)] in the
convex function setting [24].
Here we make an at-
tempt to understand the convergence of Eq. (3) w.r.t.
E[∥WT − W⋆∥2] keeping in mind the assumed charac-
teristics roughly illustrated in Figure (1.b). In [19], a
convergence rate is given for M-estimators with locally
convex function with compositional loss and regular-
ization terms.
Definition We denote by Sγ(F) = {W | F(W) ≤ γ}
the γ-sublevel set, stated here for the function F(W) ≡
P(W) + Q(W).
First we show that W ∈ Sγ (Q) implies that W ∈
Sγ (P). That is:
Lemma 1 ∀m, m′ = 1 . . . M − 1, and m′ > m
if
∥w(m)∥2 + ℓ(( ˆW(1), .., ˆW(m)), w(m)) ≤ γ then there ex-
ists ( ˆW(1), .., ˆW(m), .., ˆW(m′)) such that
∥w(m′)∥2 +
ℓ(( ˆW(1), .., ˆW(m).., ˆW(m′)), w(m′)) ≤ γ. 2
2Note that we drop the W(j), j > m since the filters
565
Chen-Yu Lee*, Saining Xie*, Patrick W. Gallagher, Zhengyou Zhang, Zhuowen Tu
Proof As we can see from an illustration of our
network architecture shown in fig.
(1.a), for all
( ˆW(1), .., ˆW(m)) such that ℓ(( ˆW(1), .., ˆW(m)), w(m)) ≤
γ there is a "trivial" way to achieve this performance
at the output layer: for each layer j > m up to m′,
we let ˆW(j) = I and w(j) = w(m), meaning that
the filters will be identity matrices.
This results in
ℓ(( ˆW(1), .., ˆW(m).., ˆW(m′)), w(m′)) ≤ γ.
□
Remark Lemma 1 shows that a good solution for
Q(W) is also a good one for P(W); of course, the
converse needs not be the case.
That is: a W that
makes P(W) small may not necessarily produce fea-
tures for which the hidden layers will have a small
Q(W). As mentioned previously, Q(W) can be viewed
as a regularization term. If it happens that P(W) pos-
sesses an essentially flat area (relative to the training
data) in the vicinity of some local optimum of inter-
est and it is ultimately the test error that we really
care about, we would be able to focus on the set-
ting of W, W⋆, that will make both Q(W) and P(W)
small. Therefore, it is not unreasonable to assume that
F(W) ≡ P(W) + Q(W) and P(W) share the same op-
timal W⋆.
Let P(W) and Q(W) be locally strongly convex around
W⋆, ∥W′ − W⋆∥2 ≤ D and ∥W − W⋆∥2 ≤ D, with
P(W′) ≥ P(W)+ < gp, W′ −W > + λ1
2 ∥W′ −W∥2 and
Q(W′) ≥ Q(W)+ < gq, W′ − W > + λ2
2 ∥W′ − W∥2,
where gp and gq are subgradients for W for P and Q,
respectively. It can be directly seen that F(W) is also
strongly convex and that gf = gp+gq is a subgradient
of F(W) at W.
Lemma 2 Suppose E[∥ ˆ
gpt∥2] ≤ G2 and E[∥ ˆ
gqt∥2] ≤
G2, and we use the update rule of Wt+1 = ΠW(Wt −
ηt( ˆ
gpt + ˆ
gqt)) where E[ ˆ
gpt] = gpt and E[ ˆ
gqt] = gqt.
If we use ηt = 1/(λ1 + λ2)t, then at iteration T
E[∥WT − W⋆∥2] ≤
4G2
(λ1 + λ2)2T
(8)
Proof Since F(W) = P(W)+Q(W), it can be directly
seen that
F (W′)≥F (W)+<gp+gq,W′−W>+ λ1+λ2
2
∥W′−W∥2.
Based on lemma 1 in [21], this upper bound directly
holds.
□
Lemma 3 We follow the assumptions in lemma 2,
with the exception that we assume ηt = 1/t since λ1
and λ2 are not always readily available; as we discuss
in the appendix, we also expect the combined λ to be
above layer m do not participate in the computation for
the loss function of this layer.
small. When we begin in the region ∥W1 − W⋆∥2 ≤ D
and the assumptions used in the appendix hold, the
convergence rate is bounded by
E[∥WT −W⋆∥2]≤e−2λ(ln(T −1)+0.578)D+4G2 ∑T −1
t=1
1
t2 (
t
T −1)
2λ
(9)
Proof See appendix.
Theorem 1 Let P(W) be λ1-strongly convex and
Q(W) be λ2-strongly convex near optimal W⋆ and de-
note by W(F )
T
and W(P)
T
the solution after T iterations
when following SGD on F(W) and P(W), respectively.
Then DSN framework in Eq. (4) improves the relative
convergence speed E[∥W(P)
T
−W⋆∥2]
E[∥W(F )
T
−W⋆∥2], viewed from the ratio
of their upper bounds as Θ( (λ1+λ2)2
λ2
1
), when ηt = 1/λt.
Proof Lemma 1 shows the compatibility of the com-
panion objective of Q w.r.t the output objective P.
This equation can be directly derived from lemma 2.
Remark When ηt = 1/t, T is large, and the first
term of Eq. (9) dominates, the upper bound ratio for
E[∥W(P)
T
−W⋆∥2]
E[∥W(F )
T
−W⋆∥2] is roughly at the order of Θ(e2 ln(T )λ2).
If the second term of Eq. (9) dominates, the conver-
gence of F(W) over P(W) is also advantageous with
the ratio at an order of Θ(e2 ln((T −1)/(T −2))λ2) loosely.
In general we might expect λ2 ≫ λ1 which would lead
to a great improvement in convergence speed.
□
3
Experiments
We evaluate our proposed DSN on four datasets:
MNIST, CIFAR-10, CIFAR-100, and SVHN. We also
use ImageNet to assess the behavior of DSN on a large
dataset. We use MNIST as our primary running ex-
ample. For our MNIST experiments we used an imple-
mentation of DSN built on top of the Theano platform
[2]. The rest of the experiments are performed using
DSN built on top of [18], except for ImageNet where
we use [14] due to its parallel computing infrastructure.
We use the SGD solver with mini-batch size of 128 and
fixed momentum of 0.9. Initial values for learning rate
and weight decay factor are determined from the val-
idation set. For fair comparison and clear illustration
of the effectiveness of DSN, we match the complexity
of our model to the complexity of the network archi-
tectures used in [18] and [10] so that there are a com-
parable number of parameters. We also incorporate
two dropout layers with dropout rate 0.5. Compan-
ion objective at the convolutional layers is imposed to
back-propagate the classification error guidance at the
associated layer. Our proposed DSN framework is not
difficult to train and no particular engineering tricks
are adopted.
566
Deeply-Supervised Nets
0
20
40
60
80
100
10
−2
epoch
error
MNIST testing error
CNN−Softmax  [0.56%]
DSN−Softmax (ours)  [0.51%]
CNN−SVM  [0.48%]
DSN−SVM (ours)  [0.39%]
0.5
1
3
5
10
20
35
60
0
1
2
3
4
5
6
7
8
9
10
number of training samples (k)
error (%)
MNIST testing error
CNN−Softmax
DSN−Softmax (ours)
CNN−SVM
DSN−SVM (ours)
0
20
40
60
80
100
0
0.005
0.01
0.015
0.02
0.025
0.03
epoch
error
Training and testing error
CNN−SVM: Training error [0.03%]
CNN−SVM: Testing error [0.50%]
DSN−SVM: Training error [0.03%]
DSN−SVM: Testing error [0.39%]
(a)
(b)
(c)
Figure 2: Classification error on MNIST test. (a) shows test error of competing methods; (b) shows test error
versus training sample size. (c) train and test error comparison.
DSN can be equipped with different types of classifier
objective functions; we consider L2SVM and softmax
and show DSN-L2SVM and DSN-Softmax yield gains
over corresponding CNN-L2SVM and CNN-Softmax
approaches (see Figure (2.a)). The performance gain
is more evident in the small training data regime (see
Figure (2.b)); this might partially ease the burden
of requiring large training data for DL. Overall, we
observe state-of-the-art classification error on all four
datasets. All results are achieved without using aver-
aging [22], which could lead to further improvement
in classification accuracy. Figure (4) gives an illustra-
tion of some learned features. The ImageNet classifi-
cation result is also very encouraging (recent winning
systems [28, 25] used many specific engineering steps
and a cluster for training). Again, our DSN method
can be combined with many existing CNN-type meth-
ods. Overall, as stated before: for small training data
and relatively shallow networks, DSN functions as a
strong "regularization"; for large training data and
very deep networks, DSN makes the training process
convenient, which might be otherwise problematic in
standard CNN.
MNIST
The MNIST dataset consists of 28 × 28 images from
10 different classes (0 to 9) with 60,000 training and
10,000 test samples.
We use a fairly standard net-
work containing 2 convolutional layers followed by a
fully-connected layer.
Both convolutional layers use
5 × 5 filter size with 32 and 64 channels, respectively.
Relu hidden units and 2 × 2 max pooling with stride
2 are used after each convolutional layer. The fully-
connected layer has 500 hidden nodes that uses Relu
activation with dropout rate 0.5.
Figure (2.a) and (b) show results from four meth-
ods, namely: (1) conventional CNN with softmax loss
(CNN-Softmax), (2) the proposed DSN with softmax
loss (DSN-Softmax), (3) CNN with L2SVM objec-
tive (CNN-L2SVM) , and (4) the proposed DSN with
L2SVM objective (DSN-L2SVM). DSN-Softmax and
DSN-L2SVM outperform their respective competing
CNN algorithms (DSN-L2SVM shows classification er-
ror of 0.39% under a single model without data whiten-
ing and augmentation). Figure (2.b) shows the classi-
fication error of the competing methods when trained
with varying sizes of training samples (26% gain of
DSN-L2SVM over CNN-Softmax at 500 samples. Fig-
ure (2.c) shows a comparison of generalization error
between CNN and DSN.
We also plot averaged absolute values of gradient
matrices during training on MNIST for both CNN
and DSN, shown in Figure (3). DSN backpropagates
stronger feedback than CNN for both weights and bi-
ases, thanks to deep supervision. Table 1 compares
performance.
To investigate the difference between
DSN and layer-wise pre-training (shown as CNN with
pre-training in Table 1), we train a CNN model us-
ing greedy layer-wise pre-training as in [1]. Under the
same network architecture, layer-wise pre-training per-
forms better than CNN without pre-training but worse
than DSN.
Table 1: MNIST classification result (without using
data augmentation and model averaging).
Method
Error(%)
CNN [13]
0.53
Stochastic Pooling [32]
0.47
Network in Network [18]
0.47
Maxout Networks [10]
0.45
CNN (layer-wise pre-training)
0.43
DSN (ours)
0.39
CIFAR-10 and CIFAR-100
The CIFAR-10 dataset consists of 32×32 color images.
A collection of 60,000 images is split into 50,000 train-
567
Chen-Yu Lee*, Saining Xie*, Patrick W. Gallagher, Zhengyou Zhang, Zhuowen Tu
0
20
40
60
80
100
120
0
0.5
1
1.5
2
2.5
x 10
−3
epoch
gradient magnitude
Mean gradient of all weights
CNN−SVM gradients
DSN−SVM gradients
0
20
40
60
80
100
120
0
0.5
1
1.5
2
2.5
x 10
−3
epoch
gradient magnitude
Mean gradient of all bias
CNN−SVM gradients
DSN−SVM gradients
(a)
(b)
Figure 3: Average absolute value of gradient matri-
ces during the training on MNIST for (a) weights; (b)
biases.
ing and 10,000 testing images. The dataset is prepro-
cessed by global contrast normalization. To compare
our results to previous state-of-the-art, for this dataset
we also augmented the dataset by zero-padding 4 pix-
els on each side; we also perform corner cropping and
random flipping on the fly during training. For a fair
comparison, we adopt a network architecture similar
to [18] that contains 3 convolutional layers with 192
filter channels. We also use the mlpconv layer after
each convolutional layer and a global averaged pool-
ing scheme with kernel size 8 for the output predic-
tion.
Relu neuron with 0.5 dropout rate and 3 × 3
max pooling with stride 2 are used after the first two
convolutional layers.
No model averaging is done at test phase. Table (2)
shows our results. Our DSN model achieves an error
rate of 9.69% without data augmentation and of 7.97%
with data augmentation (the best result to our knowl-
edge). Note that DSN still outperforms greedy layer-
wise pre-training (shown as CNN with pre-training in
Table 2) under the same network architecture.
(a) by DSN
(b) by CNN
Figure 4: Visualization of the feature maps learned in
the convolutional layer.
DSN also provides added robustness to hyperparame-
ter choice, in that the early layers are guided with di-
rect classification loss, leading to a faster convergence
rate and reduced dependence on heavy hyperparame-
ter tuning. We also compared the gradients in DSN
with those in CNN, observing 4.55 times greater gra-
dient variance of DSN over CNN in the first convo-
lutional layer. This is consistent with an observation
in [10] and with the assumptions and motivations we
make in this work. To see what features have been
learned in DSN vs. CNN, we select one example image
from each of the ten categories of CIFAR-10, run one
forward pass, and show the feature maps learned from
the bottom convolutional layer in Figure (4). Only the
top 30% of activations are shown in each of the fea-
ture maps. Feature maps learned by DSN appear to
be more intuitive than those learned by CNN.
Table 2: CIFAR-10 classification error.
Method
Error(%)
No Data Augmentation
Stochastic Pooling [32]
15.13
Maxout Networks [10]
11.68
Network in Network [18]
10.41
NIN (layer-wise pre-training)
9.92
DSN (ours)
9.69
With Data Augmentation
Maxout Networks [10]
9.38
Dropconnect [17]
9.32
Network in Network [18]
8.81
DSN (ours)
7.97
CIFAR-100 is similar to CIFAR-10, but with 100
classes rather than 10.
The number of images for
each class is then 500 instead of 5, 000 as in CIFAR-
10, which makes the classification task more challeng-
ing. We use the same network settings as in CIFAR-
10. The consistent performance boost (shown on both
CIFAR-10 and CIFAR-100) again demonstrates the
advantage of the DSN method.
Table 3: CIFAR-100 classification error.
Method
Error(%)
Stochastic Pooling [32]
42.51
Maxout Networks [10]
38.57
Tree based Priors [27]
36.85
Network in Network [18]
35.68
DSN (ours)
34.57
Street View House Numbers
The Street View House Numbers (SVHN) dataset con-
sists of 32×32 color images: 73, 257 digits for training,
26, 032 digits for testing, and 531, 131 extra training
samples. We prepared the data in keeping with pre-
vious work, namely: we select 400 samples per class
from the training set and 200 samples per class from
the extra set. The remaining 598,388 images are used
for training. We followed [10] to preprocess the dataset
by Local Contrast Normalization (LCN). We do not
do data augmentation in training and use only a sin-
gle model in testing. Table 4 shows recent comparable
results. Note that Dropconnect [17] uses data augmen-
tation and multiple model voting.
568
Deeply-Supervised Nets
Table 4: SVHN classification error.
Method
Error(%)
Stochastic Pooling [32]
2.80
Maxout Networks [10]
2.47
Network in Network [18]
2.35
Dropconnect [17]
1.94
DSN (ours)
1.92
3.1
ImageNet Challenge
To further illustrate the effectiveness of DSN on large
training datasets and with deeper networks, we test
both CNN and DSN (8-layer and 11-layer networks) on
the ImageNet 2012 dataset, which consists of 1.2 mil-
lion training images, 50, 000 validation, and 100, 000
testing. We emphasize that our intention here is not
to directly compete with the best performing result in
the challenge (since the winning methods [28] require
many additional aspects), but to provide an illustra-
tive comparison of the relative benefits of DSN versus
CNN on this data set. That said, it is worth noting
that the supervision applied to two hidden layers in
[28] is a suggestive indication of additional support for
our method.
The 8-layer network's configuration is
based on the AlexNet [14] with 5 convolutional layers
and 3 fully-connected layers, while the 11-layer net-
work's infrastructure contains 8 convolutional layers
and 3 fully-connected layers. To decouple the effect of
parameter tuning, we use the same network and pa-
rameter setup as in [14] for both CNN and DSN. We
simply add three more convolutional layers for the 11-
layer networks. We start with learning rate 0.01 and
lower by a factor of 10 when validation error stops
decreasing. This procedure is repeated until learning
rate reaches 10−5. All results are tested under a single
net without multi-scale training/testing in order to re-
duce the effect of model/prediction averaging. Table
5 shows a direct comparison between CNN and DSN.
When the depth of the network increases to 11 lay-
ers, standard backpropagation for training CNN faces
a big challenge.
We observe that the average abso-
lute gradient of the weight matrix of conv1 layer to
be at magnitude of 10−10, even during the first few
epochs of the training process, making the networks
significantly difficult to converge. Therefore, we have
to adopt a pre-training strategy suggested in [25]: we
first pre-train an 8-layer network and then restart the
training process with another three convolutional lay-
ers for the 11-layer CNN configuration. In contrast,
our DSN of 11 layers behaves regularly during train-
ing and the objective function moves down directly
starting at the first few epochs without pre-training;
CNN converges for 95 epochs with pre-training while
DSN converges for 58 epochs. Table 5 shows the per-
formance advantage of DSN over CNN, in addition to
a significant gain in training convenience and saved
manual adjustments.
Table 5: ImageNet 2012 classification error.
Method
top-1 val.
error(%)
top-5 val.
error(%)
CNN 8-layer [14]
40.7
18.2
DSN 8-layer (ours)
39.6
17.8
CNN 11-layer
34.5
13.9
DSN 11-layer (ours)
33.7
13.1
Acknowledgments
This work is supported by NSF awards IIS-1216528
(IIS-1360566) and IIS-0844566(IIS-1360568).
We
thank Min Lin, Naiyan Wang, Baoyuan Wang, Jing-
dong Wang, Liwei Wang, Ying Nian Wu, and David
Wipf for helpful discussions. We gratefully acknowl-
edge the support of NVIDIA Corporation with their
donation of the GPUs used for this research.
Appendix
Proof for Lemma 3. Letting λ = λ1 + λ2, we have
F(W⋆)−F(Wt) ≥ < gf t, W⋆ −Wt > +λ
2 ∥W⋆ −Wt∥2,
and
F(Wt) − F(W⋆) ≥ λ
2 ∥Wt − W⋆∥2.
Thus,
< gf t, Wt − W⋆ > ≥ λ∥Wt − W⋆∥2.
Therefore, with ηt = 1/t,
E[∥Wt+1 − W⋆∥2] = E[∥ΠW(Wt − ηt ˆgf t) − W⋆∥2]
≤
E[∥Wt − ηt ˆgf t − W⋆∥2]
=
E[∥Wt−W⋆∥2]−2ηtE[<gf t,Wt−W⋆>]+η2
t E[∥ ˆ
gf t∥2]
≤
(1 − 2λ/t)E[∥Wt − W⋆∥2] + 4G2/t2
(10)
Since our focus is on the setting of "nearly flat" ob-
jective behavior, we expect 2λ/t to be small; in such a
case 1 − 2λ/t ≈ e−2λ/t.
E[∥WT − W⋆∥2]
≤ e−2λ( 1
1 + 1
2 +···
1
T −1 )D +
T −1
∑
t=1
4G2
t2 e−2λ( 1
t +
1
t+1 +···
1
T −1 )
= e−2λ(ln(T −1)+0.578)D + 4G2
T −1
∑
t=1
1
t2 e−2λ ln(T −1)+2λ ln(t)
≤ e−2λ(ln(T −1)+0.578)D + 4G2
T −1
∑
t=1
1
t2
(
t
T − 1
)2λ
□
Further, we can approximate
E[∥Wt+1−W⋆∥2]<e−2λ(ln(T −1)+0.578)D+e−2λ(ln((T −1)/(T −2)))4G2ζ(2),
(11)
where ζ(2) is the Riemann zeta function.
569
Chen-Yu Lee*, Saining Xie*, Patrick W. Gallagher, Zhengyou Zhang, Zhuowen Tu
References
[1] Y.
Bengio,
P.
Lamblin,
D.
Popovici,
and
H. Larochelle.
Greedy layer-wise training of deep
networks. In NIPS, 2007.
[2] J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin,
R. Pascanu, G. Desjardins, J. Turian, D. Warde-
Farley, and Y. Bengio.
Theano: a CPU and GPU
math expression compiler.
In Proceedings of the
Python for Scientific Computing Conference (SciPy),
June 2010.
[3] L. Bottou. Online algorithms and stochastic approxi-
mations. Cambridge University Press, 1998.
[4] M. Carreira-Perpinan and W. Wang. Distributed op-
timization of deeply nested systems.
In AISTATS,
2014.
[5] G. E. Dahl, D. Yu, L. Deng, and A. Acero. Context-
dependent pre-trained deep neural networks for large-
vocabulary speech recognition. IEEE Tran. on Audio,
Speech, and Lang. Proc., 20(1):30–42, 2012.
[6] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang,
E. Tzeng, and T. Darrell. Decaf: A deep convolutional
activation feature for generic visual recognition.
In
arXiv, 2013.
[7] D. Eigen, J. Rolfe, R. Fergus, and Y. LeCun. Under-
standing deep architectures using a recursive convolu-
tional network. In arXiv:1312.1847v2, 2014.
[8] J. L. Elman. Distributed representations, simple re-
current networks, and grammatical. Machine Learn-
ing, 7:195–225, 1991.
[9] X. Glorot and Y. Bengio. Understanding the difficulty
of training deep feedforward neural networks. In AI-
STAT, 2010.
[10] I. J. Goodfellow, D. Warde-Farley, M. Mirza, A. C.
Courville, and Y. Bengio. Maxout networks. In ICML,
2013.
[11] G. E. Hinton, S. Osindero, and Y. W. Teh. A fast
learning algorithm for deep belief nets. Neural com-
putation, 18:1527–1554, 2006.
[12] G.
E.
Hinton,
N.
Srivastava,
A.
Krizhevsky,
I. Sutskever, and R. Salakhutdinov. Improving neu-
ral networks by preventing co-adaptation of feature
detectors. In CoRR, abs/1207.0580, 2012.
[13] K. Jarrett, K. Kavukcuoglu, M. Ranzato, and Y. Le-
Cun.
What is the best multi-stage architecture for
object recognition? In ICCV, 2009.
[14] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Im-
agenet classification with deep convolutional neural
networks. In NIPS, 2012.
[15] Q. Le, J. Ngiam, Z. Chen, D. Chia, P. W. Koh, and
A. Ng. Tiled convolutional neural networks. In NIPS,
2010.
[16] Y. LeCun, B. Boser, J. S. Denker, D. Henderson,
R. Howard, W. Hubbard, and L. Jackel. Backprop-
agation applied to handwritten zip code recognition.
In Neural Computation, 1989.
[17] W. Li, M. Zeiler, S. Zhang, Y. LeCun, and R. Fergus.
Regularization of neural networks using dropconnect.
In ICML, 2013.
[18] M. Lin, Q. Chen, and S. Yan. Network in network. In
ICLR, 2014.
[19] P.-L. Loh and M. J. Wainwright.
Regularized m-
estimators with nonconvexity : statistical and algo-
rithmic theory for local optima. In arXiv:1305.2436v1,
2013.
[20] R. Pascanu, T. Mikolov, and Y. Bengio.
On the
difficulty of training recurrent neural networks.
In
arXiv:1211.5063v2, 2014.
[21] A. Rakhlin, O. Shamir, and K. Sridharan.
Making
gradient descent optimal for strongly convex stochas-
tic optimization. In ICML, 2012.
[22] J. Schmidhuber. Multi-column deep neural networks
for image classification. In CVPR, 2012.
[23] P. Sermanet and Y. LeCun. Traffic sign recognition
with multi-scale convolutional networks. In Interna-
tional Joint Conference on Neural Networks (IJCNN),
2011.
[24] O. Shamir and T. Zhang.
Stochastic gradient de-
scent for non-smooth optimization: Convergence re-
sults and optimal averaging schemes. In ICML, 2013.
[25] K. Simonyan and A. Zisserman. Very deep convolu-
tional networks for large-scale image recognition. In
arXiv:1409.1556, Sept. 4, 2014.
[26] J. Snoek, R. P. Adams, and H. Larochelle. Nonpara-
metric guidance of autoencoder representations using
label information. J. of Machine Learning Research,
13:2567–2588, 2012.
[27] N. Srivastava and R. Salakhutdinov. Discriminative
transfer learning with tree-based priors.
In NIPS,
2013.
[28] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Ra-
binovich.
Going deeper with convolutions.
In
arXiv:1409.4842, Sept. 17, 2014.
[29] Y. Tang. Deep learning using linear support vector
machines. In Workshop on Representational Learning,
ICML, 2013.
[30] J. Weston and F. Ratle.
Deep learning via semi-
supervised embedding. In ICML, 2008.
[31] M. Zeiler and R. Fergus. Visualizing and understand-
ing convolutional networks. In arXiv 1311.2901, 2013.
[32] M. D. Zeiler and R. Fergus.
Stochastic pooling for
regularization of deep convolutional neural networks.
In ICLR, 2013.
570
