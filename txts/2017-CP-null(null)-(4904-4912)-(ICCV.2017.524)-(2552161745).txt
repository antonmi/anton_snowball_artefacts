Boosting Image Captioning with Attributes∗
Ting Yao †, Yingwei Pan ‡, Yehao Li §, Zhaofan Qiu ‡, and Tao Mei †
† Microsoft Research, Beijing, China
‡ University of Science and Technology of China, Hefei, China
§ Sun Yat-Sen University, Guangzhou, China
{tiyao, tmei}@microsoft.com, {panyw.ustc, yehaoli.sysu, zhaofanqiu}@gmail.com
Abstract
Automatically describing an image with a natural lan-
guage has been an emerging challenge in both fields of
computer vision and natural language processing. In this
paper, we present Long Short-Term Memory with Attributes
(LSTM-A) - a novel architecture that integrates attributes
into the successful Convolutional Neural Networks (CNNs)
plus Recurrent Neural Networks (RNNs) image captioning
framework, by training them in an end-to-end manner. Par-
ticularly, the learning of attributes is strengthened by in-
tegrating inter-attribute correlations into Multiple Instance
Learning (MIL). To incorporate attributes into captioning,
we construct variants of architectures by feeding image
representations and attributes into RNNs in different ways
to explore the mutual but also fuzzy relationship between
them. Extensive experiments are conducted on COCO im-
age captioning dataset and our framework shows clear im-
provements when compared to state-of-the-art deep mod-
els.
More remarkably, we obtain METEOR/CIDEr-D of
25.5%/100.2% on testing data of widely used and publicly
available splits in [10] when extracting image representa-
tions by GoogleNet and achieve superior performance on
COCO captioning Leaderboard.
1. Introduction
Accelerated by tremendous increase in Internet band-
width and proliferation of sensor-rich mobile devices, im-
age data has been generated, published and spread explo-
sively, becoming an indispensable part of today's big da-
ta. This has encouraged the development of advanced tech-
niques for a broad range of image understanding applica-
tions. A fundamental issue that underlies the success of
these technological advances is the recognition [8, 26, 28].
Recently, researchers have strived to automatically describe
∗This work was performed when Yingwei Pan, Yehao Li and
Zhaofan Qiu were visiting Microsoft Research as research interns.
the content of an image with a complete and natural sen-
tence, which has a great potential impact for instance on
robotic vision or helping visually impaired people. Never-
theless, this problem is very challenging, as description gen-
eration model should capture not only the objects/scenes p-
resented in the image, but also be capable of expressing how
the objects/scenes relate to each other in a nature sentence.
The main inspiration of recent attempts on this problem
[5, 31, 33] are from the advances by using RNNs in machine
translation [27], which is to translate a text from one lan-
guage (e.g., English) to another (e.g., Chinese). The basic
idea is to perform a sequence to sequence learning for trans-
lation, where an encoder RNN reads the input sequential
sentence, one word at a time till the end of the sentence and
then a decoder RNN is exploited to generate the sentence
in target language, one word at each time step. Following
this philosophy, it is natural to employ a CNN instead of the
encoder RNN for image captioning, which is regarded as an
image encoder to produce image representations.
While encouraging performances are reported, these C-
NN plus RNN image captioning methods translate directly
from image representations to language, without explicitly
taking more high-level semantic information from images
into account. On the other hand, attributes are properties
observed in images with rich semantic cues and have been
proved to be effective in visual recognition [23]. Therefore,
a valid question is how to incorporate high-level image at-
tributes into CNN plus RNN image captioning architecture
as complementary knowledge in addition to image repre-
sentations. We investigate particularly in this paper the ar-
chitectures by exploiting the mutual relationship between
image representations and attributes for enhancing image
description generation. Specifically, to better demonstrate
the impact of simultaneously utilizing the two kinds of rep-
resentations, we devise variants of architectures by feeding
them into RNN in different placements and moments, e.g.,
leveraging only attributes, inserting image representations
first and then attributes or vice versa, and inputting image
representations/attributes once or at each time step. More-
over, considering attributes are vital to our proposal, we en-
dow the Multiple Instance Learning (MIL) framework with
more power of exploring inter-attribute correlations.
The main contribution of this work is the proposal of at-
tribute augmented architectures by integrating the attributes
into CNN plus RNN image captioning framework, which
is a problem not yet fully understood in the literature. By
leveraging more knowledge for building richer representa-
tions and description models, our work takes a further step
forward to enhance image captioning. More importantly,
the utilization of attributes also has a great potential to be an
elegant solution of generating open-vocabulary sentences,
making image captioning system really practical.
2. Related Work
The research on image captioning has proceeded a-
long three different dimensions: template-based methods
[11, 18, 34], search-based approaches [4, 7], and language-
based models [5, 16, 31, 32, 33, 36, 38].
The template-based methods firstly align each sentence
fragments (e.g., subject, verb, object) with detected words
from image content and then generate the sentence with pre-
defined language templates. Obviously, most of them high-
ly depend on the templates of sentence and always generate
sentences with syntactical structure. For example, Kulkarni
et al. employ Conditional Random Field (CRF) model to
predict labeling based on the detected objects, attributes and
prepositions, and then generate sentence with a template by
filling in slots with the most likely labeling [11]. Similarly,
Yang et al. utilize HMM to select the best objects, scenes,
verbs, and prepositions with the highest log-likelihood ratio
for template-based sentence generation in [34].
Search-based approaches "generate" sentence for an im-
age by selecting the most semantically similar sentences
from sentence pool or directly copying sentences from other
visually similar images. This direction indeed can achieve
human-level descriptions as all sentences are from existing
human-generated sentences. The need to collect human-
generated sentences, however, makes the sentence pool hard
to be scaled up. Moreover, the approaches in this dimension
cannot generate novel descriptions. For instance, in [7], an
intermediate meaning space based on the triplet of objec-
t, action, and scene is proposed to measure the similarity
between image and sentence, where the top sentences are
regarded as the generated sentences for the target image.
Recently, a simple k-nearest neighbor retrieval model is u-
tilized in [4] and the best or consensus caption is selected
from the returned candidate captions, which even performs
as well as several state-of-the-art language-based models.
Different from template-based and search-based models,
language-based models aim to learn the probability distribu-
tion in the common space of visual content and textual sen-
tence to generate novel sentences with more flexible syntac-
tical structures. In this direction, recent works explore such
probability distribution mainly by using neural networks for
image captioning. For instance, in [31], Vinyals et al. pro-
pose an end-to-end neural networks architecture by utiliz-
ing LSTM to generate sentence for an image, which is fur-
ther incorporated with attention mechanism in [33] to au-
tomatically focus on salient objects when generating corre-
sponding words. More recently, in [32], high-level concept-
s/attributes are shown to obtain clear improvements on im-
age captioning when injected into existing state-of-the-art
RNN-based model and such attributes are further utilized
as semantic attention [38] to enhance image captioning. In
another work by Yao et al. [36], attribute/object detectors
are developed and leveraged into image captioning to de-
scribe novel objects.
In short, our work in this paper belongs to the language-
based models. Different from most of the aforementioned
language-based models which mainly focus on sentence
generation by solely depending on image representations
[5, 16, 31, 33] or attributes [32], our work contributes by
studying not only jointly exploiting image representations
and attributes for image captioning, but also how the ar-
chitecture can be better devised by exploring mutual rela-
tionship in between. It is also worth noting that [38] also
involves attributes for image captioning. Ours is fundamen-
tally different in the way that [38] is as a result of utilizing
attributes to model semantic attention to the locally previ-
ous words, as opposed to holistically employing attributes
as a kind of complementary representations in this work.
3. Boosting Image Captioning with Attributes
In this paper, we devise our CNN plus RNN architectures
to generate descriptions for images under the umbrella of
additionally incorporating the detected high-level attributes.
Specifically, we begin this section by presenting the prob-
lem formulation. Then, an attributes prediction method by
further exploring inter-attribute correlations is provided. Fi-
nally, five variants of our image captioning frameworks with
attributes are investigated and discussed.
3.1. Problem Formulation
Suppose we have an image I to be described by a tex-
tual sentence S, where S = {w1, w2, ..., wNs} consist-
ing of Ns words.
Let I ∈ RDv and wt ∈ RDs de-
note the Dv-dimensional image representations of the im-
age I and the Ds-dimensional textual features of the t-th
word in sentence S, respectively. Furthermore, we have
feature vector A ∈ RDa to represent the probability dis-
tribution over all the high-level attributes A for image I,
where A = {a1, a2, ..., aDa} consisting of Da attributes
in the whole image captioning dataset. More details about
how we mine and represent the attributes will be elaborat-
ed in Section 3.2. Taking inspiration from the recent suc-
cesses of probabilistic sequence models leveraged in sta-
tistical machine translation [27] and image/video caption-
ing [20, 21, 31], we aim to formulate our image captioning
models in an end-to-end fashion based on RNNs which en-
code the given image and/or its detected attributes into a
fixed dimensional vector and then decode it to the target
output sentence. Hence, the sentence generation problem
we explore here can be formulated by minimizing the fol-
lowing energy loss function as
E(I, A, S) = − log Pr (S|I, A),
(1)
which is the negative log probability of the correct textual
sentence given the image and detected attributes.
Since the model produces one word in the sentence at
each time step, it is natural to apply chain rule to model the
joint probability over the sequential words. Thus, the log
probability of the sentence is given by the sum of the log
probabilities over the word and can be expressed as
log Pr (S|I, A) =
Ns
�
t=1
log Pr ( wt| I, A, w0, . . . , wt−1).
(2)
By minimizing this loss, the contextual relationship among
the words in the sentence can be guaranteed given the image
and its detected attributes.
We formulate this task as a variable-length sequence to
sequence problem and model the parametric distribution
Pr (wt| I, A, w0, . . . , wt−1) in Eq.(2) with Long Short-
Term Memory (LSTM) network, which is a widely used
type of RNN and can capture long-term information in the
sequential data by mapping sequences to sequences.
3.2. Attributes Prediction
An image generally contains not only multiple semantic
attributes but also the interactions between the attributes.
To detect attributes from images, one way is to train Ful-
ly Convolutional Networks (FCNs) by using the weakly-
supervised multi-label classification approach of Multiple
Instance Learning (MIL) in [6]. This method can easily
predict the attributes probability distribution over massive
attributes, but leaving the inherent semantic correlations be-
tween attributes unexploited as all the attributes detectors
are learnt independently.
To further explore the seman-
tic correlations between attributes, a new MIL-based mod-
el with Inter-Attributes Correlations (MIL-IAC) is devised.
Technically, for an attribute aj, one image I is regarded as
a positive bag of regions (instances) if aj exists in image
I's ground-truth sentences, and negative bag otherwise. By
inputting all the bags into a noisy-OR MIL model [6], the
probability of the bag bI which contains attribute aj is mea-
sured on the probabilities of all the regions in the bag:
Pr
aj
I
= 1 −
�
ri∈bI
�
1 − p
aj
i
�
,
(3)
where paj
i
is the probability of the attribute aj predicted by
region ri. We calculate paj
i through a sigmoid layer after the
last convolutional layer in the fully convolutional network:
p
aj
i
=
1
1 + e
−T⊤
j ri
,
(4)
where Tj ∈ RDt denotes the detection parameter matrix in
sigmoid layer for measuring the prediction score of j-th at-
tribute aj and ri is the corresponding representation for im-
age region ri. In particular, the dimension of convolutional
activations from the last convolutional layer is x × x × Dt
and Dt represents the representation dimension of each re-
gion, resulting in x × x response map which preserves the
spatial dependency of the image. Then, a cross entropy loss
is calculated based on the probabilities of all the attributes
at the top of the whole FCNs architecture as
lc(I) = −
Da
�
j=1
�
I(Cj =1) log
�
Pr
aj
I
�
+ (1 − I(Cj =1)) log
�
1 − Pr
aj
I
� �
,
(5)
where Praj
I is measured as in Eq.(3), the indicator function
Icondition = 1 if condition is true; otherwise Icondition = 0,
and Cj denotes the j-th element in attributes label vector
C. Note that each element of the attributes label vector C ∈
{0, 1}Da is an attribute indicator. The indicator is 1 if the
image contains this attribute otherwise the indicator is 0.
Inspired by the idea of structure preservation or manifold
regularization in [19, 37], the inter-attribute correlation here
is integrated into the learning of attributes detector as a reg-
ularizer in the sigmoid layer to further explore the inherent
semantic correlations between attributes. This regularizer
indicates that the detectors, i.e., detection parameter ma-
trices in sigmoid layer, of semantically relevant attributes
should be similar. The estimation of the underlying seman-
tic correlations can be measured by the appropriate pairwise
similarity between attributes. Specifically, the regulariza-
tion of inter-attribute correlations could be given by
la(T) =
Da
�
m,n=1
Smn∥Tm − Tn∥2,
(6)
where S ∈ RDa×Da is the affinity matrix defined on the at-
tributes, T ∈ RDt×Da is the whole detection parameter ma-
trix in sigmoid layer, and Tm denotes the m-th column of
T representing the detection parameter matrix for attribute
am. It is reasonable to minimize Eq.(6), since it will incur
a heavy penalty if the distance between two similar detec-
tion parameter matrices is very far. There are many ways
of defining the affinity matrix S. Here, we calculate the
elements through the normalized cosine similarity between
two attributes:
Smn =
am · an
∥am∥ ∥an∥ ,
(7)
where am is a 300-dimensional word representation gener-
ated from word2vector neural network [17] for attribute am.
Please note that each cosine similarity score Smn is further
linearly normalized into the range of [0, 1].
Attributes
Image
LSTM
LSTM
Attributes
Image
LSTM
LSTM
Attributes
LSTM
w0
LSTM
w1
w1
LSTM
w2
wNs-1
LSTM
wNs
...
Attributes
LSTM
Image
w0
LSTM
w1
w1
LSTM
w2
wNs-1
LSTM
wNs
...
LSTM
Attributes
Image
w0
LSTM
w1
w1
LSTM
w2
wNs-1
LSTM
wNs
...
Figure 1. Five variants of our LSTM-A framework (better viewed in color).
By defining the graph Laplacian L = D − S, where D
is a diagonal matrix with its elements defined as Dmm =
�
n Smn, Eq.(6) can be rewritten as
la(T) = tr(TLT⊤).
(8)
By minimizing this term, the inherent semantic corrections
between attributes could be preserved in the learnt attributes
detectors. The overall objective function integrates the cross
entropy loss in Eq.(5) on the image set I and inter-attribute
correlations regularization in Eq.(8). Hence, we obtain the
following overall loss objective:
l = λ
�
I∈I
lc(I) + (1 − λ) la(T),
(9)
where λ is the tradeoff parameter. After optimizing the w-
hole FCN architecture with the overall loss objective in E-
q.(9), we complete the learning of our MIL-IAC attributes
prediction model and treat its final prediction scores on all
the attributes as A.
3.3. Long Short-Term Memory with Attributes
Unlike the existing image captioning models in [5, 31]
which solely encode image representations for sentence
generation, our proposed Long Short-Term Memory with
Attributes (LSTM-A) model additionally integrates the de-
tected high-level attributes into LSTM. We devise five vari-
ants of LSTM-A for involvement of two design purposes.
The first purpose is about where to feed attributes into LST-
M and three architectures, i.e., LSTM-A1 (leveraging only
attributes), LSTM-A2 (inserting image representations first)
and LSTM-A3 (feeding attributes first), are derived from
this view. The second is about when to input attributes or
image representations into LSTM and we design LSTM-
A4 (inputting image representations at each time step) and
LSTM-A5 (inputting attributes at each time step) for this
purpose. An overview of LSTM-A is depicted in Figure 1.
3.3.1
LSTM-A1 (Leveraging only Attributes)
Given the detected attributes, one natural way is to direct-
ly inject the attributes as representations at the initial time
to inform the LSTM about the high-level attributes. This
kind of architecture with only attributes input is named
as LSTM-A1. It is also worth noting that the attributes-
based model in [32] is similar to LSTM-A1 and can be re-
garded as one special case of our LSTM-A. Given the at-
tribute representations A and the corresponding sentence
W ≡ [w0, w1, ..., wNs], the LSTM updating procedure in
LSTM-A1 is as
x−1 = TaA,
xt = Tswt,
and ht = f
�
xt�
, t ∈ {0, . . . , Ns − 1} ,
(10)
where De is the dimensionality of LSTM input, Ta ∈
RDe×Da and Ts ∈ RDe×Ds is the transformation matrix
for attribute representations and textual features of word,
respectively, f is the updating function within LSTM unit,
and ht is the cell output of the LSTM unit. Please note that
for the input sentence W ≡ [w0, w1, ..., wNs], we take w0
as the start sign word to inform the beginning of sentence
and wNs as the end sign word which indicates the end of
sentence. Both of the special sign words are included in our
vocabulary. Most specifically, at the initial time step, the
attribute representations are transformed as the input to L-
STM, and then in the next steps, word embedding xt will be
input into the LSTM along with the previous step's hidden
state ht−1. In each time step (except the initial step), we use
the LSTM cell output ht to predict the next word through a
softmax layer.
3.3.2
LSTM-A2 (Inserting image first)
To further leverage both image representations and high-
level attributes in the encoding stage of our LSTM-A, we
design the second architecture LSTM-A2 by treating both
of them as atoms in the input sequence to LSTM. Specif-
ically, at the initial step, the image representations I are
firstly transformed into LSTM to inform the LSTM about
the image content, followed by the attribute representations
A which are encoded into LSTM at the next time step to
inform the high-level attributes. Then, LSTM decodes each
output word based on previous word xt and previous step's
hidden state ht−1, which is similar to the decoding stage in
LSTM-A1. The LSTM updating procedure in LSTM-A2 is
designed as
x−2 = TvI and x−1 = TaA,
xt = Tswt,
and
ht = f
�
xt�
, t ∈ {0, . . . , Ns − 1} ,
(11)
where Tv ∈ RDe×Dv is the transformation matrix for im-
age representations.
3.3.3
LSTM-A3 (Feeding attributes first)
The third design LSTM-A3 is similar to LSTM-A2 as
both designs utilize image representations and high-level
attributes to form the input sequence to LSTM in the en-
coding stage, except that the orders of encoding are differ-
ent. In LSTM-A3, the attribute representations are firstly
encoded into LSTM and then the image representations are
transformed into LSTM at the second time step. The whole
LSTM updating procedure in LSTM-A3 is as
x−2 = TaA and x−1 = TvI,
xt = Tswt,
and
ht = f
�
xt�
, t ∈ {0, . . . , Ns − 1} .
(12)
3.3.4
LSTM-A4 (Inputting image each time step)
Different from the former three designed architectures
which mainly inject high-level attributes and image repre-
sentations at the encoding stage of LSTM, we next modify
the decoding stage in our LSTM-A by additionally incorpo-
rating image representations or high-level attributes. More
specifically, in LSTM-A4, the attribute representations are
injected once at the initial step to inform the LSTM about
the high-level attributes, and then image representations are
fed at each time step as an extra input to LSTM to empha-
size the image content frequently among memory cells in
LSTM. Hence, the LSTM updating procedure in LSTM-A4
is:
x−1 = TaA,
xt = Tswt + TvI, and ht = f
�
xt�
, t ∈ {0, . . . , Ns − 1} . (13)
3.3.5
LSTM-A5 (Inputting attributes each time step)
The last design LSTM-A5 is similar to LSTM-A4 except
that it firstly encodes image representations and then feed-
s attribute representations as an additional input to LSTM
at each step in decoding stage to emphasize the high-level
attributes frequently. Accordingly, the LSTM updating pro-
cedure in LSTM-A5 is as
x−1 = TvI,
xt = Tswt + TaA, and ht = f
�
xt�
, t ∈ {0, . . . , Ns − 1} . (14)
4. Experiments
We conducted the experiments and evaluated our ap-
proaches on COCO captioning dataset (COCO) [13].
4.1. Dataset and Experimental Settings
The dataset, COCO, is the most popular benchmark for
image captioning, which contains 82,783 training images
and 40,504 validation images. There are 5 human-annotated
descriptions per image. As the annotations of the official
testing set are not publicly available, we follow the widely
used settings in [38, 39] and take 82,783 images for training,
5,000 for validation and 5,000 for testing.
Data Preprocessing. Following [10], we convert all the
descriptions in training set to lower case and discard rare
words which occur less than 5 times, resulting in the final
vocabulary with 8,791 unique words in COCO dataset.
Features and Parameter Settings. Each word in the
sentence is represented as "one-hot" vector (binary index
vector in a vocabulary).
For image representations, we
take the output of 1,024-way pool5/7 × 7 s1 layer from
GoogleNet [28] pre-trained on Imagenet ILSVRC12 dataset
[25]. For attribute representations, we select 1,000 most
common words on COCO as the high-level attributes and
train our MIL-IAC attributes prediction model purely on the
training data of COCO, resulting in the final 1,000-way vec-
tor of probabilities of attributes. The tradeoff parameter λ
is empirically set as 0.8. The dimension of the input and
hidden layers in LSTM of LSTM-A are both set to 1,024.
Implementation Details. We mainly implement our im-
age captioning models based on Caffe [9], which is one
of widely adopted deep learning frameworks. Specifical-
ly, with an initial learning rate 0.01 and mini-batch size of
1,024, the objective value can decrease to 25% of the initial
loss and reach a reasonable result after 50,000 iterations.
Testing Strategies. For sentence generation in testing
stage, we adopt the beam search strategy which selects the
top-k best sentences at each time step and considers them as
the candidates to generate new top-k best sentences at the
next time step. The beam size k is empirically set to 3.
Evaluation Metrics. For the evaluation of our proposed
models, we adopt five types of metrics: BLEU@N [22],
METEOR [2], ROUGE-L [12], CIDEr-D [30] and SPICE
[1]. All the metrics are computed by using the codes1 re-
leased by COCO Evaluation Server [3].
4.2. Compared Approaches
To verify the merit of our LSTM-A models, we com-
pared the following state-of-the-art methods.
(1) NIC & LSTM [31]: NIC is the standard RNN-based
model which only injects image into LSTM at the initial
time step. We directly extract results reported in [38] and
1https://github.com/tylin/coco-caption
Table 1. Performance of our proposed models and other state-of-the-art methods on COCO, where B@N, M, R, C and S are short for
BLEU@N, METEOR, ROUGE-L, CIDEr-D and SPICE scores. All values are reported as percentage (%).
Model
B@1
B@2
B@3
B@4
M
R
C
S
NIC [31]
66.6
45.1
30.4
20.3
-
-
-
-
LRCN [5]
69.7
51.9
38.0
27.8
22.9
50.8
83.7
15.8
HA [33]
71.8
50.4
35.7
25
23
-
-
-
SA [33]
70.7
49.2
34.4
24.3
23.9
-
-
-
ATT [38]
70.9
53.7
40.2
30.4
24.3
-
-
-
SC [39]
72
54.6
40.4
29.8
24.5
-
95.9
-
LSTM [31]
68.4
51.2
38
28.4
23.1
50.7
84.3
16
LSTM-A1
72.9
56.2
42.4
31.9
25.1
53.4
97.5
18.1
LSTM-A2
73.3
56.5
42.7
32.2
25.3
53.9
99.1
18.3
LSTM-A3
73.5
56.6
42.9
32.4
25.5
53.9
99.8
18.5
LSTM-A4
72.1
55.5
41.7
31.4
24.9
53.2
95.7
17.8
LSTM-A5
73.4
56.7
43.0
32.6
25.4
54.0
100.2
18.6
LSTM-A∗
95.7
82.5
68.5
55.9
34.1
67.3
150.5
26.8
name this run as NIC. Moreover, for fair comparison, we
also include our implementation of NIC, named as LSTM.
(2) LRCN [5]: LRCN inputs both image representations
and previous word into LSTM at each time step.
(3) Hard-Attention (HA) & Soft-Attention (SA) [33]: S-
patial attention on convolutional features of an image is in-
corporated into the encoder-decoder framework through t-
wo kinds of mechanisms: 1) "hard" stochastic attention e-
quivalently by reinforce learning and 2) "soft" deterministic
attention with standard back-propagation.
(4) ATT [38]: ATT utilizes attributes as semantic atten-
tion to combine image and attributes in RNN for captioning.
(5) Sentence-Condition (SC) [39]: Sentence-condition
exploits text-conditional semantic attention to generate se-
mantic guidance for sentence generation by conditioning
image features on current text content.
(6) MSR Captivator [4]: MSR Captivator employs both
Multimodal Recurrent Neural Network (MRNN) and Max-
imum Entropy Language Model (MELM) [6] for sentence
generation. Deep Multimodal Similarity Model (DMSM)
[6] is further exploited for sentence re-ranking.
(7) CaptionBot [29]: CaptionBot is a publicly image cap-
tioning system2 which is mainly built on vision models by
using Deep residual networks (ResNets) [8] to detect visual
concepts, MELM [6] language model for sentence genera-
tion and DMSM [6] for caption ranking.
(8) LSTM-A: LSTM-A1 ∼ LSTM-A5 are five variants
derived from our proposed LSTM-A framework. In addi-
tion, LSTM-A∗ is an oracle run that inputs ground-truth at-
tributes into the LSTM-A3 architecture.
4.3. Performance Comparison on COCO
Table 1 shows the performances of different model-
s on COCO image captioning dataset.
It is worth not-
ing that the performances of different approaches here
2https://www.captionbot.ai
are based on different image representations. Specifical-
ly, VGG architecture [26] is utilized as image feature ex-
tractor in the methods of Hard-Attention & Soft-Attention
and Sentence-Condition, while GoogleNet [28] is exploit-
ed in NIC, LRCN, ATT, LSTM and our LSTM-A. In view
that the GoogleNet and VGG features are comparable, we
compare directly with results. Overall, the results across
eight evaluation metrics consistently indicate that our pro-
posed LSTM-A exhibits better performance than all the
state-of-the-art techniques including non-attention models
(NIC, LSTM, LRCN) and attention-based methods (Hard-
Attention, Soft-Attention, ATT, Sentence-Condition).
In
particular, the CIDEr-D and SPICE can achieve 100.2% and
18.6%, respectively, when extracting image representation-
s by GoogleNet. LSTM-A1 inputting only high-level at-
tributes as representations makes the relative improvement
over LSTM which feeds into image representations instead
by 12.3%, 8.7%, 5.3%, 15.7% and 13.1% in BLEU@4,
METEOR, ROUGR-L, CIDEr-D and SPICE, respectively.
The results basically indicate the advantage of exploiting
high-level attributes than image representations for image
captioning. Furthermore, by additionally incorporating at-
tributes to LSTM model, LSTM-A2, LSTM-A3 and LSTM-
A5 lead to a performance boost, indicating that image repre-
sentations and attributes are complementary and thus have
mutual reinforcement for image captioning. Similar in spir-
it, LSTM-A4 improves LRCN by further taking attributes
into account. There is a significant performance gap be-
tween ATT and LSTM-A5. Though both runs involve the
utilization of image representations and attributes, they are
fundamentally different in the way that the performance of
ATT is as a result of modulating the strength of attention on
attributes to the previous words, and LSTM-A5 is by em-
ploying attributes as auxiliary knowledge to complement
image representations. This somewhat reveals the weak-
ness of semantic attention model, where the prediction er-
rors will accumulate along the generated sequence.
Table 2. Leaderboard of the published state-of-the-art image captioning models on the online COCO testing server, where B@N, M, R,
and C are short for BLEU@N, METEOR, ROUGE-L, and CIDEr-D scores. All values are reported as percentage (%).
Model
B@1
B@2
B@3
B@4
M
R
C
c5
c40
c5
c40
c5
c40
c5
c40
c5
c40
c5
c40
c5
c40
LSTM-A3 (Ours)
78.7
93.7
62.7
86.7
47.6
76.5
35.6
65.2
27
35.4
56.4
70.5
116
118
Watson Multimodal [24]
77.3
92.9
60.9
85.6
46.1
75.1
34.4
63.6
26.8
35.3
55.9
70.4
112.3
114.6
G-RMI(PG-SPIDEr-TAG) [14]
75.1
91.6
59.1
84.2
44.5
73.8
33.1
62.4
25.5
33.9
55.1
69.4
104.2
107.1
MetaMind/VT GT [15]
74.8
92.0
58.4
84.5
44.4
74.4
33.6
63.7
26.4
35.9
55.0
70.5
104.2
105.9
reviewnet [35]
72.0
90.0
55.0
81.2
41.4
70.5
31.3
59.7
25.6
34.7
53.3
68.6
96.5
96.9
ATT [38]
73.1
90
56.5
81.5
42.4
70.9
31.6
59.9
25
33.5
53.5
68.2
94.3
95.8
Google [31]
71.3
89.5
54.2
80.2
40.7
69.4
30.9
58.7
25.4
34.6
53
68.2
94.3
94.6
MSR Captivator [4]
71.5
90.7
54.3
81.9
40.7
71
30.8
60.1
24.8
33.9
52.6
68
93.1
93.7
Table 3. BLEU@4, METEOR, ROUGE-L, CIDEr-D, and SPICE
scores of our proposed LSTM-A3 with attributes learnt by differ-
ent attributes prediction models on COCO.
Model
B@4
M
R
C
S
Fine-tune
30.2
24.3
52.4
91.7
17.2
MIL [6]
32.1
25.2
53.7
98.4
18.2
MIL-IAC
32.4
25.5
53.9
99.8
18.5
Compared to LSTM-A1, LSTM-A2 which is augment-
ed by integrating image representations performs better, but
the performances are lower than LSTM-A3. The results
indicate that LSTM-A3, in comparison, benefits from the
mechanism of first feeding high-level attributes into LSTM
instead of starting from inserting image representations in
LSTM-A2. The chance that a good start point can be at-
tained and lead to performance gain is better. LSTM-A4
feeding the image representations at each time step yields
inferior performances to LSTM-A3, which only inputs im-
age representations once. We speculate that this may be-
cause the noise in the image can be explicitly accumulated
and thus the network overfits more easily. In contrast, the
performances of LSTM-A5 which feeds attributes at each
time step show the improvements on LSTM-A3. The results
demonstrate that the high-level attributes are more accurate
and easily translated into human understandable sentence.
Among the five proposed LSTM-A architectures, LSTM-
A3 achieves the best performances in terms of BLEU@1
and METEOR, while LSTM-A5 performs the best in oth-
er six evaluation metrics. The performances of the oracle
run LSTM-A∗ could be regarded as the upper bound of em-
ploying attributes in our framework and lead to large per-
formance gain against LSTM-A3. Such an upper bound en-
ables us to obtain more insights on the factor accounting for
the success of the current attribute augmented architecture
and also provides guidance to future research in this direc-
tion. More specifically, the results, on one hand, indicate
the advantage and great potential of leveraging attributes
for boosting image captioning, and on the other, suggest
that more efforts are further required towards mining and
representing attributes more effectively.
4.4. Evaluation of Attributes Prediction Model
We further verify the effectiveness of our MIL-IAC at-
tributes prediction model. We compared two baselines here.
One is to directly fine-tune the VGG architecture with cross
entropy loss for attributes prediction, named as Fine-tune,
and the other, namely MIL, exploits a weakly supervised
MIL model [6] based on VGG to learn region-based detec-
tors for attributes. Table 3 compares the sentence generation
performances of our LSTM-A3 model with attributes learnt
by different attributes prediction models on COCO dataset.
Compared to Fine-tune, MIL method using region-based
detectors consistently exhibits better performance across d-
ifferent evaluation metric. Moreover, by additionally ex-
ploring the inter-attribute correlations in MIL framework,
our proposed MIL-IAC leads to larger performance gains.
4.5. Performance on COCO Online Testing Server
We also submitted our best run in terms of METEOR,
i.e., LSTM-A3, to online COCO testing server and evaluat-
ed the performance on official testing set. Table 2 shows the
performance Leaderboard on official testing image set with
5 reference captions (c5) and 40 reference captions (c40).
Please note that here we utilize the outputs of 2,048-way
pool5 layer from ResNet-152 as image representations and
train the attribute detectors by ResNet-152 in our final sub-
mission. Moreover, inspired by [24], we adopt the policy
gradient optimization to specifically boost CIDEr-D perfor-
mance. The latest top-8 performing methods which have
been officially published are included in the table. Com-
pared to the top performing methods on the leaderboard, our
proposed LSTM-A3 achieves the best performances across
all the evaluation metrics on both c5 and c40 testing sets.
4.6. Human Evaluation
To better understand how satisfactory are the sentences
generated from different methods, we also conducted a
human study to compare our LSTM-A3 against three ap-
proaches, i.e., CaptionBot, LRCN and LSTM. A total num-
ber of 12 evaluators (6 females and 6 males) from differ-
ent education backgrounds, including computer science (4),
business (2), linguistics (2) and engineering (4), are invited
and a subset of 1K images is randomly selected from testing
set for the subjective evaluation. The evaluation process is
as follows. All the evaluators are organized into two groups.
We show the first group all the four sentences generated by
each approach plus five human-annotated sentences and ask
Generated Sentences:
LSTM: a man riding a skateboard down a street
CaptionBot:  I think it's a group of people walking down the 
road.
LSTM-A3: a man walking down a street with a herd of sheep
Ground Truth: 
  a man walks while a large number of sheep follow 
  a man leading a herd of sheep down the sheep
  the man is walking a herd of sheep on the road 
through a town
Attributes:
sheep: 0.976  herd: 0.778  street: 0.702
walking: 0.702  road: 0.635  man: 0.555
standing: 0.430  animals: 0.388
Generated Sentences:
LSTM: a group of people standing around a market
CaptionBot:  I think it's a bunch of yellow flowers.
LSTM-A3: a group of people standing around a bunch of 
bananas
Ground Truth: 
  bunches of bananas for sale at an outdoor market  
  a person at a table filled with bananas
  there are many bananas layer across this table at a 
farmers market
Attributes:
bananas: 1  people: 0.956  market: 0.708
standing: 0.612  outdoor: 0.558
blue: 0.514  large: 0.407  table: 0.381
Generated Sentences:
LSTM: a cell phone sitting on top of a table
CaptionBot:  I think it's a laptop that is on the phone.
LSTM-A3: a person holding a cell phone in front of a laptop
Ground Truth: 
  a smart phone being held up in front of a lap top  
  the person is holding his cell phone while on his laptop
  someone holding a cell phone in front of a laptop
Attributes:
phone: 0.867  cell: 0.839  computer: 0.735  
laptop: 0.641  keyboard: 0.581 
screen: 0.546 holding: 0.505  person: 0.334
Generated Sentences:
LSTM: a group of people flying kites in the sky
CaptionBot:  I think it's a plane is flying over the water.
LSTM-A3: a red and white plane flying over a body of water
Ground Truth: 
  a plane with water skies for landing gear coming in for 
a landing at a lake  
  a plane flying through a sky above a lake
  a red and white plane is flying over some water
Attributes:
flying: 0.997  airplane: 0.957  plane: 0.941  
water: 0.893  red: 0.837  lake: 0.751
white: 0.566  sky: 0.565
Attributes:
boat: 1  water: 0.838  man: 0.762  
riding: 0.728  dog: 0.547  small: 0.485
person: 0.471  river: 0.461
Generated Sentences:
LSTM: a group of people on a boat in the water
CaptionBot: I think it's a man with a small boat in a body of 
water.
LSTM-A3: a man and a dog on a boat in the water
Ground Truth: 
  an image of a man in a boat with a dog 
  a person on a rowboat with a dalmatian dog on the boat
  old woman rowing a boat with a dog
Figure 2. Attributes and sentences generation results on COCO. The attributes are detected by our attributes prediction model and the
output sentences are generated by 1) LSTM, 2) CaptionBot2, 3) our LSTM-A3 and 4) Ground Truth: three ground truth sentences.
Table 4. User study on two criteria: M1 - percentage of captions
generated by different methods that are evaluated as better/equal to
human caption; M2 - percentage of captions that pass Turing Test.
Human
LSTM-A3
CaptionBot
LSTM
LRCN
M1
-
64.9
58.2
49.2
43.9
M2
90.1
75.3
66.3
57.3
55.9
them the question: Do the systems produce captions resem-
bling human-generated sentences? In contrast, we show the
second group once only one sentence generated by differ-
ent approach or human annotation and they are asked: Can
you determine whether the given sentence has been gener-
ated by a system or by a human being? From evaluators'
responses, we calculate two metrics: 1) M1: percentage of
captions that are evaluated as better or equal to human cap-
tion; 2) M2: percentage of captions that pass the Turing
Test. Table 4 lists the result of the user study. Overall, our
LSTM-A3 is clearly the winner for all two criteria. In par-
ticular, the percentage achieves 64.9% and 75.3% in terms
of M1 and M2, respectively, making the absolute improve-
ment over the best competitor CaptionBot by 6.7% and 9%.
4.7. Qualitative Analysis
Figure 2 showcases a few sentence examples generated
by different methods, the detected high-level attributes, and
human-annotated ground truth sentences. From these ex-
emplar results, it is easy to see that all of these automatic
methods can generate somewhat relevant sentences, while
our proposed LSTM-A3 can predict more relevant keyword-
s by jointly exploiting high-level attributes and image rep-
resentations for image captioning. For example, compared
to subject term "a group of people" and "a man" in the sen-
tence generated by LSTM and CaptionBot respectively, "a
man and a dog" in our LSTM-A3 is more precise to de-
scribe the image content in the first image, since the key-
word "dog" is one of the detected attributes and directly
injected into LSTM to guide the sentence generation. Sim-
ilarly, verb term "holding" which is also detected as one
high-level attribute presents the fourth image more exact-
ly. Moreover, our LSTM-A3 generate more descriptive sen-
tence by enriching the semantics with high-level attributes.
For instance, with the detected adjective "red," the gener-
ated sentence "a red and white plane flying over a body of
water" of the fifth image depicts the image content more
comprehensive. We refer the readers to supplementary ma-
terials for more examples.
5. Discussions and Conclusions
We have presented Long Short-Term Memory with At-
tributes (LSTM-A) architectures which explores both image
representations and high-level attributes for image caption-
ing. Particularly, we detect attributes by additionally explor-
ing the inter-attribute correlations in the Multiple Instance
Learning framework and study the problem of augment-
ing high-level attributes from images to complement im-
age representations for enhancing sentence generation. To
verify our claim, we have devised variants of architectures
by modifying the placement and moment, where and when
to feed into the two kinds of representations. Experiments
conducted on COCO image captioning dataset validate our
proposal and analysis. Performance improvements are ob-
served when comparing to other captioning techniques.
Our future works are as follows. First, more attributes
will be learnt from large-scale image benchmarks, e.g.,
YFCC-100M dataset, and integrated into image captioning.
Second, how to generate free-form and open-vocabulary
sentences with the learnt attributes is also expected.
References
[1] P. Anderson, B. Fernando, M. Johnson, and S. Gould. Spice:
Semantic propositional image caption evaluation. In ECCV,
2016.
[2] S. Banerjee and A. Lavie. Meteor: An automatic metric for
mt evaluation with improved correlation with human judg-
ments. In ACL workshop, 2005.
[3] X. Chen, H. Fang, T.-Y. Lin, R. Vedantam, S. Gupta, P. Dol-
l´ar, and C. L. Zitnick.
Microsoft COCO captions: Da-
ta collection and evaluation server.
arXiv preprint arX-
iv:1504.00325, 2015.
[4] J. Devlin, H. Cheng, H. Fang, S. Gupta, L. Deng, X. He,
G. Zweig, and M. Mitchell. Language models for image cap-
tioning: The quirks and what works. In ACL, 2015.
[5] J.
Donahue,
L.
Anne
Hendricks,
S.
Guadarrama,
M. Rohrbach, S. Venugopalan, K. Saenko, and T. Dar-
rell. Long-term recurrent convolutional networks for visual
recognition and description. In CVPR, 2015.
[6] H. Fang, S. Gupta, et al. From captions to visual concepts
and back. In CVPR, 2015.
[7] A. Farhadi,
M. Hejrati,
M. A. Sadeghi,
P. Young,
C. Rashtchian, J. Hockenmaier, and D. Forsyth. Every pic-
ture tells a story: Generating sentences from images. In EC-
CV, 2010.
[8] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
for image recognition. In CVPR, 2016.
[9] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-
shick, S. Guadarrama, and T. Darrell. Caffe: Convolutional
architecture for fast feature embedding. In MM, 2014.
[10] A. Karpathy and L. Fei-Fei.
Deep visual-semantic align-
ments for generating image descriptions. In CVPR, 2015.
[11] G. Kulkarni, V. Premraj, et al. Babytalk: Understanding and
generating simple image descriptions. IEEE Trans. on PAMI,
2013.
[12] C.-Y. Lin. Rouge: A package for automatic evaluation of
summaries. In ACL Workshop, 2004.
[13] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-
manan, P. Doll´ar, and C. L. Zitnick. Microsoft coco: Com-
mon objects in context. In ECCV, 2014.
[14] S. Liu, Z. Zhu, N. Ye, S. Guadarrama, and K. Murphy. Opti-
mization of image description metrics using policy gradient
methods. arXiv preprint arXiv:1612.00370, 2016.
[15] J. Lu, C. Xiong, D. Parikh, and R. Socher. Knowing when
to look: Adaptive attention via a visual sentinel for image
captioning. In CVPR, 2017.
[16] J. Mao, W. Xu, Y. Yang, J. Wang, and A. L. Yuille. Explain
images with multimodal recurrent neural networks. In NIPS
Workshop on Deep Learning, 2014.
[17] T. Mikolov, K. Chen, G. Corrado, and J. Dean.
Efficient
estimation of word representations in vector space. In ICLR
workshop, 2013.
[18] M. Mitchell, X. Han, et al. Midge: Generating image de-
scriptions from computer vision detections. In EACL, 2012.
[19] Y. Pan, Y. Li, T. Yao, T. Mei, H. Li, and Y. Rui. Learning
deep intrinsic video representation by exploring temporal co-
herence and graph structure. In IJCAI, 2016.
[20] Y. Pan, T. Mei, T. Yao, H. Li, and Y. Rui. Jointly modeling
embedding and translation to bridge video and language. In
CVPR, 2016.
[21] Y. Pan, T. Yao, H. Li, and T. Mei. Video captioning with
transferred semantic attributes. In CVPR, 2017.
[22] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. Bleu: a
method for automatic evaluation of machine translation. In
ACL, 2002.
[23] D. Parikh and K. Grauman. Relative attributes. In ICCV,
2011.
[24] S. J. Rennie, E. Marcheret, Y. Mroueh, J. Ross, and V. Goel.
Self-critical sequence training for image captioning. arXiv
preprint arXiv:1612.00563, 2016.
[25] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual
Recognition Challenge. IJCV, 2015.
[26] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In ICLR, 2015.
[27] I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence
learning with neural networks. In NIPS, 2014.
[28] C. Szegedy,
W. Liu,
Y. Jia,
P. Sermanet,
S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In CVPR, 2015.
[29] K. Tran, X. He, L. Zhang, J. Sun, C. Carapcea, C. Thrasher,
C. Buehler, and C. Sienkiewicz. Rich image captioning in
the wild. arXiv preprint arXiv:1603.09016, 2016.
[30] R. Vedantam, C. Lawrence Zitnick, and D. Parikh. Cider:
Consensus-based image description evaluation.
In CVPR,
2015.
[31] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and
tell: A neural image caption generator. In CVPR, 2015.
[32] Q. Wu, C. Shen, L. Liu, A. Dick, and A. v. d. Hengel. What
value do explicit high level concepts have in vision to lan-
guage problems? In CVPR, 2016.
[33] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudi-
nov, R. Zemel, and Y. Bengio. Show, attend and tell: Neural
image caption generation with visual attention.
In ICML,
2015.
[34] Y. Yang, C. L. Teo, H. Daum´e III, and Y. Aloimonos.
Corpus-guided sentence generation of natural images.
In
EMNLP, 2011.
[35] Z. Yang, Y. Yuan, Y. Wu, W. W. Cohen, and R. R. Salakhut-
dinov. Review networks for caption generation. In NIPS,
2016.
[36] T. Yao, Y. Pan, Y. Li, and T. Mei. Incorporating copying
mechanism in image captioning for learning novel objects.
In CVPR, 2017.
[37] T. Yao, Y. Pan, C.-W. Ngo, H. Li, and T. Mei.
Semi-
supervised domain adaptation with subspace learning for vi-
sual recognition. In CVPR, 2015.
[38] Q. You, H. Jin, Z. Wang, C. Fang, and J. Luo. Image cap-
tioning with semantic attention. In CVPR, 2016.
[39] L. Zhou, C. Xu, P. Koch, and J. J. Corso. Image caption
generation with text-conditional semantic attention. arXiv
preprint arXiv:1606.04621, 2016.
